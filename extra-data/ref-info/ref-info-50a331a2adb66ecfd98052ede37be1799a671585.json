{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742688"
                        ],
                        "name": "H. Escalante",
                        "slug": "H.-Escalante",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Escalante",
                            "middleNames": [
                                "Jair"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Escalante"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404036758"
                        ],
                        "name": "C. Hern\u00e1ndez-Gracidas",
                        "slug": "C.-Hern\u00e1ndez-Gracidas",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Hern\u00e1ndez-Gracidas",
                            "middleNames": [
                                "Arturo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hern\u00e1ndez-Gracidas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145876100"
                        ],
                        "name": "Jesus A. Gonzalez",
                        "slug": "Jesus-A.-Gonzalez",
                        "structuredName": {
                            "firstName": "Jesus",
                            "lastName": "Gonzalez",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesus A. Gonzalez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110054805"
                        ],
                        "name": "Aurelio L\u00f3pez",
                        "slug": "Aurelio-L\u00f3pez",
                        "structuredName": {
                            "firstName": "Aurelio",
                            "lastName": "L\u00f3pez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aurelio L\u00f3pez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145364363"
                        ],
                        "name": "Manuel Montes",
                        "slug": "Manuel-Montes",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Montes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manuel Montes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34970419"
                        ],
                        "name": "E. Morales",
                        "slug": "E.-Morales",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Morales",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Morales"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7801146"
                        ],
                        "name": "E. Sucar",
                        "slug": "E.-Sucar",
                        "structuredName": {
                            "firstName": "Enrique",
                            "lastName": "Sucar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sucar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088947252"
                        ],
                        "name": "Luis Villase\u00f1or",
                        "slug": "Luis-Villase\u00f1or",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Villase\u00f1or",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Villase\u00f1or"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 191
                            }
                        ],
                        "text": "Similarly, the collection could contribute to the research of the extent that automatic segmentation affects the retrieval and annotation performance, among several other interesting aspects [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "We only focus on describing the main aspects of the project; further technical details can be found in [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Those include adjacent, disjoint, beside, X-aligned, above, below and Y-aligned [36]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "4 shows the \u2018landscape\u2019 branch of the hierarchy, using different colors to describe different levels; other branches and further details can be found in [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54877267,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb88241892cd62b199bdd7a7723353f9d6b69a29",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "With the increasing storage of images worldwide, automatic image annotation has become a very active and relevant research area, however, it still lacks a benchmark specifically designed for this task, and in particular for region-level annotation. In this report we introduce the segmented and annotated IAPR-TC12 benchmark, an extended resource for the evaluation of automatic image annotation (AIA) methods. We present a methodology for the manual segmentation and annotation of the images in this collection. The goal of this methodology is to obtain reliable ground truth data for benchmarking AIA and related tasks. For annotation, an ad-hoc vocabulary is defined and hierarchically organized. This hierarchy proved to be very useful for obtaining objective and structured annotations. Also, a soft measure for the evaluation of annotation performance is proposed, based on this hierarchy. Statistics on the segmentation and annotation processes give evidence of the reliability of the proposed approach. Visual attributes and spatial relations are also extracted from regions in segmented images. The latter feature will promote research on the use of (spatial) contextual information for AIA and image retrieval. The extended collection is publicly available and can be used to evaluate a variety of tasks besides image annotation; this resource can also serve to study the use of automatic annotations for multimedia image retrieval; the latter is a distinctive feature of the collection because, although there are several image annotation benchmarks, there is currently no collection that can be used to effectively evaluate the performance of annotation methods in the task they are designed for (i.e. image retrieval). We outline several applications and raise important questions that might be answered with the annotated collection; motivating research in the areas of image segmentation, annotation and retrieval as well as on machine learning."
            },
            "slug": "Segmenting-and-annotating-the-IAPR-TC12-Benchmark-Escalante-Hern\u00e1ndez-Gracidas",
            "title": {
                "fragments": [],
                "text": "Segmenting and annotating the IAPR-TC12 Benchmark"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The segmented and annotated IAPR-TC12 benchmark is introduced, an extended resource for the evaluation of automatic image annotation (AIA) methods and a methodology for the manual segmentation and annotation of the images in this collection is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[22], with the goal of creating benchmark collections for diverse computer vision applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": false,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791802"
                        ],
                        "name": "J. Jeon",
                        "slug": "J.-Jeon",
                        "structuredName": {
                            "firstName": "Jiwoon",
                            "lastName": "Jeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jeon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, this analysis only gives partial evidence of the true localization performance as in most cases, when AIA methods are evaluated, this type of evaluation is not carried out [4\u20137]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, due to the lack of a benchmark collection specifically designed for the requirements of AIA, most methods have been evaluated in small collections of unrealistic images [3\u20139]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A widely used collection to evaluate AIA is the Corel data set [1,4\u20136,8,10,17]; it consists of around 800 CDs, each containing 100 images related to a common semantic concept."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "region-level or image-level) [4\u20137], due to the lack of benchmark collections with region-level annotations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The AIA challenge has been approached with semi-supervised and supervised machine learning techniques [3\u201311,17,18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Despite being relatively new, significant progress has been achieved in this task within the last decade [2\u20139]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14303727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "228029e7533e32a025071e31e3f4f08d2bea5f5a",
            "isKey": true,
            "numCitedBy": 1301,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Libraries have traditionally used manual image annotation for indexing and then later retrieving their image collections. However, manual image annotation is an expensive and labor intensive procedure and hence there has been great interest in coming up with automatic ways to retrieve images based on content. Here, we propose an automatic approach to annotating and retrieving images based on a training set of images. We assume that regions in an image can be described using a small vocabulary of blobs. Blobs are generated from image features using clustering. Given a training set of images with annotations, we show that probabilistic models allow us to predict the probability of generating a word given the blobs in an image. This may be used to automatically annotate and retrieve images given a word as a query. We show that relevance models allow us to derive these probabilities in a natural way. Experiments show that the annotation performance of this cross-media relevance model is almost six times as good (in terms of mean precision) than a model based on word-blob co-occurrence model and twice as good as a state of the art model derived from machine translation. Our approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval."
            },
            "slug": "Automatic-image-annotation-and-retrieval-using-Jeon-Lavrenko",
            "title": {
                "fragments": [],
                "text": "Automatic image annotation and retrieval using cross-media relevance models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval by assuming that regions in an image can be described using a small vocabulary of blobs."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699657"
                        ],
                        "name": "A. Hanbury",
                        "slug": "A.-Hanbury",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Hanbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanbury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "This work is based on a study carried out by Hanbury [37], in which a list of 494 labels was obtained by analyzing several AIA benchmark collections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 822037,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "450a70b2dfe0c2745730776d10e5472623842457",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In the field of computer vision, automated image annotation and object recognition are currently important research topics. It is hoped that these will lead to improved general image understanding which can be usefully applied in Content-based Image Retrieval. Three approaches to image annotation are reviewed: free text annotation, keyword annotation and annotation based on ontologies. An analysis of the keywords which have been used in automated image and video annotation research and evaluation campaigns is then presented. The outcome of this analysis is a list of 525 keywords divided into 15 categories. Given that this list is collected from existing image annotations, it could be used to check the applicability of ontologies describing entities which are portrayable in images."
            },
            "slug": "Review-of-Image-Annotation-for-the-Evaluation-of-Hanbury",
            "title": {
                "fragments": [],
                "text": "Review of Image Annotation for the Evaluation of Computer Vision Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An analysis of the keywords which have been used in automated image and video annotation research and evaluation campaigns is presented, resulting in a list of 525 keywords divided into 15 categories which could be used to check the applicability of ontologies describing entities which are portrayable in images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742688"
                        ],
                        "name": "H. Escalante",
                        "slug": "H.-Escalante",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Escalante",
                            "middleNames": [
                                "Jair"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Escalante"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145876100"
                        ],
                        "name": "Jesus A. Gonzalez",
                        "slug": "Jesus-A.-Gonzalez",
                        "structuredName": {
                            "firstName": "Jesus",
                            "lastName": "Gonzalez",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesus A. Gonzalez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1383223659"
                        ],
                        "name": "Carlos A. Hern\u00e1ndez",
                        "slug": "Carlos-A.-Hern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Hern\u00e1ndez",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos A. Hern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83445446"
                        ],
                        "name": "A. L\u00f3pez-L\u00f3pez",
                        "slug": "A.-L\u00f3pez-L\u00f3pez",
                        "structuredName": {
                            "firstName": "Aurelio",
                            "lastName": "L\u00f3pez-L\u00f3pez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. L\u00f3pez-L\u00f3pez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389780530"
                        ],
                        "name": "M. Montes-y-G\u00f3mez",
                        "slug": "M.-Montes-y-G\u00f3mez",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Montes-y-G\u00f3mez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Montes-y-G\u00f3mez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34970419"
                        ],
                        "name": "E. Morales",
                        "slug": "E.-Morales",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Morales",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Morales"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144763689"
                        ],
                        "name": "L. Sucar",
                        "slug": "L.-Sucar",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sucar",
                            "middleNames": [
                                "Enrique"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sucar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112901874"
                        ],
                        "name": "Luis Villase\u00f1or Pineda",
                        "slug": "Luis-Villase\u00f1or-Pineda",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Pineda",
                            "middleNames": [
                                "Villase\u00f1or"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Villase\u00f1or Pineda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14096930,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c6b40fb5b0c43b027028dfb799f6d8a53fe03",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes experimental results of two approaches to multimedia image retrieval: annotation-based expansion and late fusion of mixed methods. The former formulation consists of expanding manual annotations with labels generated by automatic annotation methods. Experimental results show that the performance of text-based methods can be improved with this strategy, specially, for visual topics; motivating further research in several directions. The second approach consists of combining the outputs of diverse image retrieval models based on different information. Experimental results show that competitive performance, in both retrieval and results diversification, can be obtained with this simple strategy. It is interesting that, contrary to previous work, the best results of the fusion were obtained by assigning a high weight to visual methods. Furthermore, a probabilistic modeling approach to result-diversification is proposed; experimental results reveal that some modifications are needed to achieve satisfactory results with this method."
            },
            "slug": "Annotation-Based-Expansion-and-Late-Fusion-of-Mixed-Escalante-Gonzalez",
            "title": {
                "fragments": [],
                "text": "Annotation-Based Expansion and Late Fusion of Mixed Methods for Multimedia Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experimental results show that competitive performance, in both retrieval and results diversification, can be obtained with this simple strategy, and a probabilistic modeling approach to result-diversification is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33421444"
                        ],
                        "name": "Quanfu Fan",
                        "slug": "Quanfu-Fan",
                        "structuredName": {
                            "firstName": "Quanfu",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quanfu Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3246962"
                        ],
                        "name": "R. Swaminathan",
                        "slug": "R.-Swaminathan",
                        "structuredName": {
                            "firstName": "Ranjini",
                            "lastName": "Swaminathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Swaminathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642913"
                        ],
                        "name": "A. Hoogs",
                        "slug": "A.-Hoogs",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Hoogs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hoogs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34265270"
                        ],
                        "name": "Roderic Collins",
                        "slug": "Roderic-Collins",
                        "structuredName": {
                            "firstName": "Roderic",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roderic Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073488753"
                        ],
                        "name": "Pascale Rondot",
                        "slug": "Pascale-Rondot",
                        "structuredName": {
                            "firstName": "Pascale",
                            "lastName": "Rondot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascale Rondot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781137"
                        ],
                        "name": "J. Kaufhold",
                        "slug": "J.-Kaufhold",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kaufhold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kaufhold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] considered WordNet for the annotation of images; however, we did not base our hierarchy on WordNet as assigning a region to its nearest WordNet meaning would lead to ambiguities due to the subjective knowledge of annotators(7); the use of WordNet would also make annotation slower."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 218
                            }
                        ],
                        "text": "Furthermore, the lack of region-level AIA benchmarks lead to many region-level methods being evaluated by their annotation performance at image-level, which can yield unreliable estimations of localization performance [5,10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "A straightforward methodology was proposed for the evaluation of localization performance in region-level AIA, which facilitates the performance evaluation of methods that do not use the same vocabulary as that used in [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "A better and simpler methodology would be to average the number of correctly labeled regions [8,10]; this measure would adequately evaluate the localization performance of both annotations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 79
                            }
                        ],
                        "text": "The proposed hierarchy thereby resembles hierarchies proposed in related works [10,23,35]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 14110947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be4ca1d090363b6b641e01b3d895d6c380fb0603",
            "isKey": true,
            "numCitedBy": 45,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe present a new data set of 1014 images with manual segmentations and semantic labels for each segment, together with a methodology for using this kind of data for recognition evaluation. The images and segmentations are from the UCB segmentation benchmark database (Martin et\u00a0al., in International conference on computer vision, vol.\u00a0II, pp.\u00a0416\u2013421, 2001). The database is extended by manually labeling each segment with its most specific semantic concept in WordNet (Miller et\u00a0al., in Int. J. Lexicogr. 3(4):235\u2013244, 1990). The evaluation methodology establishes protocols for mapping algorithm specific localization (e.g., segmentations) to our data, handling synonyms, scoring matches at different levels of specificity, dealing with vocabularies with sense ambiguity (the usual case), and handling ground truth regions with multiple labels. Given these protocols, we develop two evaluation approaches. The first measures the range of semantics that an algorithm can recognize, and the second measures the frequency that an algorithm recognizes semantics correctly. The data, the image labeling tool, and programs implementing our evaluation strategy are all available on-line (kobus.ca//research/data/IJCV_2007).\n\nWe apply this infrastructure to evaluate four algorithms which learn to label image regions from weakly labeled data. The algorithms tested include two variants of multiple instance learning (MIL), and two generative multi-modal mixture models. These experiments are on a significantly larger scale than previously reported, especially in the case of MIL methods. More specifically, we used training data sets up to 37,000 images and training vocabularies of up to 650 words.\n\nWe found that one of the mixture models performed best on image annotation and the frequency correct measure, and that variants of MIL gave the best semantic range performance. We were able to substantively improve the performance of MIL methods on the other tasks (image annotation and frequency correct region labeling) by providing an appropriate prior.\n"
            },
            "slug": "Evaluation-of-Localized-Semantics:-Data,-and-Barnard-Fan",
            "title": {
                "fragments": [],
                "text": "Evaluation of Localized Semantics: Data, Methodology, and Experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new data set of 1014 images with manual segmentations and semantic labels for each segment is presented, together with a methodology for using this kind of data for recognition evaluation, and four algorithms which learn to label image regions from weakly labeled data are evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151194032"
                        ],
                        "name": "H. M\u00fcller",
                        "slug": "H.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Henning",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398659765"
                        ],
                        "name": "S. Marchand-Maillet",
                        "slug": "S.-Marchand-Maillet",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Marchand-Maillet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marchand-Maillet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809085"
                        ],
                        "name": "T. Pun",
                        "slug": "T.-Pun",
                        "structuredName": {
                            "firstName": "Thierry",
                            "lastName": "Pun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 620,
                                "start": 616
                            }
                        ],
                        "text": "Although this collection is large enough for obtaining significant results, it exhibits several limitations that make it an unsuitable and unrealistic resource for the evaluation of image retrieval algorithms: (i) most of its images were taken in difficult poses and under controlled situations; (ii) it contains the same number of images related to each of the semantic concepts, which is rarely found in realistic collections; (iii) its images are annotated at image-level and therefore cannot be used for region-level AIA; (iv) it has been shown that subsets of this database can be tailored to show improvements [19]; (v) it is copyright protected, hence its images cannot be freely distributed among researchers, which makes the collection expensive; and (vi) it is no longer available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13507478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8d5814e795b1a9ea7c1b5652d03b572ee418c98",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "To demonstrate the performance of content-based image retrieval systems (CBIRSs), there is not yet any standard data set that is widely used. The only dataset used by a large number of research groups are the Corel Photo CDs. There are more than 800 of those CDs, each containing 100 pictures roughly similar in theme. Unfortunately, basically every evaluation is done on a different subset of the image sets thus making comparison impossible.In this article, we compare different ways of evaluating the performance using a subset of the Corel images with the same CBIRSan d the same set of evaluation measures. The aim is to show how easy it is to get differing results, even when using the same image collection, the same CBIRS and the same performance measures. This pinpoints the fact that we need a standard database of images with a query set and corresponding relevance judgments (RJs) to really compare systems.The techniques used in this article to \"enhance\" the apparent performance of a CBIRSa re commonly used, sometimes described, sometimes not. They all have a justification and seem to change the performance of a CBIRS but they do actually not. With a larger subset of images it is of course much easier to generate even bigger differences in performance. The goal of this article is not to be a guide of how to make the \"apparent\" performance of systems look good, but rather to make readers aware of CBIRS evaluations and the importance of standardized image databases, queries and RJ."
            },
            "slug": "The-Truth-about-Corel-Evaluation-in-Image-Retrieval-M\u00fcller-Marchand-Maillet",
            "title": {
                "fragments": [],
                "text": "The Truth about Corel - Evaluation in Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This article compares different ways of evaluating the performance of content-based image retrieval systems using a subset of the Corel images with the same CBIRSan d the same set of evaluation measures to show how easy it is to get differing results, even when using the same image collection, thesame CBIRS and the same performance measures."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704149"
                        ],
                        "name": "Paul D. Clough",
                        "slug": "Paul-D.-Clough",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clough",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul D. Clough"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426894"
                        ],
                        "name": "Michael Grubinger",
                        "slug": "Michael-Grubinger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Grubinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Grubinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699657"
                        ],
                        "name": "A. Hanbury",
                        "slug": "A.-Hanbury",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Hanbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151194032"
                        ],
                        "name": "H. M\u00fcller",
                        "slug": "H.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Henning",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1029021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6144ea17732dbd8d576c3525a90b9edb02dfe70e",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the general photographic retrieval and object annotation tasks of the ImageCLEF 2006 evaluation campaign. These tasks provided both the resources and the framework necessary to perform comparative laboratory-style evaluation of visual information systems for image retrieval and automatic image annotation. Both tasks offered something new for 2006 and attracted a large number of submissions: 12 groups participated in ImageCLEFphoto and 3 groups in the automatic annotation task. This paper summarises these two tasks including collections used in the benchmark, the tasks proposed, a summary of submissions from participating groups and the main findings."
            },
            "slug": "Overview-of-the-ImageCLEF-2006-Photographic-and-Clough-Grubinger",
            "title": {
                "fragments": [],
                "text": "Overview of the ImageCLEF 2006 Photographic Retrieval and Object Annotation Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper describes the general photographic retrieval and object annotation tasks of the ImageCLEF 2006 evaluation campaign, including collections used in the benchmark, the tasks proposed, a summary of submissions from participating groups and the main findings."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742688"
                        ],
                        "name": "H. Escalante",
                        "slug": "H.-Escalante",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Escalante",
                            "middleNames": [
                                "Jair"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Escalante"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389780530"
                        ],
                        "name": "M. Montes-y-G\u00f3mez",
                        "slug": "M.-Montes-y-G\u00f3mez",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Montes-y-G\u00f3mez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Montes-y-G\u00f3mez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144763689"
                        ],
                        "name": "L. Sucar",
                        "slug": "L.-Sucar",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sucar",
                            "middleNames": [
                                "Enrique"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sucar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 94
                            }
                        ],
                        "text": "All these features and spatial relationships have successfully been used in previous AIA work [8,17,18,38]; however, each user can extract their own set of features and spatial relationships since segmentation masks and images are publicly available."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 96
                            }
                        ],
                        "text": "Supervised methods have thereby reported better results than their semi-supervised counterparts [9,17,18], but they also require a training set of region-label pairs, compared to semi-supervised methods that only need weakly annotated images."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14037958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca7999e328a35d5ca1b399de119d32f338e21910",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a novel approach for improving automatic image annotation methods is proposed. The approach is based on the fact that accuracy of current image annotation methods is low if we look at the most confident label only. Instead, accuracy is improved if we look for the correct label within the set of the top k candidate labels. We take advantage of this fact and propose a Markov random field ( MRF) based on word co-occurrence information for the improvement of annotation systems. Through the MRF structure we take into account spatial dependencies between connected regions. As a result, we are considering semantic relationships between labels. We performed experiments with iterated conditional modes and simulated annealing as optimization strategies in a subset of the Corel benchmark collection. Experimental results of the proposed method together with a k nearest neighbors classifier as our annotation method show important error reductions."
            },
            "slug": "Word-Co-occurrence-and-Markov-Random-Fields-for-Escalante-Montes-y-G\u00f3mez",
            "title": {
                "fragments": [],
                "text": "Word Co-occurrence and Markov Random Fields for Improving Automatic Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Markov random field ( MRF) based on word co-occurrence information for the improvement of annotation systems is proposed and takes into account spatial dependencies between connected regions."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47713710"
                        ],
                        "name": "Benjamin Z. Yao",
                        "slug": "Benjamin-Z.-Yao",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Yao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Z. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112063737"
                        ],
                        "name": "Xiong Yang",
                        "slug": "Xiong-Yang",
                        "structuredName": {
                            "firstName": "Xiong",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiong Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5626877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d1813749fcb36351fff850f0391968b62fc73b7",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a large scale general purpose image database with human annotated ground truth. Firstly, an all-in-all labeling framework is proposed to group visual knowledge of three levels: scene level (global geometric description), object level (segmentation, sketch representation, hierarchical decomposition), and low-mid level (2.1D layered representation, object boundary attributes, curve completion, etc.). Much of this data has not appeared in previous databases. In addition, And-Or Graph is used to organize visual elements to facilitate top-down labeling. An annotation tool is developed to realize and integrate all tasks. With this tool, we've been able to create a database consisting of more than 636,748 annotated images and video frames. Lastly, the data is organized into 13 common subsets to serve as benchmarks for diverse evaluation endeavors."
            },
            "slug": "Introduction-to-a-Large-Scale-General-Purpose-Truth-Yao-Yang",
            "title": {
                "fragments": [],
                "text": "Introduction to a Large-Scale General Purpose Ground Truth Database: Methodology, Annotation Tool and Benchmarks"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A large scale general purpose image database with human annotated ground truth consisting of more than 636,748 annotated images and video frames is presented."
            },
            "venue": {
                "fragments": [],
                "text": "EMMCVPR"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699657"
                        ],
                        "name": "A. Hanbury",
                        "slug": "A.-Hanbury",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Hanbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279074"
                        ],
                        "name": "Ville Viitaniemi",
                        "slug": "Ville-Viitaniemi",
                        "structuredName": {
                            "firstName": "Ville",
                            "lastName": "Viitaniemi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ville Viitaniemi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3286520"
                        ],
                        "name": "A. Bencz\u00far",
                        "slug": "A.-Bencz\u00far",
                        "structuredName": {
                            "firstName": "Andr\u00e1s",
                            "lastName": "Bencz\u00far",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bencz\u00far"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144184144"
                        ],
                        "name": "M. Brendel",
                        "slug": "M.-Brendel",
                        "structuredName": {
                            "firstName": "M\u00e1ty\u00e1s",
                            "lastName": "Brendel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brendel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1886154"
                        ],
                        "name": "B. Dar\u00f3czy",
                        "slug": "B.-Dar\u00f3czy",
                        "structuredName": {
                            "firstName": "B\u00e1lint",
                            "lastName": "Dar\u00f3czy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dar\u00f3czy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742688"
                        ],
                        "name": "H. Escalante",
                        "slug": "H.-Escalante",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Escalante",
                            "middleNames": [
                                "Jair"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Escalante"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404036758"
                        ],
                        "name": "C. Hern\u00e1ndez-Gracidas",
                        "slug": "C.-Hern\u00e1ndez-Gracidas",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Hern\u00e1ndez-Gracidas",
                            "middleNames": [
                                "Arturo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hern\u00e1ndez-Gracidas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741126"
                        ],
                        "name": "S. Hoi",
                        "slug": "S.-Hoi",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Hoi",
                            "middleNames": [
                                "C.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hoi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708642"
                        ],
                        "name": "Jorma T. Laaksonen",
                        "slug": "Jorma-T.-Laaksonen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Laaksonen",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorma T. Laaksonen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8392859"
                        ],
                        "name": "Mingjing Li",
                        "slug": "Mingjing-Li",
                        "structuredName": {
                            "firstName": "Mingjing",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingjing Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788472"
                        ],
                        "name": "Heidy Marisol Mar\u00edn Castro",
                        "slug": "Heidy-Marisol-Mar\u00edn-Castro",
                        "structuredName": {
                            "firstName": "Heidy",
                            "lastName": "Castro",
                            "middleNames": [
                                "Marisol",
                                "Mar\u00edn"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heidy Marisol Mar\u00edn Castro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3334955"
                        ],
                        "name": "Xiaoguang Rui",
                        "slug": "Xiaoguang-Rui",
                        "structuredName": {
                            "firstName": "Xiaoguang",
                            "lastName": "Rui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoguang Rui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703601"
                        ],
                        "name": "N. Sebe",
                        "slug": "N.-Sebe",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Sebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sebe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3245731"
                        ],
                        "name": "Julian St\u00f6ttinger",
                        "slug": "Julian-St\u00f6ttinger",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "St\u00f6ttinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian St\u00f6ttinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116984408"
                        ],
                        "name": "Lei Wu",
                        "slug": "Lei-Wu",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1590048,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e9a2f23c2a7a284afa3d10a307c3a7cd0ed2162",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the object retrieval task of ImageCLEF 2007, give an overview of the methods of the participating groups, and present and discuss the results. \n \nThe task was based on the widely used PASCAL object recognition data to train object recognition methods and on the IAPR TC-12 benchmark dataset from which images of objects of the ten different classes bicycles, buses, cars, motorbikes, cats, cows, dogs, horses, sheep, and persons had to be retrieved. \n \nSeven international groups participated using a wide variety of methods. The results of the evaluation show that the task was very challenging and that different methods for relevance assessment can have a strong influence on the results of an evaluation."
            },
            "slug": "Overview-of-the-ImageCLEF-2007-Object-Retrieval-Deselaers-Hanbury",
            "title": {
                "fragments": [],
                "text": "Overview of the ImageCLEF 2007 Object Retrieval Task"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The results of the evaluation show that the object retrieval task of ImageCLEF 2007 was very challenging and that different methods for relevance assessment can have a strong influence on the results of an evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426894"
                        ],
                        "name": "Michael Grubinger",
                        "slug": "Michael-Grubinger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Grubinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Grubinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704149"
                        ],
                        "name": "Paul D. Clough",
                        "slug": "Paul-D.-Clough",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clough",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul D. Clough"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699657"
                        ],
                        "name": "A. Hanbury",
                        "slug": "A.-Hanbury",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Hanbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151194032"
                        ],
                        "name": "H. M\u00fcller",
                        "slug": "H.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Henning",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5957292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3174525fd67fff2ccbdd7ce926487f85347e83fd",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The general photographic ad-hoc retrieval task of the ImageCLEF 2007 evaluation campaign is described. This task provides both the resources and the framework necessary to perform comparative laboratory-style evaluation of visual information retrieval from generic photographic collections. In 2007, the evaluation objective concentrated on retrieval of lightly annotated images, a new challenge that attracted a large number of submissions: a total of 20 participating groups submitted 616 system runs. This paper summarises the components used in the benchmark, including the document collection and the search tasks, and presents an analysis of the submissions and the results."
            },
            "slug": "Overview-of-the-ImageCLEFphoto-2007-Photographic-Grubinger-Clough",
            "title": {
                "fragments": [],
                "text": "Overview of the ImageCLEFphoto 2007 Photographic Retrieval Task"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The general photographic ad-hoc retrieval task of the ImageCLEF 2007 evaluation campaign provides both the resources and the framework necessary to perform comparative laboratory-style evaluation of visual information retrieval from generic photographic collections."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40490812"
                        ],
                        "name": "R. Datta",
                        "slug": "R.-Datta",
                        "structuredName": {
                            "firstName": "Ritendra",
                            "lastName": "Datta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Datta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5113463"
                        ],
                        "name": "D. Joshi",
                        "slug": "D.-Joshi",
                        "structuredName": {
                            "firstName": "Dhiraj",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7060187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dfa5679a15d0125ecec8539b79e8ba0babb8f73",
            "isKey": false,
            "numCitedBy": 3618,
            "numCiting": 325,
            "paperAbstract": {
                "fragments": [],
                "text": "We have witnessed great interest and a wealth of promise in content-based image retrieval as an emerging technology. While the last decade laid foundation to such promise, it also paved the way for a large number of new techniques and systems, got many new people involved, and triggered stronger association of weakly related fields. In this article, we survey almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation, and in the process discuss the spawning of related subfields. We also discuss significant challenges involved in the adaptation of existing image retrieval techniques to build systems that can be useful in the real world. In retrospect of what has been achieved so far, we also conjecture what the future may hold for image retrieval research."
            },
            "slug": "Image-retrieval:-Ideas,-influences,-and-trends-of-Datta-Joshi",
            "title": {
                "fragments": [],
                "text": "Image retrieval: Ideas, influences, and trends of the new age"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation are surveyed, and the spawning of related subfields are discussed, to discuss the adaptation of existing image retrieval techniques to build systems that can be useful in the real world."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404036758"
                        ],
                        "name": "C. Hern\u00e1ndez-Gracidas",
                        "slug": "C.-Hern\u00e1ndez-Gracidas",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Hern\u00e1ndez-Gracidas",
                            "middleNames": [
                                "Arturo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hern\u00e1ndez-Gracidas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144763689"
                        ],
                        "name": "L. Sucar",
                        "slug": "L.-Sucar",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sucar",
                            "middleNames": [
                                "Enrique"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sucar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17859030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7f24a24165203f9b318f26b98bc92034c36f40f",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Content-based image retrieval (CBIR) is currently limited because of the lack of representational power of the low-level image features, which fail to properly represent the actual contents of an image, and consequently poor results are achieved with the use of this sole information. Spatial relations represent a class of high-level image features which can improve image annotation. We apply spatial relations to automatic image annotation, a task which is usually a first step towards CBIR. We follow a probabilistic approach to represent different types of spatial relations to improve the automatic annotations which are obtained based on low-level features. Different configurations and subsets of the computed spatial relations were used to perform experiments on a database of landscape images. Results show a noticeable improvement of almost 9% compared to the base results obtained using the k-Nearest Neighbor classifier."
            },
            "slug": "Markov-Random-Fields-and-Spatial-Information-to-Hern\u00e1ndez-Gracidas-Sucar",
            "title": {
                "fragments": [],
                "text": "Markov Random Fields and Spatial Information to Improve Automatic Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work applies spatial relations to automatic image annotation, a task which is usually a first step towards CBIR, and follows a probabilistic approach to represent different types of spatial Relations to improve the automatic annotations which are obtained based on low-level features."
            },
            "venue": {
                "fragments": [],
                "text": "PSIVT"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426894"
                        ],
                        "name": "Michael Grubinger",
                        "slug": "Michael-Grubinger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Grubinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Grubinger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": ", pictures of people on vacation trips [12])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "The IAPR TC-12 benchmark was created with the goal of providing a realistic collection of images suitable for a wide number of evaluation purposes [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "This collection is a well-established image retrieval benchmark comprising 20,000 images manually annotated with free-text descriptions in three languages [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60740304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "327edcc76d35e0e1014b4d9d73d1e16cc42697a5",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation investigates the system-centred evaluation of visual information retrieval from generic photographic collections. The development of visual information retrieval systems has long been hindered by the lack of standardised benchmarks. Researchers have proposed numerous systems and techniques, and although different systems clearly have their particular strength, there is a tendency by researchers to use different means of showing retrieval performance to highlight the own algorithm\u2019s benefits. For the field of visual information search to advance, however, objective evaluation to identify, compare and validate the strengths and merits of different systems is therefore essential. Benchmarks to carry out such evaluation have recently been developed, and evaluation events have also been organised for several domains. Yet, no efforts have considered the evaluation of retrieval from generic photographic collections (i.e. containing everyday real-world photographs akin to those that can frequently be found in private photographic collections as well, e.g. pictures of holidays and events). We therefore first analyse a multitude of variables and factors with respect to the performance and requirements of visual information systems, and we then design and implement the framework and resources necessary to carry out such an evaluation. These resources include: a parametric image collection, representative search requests, relevance assessments and a set of performance measures. In addition, we organise the first evaluation event for retrieval from generic photographic collections and report on its realisation. Finally, we present an analysis and the evaluation of the participating retrieval systems as well as of the evaluation event itself. Filling this particular gap by making possible a systematic calibration and comparison of system performance for retrieval from generic photographic collections constitutes the main scientific contribution of this research. This dissertation thereby enables a deeper understanding of the complex conditions and constraints associated with visual information identification, the accurate capturing of user requirements, the appropriate specification and complexity of user queries, the execution of searches, and the reliability of performance indicators."
            },
            "slug": "Analysis-and-evaluation-of-visual-information-Grubinger",
            "title": {
                "fragments": [],
                "text": "Analysis and evaluation of visual information systems performance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A deeper understanding of the complex conditions and constraints associated with visual information identification, the accurate capturing of user requirements, the appropriate specification and complexity of user queries, the execution of searches, and the reliability of performance indicators is enabled."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 868535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e",
            "isKey": false,
            "numCitedBy": 1760,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data."
            },
            "slug": "Matching-Words-and-Pictures-Barnard-Sahin",
            "title": {
                "fragments": [],
                "text": "Matching Words and Pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text, is presented, and a number of models for the joint distribution of image regions and words are developed, including several which explicitly learn the correspondence between regions and Words."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145575177"
                        ],
                        "name": "G. Carneiro",
                        "slug": "G.-Carneiro",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Carneiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carneiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3651407"
                        ],
                        "name": "Antoni B. Chan",
                        "slug": "Antoni-B.-Chan",
                        "structuredName": {
                            "firstName": "Antoni",
                            "lastName": "Chan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoni B. Chan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47690405"
                        ],
                        "name": "P. Moreno",
                        "slug": "P.-Moreno",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Moreno",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Moreno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699559"
                        ],
                        "name": "N. Vasconcelos",
                        "slug": "N.-Vasconcelos",
                        "structuredName": {
                            "firstName": "Nuno",
                            "lastName": "Vasconcelos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Vasconcelos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 96
                            }
                        ],
                        "text": "Supervised methods have thereby reported better results than their semi-supervised counterparts [9,17,18], but they also require a training set of region-label pairs, compared to semi-supervised methods that only need weakly annotated images."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2717049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e638e2c7f7bbc788eb4adb5b5c67bde5ffc11bc5",
            "isKey": false,
            "numCitedBy": 955,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "A probabilistic formulation for semantic image annotation and retrieval is proposed. Annotation and retrieval are posed as classification problems where each class is defined as the group of database images labeled with a common semantic label. It is shown that, by establishing this one-to-one correspondence between semantic labels and semantic classes, a minimum probability of error annotation and retrieval are feasible with algorithms that are 1) conceptually simple, 2) computationally efficient, and 3) do not require prior semantic segmentation of training images. In particular, images are represented as bags of localized feature vectors, a mixture density estimated for each image, and the mixtures associated with all images annotated with a common semantic label pooled into a density estimate for the corresponding semantic class. This pooling is justified by a multiple instance learning argument and performed efficiently with a hierarchical extension of expectation-maximization. The benefits of the supervised formulation over the more complex, and currently popular, joint modeling of semantic label and visual feature distributions are illustrated through theoretical arguments and extensive experiments. The supervised formulation is shown to achieve higher accuracy than various previously published methods at a fraction of their computational cost. Finally, the proposed method is shown to be fairly robust to parameter tuning"
            },
            "slug": "Supervised-Learning-of-Semantic-Classes-for-Image-Carneiro-Chan",
            "title": {
                "fragments": [],
                "text": "Supervised Learning of Semantic Classes for Image Annotation and Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The supervised formulation is shown to achieve higher accuracy than various previously published methods at a fraction of their computational cost and to be fairly robust to parameter tuning."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699657"
                        ],
                        "name": "A. Hanbury",
                        "slug": "A.-Hanbury",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Hanbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanbury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6272907,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "2a9e4b8ae9971c39e7913af670a406250f91e98a",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The Visual Concept Detection Task (VCDT) of ImageCLEF 2008 is described. A database of 2,827 images were manually annotated with 17 concepts. Of these, 1,827 were used for training and 1,000 for testing the automated assignment of categories. In total 11 groups participated and submitted 53 runs. The runs were evaluated using ROC curves, from which the Area Under the Curve (AUC) and Equal Error Rate (EER) were calculated. For each concept, the best runs obtained an AUC of 80% or above."
            },
            "slug": "The-Visual-Concept-Detection-Task-in-ImageCLEF-2008-Deselaers-Hanbury",
            "title": {
                "fragments": [],
                "text": "The Visual Concept Detection Task in ImageCLEF 2008"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The Visual Concept Detection Task (VCDT) of ImageCLEF 2008 is described, where a database of 2,827 images were manually annotated with 17 concepts and the best runs obtained an AUC of 80% or above."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6075144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb444dc25bab36a8e273ed654d49e3841905e5af",
            "isKey": false,
            "numCitedBy": 1349,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods. \n \nHigh classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow)."
            },
            "slug": "TextonBoost:-Joint-Appearance,-Shape-and-Context-Shotton-Winn",
            "title": {
                "fragments": [],
                "text": "TextonBoost: Joint Appearance, Shape and Context Modeling for Multi-class Object Recognition and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently, is proposed, which is used for automatic visual recognition and semantic segmentation of photographs."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38614318"
                        ],
                        "name": "Masashi Inoue",
                        "slug": "Masashi-Inoue",
                        "structuredName": {
                            "firstName": "Masashi",
                            "lastName": "Inoue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masashi Inoue"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 199
                            }
                        ],
                        "text": "This type of image search is referred to as annotation-based image retrieval (ABIR) and is different from text-based image retrieval (TBIR), which uses text that has been manually assigned to images [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2149792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7ab1e41c5d1bb0b5160f1064aa3cb8bedcb32ba",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Compared with content-based image retrieval, annotationbased image retrieval is more practical in some application domains. Users\u2019 information needs and the semantic contents of images can be represented by textual information more easily. We describe two problems which are unique to annotation-based image retrieval and would be worthy of further research. Contextual information embedded in data may be used to address these problems."
            },
            "slug": "On-the-need-for-annotation-based-image-retrieval-Inoue",
            "title": {
                "fragments": [],
                "text": "On the need for annotation-based image retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Two problems which are unique to annotation-based image retrieval are described which would be worthy of further research and may be used to address these problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724876"
                        ],
                        "name": "P. Carbonetto",
                        "slug": "P.-Carbonetto",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Carbonetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Carbonetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8270758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7925221db50d93142cb5e5a4e31d6b72b2c5af8b",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider object recognition as the process of attaching meaningful labels to specific regions of an image, and propose a model that learns spatial relationships between objects. Given a set of images and their associated text (e.g. keywords, captions, descriptions), the objective is to segment an image, in either a crude or sophisticated fashion, then to find the proper associations between words and regions. Previous models are limited by the scope of the representation. In particular, they fail to exploit spatial context in the images and words. We develop a more expressive model that takes this into account. We formulate a spatially consistent probabilistic mapping between continuous image feature vectors and the supplied word tokens. By learning both word-to-region associations and object relations, the proposed model augments scene segmentations due to smoothing implicit in spatial consistency. Context introduces cycles to the undirected graph, so we cannot rely on a straightforward implementation of the EM algorithm for estimating the model parameters and densities of the unknown alignment variables. Instead, we develop an approximate EM algorithm that uses loopy belief propagation in the inference step and iterative scaling on the pseudo-likelihood approximation in the parameter update step. The experiments indicate that our approximate inference and learning algorithm converges to good local solutions. Experiments on a diverse array of images show that spatial context considerably improves the accuracy of object recognition. Most significantly, spatial context combined with a nonlinear discrete object representation allows our models to cope well with over-segmented scenes."
            },
            "slug": "A-Statistical-Model-for-General-Contextual-Object-Carbonetto-Freitas",
            "title": {
                "fragments": [],
                "text": "A Statistical Model for General Contextual Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experiments show that spatial context considerably improves the accuracy of object recognition, and an approximate EM algorithm that uses loopy belief propagation in the inference step and iterative scaling on the pseudo-likelihood approximation in the parameter update step converges to good local solutions."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43289321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "716d218b0831157b6ff431f5418094bf776ee3f2",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This book is the outcome of two workshops that brought together about 40 prominent vision and machine learning researchers interested in the fundamental and applicative aspects of object recognition, as well as representatives of industry. The main goals of these two workshops were (1) to promote the creation of an international object recognition community, with common datasets and evaluation procedures, (2) to map the state of the art and identify the main open problems and opportunities for synergistic research, and (3) to articulate the industrial and societal needs and opportunities for object recognition research worldwide. These goals are reflected in a relatively small number of papers that illustrate the breadth of today's object recognition research and the arsenal of techniques at its disposal, and discuss current achievements and outstanding challenges. Most of the chapters are descriptions of technical approaches, intended to capture the current state of the art. Some of the chapters are of a tutorial nature. They cover fundamental building blocks for object recognition techniques."
            },
            "slug": "Toward-Category-Level-Object-Recognition-Ponce-Hebert",
            "title": {
                "fragments": [],
                "text": "Toward Category-Level Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This book is the outcome of two workshops that brought together about 40 prominent vision and machine learning researchers interested in the fundamental and applicative aspects of object recognition, as well as representatives of industry to promote the creation of an international object recognition community."
            },
            "venue": {
                "fragments": [],
                "text": "Toward Category-Level Object Recognition"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797063"
                        ],
                        "name": "S. Agarwal",
                        "slug": "S.-Agarwal",
                        "structuredName": {
                            "firstName": "Shivani",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32177941"
                        ],
                        "name": "A. Awan",
                        "slug": "A.-Awan",
                        "structuredName": {
                            "firstName": "Aatif",
                            "lastName": "Awan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Awan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Most of these, however, are restricted to specific domains, such as cars [26], nature-roadways [27], animals [28], landscape vs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8855331,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b82d251ed367593366680acebc81fdb070b04a18",
            "isKey": false,
            "numCitedBy": 963,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of detecting objects in still, gray-scale images. Our primary focus is the development of a learning-based approach to the problem that makes use of a sparse, part-based representation. A vocabulary of distinctive object parts is automatically constructed from a set of sample images of the object class of interest; images are then represented using parts from this vocabulary, together with spatial relations observed among the parts. Based on this representation, a learning algorithm is used to automatically learn to detect instances of the object class in new images. The approach can be applied to any object with distinguishable parts in a relatively fixed spatial configuration; it is evaluated here on difficult sets of real-world images containing side views of cars, and is seen to successfully detect objects in varying conditions amidst background clutter and mild occlusion. In evaluating object detection approaches, several important methodological issues arise that have not been satisfactorily addressed in the previous work. A secondary focus of this paper is to highlight these issues, and to develop rigorous evaluation standards for the object detection problem. A critical evaluation of our approach under the proposed standards is presented."
            },
            "slug": "Learning-to-detect-objects-in-images-via-a-sparse,-Agarwal-Awan",
            "title": {
                "fragments": [],
                "text": "Learning to detect objects in images via a sparse, part-based representation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A learning-based approach to the problem of detecting objects in still, gray-scale images that makes use of a sparse, part-based representation is developed and a critical evaluation of the approach under the proposed standards is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059257793"
                        ],
                        "name": "Jo\u00e3o Freitas",
                        "slug": "Jo\u00e3o-Freitas",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Freitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12561212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d9f55b445f36578802e7eef4393cfa914b11620",
            "isKey": false,
            "numCitedBy": 1765,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well -- for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach."
            },
            "slug": "Object-Recognition-as-Machine-Translation:-Learning-Sahin-Barnard",
            "title": {
                "fragments": [],
                "text": "Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows how to cluster words that individually are difficult to predict into clusters that can be predicted well, and cannot predict the distinction between train and locomotive using the current set of features, but can predict the underlying concept."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40538579"
                        ],
                        "name": "J. Vogel",
                        "slug": "J.-Vogel",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Vogel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "structured classification [29], and natural scene classification [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12322757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e264e1e55433f158bf8aa8b260bf430d76d5fa28",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel image representation that renders it possible to access natural scenes by local semantic description. Our work is motivated by the continuing effort in content-based image retrieval to extract and to model the semantic content of images. The basic idea of the semantic modeling is to classify local image regions into semantic concept classes such as water, rocks, or foliage. Images are represented through the frequency of occurrence of these local concepts. Through extensive experiments, we demonstrate that the image representation is well suited for modeling the semantic content of heterogenous scene categories, and thus for categorization and retrieval.The image representation also allows us to rank natural scenes according to their semantic similarity relative to certain scene categories. Based on human ranking data, we learn a perceptually plausible distance measure that leads to a high correlation between the human and the automatically obtained typicality ranking. This result is especially valuable for content-based image retrieval where the goal is to present retrieval results in descending semantic similarity from the query."
            },
            "slug": "Semantic-Modeling-of-Natural-Scenes-for-Image-Vogel-Schiele",
            "title": {
                "fragments": [],
                "text": "Semantic Modeling of Natural Scenes for Content-Based Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A novel image representation is presented that renders it possible to access natural scenes by local semantic description by using a perceptually plausible distance measure that leads to a high correlation between the human and the automatically obtained typicality ranking."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[31], for example, segmented and annotated a small set of 240 images, considering nine labels only."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5893207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03a073589eaf8ce3440464d020e0d0b26df5869b",
            "isKey": false,
            "numCitedBy": 997,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new algorithm for the automatic recognition of object classes from images (categorization). Compact and yet discriminative appearance-based object class models are automatically learned from a set of training images. The method is simple and extremely fast, making it suitable for many applications such as semantic image retrieval, Web search, and interactive image editing. It classifies a region according to the proportions of different visual words (clusters in feature space). The specific visual words and the typical proportions in each object are learned from a segmented training set. The main contribution of this paper is twofold: i) an optimally compact visual dictionary is learned by pair-wise merging of visual words from an initially large dictionary. The final visual words are described by GMMs. ii) A novel statistical measure of discrimination is proposed which is optimized by each merge operation. High classification accuracy is demonstrated for nine object classes on photographs of real objects viewed under general lighting conditions, poses and viewpoints. The set of test images used for validation comprise: i) photographs acquired by us, ii) images from the Web and iii) images from the recently released Pascal dataset. The proposed algorithm performs well on both texture-rich objects (e.g. grass, sky, trees) and structure-rich ones (e.g. cars, bikes, planes)"
            },
            "slug": "Object-categorization-by-learned-universal-visual-Winn-Criminisi",
            "title": {
                "fragments": [],
                "text": "Object categorization by learned universal visual dictionary"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An optimally compact visual dictionary is learned by pair-wise merging of visual words from an initially large dictionary, and a novel statistical measure of discrimination is proposed which is optimized by each merge operation."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, this analysis only gives partial evidence of the true localization performance as in most cases, when AIA methods are evaluated, this type of evaluation is not carried out [4\u20137]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, due to the lack of a benchmark collection specifically designed for the requirements of AIA, most methods have been evaluated in small collections of unrealistic images [3\u20139]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "region-level or image-level) [4\u20137], due to the lack of benchmark collections with region-level annotations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The AIA challenge has been approached with semi-supervised and supervised machine learning techniques [3\u201311,17,18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Despite being relatively new, significant progress has been achieved in this task within the last decade [2\u20139]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63473035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2dac26f6dacf5679a155f138b44a51081400f25",
            "isKey": true,
            "numCitedBy": 80,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Managing large and growing collections of information is a central goal of modern computer science. Data repositories of texts, images, sounds, and genetic information have become widely accessible, thus necessitating good methods of retrieval, organization, and exploration. In this thesis, we describe a suite of probabilistic models of information collections for which the above problems can be cast as statistical queries. \nWe use directed graphical models as a flexible, modular framework for describing appropriate modeling assumptions about the data. Fast approximate posterior inference algorithms based on variational methods free us from having to specify tractable models, and further allow us to take the Bayesian perspective, even in the face of large datasets. \nWith this framework in hand, we describe latent Dirichlet allocation (LDA), a graphical model particularly suited to analyzing text collections. LDA posits a finite index of hidden topics which describe the underlying documents. New documents are situated into the collection via approximate posterior inference of their associated index terms. Extensions to LDA can index a set of images, or multimedia collections of interrelated text and images. \nFinally, we describe nonparametric Bayesian methods for relaxing the assumption of a fixed number of topics, and develop models based on the natural assumption that the size of the index can grow with the collection. This idea is extended to trees, and to models which represent the hidden structure and content of a topic hierarchy that underlies a collection."
            },
            "slug": "Probabilistic-models-of-text-and-images-Blei-Jordan",
            "title": {
                "fragments": [],
                "text": "Probabilistic models of text and images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A suite of probabilistic models of information collections for which the above problems can be cast as statistical queries are described, and directed graphical models are used as a flexible, modular framework for describing appropriate modeling assumptions about the data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699657"
                        ],
                        "name": "A. Hanbury",
                        "slug": "A.-Hanbury",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Hanbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2310744"
                        ],
                        "name": "Alireza Tavakoli Targhi",
                        "slug": "Alireza-Tavakoli-Targhi",
                        "structuredName": {
                            "firstName": "Alireza",
                            "lastName": "Targhi",
                            "middleNames": [
                                "Tavakoli"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alireza Tavakoli Targhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Most of these, however, are restricted to specific domains, such as cars [26], nature-roadways [27], animals [28], landscape vs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 66
                            }
                        ],
                        "text": "Although several tools are available for interactive segmentation [28,33], most of them are designed to provide rather detailed segmentations of images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18277834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ca88880aca7c47cbf169ebb792c605fbd88b29d",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "An annotation of 59 795 images from the Corel database is presented. Each image has been labelled as containing an animal or not. The type of animal is also specified for most images. In addition, 1289 images have been manually segmented into animals and background. This annotated dataset will allow the evaluation of segmentation, feature extraction and object recognition algorithms for the well-defined task of recognising animals in images."
            },
            "slug": "A-Dataset-of-Annotated-Animals-Hanbury-Targhi",
            "title": {
                "fragments": [],
                "text": "A Dataset of Annotated Animals"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "An annotation of 59 795 images from the Corel database is presented to allow the evaluation of segmentation, feature extraction and object recognition algorithms for the well-defined task of recognising animals in images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277853"
                        ],
                        "name": "A. Vailaya",
                        "slug": "A.-Vailaya",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Vailaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vailaya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15065442,
            "fieldsOfStudy": [
                "Environmental Science",
                "Computer Science"
            ],
            "id": "c411f93539714f512e437c45a7a9d0a6d5a7675e",
            "isKey": false,
            "numCitedBy": 442,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-image-classification:-city-images-vs.-landscapes-Vailaya-Jain",
            "title": {
                "fragments": [],
                "text": "On image classification: city images vs. landscapes"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3502855"
                        ],
                        "name": "Marcin Marszalek",
                        "slug": "Marcin-Marszalek",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Marszalek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Marszalek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2875887"
                        ],
                        "name": "Jianguo Zhang",
                        "slug": "Jianguo-Zhang",
                        "structuredName": {
                            "firstName": "Jianguo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguo Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "2), yet even their use for the evaluation of object recognition methods has been questioned [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "There are several other collections available for the evaluation of object recognition algorithms [24], most notably the Caltech101 [25] and Caltech-256 [16] benchmarks as well as the PASCAL VOC-2006 [15] and VOC-2007(3) collections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2451202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1eca4ee67c65624a2949f33cfc7ccb4c14b0a15",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Appropriate datasets are required at all stages of object recognition research, including learning visual models of object and scene categories, detecting and localizing instances of these models in images, and evaluating the performance of recognition algorithms. Current datasets are lacking in several respects, and this paper discusses some of the lessons learned from existing efforts, as well as innovative ways to obtain very large and diverse annotated datasets. It also suggests a few criteria for gathering future datasets."
            },
            "slug": "Dataset-Issues-in-Object-Recognition-Ponce-Berg",
            "title": {
                "fragments": [],
                "text": "Dataset Issues in Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Current datasets are lacking in several respects, and this paper discusses some of the lessons learned from existing efforts, as well as innovative ways to obtain very large and diverse annotated datasets."
            },
            "venue": {
                "fragments": [],
                "text": "Toward Category-Level Object Recognition"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328108"
                        ],
                        "name": "Luis von Ahn",
                        "slug": "Luis-von-Ahn",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis von Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3319095"
                        ],
                        "name": "Ruoran Liu",
                        "slug": "Ruoran-Liu",
                        "structuredName": {
                            "firstName": "Ruoran",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruoran Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143624243"
                        ],
                        "name": "M. Blum",
                        "slug": "M.-Blum",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Blum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207158556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fff3876663581b51273cde3f6151960b75a0e12f",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Peekaboom, an entertaining web-based game that can help computers locate objects in images. People play the game because of its entertainment value, and as a side effect of them playing, we collect valuable image metadata, such as which pixels belong to which object in the image. The collected data could be applied towards constructing more accurate computer vision algorithms, which require massive amounts of training and testing data not currently available. Peekaboom has been played by thousands of people, some of whom have spent over 12 hours a day playing, and thus far has generated millions of data points. In addition to its purely utilitarian aspect, Peekaboom is an example of a new, emerging class of games, which not only bring people together for leisure purposes, but also exist to improve artificial intelligence. Such games appeal to a general audience, while providing answers to problems that computers cannot yet solve."
            },
            "slug": "Peekaboom:-a-game-for-locating-objects-in-images-Ahn-Liu",
            "title": {
                "fragments": [],
                "text": "Peekaboom: a game for locating objects in images"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Peekaboom is an entertaining web-based game that can help computers locate objects in images and is an example of a new, emerging class of games, which not only bring people together for leisure purposes, but also exist to improve artificial intelligence."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328108"
                        ],
                        "name": "Luis von Ahn",
                        "slug": "Luis-von-Ahn",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis von Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784365"
                        ],
                        "name": "Laura A. Dabbish",
                        "slug": "Laura-A.-Dabbish",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Dabbish",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura A. Dabbish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 338469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2d4a6e4900ec0f096c87bb2b1272eeceaa584a6",
            "isKey": false,
            "numCitedBy": 2386,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained."
            },
            "slug": "Labeling-images-with-a-computer-game-Ahn-Dabbish",
            "title": {
                "fragments": [],
                "text": "Labeling images with a computer game"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new interactive system: a game that is fun and can be used to create valuable output that addresses the image-labeling problem and encourages people to do the work by taking advantage of their desire to be entertained."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2333082"
                        ],
                        "name": "Y. Mori",
                        "slug": "Y.-Mori",
                        "structuredName": {
                            "firstName": "Yasuhide",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079483"
                        ],
                        "name": "Hironobu Takahashi",
                        "slug": "Hironobu-Takahashi",
                        "structuredName": {
                            "firstName": "Hironobu",
                            "lastName": "Takahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hironobu Takahashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776022"
                        ],
                        "name": "R. Oka",
                        "slug": "R.-Oka",
                        "structuredName": {
                            "firstName": "Ryu-ichi",
                            "lastName": "Oka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Oka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18574318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b29ffb4207435540ddecf4b14a8a32106b33830",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method to make a relationship between images and words. We adopt two processes in the method, one is a process to uniformly divide each image into sub-images with key words, and the other is a process to carry out vector quantization of the sub-images. These processes lead to results which show that each sub-image can be correlated to a set of words each of which is selected from words assigned to whole images. Original aspects of the method are, (1) all words assigned to a whole image are inherited to each divided sub-image, (2) the voting probability of each word for a set of divided images is estimated by the result of a vector quantization of the feature vector of sub-images. Some experiments show the e ectiveness of the proposed method."
            },
            "slug": "Image-to-word-transformation-based-on-dividing-Mori-Takahashi",
            "title": {
                "fragments": [],
                "text": "Image-to-word transformation based on dividing"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "All words assigned to a whole image are inherited to each divided sub-image and the voting probability of each word for a set of divided images is estimated by the result of a vector quantization of the feature vector of sub-images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082299938"
                        ],
                        "name": "D. Tal",
                        "slug": "D.-Tal",
                        "structuredName": {
                            "firstName": "Doron",
                            "lastName": "Tal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "It consists of 1041 images taken from a previous study for benchmarking segmentation algorithms [33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 66
                            }
                        ],
                        "text": "Although several tools are available for interactive segmentation [28,33], most of them are designed to provide rather detailed segmentations of images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a1ed876196ec9733acb1daa6d65e35ff0414291",
            "isKey": false,
            "numCitedBy": 6039,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties."
            },
            "slug": "A-database-of-human-segmented-natural-images-and-to-Martin-Fowlkes",
            "title": {
                "fragments": [],
                "text": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes is presented and an error measure is defined which quantifies the consistency between segmentations of differing granularities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 121
                            }
                        ],
                        "text": "There are several other collections available for the evaluation of object recognition algorithms [24], most notably the Caltech101 [25] and Caltech-256 [16] benchmarks as well as the PASCAL VOC-2006 [15] and VOC-20073 collections."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "There are several other collections available for the evaluation of object recognition algorithms [24], most notably the Caltech101 [25] and Caltech-256 [16] benchmarks as well as the PASCAL VOC-2006 [15] and VOC-2007(3) collections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2156851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aedb8df8f953429ec5a6df99fda5c5d71dbee4ff",
            "isKey": false,
            "numCitedBy": 2326,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Generative-Visual-Models-from-Few-Training-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"
            },
            "venue": {
                "fragments": [],
                "text": "2004 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61615905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a2ed19ac684022aa3186887cd4893484ab8f80c",
            "isKey": false,
            "numCitedBy": 2169,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This report presents the results of the 2006 PASCAL Visual Object Classes Challenge (VOC2006). Details of the challenge, data, and evaluation are presented. Participants in the challenge submitted descriptions of their methods, and these have been included verbatim. This document should be considered preliminary, and subject to change."
            },
            "slug": "The-PASCAL-visual-object-classes-challenge-2006-Everingham-Zisserman",
            "title": {
                "fragments": [],
                "text": "The PASCAL visual object classes challenge 2006 (VOC2006) results"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This report presents the results of the 2006 PASCAL Visual Object Classes Challenge (VOC2006)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083942236"
                        ],
                        "name": "Francesco Vivarelli",
                        "slug": "Francesco-Vivarelli",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Vivarelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesco Vivarelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Most of these, however, are restricted to specific domains, such as cars [26], nature-roadways [27], animals [28], landscape vs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 957130,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d0c4d8cb0e4c823b041b7dbb9cd9591399f139e",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present results that compare the performance of neural networks trained with two Bayesian methods, (i) the Evidence Framework of MacKay (1992) and (ii) a Markov Chain Monte Carlo method due to Neal (1996) on a task of classifying segmented outdoor images. We also investigate the use of the Automatic Relevance Determination method for input feature selection."
            },
            "slug": "Using-Bayesian-neural-networks-to-classify-images-Vivarelli-Williams",
            "title": {
                "fragments": [],
                "text": "Using Bayesian neural networks to classify segmented images"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Results are presented that compare the performance of neural networks trained with two Bayesian methods, (i) the Evidence Framework of MacKay (1992) and (ii) a Markov Chain Monte Carlo method due to Neal (1996) on a task of classifying segmented outdoor images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2772109"
                        ],
                        "name": "Renzo Angles",
                        "slug": "Renzo-Angles",
                        "structuredName": {
                            "firstName": "Renzo",
                            "lastName": "Angles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Renzo Angles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768011"
                        ],
                        "name": "C. Guti\u00e9rrez",
                        "slug": "C.-Guti\u00e9rrez",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Guti\u00e9rrez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Guti\u00e9rrez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207166126,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d57ca29d73272e139c04f118d5c3107dfb964596",
            "isKey": false,
            "numCitedBy": 1569,
            "numCiting": 182,
            "paperAbstract": {
                "fragments": [],
                "text": "Graph database models can be defined as those in which data structures for the schema and instances are modeled as graphs or generalizations of them, and data manipulation is expressed by graph-oriented operations and type constructors. These models took off in the eighties and early nineties alongside object-oriented models. Their influence gradually died out with the emergence of other database models, in particular geographical, spatial, semistructured, and XML. Recently, the need to manage information with graph-like nature has reestablished the relevance of this area. The main objective of this survey is to present the work that has been conducted in the area of graph database modeling, concentrating on data structures, query languages, and integrity constraints."
            },
            "slug": "Survey-of-graph-database-models-Angles-Guti\u00e9rrez",
            "title": {
                "fragments": [],
                "text": "Survey of graph database models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main objective of this survey is to present the work that has been conducted in the area of graph database modeling, concentrating on data structures, query languages, and integrity constraints."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 171
                            }
                        ],
                        "text": "One of the main challenges in OVA classification is choosing a way to combine the outputs of binary classifiers such that errors with respect to unseen data are minimized [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31993898,
            "fieldsOfStudy": [
                "Mathematics",
                "Art"
            ],
            "id": "3bb5a439a0d610a7eac68f73068cdd278b8c9775",
            "isKey": false,
            "numCitedBy": 21004,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "the selection of symmetric factorial designs, that is, a design where all factors have the same number of levels. Chapter 3 focuses on selection of two-level factorial designs and discusses complementary design theory and related topics in the selection of designs. Chapter 4 covers the selection of three level designs followed by the general case of s-levels. Chapter 5 discusses estimation capacity, presenting the connections with complementary designs followed by the estimation capacity for two-level and s-level designs. Chapter 6 discusses and presents results for the construction of mixed-level designs. Giving many examples of the use of mixed two and four-level designs. The final unit of the book discusses designs where there are two-different groups of factors. Chapters 7 and 8 discuss factorial designs with restricted randomization. Focusing first on blocked designs for full factorials as well as blocked fractional factorial designs. Chapter 8 focuses on split-plot designs. The booked is concluded with a chapter on robust parameter designs. This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion. However, the authors do a wonderful job of keeping the statistical methodology at the forefront of the book and the mathematical detail is presented as the necessary tool to study these designs. The book will serve as a great text for an advanced graduate level course in design theory for students with the necessary mathematical background. The book will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments. In addition, practitioners will also find the book useful for the comprehensive collection of optimal designs presented at the end of many chapters. Overall, this is a very well written book and a necessary addition to the existing literature on the design of factorial experiments."
            },
            "slug": "Pattern-Recognition-and-Machine-Learning-Neal",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion and will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60688891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "932a106c21a1db1e1876459c1521d27fd152caac",
            "isKey": false,
            "numCitedBy": 8461,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Looking for competent reading resources? We have pattern recognition and machine learning information science and statistics to read, not only read, but also download them or even check out online. Locate this fantastic book writtern by by now, simply here, yeah just here. Obtain the reports in the kinds of txt, zip, kindle, word, ppt, pdf, as well as rar. Once again, never ever miss to review online and download this book in our site right here. Click the link."
            },
            "slug": "Pattern-Recognition-and-Machine-Learning-Science-Bishop",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Machine Learning (Information Science and Statistics)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[32] reports on the creation of a larger collection with 591 images and 21 labels, yet again the size of these data sets and the number of concepts are not adequate for evaluating AIA."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "TextonBoost: joint appearance"
            },
            "venue": {
                "fragments": [],
                "text": "shape and context modeling for multi-class object recognition and segmentation, in: ECCV\u201906: Proceedings of the 9th European Conference on Computer Vision, Graz, Austria"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schiele, semantic modeling of natural scenes for content-based image retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "structured classification [29], and natural scene classification [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On Image classification: city versus landscape"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition 31 "
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 94
                            }
                        ],
                        "text": "All these features and spatial relationships have successfully been used in previous AIA work [8,17,18,38]; however, each user can extract their own set of features and spatial relationships since segmentation masks and images are publicly available."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 96
                            }
                        ],
                        "text": "Supervised methods have thereby reported better results than their semi-supervised counterparts [9,17,18], but they also require a training set of region-label pairs, compared to semi-supervised methods that only need weakly annotated images."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-Class PSMS for Automatic Image Annotation"
            },
            "venue": {
                "fragments": [],
                "text": "Applications on Swarm Intelligence, Springer"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 204
                            }
                        ],
                        "text": "The task of automatically assigning semantic labels to images is known as automatic image annotation (AIA), a challenge that has been identified as one of the hot-topics in the new age of image retrieval [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image retrieval: ideas"
            },
            "venue": {
                "fragments": [],
                "text": "influences, and trends of the new age, ACM Computing Surveys 40 (2) "
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 141
                            }
                        ],
                        "text": "There are several other collections available for the evaluation of object recognition algorithms [24], most notably the Caltech101 [25] and Caltech-256 [16] benchmarks as well as the PASCAL VOC-2006 [15] and VOC-20073 collections."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 290
                            }
                        ],
                        "text": "1 depicts sample images for both approaches, taken from three related tasks (from left to right): image-level annotation and region-level annotation (from the Corel subsets [8]), object detection (from the PASCAL VOC-2006 data set [15]) and object recognition (from the Caltech256 data set [16])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "There are several other collections available for the evaluation of object recognition algorithms [24], most notably the Caltech101 [25] and Caltech-256 [16] benchmarks as well as the PASCAL VOC-2006 [15] and VOC-2007(3) collections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Caltech-256"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. rep., California Institute of Technology, Pasadena, California, USA"
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 10,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 46,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/The-segmented-and-annotated-IAPR-TC-12-benchmark-Escalante-Hern\u00e1ndez/50a331a2adb66ecfd98052ede37be1799a671585?sort=total-citations"
}