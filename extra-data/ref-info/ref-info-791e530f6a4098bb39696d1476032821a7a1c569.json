{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152292143"
                        ],
                        "name": "Jeffrey R. Huang",
                        "slug": "Jeffrey-R.-Huang",
                        "structuredName": {
                            "firstName": "Jeffrey R.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey R. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313513"
                        ],
                        "name": "Patrick J. Rauss",
                        "slug": "Patrick-J.-Rauss",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Rauss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick J. Rauss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17779599,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc8b25e35a3acb812beb499844734081722319b4",
            "isKey": false,
            "numCitedBy": 2398,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-FERET-database-and-evaluation-procedure-for-Phillips-Wechsler",
            "title": {
                "fragments": [],
                "text": "The FERET database and evaluation procedure for face-recognition algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313513"
                        ],
                        "name": "Patrick J. Rauss",
                        "slug": "Patrick-J.-Rauss",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Rauss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick J. Rauss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231915"
                        ],
                        "name": "S. Der",
                        "slug": "S.-Der",
                        "structuredName": {
                            "firstName": "Sandor",
                            "lastName": "Der",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Der"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We designed the digitally modified images to test the effects of illumination and scale [ 5 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The traditional method of testing a face-recognition algorithm is to provide the algorithm with two sets of images [ 5 ], the gallery and the probe set, which do not intersect."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14238832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39577ba5d7020bc1750b3a417b7d6432ebb7f00c",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As part of the Face Recognition Technology (FERET) program, the U.S. Army Research Laboratory (ARL) conducted supervised government tests and evaluations of automatic face recognition algorithms. The goal of the tests was to provide an independent method of evaluating algorithms and assessing the state of the art in automatic face recognition. This report describes the design and presents the results of the August 1994 and March 1995 FERET tests. Results for FERET tests administered by ARL between August 1994 and August 1996 are reported."
            },
            "slug": "FERET-(Face-Recognition-Technology)-Recognition-and-Phillips-Rauss",
            "title": {
                "fragments": [],
                "text": "FERET (Face Recognition Technology) Recognition Algorithm Development and Test Results."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The design and results of the August 1994 and March 1995 FERET tests are described and the goal was to provide an independent method of evaluating algorithms and assessing the state of the art in automatic face recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313513"
                        ],
                        "name": "Patrick J. Rauss",
                        "slug": "Patrick-J.-Rauss",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Rauss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick J. Rauss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113989911"
                        ],
                        "name": "J. Phillips",
                        "slug": "J.-Phillips",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Phillips",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3156855"
                        ],
                        "name": "M. K. Hamilton",
                        "slug": "M.-K.-Hamilton",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Hamilton",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. K. Hamilton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391748072"
                        ],
                        "name": "A. Trent DePersia",
                        "slug": "A.-Trent-DePersia",
                        "structuredName": {
                            "firstName": "A. Trent",
                            "lastName": "DePersia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Trent DePersia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 173179919,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e70d2477af17d68b13c554e9f4f62d4f739c029d",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The mission of the Department of Defense Counter-drug Technology Development Program Office's face recognition technology (FERET) program is to develop automatic face- recognition systems for intelligence and law enforcement applications. To achieve this objective, the program supports research in face-recognition algorithms, the collection of a large database of facial images, independent testing and evaluation of face-recognition algorithms, construction of real-time demonstration systems, and the integration of algorithms into the demonstration systems. The FERET program has established baseline performance for face recognition. The Army Research Laboratory (ARL) has been the program's technical agent since 1993, managing development of the recognition algorithms, database collection, and conduction algorithm testing and evaluation. Currently, ARL is managing the development of several prototype face-recognition systems that will demonstrate complete real-time video face identification in an access control scenario. This paper gives an overview of the FERET program, presents performance results of the face- recognition algorithms evaluated, and addresses the future direction of the program and applications for DoD and law enforcement."
            },
            "slug": "FERET-(Face-Recognition-Technology)-program-Rauss-Phillips",
            "title": {
                "fragments": [],
                "text": "FERET (Face Recognition Technology) program"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An overview of the FERET program is given, performance results of the face- recognition algorithms evaluated are presented, and the future direction of the program and applications for DoD and law enforcement are addressed."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313513"
                        ],
                        "name": "Patrick J. Rauss",
                        "slug": "Patrick-J.-Rauss",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Rauss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick J. Rauss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113989911"
                        ],
                        "name": "J. Phillips",
                        "slug": "J.-Phillips",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Phillips",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40638847"
                        ],
                        "name": "Hyeonjoon Moon",
                        "slug": "Hyeonjoon-Moon",
                        "structuredName": {
                            "firstName": "Hyeonjoon",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeonjoon Moon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2958806"
                        ],
                        "name": "S. A. Rizvi",
                        "slug": "S.-A.-Rizvi",
                        "structuredName": {
                            "firstName": "Syed",
                            "lastName": "Rizvi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. A. Rizvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3156855"
                        ],
                        "name": "M. K. Hamilton",
                        "slug": "M.-K.-Hamilton",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Hamilton",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. K. Hamilton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391748072"
                        ],
                        "name": "A. Trent DePersia",
                        "slug": "A.-Trent-DePersia",
                        "structuredName": {
                            "firstName": "A. Trent",
                            "lastName": "DePersia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Trent DePersia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61144753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17ef6f8ff78ea531b54a973d2691f5f7c1923a16",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The mission of the Department of Defense (DoD) Counter-drug Technology Development Program Office's Face Recognition Technology (FERET) program is to develop automatic face recognition systems from the development of detection and recognition algorithms in the laboratory through their demonstration in a prototype real-time system. To achieve this objective, the program supports research in face recognition algorithms, the collection of a large database of facial images, independent testing and evaluation of face recognition algorithms, construction of a real-time demonstration systems, and the integration of algorithms into the demonstration systems. The FERET program has established baseline performance for face recognition. The Army Research Laboratory (ARL) has been the technical agent for the Advanced Research Projects Agency since 1993, managing development of the recognition algorithms, database collection, and algorithm testing. Currently ARL is managing the development of several prototype face recognition systems that will demonstrate complete real-time video face identification in an access control mission. This paper gives an overview of the FERET program, presents recent performance results of face recognition algorithms evaluated, and addresses the future direction of the program and applications for DoD and law enforcement."
            },
            "slug": "FERET-(Face-Recognition-Technology)-program-Rauss-Phillips",
            "title": {
                "fragments": [],
                "text": "FERET (Face Recognition Technology) program"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An overview of the FERET program is given, recent performance results of face recognition algorithms evaluated are presented, and the future direction of the program and applications for DoD and law enforcement are addressed."
            },
            "venue": {
                "fragments": [],
                "text": "Defense + Security Symposium"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2958806"
                        ],
                        "name": "S. A. Rizvi",
                        "slug": "S.-A.-Rizvi",
                        "structuredName": {
                            "firstName": "Syed",
                            "lastName": "Rizvi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. A. Rizvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40638847"
                        ],
                        "name": "Hyeonjoon Moon",
                        "slug": "Hyeonjoon-Moon",
                        "structuredName": {
                            "firstName": "Hyeonjoon",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeonjoon Moon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9512074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6136d4e52a6ffd0b21ae85a36fd2e400af41ca78",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Two critical performance characterizations of biometric algorithms, including face recognition, are identification and verification. Identification performance of face recognition algorithms on the FERET tests has been previously reported. We report on verification performance obtained from the Sept96 FERET test. The databases used to develop and test algorithms are usually smaller than the databases that will be encountered in applications. We examine the effects of size of the database on performance for both identification and verification."
            },
            "slug": "The-FERET-verification-testing-protocol-for-face-Rizvi-Phillips",
            "title": {
                "fragments": [],
                "text": "The FERET verification testing protocol for face recognition algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The effects of size of the database on performance for both identification and verification of face recognition algorithms, including face recognition, are examined."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1971263"
                        ],
                        "name": "K. Etemad",
                        "slug": "K.-Etemad",
                        "structuredName": {
                            "firstName": "Kamran",
                            "lastName": "Etemad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Etemad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69416958"
                        ],
                        "name": "Ramalingam Chellappa",
                        "slug": "Ramalingam-Chellappa",
                        "structuredName": {
                            "firstName": "Ramalingam",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ramalingam Chellappa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221895897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6bfe51abb006ab56ee039b4b54a643fd53ed7fa",
            "isKey": false,
            "numCitedBy": 425,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper the discriminatory power of various human facial features is studied and a new scheme for Automatic Face Recognition (AFR) is proposed. Using Linear Discriminant Analysis (LDA) of different aspects of human faces in spatial domain, we first evaluate the significance of visual information in different parts/features of the face for identifying the human subject. The LDA of faces also provides us with a small set of features that carry the most relevant information for classification purposes. The features are obtained through eigenvector analysis of scatter matrices with the objective of maximizing between-class and minimizing within-class variations. The result is an efficient projection-based feature extraction and classification scheme for AFR. Soft decisions made based on each of the projections are combined, using probabilistic or evidential approaches to multisource data analysis. For medium-sized databases of human faces, good classification accuracy is achieved using very low-dimensional feature vectors."
            },
            "slug": "Discriminant-analysis-for-recognition-of-human-face-Etemad-Chellappa",
            "title": {
                "fragments": [],
                "text": "Discriminant analysis for recognition of human face images"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The discriminatory power of various human facial features is studied and a new scheme for Automatic Face Recognition (AFR) is proposed and an efficient projection-based feature extraction and classification scheme for AFR is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738894"
                        ],
                        "name": "T. Starner",
                        "slug": "T.-Starner",
                        "structuredName": {
                            "firstName": "Thad",
                            "lastName": "Starner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 136280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0bf5d558220d39698ce96d59ee5772e8e1a0663",
            "isKey": false,
            "numCitedBy": 2234,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated.<<ETX>>"
            },
            "slug": "View-based-and-modular-eigenspaces-for-face-Pentland-Moghaddam",
            "title": {
                "fragments": [],
                "text": "View-based and modular eigenspaces for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer, which yields higher recognition rates as well as a more robust framework for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13900,
                                "start": 13897
                            }
                        ],
                        "text": "Two of the most critical requirements in support of producing reliable face recognition systems are a large database of facial images and a testing procedure to evaluate systems The Face Recognition Technology FERET program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests To date images from individuals are included in the FERET database which is divided into development and sequestered portions of the database In September the FERET program administered the third in a series of FERET face recognition tests The primary objectives of the third test were to assess the state of the art identify future areas of research and measure algorithm performance\nIntroduction\nOver the last decade face recognition has become an active area of research in com puter vision neuroscience and psychology Progress has advanced to the point that face recognition systems are being demonstrated in real world settings The rapid de velopment of face recognition is due to a combination of factors active development of\nThe work reported here is part of the Face Recognition Technology FERET program which is sponsored by the U S Department of Defense Counterdrug Technology Development Program Portions of this work was done while Jonathon Phillips was at the U S Army Research Laboratory ARL Jonathon Phillips acknowledges the support of the National Institute of Justice\nalgorithms the availability of a large database of facial images and a method for evaluat ing the performance of face recognition algorithms The FERET database and evaluation methodology address the latter two points and are de facto standards There have been three FERET evaluations with the most recent being the Sep FERET test\nThe Sep FERET test provides a comprehensive picture of the state of the art in face recognition from still images This was accomplished by evaluating algorithms ability on di erent scenarios categories of images and versions of algorithms Performance was computed for identi cation and veri cation scenarios In an identi cation application an algorithm is presented with a face that it must identify the face whereas in a veri cation application an algorithm is presented with a face and a claimed identity and the algorithm must accept or reject the claim In this paper we describe the FERET database the Sep FERET evaluation protocol and present identi cation results Veri cation results are presented in Rizvi et al\nTo obtain a robust assessment of performance algorithms are evaluated against dif ferent categories of images The categories are broken out by lighting changes people wearing glasses and the time between the acquisition date of the database image and the image presented to the algorithm By breaking out performance into these categories a better understanding of the face recognition eld in general as well as the strengths and weakness of individual algorithms is obtained This detailed analysis helps to assess which applications can be successfully addressed\nAll face recognition algorithms known to the authors consist of two parts face detection and normalization and face identi cation Algorithms that consist of both parts are referred to as fully automatic algorithms and those that consist of only the second part are partially automatic algorithms The Sep test evaluated both fully and partially automatic algorithms Partially automatic algorithms are given a facial image and the coordinates of the center of the eyes Fully automatic algorithms are only given facial images\nThe availability of the FERET database and evaluation methodology has made a signi cant di erence in the progress of development of face recognition algorithms Be fore the FERET database was created a large number of papers reported outstanding recognition results usually correct recognition on limited size databases usually individuals In fact this is still true Only a few of these algorithms reported results on images utilizing a common database let alone met the desirable goal of being evaluated on a standard testing protocol that included separate training and testing sets As a consequence there was no method to make informed comparisons among various algorithms\nThe FERET database has made it possible for researchers to develop algorithms on a common database and to report results in the literature using this database Results reported in the literature do not provide a direct comparison among algorithms because each researcher reported results using di erent assumptions scoring methods and images The independently administered FERET test allows for a direct quantitative assessment of the relative strengths and weaknesses of di erent approaches\nMore importantly the FERET database and tests clarify the current state of the art in face recognition and point out general directions for future research The FERET tests allow the computer vision community to assess overall strengths and weaknesses in the eld not only on the basis of the performance of an individual algorithm but\nin addition on the aggregate performance of all algorithms tested Through this type of assessment the community learns in an unbiased and open manner of the important technical problems to be addressed and how the community is progressing toward solving these problems\nBackground\nThe rst FERET tests took place in August and March for details of these tests and the FERET database and program see Phillips et al and Rauss et al The FERET database collection began in September along with the FERET program\nThe August test established for the rst time a performance baseline for face recognition algorithms This test was designed to measure performance on algorithms that could automatically locate normalize and identify faces from a database The test consisted of three subtests each with a di erent gallery and probe set The gallery contains the set of known individuals An image of an unknown face presented to the algorithm is called a probe and the collection of probes is called the probe set The rst subtest examined the ability of algorithms to recognize faces from a gallery of individuals The second was the false alarm test which measured how well an algorithm rejects faces not in the gallery The third baselined the e ects of pose changes on performance\nThe second FERET test that took place in March measured progress since August and evaluated algorithms on larger galleries The March evaluation consisted of a single test with a gallery of known individuals One emphasis of the test was on probe sets that contained duplicate images A duplicate is de ned as an image of a person whose corresponding gallery image was taken on a di erent date\nThe FERET database is designed to advance the state of the art in face recognition with the images collected directly supporting both algorithm development and the FERET evaluation tests The database is divided into a development set provided to researchers and a set of sequestered images for testing The images in the development set are representative of the sequestered images\nThe facial images were collected in sessions between August and July Collection sessions lasted one or two days In an e ort to maintain a degree of consistency throughout the database the same physical setup and location was used in each photog raphy session However because the equipment had to be reassembled for each session there was variation from session to session gure\nImages of an individual were acquired in sets of to images collected under rel atively unconstrained conditions Two frontal views were taken fa and fb a di erent facial expression was requested for the second frontal image For sets of images a third frontal image was taken with a di erent camera and di erent lighting this is referred to as the fc image The remaining images were collected at various aspects between right and left pro le To add simple variations to the database photographers sometimes took a second set of images for which the subjects were asked to put on their glasses and or pull their hair back Sometimes a second set of images of a person was taken on a later date such a set of images is referred to as a duplicate set Such duplicates sets result in variations in scale pose expression and illumination of the face\nBy July sets of images were in the database consisting of total images The database contains individuals and duplicate sets of images For\nfa fb duplicate I fc duplicate II\nFigure Examples of di erent categories of probes image The duplicate I image was taken within one year of the fa image and the duplicate II and fa images were taken at least one year apart\nsome people over two years elapsed between their rst and most recent sittings with some subjects being photographed multiple times gure The development portion of the database consisted of sets of images and was released to researchers The remaining images were sequestered by the Government\nTest Design\nTest Design Principles\nThe FERET Sep evaluation protocol was designed to assess the state of the art advance the state of the art and point to future directions of research To succeed at this the test design must solve the three bears problem The test cannot be neither too hard nor too easy If the test is too easy the testing process becomes an exercise in tuning existing algorithms If the test is too hard the test is beyond the ability of existing algorithmic techniques The results from the test are poor and do not allow for an accurate assessment of algorithmic capabilities\nThe solution to the three bears problem is through the selection of images in the test set and the testing protocol Tests are administered using a testing protocol that states the mechanics of the tests and the manner in which the test will be scored In face recognition the protocol states the number of images of each person in the test how the output from the algorithm is recorded and how the performance results are reported\nThe characteristics and quality of the images are major factors in determining the di culty of the problem being evaluated For example if faces are in a predetermined position in the images the problem is di erent from that for images in which the faces can be located anywhere in the image In the FERET database variability was introduced by the inclusion of images taken at di erent dates and locations see section This resulted in changes in lighting scale and background\nThe testing protocol is based on a set of design principles Stating the design principle allows one to assess how appropriate the FERET test is for a particular face recognition algorithm Also design principles assist in determining if an evaluation methodology for testing algorithm s for a particular application is appropriate Before discussing the\ndesign principles we state the evaluation protocol In the testing protocol an algorithm is given two sets of images the target set and the query set We introduce this terminology to distinguish these sets from the gallery and probe sets that are used in computing performance statistics The target set is given to the algorithm as the set of known facial images The images in the query set consists of unknown facial images to be identi ed For each image qi in the query set Q an algorithm reports a similarity si k between qi and each image tk in the target set T The testing protocol is designed so that each algorithm can use a di erent similarity measure and we do not compare similarity measures from di erent algorithms The key property of the new protocol which allows for greater exibility in scoring is that for any two images qi and tk we know si k\nThis exibility allows the evaluation methodology to be robust and comprehensive it is achieved by computing scores for virtual galleries and probe sets A gallery G is a virtual gallery if G is a subset of the target set i e G T Similarly P is a virtual probe set if P Q For a given gallery G and probe set P the performance scores are computed by examination of similarity measures si k such that qi P and tk G\nThe virtual gallery and probe set technique allows us to characterize algorithm per formance by di erent categories of images The di erent categories include rotated images duplicates taken within a week of the gallery image duplicates where the time between the images is at least one year galleries containing one image per person and galleries containing duplicate images of the same person We can create a gallery of people and estimate an algorithm s performance by recognizing people in this gallery Using this as a starting point we can then create virtual galleries of people and determine how performance changes as the size of the gallery increases An other avenue of investigation is to create n di erent galleries of size and calculate the variation in algorithm performance with the di erent galleries\nTo take full advantage of virtual galleries and probe sets we selected multiple images of the same person and placed them into the target and query sets If such images were marked as the same person the algorithms being tested could use the information in the evaluation process To prevent this from happenning we require that each image in the target set be treated as an unique face In practice this condition is enforced by giving every image in the target and query set a unique random identi cation This is the rst design principle\nThe second design principle is that training is completed prior to the start of the test This forces each algorithm to have a general representation for faces not a representation tuned to a speci c gallery Without this condition virtual galleries would not be possible\nFor algorithms to have a general representation for faces they must be gallery class insensitive Examples are algorithms based on normalized correlation or principal com ponent analysis PCA An algorithm is class sensitive if the representation is tuned to a speci c gallery Examples are straight forward implementation of Fisher discriminant analysis Fisher discriminant algorithms were adapted to class insensitive testing methodologies by Zhao et al with performance results of these extensions being reported in this paper\nThe third design rule is that all algorithms tested compute a similarity measure be tween two facial images this similarity measure was computed for all pairs of images between the target and query sets Knowing the similarity score between all pairs of\nFace Recognition Algorithm\n(run at Testee\u2019s site)\nOutput File\nScoring Code\nGovernment Site)\n(run at\nResults\nMMMM\n0001\n0002\n0002\n0001\nNNNN\nProbe Image Name\nGallery Image Name\n(One Image/Person) Gallery images\nList of Probes\nFigure Schematic of the FERET testing procedure\nimages from the target and query sets allows for the construction of virtual galleries and probe sets\nTest Details\nIn the Sep FERET test the target set contained images and the query set images All the images in the target set were frontal images The query set consisted of all the images in the target set plus rotated images and digitally modi ed images We designed the digitally modi ed images to test the e ects of illumination and scale Results from the rotated and digitally modi ed images are not reported here For each query image qi an algorithm outputs the similarity measure si k for all images tk in the target set For a given query image qi the target images tk are sorted by the similarity scores si Since the target set is a subset of the query set the test output contains the similarity score between all images in the target set\nThere were two versions of the Sep test The target and query sets were the same for each version The rst version tested partially automatic algorithms by providing them with a list of images in the target and query sets and the coordinates of the center of the eyes for images in the target and query sets In the second version of the test the coordinates of the eyes were not provided By comparing the performance between the two versions we estimate performance of the face locating portion of a fully automatic algorithm at the system level\nThe test was administered at each group s site under the supervision of one of the au thors Each group had three days to complete the test on less than UNIX workstations this limit was not reached We did not record the time or number of workstations because execution times can vary according to the type of machines used machine and network con guration and the amount of time that the developers spent optimizing their code we wanted to encourage algorithm development not code optimization We imposed the\ntime limit to encourage the development of algorithms that could be incorporated into operational eldable systems\nThe images contained in the gallery and probe sets consisted of images from both the developmental and sequestered portions of the FERET database Only images from the FERET database were included in the test however algorithm developers were not prohibited from using images outside the FERET database to develop or tune parameters in their algorithms\nThe FERET test is designed to measure laboratory performance The test is not concerned with speed of the implementation real time implementation issues and speed and accuracy trade o s These issues and others need to be addressed in an operational elded system were beyond the scope of the Sep FERET test\nFigure presents a schematic of the testing procedure To ensure that matching was not done by le name we gave the images random names The nominal pose of each face was provided to the testee\nDecision Theory and Performance Evaluation\nThe basic models for evaluating the performance of an algorithm are the closed and open universes In the closed universe every probe is in the gallery In an open universe some probes are not in the gallery Both models re ect di erent and important aspects of face recognition algorithms and report di erent performance statistics The open universe models veri cation applications The FERET scoring procedures for veri cation is given in Rizvi et al\nThe closed universe model allows one to ask how good an algorithm is at identifying a probe image the question is not always is the top match correct but is the correct answer in the top n matches This lets one know how many images have to be examined to get a desired level of performance The performance statistics are reported as cumula tive match scores The rank is plotted along the horizontal axis and the vertical axis is the percentage of correct matches The cumulative match score can be calculated for any subset of the probe set We calculated this score to evaluate an algorithm s performance on di erent categories of probes i e rotated or scaled probes\nThe computation of an identi cation score is quite simple Let P be a probe set and jPj the size of P We score probe set P against gallery G where G fg gMg and P fp pNg by comparing the similarity scores si such that pi P and gk G For each probe image pi P we sort si for all gallery images gk G We assume that a smaller similarity score implies a closer match If gk and pi are the same image then si k The function id i gives the index of the gallery image of the person in probe pi i e pi is an image of the person in gid i A probe pi is correctly identi ed if si id i is the smallest scores for gk G A probe pi is in the top k if si id i is one of the k th smallest score si for gallery G Let Rk denote the number of probes in the top k We reported Rk jPj the fraction of probes in the top k As an example let k R and jPj Based on the formula the performance score for R is\nIn reporting identi cation performance results we state the size of the gallery and the number of probes scored The size of the gallery is the number of di erent faces people contained in the images that are in the gallery For all results that we report there is one image per person in the gallery thus the size of the gallery is also the number of images\nin the gallery The number of probes scored also size of the probe set is jPj The probe set may contain more than one image of a person and the probe set may not contain an image of everyone in the gallery Every image in the probe set has a corresponding image in the gallery\nLatest Test Results\nThe Sep FERET test was designed to measure algorithm performance for identi cation and veri cation tasks Both tasks are evaluated on the same sets of images We report the results for algorithms that includes partially automatic algorithms and fully automatic algorithms The test was administered in September and March see table for details of when the test was administered to which groups and which version of the test was taken Two of these algorithms were developed at the MIT Media Laboratory The rst was the same algorithm that was tested in March This algorithm was retested so that improvement since March could be measured The second algorithm was based on more recent work Algorithms were also tested from Excalibur Corp Carlsbad CA Michigan State University MSU Rutgers University University of Southern California USC and two from University of Maryland UMD The rst algorithm from UMD was tested in September and a second version of the algorithm was tested in March For the fully automatic version of test algorithms from MIT and USC were evaluated\nThe nal two algorithms were our implementation of normalized correlation and a principal components analysis PCA based algorithm These algorithms provide a performance baseline In our implementation of the PCA based algorithm all images were translated rotated and scaled so that the center of the eyes were placed on speci c pixels faces were masked to remove background and hair and the non masked facial pixels were processed by a histogram equalization algorithm The training set consisted of faces Faces were represented by their projection onto the rst eigenvectors and were identi ed by a nearest neighbor classi er using the L metric For normalized correlation the images were translated rotated and scaled so that the center of the eyes were placed on speci c pixels and faces were masked to remove background and hair\nPartially automatic algorithms\nWe report identi cation scores for four categories of probes The rst probe category was the FB probes g For each set of images there were two frontal images One of the images was randomly placed in the gallery and the other image was placed in the FB probe set This category is denoted by FB to di erentiate it from the fb images in the FERET database The second probe category contained all duplicate frontal images in the FERET database for the gallery images We refer to this category as the duplicate I probes The third category was the fc images taken the same day but with a di erent camera and lighting The fourth consisted of duplicates where there is at least one year between the acquisition of the probe image and corresponding gallery image We refer to this category as the duplicate II probes For this category the gallery images were acquired before January and the probe images were acquired after January\nTable List of groups that took the Sept test broken out by versions taken and dates administered The by MIT indicates that two algorithms were tested\nTest Date\nSeptember March\nVersion of test Group Baseline\nFully Automatic MIT Media Lab U of So California USC\nEye Coordinates Given Baseline PCA Baseline Correlation Excalibur Corp MIT Media Lab\nMichigan State U Rutgers U\nU Maryland USC\nThe gallery for the FB duplicate I and fc probes was the same and consisted of frontal images with one image person in the gallery thus the gallery contained individuals Also none of the faces in the gallery images wore glasses The gallery for duplicate II probes was a subset of images from the gallery for the other categories\nThe results for identi cation are reported as cumulative match scores Table shows the categories corresponding to the gures presenting the results type of results and size of the gallery and probe sets gs to\nIn gures and we compare the di culty of di erent probe sets Whereas gure reports identi cation performance for each algorithm gure shows a single curve that is an average of the identi cation performance of all algorithms for each probe category For example the rst ranked score for duplicate I probe sets is computed from an average of the rst ranked score for all algorithms in gure In gure we presented current upper bound for performance on partially automatic algorithms for each probe category For each category of probe gure plots the algorithm with the highest top rank score R Figures and reports performance of four categories of probes FB duplicate I fc duplicate II\nTable Figures reporting results for partially automatic algorithms Performance is broken out by probe category\nFigure no Probe Category Gallery size Probe set size\nFB\nduplicate I\nfc\nduplicate II\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 MSU UMD 96 MIT 95\nBaseline Cor Baseline EF\nExcalibur Rutgers\na\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUMD 97 USC\nUMD 96 Baseline Cor Baseline EF\nb\nFigure Identi cation performance against FB probes a Partially automatic algo rithms tested in September b Partially automatic algorithms tested in March\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 Excalibur Baseline EF Baseline Cor\nMIT 95 MSU\nUMD 96 Rutgers\na\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC UMD 97\nBaseline EF Baseline Cor\nUMD 96\nb\nFigure Identi cation performance against all duplicate I probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 UMD 96\nMSU Excalibur Baseline EF Rutgers MIT 95 Baseline Cor\na\n0\n0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC UMD 97 UMD 96\nBaseline EF Baseline Cor\nb\nFigure Identi cation performance against fc probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 Baseline EF\nExcalibur Rutgers MIT 95 Baseline Cor UMD 96\nMSU\na\n0\n0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC Baseline EF\nUMD 97 Baseline Cor\nUMD 96\nb\nFigure Identi cation performance against duplicate II probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nFB probes Duplicate I probes\nfc probes Duplicate II probes\nFigure Average identi cation performance of partially automatic algorithms on each probe category\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nFB probes fc probes\nDuplicate I probes Duplicate II probes\nFigure Current upper bound identi cation performance of partially automatic algo rithm for each probe category\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\n0.92\n0.94\n0.96\n0.98\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nUSC partially automatic MIT partially automatic\nUSC fully automatic MIT fully automatic\nFigure Identi cation performance of fully automatic algorithms against partially auto matic algorithms for FB probes\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nUSC partially automatic USC fully automatic MIT partially automatic MIT fully automatic\nFigure Identi cation performance of fully automatic algorithms against partially au tomatic algorithms for duplicate I probes\nFully Automatic Performance\nIn this subsection we report performance for the fully automatic algorithms of the MIT Media Lab and USC To allow for a comparison between the partially and fully automatic algorithms we plot the results for the partially and fully automatic algorithms Figure shows performance for FB probes and gure shows performance for duplicate I probes The gallery and probe sets are the same as in subsection\nVariation in Performance\nFrom a statistical point of view a face recognition algorithm estimates the identity of a face Consistent with this view we can ask about the variance in performance of an algorithm For a given category of images how does performance change if the algorithm is given a di erent gallery and probe set In tables and we show how algorithm performance varies if the people in the galleries change For this experiment we constructed six galleries of approximately individuals in which an individual was in only one gallery the number of people contained within each gallery versus the number of probes scored is given in tables and Results are reported for the partially automatic algorithms For the results in this section we order algorithms by their top rank score on each gallery for example in table the UMD Mar algorithm scored highest on gallery and the baseline PCA and correlation tied for th place Also included in this table is average performance for all algorithms Table reports results for FB probes Table is organized in the same manner as table except that duplicate I probes are scored Tables and report results for the same gallery The galleries were constructed by placing images within the galleries by chronological order in which the images were collected the rst gallery contains the rst images collected and the th gallery contains the most recent images collected In table mean age refers to the average time between collection of images contained in the gallery and the corresponding duplicate probes No scores are reported in table for gallery because there are no duplicates for this gallery\nDiscussion and Conclusion\nIn this paper we presented the Sep FERET evaluation protocol for face recognition algorithms The protocol makes it possible to independently evaluate algorithms The protocol was designed to evaluate algorithms on di erent galleries and probe sets for dif ferent scenarios Using this protocol we computed performance on identi cation and veri cation tasks The veri cation results are presented in Rizvi et al and all veri cation results mentioned in this section are from that paper In this paper we presented detailed identi cation results Because of the Sep FERET evaluation protocol s ability to test algorithms performance on di erent tasks for multiple galleries and probe sets it is the de facto standard for measuring performance of face recognition algorithms These results show that factors e ecting performance include scenario date tested and probe category\nThe Sep test was the latest FERET test the others were the Aug and Mar tests One of the main goals of the FERET tests has been to improve the performance of face recognition algorithms and is seen in the Sep FERET test The rst case is the improvement in performance of the MIT Media Lab September algorithm over\nthe March algorithm the second is the improvement of the UMD algorithm between September and March\nBy looking at progress over the series of FERET tests one sees that substantial progress has been made in face recognition The most direct method is to compare the performance of fully automatic algorithms on fb probes the two earlier FERET tests only evaluated fully automatic algorithms The best top rank score for fb probes on the Aug test was on a gallery of individuals and for Mar the top score was on a gallery of individuals This compares to in September and in March gallery of individuals This method shows that over the course of the FERET tests the absolute scores increased as the size of the database increased The March score was from one of the MIT Media Lab algorithms and represents an increase from in March\nOn duplicate I probes MIT Media Lab improved from March to September USC s performance remained approximately the same at be tween March and March This improvement in performance was achieved while the gallery size increased and the number of duplicate I probes increased from to While increasing the number of probes does not necessarily increase the di culty of iden ti cation tasks we argue that the Sep duplicate I probe set was more di cult to process then the Mar set The Sep duplicate I probe set contained the duplicate II probes and the Mar duplicate I probe set did not contain a similar class of probes Overall the duplicate II probe set was the most di cult probe set\nAnother goal of the FERET tests is to identify areas of strengths and weaknesses in the eld of face recognition We addressed this issue by computing algorithm per formance for multiple galleries and probe sets From this evaluation we concluded that algorithm performance is dependent on the gallery and probe sets We observed variation in performance due to changing the gallery and probe set within a probe category and by changing probe categories The e ect of changing the gallery while keeping the probe category constant is shown in tables and For fb probes the range for performance is to for duplicate I probes the range is to Equally important tables and shows the variability in relative performance levels For example in table UMD Sep duplicate performance varies between number three and nine Similar results were found in Moon and Phillips in their study of principal component analysis based face recognition algorithms This shows that an area of future research could measure the ef fect of changing galleries and probe sets and statistical measures that characterize these variations\nFigures and shows probe categories characterized by di culty These gures show that fb probes are the easiest and duplicate II probes are the most di cult On average duplicate I probes are easier to identify than fc probes However the best performance on fc probes is signi cantly better than the best performance on duplicate I and II probes This comparative analysis shows that future areas of research could address processing of duplicate II probes and developing methods to compensate for changes in illumination\nThe scenario being tested contributes to algorithm performance For identi cation the MIT Media Lab algorithm was clearly the best algorithm tested in September However for veri cation there was not an algorithm that was a top performer for all probe categories Also for the algorithms tested in March the USC algorithm performed overall better than the UMD algorithm for identi cation however for veri cation UMD\noverall performed better This shows that performance on one task is not predictive of performance on another task\nThe September FERET test shows that de nite progress is being made in face recognition and that the upper bound in performance has not been reached The im provement in performance documented in this paper shows directly that the FERET series of tests have made a signi cant contribution to face recognition This conclusion is indi rectly supported by the improvement in performance between the algorithms tested in September and March the number of papers that use FERET images and report experimental results using FERET images and the number of groups that participated in the Sep test"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 35
                            }
                        ],
                        "text": "Eye Coordinates Given Baseline PCA [4, 10]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 131
                            }
                        ],
                        "text": "The nal two algorithms were our implementation of normalized correlation and a principal components analysis (PCA) based algorithm [4, 10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26127529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f",
            "isKey": true,
            "numCitedBy": 14954,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture."
            },
            "slug": "Eigenfaces-for-Recognition-Turk-Pentland",
            "title": {
                "fragments": [],
                "text": "Eigenfaces for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals, and that is easy to implement using a neural network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983127"
                        ],
                        "name": "C. Nastar",
                        "slug": "C.-Nastar",
                        "structuredName": {
                            "firstName": "Chahab",
                            "lastName": "Nastar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Nastar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15090755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b33dae4f6ba299189b8911d8814fe34fdd05d9e0",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel technique for face recognition based on deformable intensity surfaces which incorporates both the shape and texture components of the 2D image. The intensity surface of the facial image is modeled as a deformable 3D mesh in (z, y, I(x, y)) space. Using an efficient technique for matching two surfaces (in terms of the analytic modes of vibration), we obtain a dense correspondence field (or 3D warp) between two images. The probability distributions of two classes of warps are then estimated from training data: interpersonal and extrapersonal variations. These densities are then used in a Bayesian framework for image matching and recognition. Experimental results with facial data from the US Army FERET database demonstrate an increased recognition rate over the previous best methods."
            },
            "slug": "Bayesian-face-recognition-using-deformable-surfaces-Moghaddam-Nastar",
            "title": {
                "fragments": [],
                "text": "Bayesian face recognition using deformable intensity surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A novel technique for face recognition based on deformable intensity surfaces which incorporates both the shape and texture components of the 2D image and an increased recognition rate over the previous best methods are described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38480590"
                        ],
                        "name": "Wenyi Zhao",
                        "slug": "Wenyi-Zhao",
                        "structuredName": {
                            "firstName": "Wenyi",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyi Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2030268"
                        ],
                        "name": "A. Krishnaswamy",
                        "slug": "A.-Krishnaswamy",
                        "structuredName": {
                            "firstName": "Arvindh",
                            "lastName": "Krishnaswamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krishnaswamy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7032646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8e6580749e9cd3eebf7f9b95b58645c23d043ea",
            "isKey": false,
            "numCitedBy": 628,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a face recognition method based on PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis). The method consists of two steps: first we project the face image from the original vector space to a face subspace via PCA, second we use LDA to obtain a best linear classifier. The basic idea of combining PCA and LDA is to improve the generalization capability of LDA when only few samples per class are available. Using PCA, we are able to construct a face subspace in which we apply LDA to perform classification. Using FERET dataset we demonstrate a significant improvement when principal components rather than original images are fed to the LDA classifier. The hybrid classifier using PCA and LDA provides a useful framework for other image recognition tasks as well."
            },
            "slug": "Discriminant-analysis-of-principal-components-for-Zhao-Chellappa",
            "title": {
                "fragments": [],
                "text": "Discriminant analysis of principal components for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A hybrid classifier using PCA and LDA provides a useful framework for other image recognition tasks as well and demonstrates a significant improvement when principal components rather than original images are fed to the LDA classifier."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2995060"
                        ],
                        "name": "D. Swets",
                        "slug": "D.-Swets",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Swets",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Swets"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145926447"
                        ],
                        "name": "J. Weng",
                        "slug": "J.-Weng",
                        "structuredName": {
                            "firstName": "Juyang",
                            "lastName": "Weng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10952196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e28e81e757009d2f76b8674e0da431f5845884a",
            "isKey": false,
            "numCitedBy": 1773,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the automatic selection of features from an image training set using the theories of multidimensional discriminant analysis and the associated optimal linear projection. We demonstrate the effectiveness of these most discriminating features for view-based class retrieval from a large database of widely varying real-world objects presented as \"well-framed\" views, and compare it with that of the principal component analysis."
            },
            "slug": "Using-Discriminant-Eigenfeatures-for-Image-Swets-Weng",
            "title": {
                "fragments": [],
                "text": "Using Discriminant Eigenfeatures for Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper describes the automatic selection of features from an image training set using the theories of multidimensional discriminant analysis and the associated optimal linear projection, and demonstrates the effectiveness of these most discriminating features for view-based class retrieval from a large database of widely varying real-world objects."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49168578"
                        ],
                        "name": "J. Phillips",
                        "slug": "J.-Phillips",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Phillips",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144975752"
                        ],
                        "name": "V. Bruce",
                        "slug": "V.-Bruce",
                        "structuredName": {
                            "firstName": "Vicki",
                            "lastName": "Bruce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Bruce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66890955"
                        ],
                        "name": "F. F. Souli\u00e9",
                        "slug": "F.-F.-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Souli\u00e9",
                            "middleNames": [
                                "Fogelman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. F. Souli\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60461382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfec6ad5ab86f9a7bf96184c2c58950a4d5876a8",
            "isKey": false,
            "numCitedBy": 553,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-Recognition:-From-Theory-to-Applications-Phillips-Bruce",
            "title": {
                "fragments": [],
                "text": "Face Recognition: From Theory to Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9242811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e49ad8354bdd2fd6e8babd348df9e9a5b30bf3a6",
            "isKey": false,
            "numCitedBy": 461,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for a unimodal distributions) and a multivariate Mixture-of-Gaussians model (for multimodal distributions). These probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition. This learning technique is tested in experiments with modeling and subsequent detection of human faces and non-rigid objects such as hands.<<ETX>>"
            },
            "slug": "Probabilistic-visual-learning-for-object-detection-Moghaddam-Pentland",
            "title": {
                "fragments": [],
                "text": "Probabilistic visual learning for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "An unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition and a multivariate Mixture-of-Gaussians model is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168816"
                        ],
                        "name": "D. Stuss",
                        "slug": "D.-Stuss",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Stuss",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stuss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47248134"
                        ],
                        "name": "A. Hamer",
                        "slug": "A.-Hamer",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hamer",
                            "middleNames": [
                                "M",
                                "P"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6918045"
                        ],
                        "name": "L. Palumbo",
                        "slug": "L.-Palumbo",
                        "structuredName": {
                            "firstName": "Letizia",
                            "lastName": "Palumbo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Palumbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080862548"
                        ],
                        "name": "C. Dempster",
                        "slug": "C.-Dempster",
                        "structuredName": {
                            "firstName": "C",
                            "lastName": "Dempster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95349405"
                        ],
                        "name": "R. Binns",
                        "slug": "R.-Binns",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Binns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Binns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93562020"
                        ],
                        "name": "M. Levine",
                        "slug": "M.-Levine",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Levine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Levine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397844999"
                        ],
                        "name": "B. Izuakawa",
                        "slug": "B.-Izuakawa",
                        "structuredName": {
                            "firstName": "B",
                            "lastName": "Izuakawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Izuakawa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 889956,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "9b74a8d8da4a17096480b04192f467748b8be093",
            "isKey": false,
            "numCitedBy": 715,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A functional neuroimaging study of the variables that generate category-specific object processing differences. On the interaction of selective attention and lexical knowledge: A connectionist account of neglect dyslexia. D 1998 The effects of focal anterior and posterior brain lesions on verbal fluency. The discipline has emerged in the 1990s at the interface between the neural sciences and the cognitive and computational sciences. On one side, it grows out of the traditions of cognitive psychology and neuro-psychology, which use behavioral experiments to uncover the processes and mechanisms lying behind human cognitive functions, and of computational approaches within cognitive psychology, which rely on computational models to develop explicit mech-anistic accounts of these functions. On the other side, it grows out of the traditions of behavioral, functional, and systems neuroscience, which use neurophysio-logical and neuroanatomical methods to explore the mechanisms underlying complex functions. It draws on findings and principles of cellular and molecular neuroscience. It joins these approaches with the use of new functional brain imaging methods, such as functional magnetic imaging (fMRI), positron emission tomography (PET), as well as other methods including electroencephalography (EEG) and mag-netoencephalography (MEG), and with a growing research tradition in computational neuroscience. A starting point for cognitive neuroscience is the idea that a cognitive or mental state consists of a pattern of activity distributed over many neurons. For example, the experience an individual has when holding, sniffing , and viewing a rose is a complex pattern of neural activity, distributed over many brain regions, including the participation of neurons in visual, somato-sensory, and olfactory, and possibly extending to language areas participating in representing the sound of the word 'rose' and\\or other areas where activity represents the content of an associated memory that may be evoked by the experience. These patterns of activation arise from excitatory and inhibitory interactions among the participating neurons, mediated by connections called synapses. The inputs neurons receive cause them to 'fire' or emit impulses called spikes or action potentials, which travel down their axons to synaptic terminals where they cause the release of chemicals that then have excitatory or inhibitory influences on the neurons on the other side of the synapse. The combined effect of the incoming signals to each neuron, together with its recent history, determines whether it will fire at a particular moment. Figure 1 indicates something of the fundamental circuitry involved, though it should be noted that only one out of \u2026"
            },
            "slug": "Cognitive-neuroscience.-Stuss-Hamer",
            "title": {
                "fragments": [],
                "text": "Cognitive neuroscience."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Cognitive neuroscience has emerged in the 1990s at the interface between the neural sciences and the cognitive and computational sciences and joins these approaches with the use of new functional brain imaging methods, such as functional magnetic imaging (fMRI), positron emission tomography (PET), as well as other methods including electroencephalography (EEG) and mag-netoencephalographic (MEG)."
            },
            "venue": {
                "fragments": [],
                "text": "Current opinion in neurobiology"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21097,
                                "start": 21079
                            }
                        ],
                        "text": "Two of the most critical requirements in support of producing reliable face recognition systems are a large database of facial images and a testing procedure to evaluate systems The Face Recognition Technology FERET program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests To date images from individuals are included in the FERET database which is divided into development and sequestered portions of the database In September the FERET program administered the third in a series of FERET face recognition tests The primary objectives of the third test were to assess the state of the art identify future areas of research and measure algorithm performance\nIntroduction\nOver the last decade face recognition has become an active area of research in com puter vision neuroscience and psychology Progress has advanced to the point that face recognition systems are being demonstrated in real world settings The rapid de velopment of face recognition is due to a combination of factors active development of\nThe work reported here is part of the Face Recognition Technology FERET program which is sponsored by the U S Department of Defense Counterdrug Technology Development Program Portions of this work was done while Jonathon Phillips was at the U S Army Research Laboratory ARL Jonathon Phillips acknowledges the support of the National Institute of Justice\nalgorithms the availability of a large database of facial images and a method for evaluat ing the performance of face recognition algorithms The FERET database and evaluation methodology address the latter two points and are de facto standards There have been three FERET evaluations with the most recent being the Sep FERET test\nThe Sep FERET test provides a comprehensive picture of the state of the art in face recognition from still images This was accomplished by evaluating algorithms ability on di erent scenarios categories of images and versions of algorithms Performance was computed for identi cation and veri cation scenarios In an identi cation application an algorithm is presented with a face that it must identify the face whereas in a veri cation application an algorithm is presented with a face and a claimed identity and the algorithm must accept or reject the claim In this paper we describe the FERET database the Sep FERET evaluation protocol and present identi cation results Veri cation results are presented in Rizvi et al\nTo obtain a robust assessment of performance algorithms are evaluated against dif ferent categories of images The categories are broken out by lighting changes people wearing glasses and the time between the acquisition date of the database image and the image presented to the algorithm By breaking out performance into these categories a better understanding of the face recognition eld in general as well as the strengths and weakness of individual algorithms is obtained This detailed analysis helps to assess which applications can be successfully addressed\nAll face recognition algorithms known to the authors consist of two parts face detection and normalization and face identi cation Algorithms that consist of both parts are referred to as fully automatic algorithms and those that consist of only the second part are partially automatic algorithms The Sep test evaluated both fully and partially automatic algorithms Partially automatic algorithms are given a facial image and the coordinates of the center of the eyes Fully automatic algorithms are only given facial images\nThe availability of the FERET database and evaluation methodology has made a signi cant di erence in the progress of development of face recognition algorithms Be fore the FERET database was created a large number of papers reported outstanding recognition results usually correct recognition on limited size databases usually individuals In fact this is still true Only a few of these algorithms reported results on images utilizing a common database let alone met the desirable goal of being evaluated on a standard testing protocol that included separate training and testing sets As a consequence there was no method to make informed comparisons among various algorithms\nThe FERET database has made it possible for researchers to develop algorithms on a common database and to report results in the literature using this database Results reported in the literature do not provide a direct comparison among algorithms because each researcher reported results using di erent assumptions scoring methods and images The independently administered FERET test allows for a direct quantitative assessment of the relative strengths and weaknesses of di erent approaches\nMore importantly the FERET database and tests clarify the current state of the art in face recognition and point out general directions for future research The FERET tests allow the computer vision community to assess overall strengths and weaknesses in the eld not only on the basis of the performance of an individual algorithm but\nin addition on the aggregate performance of all algorithms tested Through this type of assessment the community learns in an unbiased and open manner of the important technical problems to be addressed and how the community is progressing toward solving these problems\nBackground\nThe rst FERET tests took place in August and March for details of these tests and the FERET database and program see Phillips et al and Rauss et al The FERET database collection began in September along with the FERET program\nThe August test established for the rst time a performance baseline for face recognition algorithms This test was designed to measure performance on algorithms that could automatically locate normalize and identify faces from a database The test consisted of three subtests each with a di erent gallery and probe set The gallery contains the set of known individuals An image of an unknown face presented to the algorithm is called a probe and the collection of probes is called the probe set The rst subtest examined the ability of algorithms to recognize faces from a gallery of individuals The second was the false alarm test which measured how well an algorithm rejects faces not in the gallery The third baselined the e ects of pose changes on performance\nThe second FERET test that took place in March measured progress since August and evaluated algorithms on larger galleries The March evaluation consisted of a single test with a gallery of known individuals One emphasis of the test was on probe sets that contained duplicate images A duplicate is de ned as an image of a person whose corresponding gallery image was taken on a di erent date\nThe FERET database is designed to advance the state of the art in face recognition with the images collected directly supporting both algorithm development and the FERET evaluation tests The database is divided into a development set provided to researchers and a set of sequestered images for testing The images in the development set are representative of the sequestered images\nThe facial images were collected in sessions between August and July Collection sessions lasted one or two days In an e ort to maintain a degree of consistency throughout the database the same physical setup and location was used in each photog raphy session However because the equipment had to be reassembled for each session there was variation from session to session gure\nImages of an individual were acquired in sets of to images collected under rel atively unconstrained conditions Two frontal views were taken fa and fb a di erent facial expression was requested for the second frontal image For sets of images a third frontal image was taken with a di erent camera and di erent lighting this is referred to as the fc image The remaining images were collected at various aspects between right and left pro le To add simple variations to the database photographers sometimes took a second set of images for which the subjects were asked to put on their glasses and or pull their hair back Sometimes a second set of images of a person was taken on a later date such a set of images is referred to as a duplicate set Such duplicates sets result in variations in scale pose expression and illumination of the face\nBy July sets of images were in the database consisting of total images The database contains individuals and duplicate sets of images For\nfa fb duplicate I fc duplicate II\nFigure Examples of di erent categories of probes image The duplicate I image was taken within one year of the fa image and the duplicate II and fa images were taken at least one year apart\nsome people over two years elapsed between their rst and most recent sittings with some subjects being photographed multiple times gure The development portion of the database consisted of sets of images and was released to researchers The remaining images were sequestered by the Government\nTest Design\nTest Design Principles\nThe FERET Sep evaluation protocol was designed to assess the state of the art advance the state of the art and point to future directions of research To succeed at this the test design must solve the three bears problem The test cannot be neither too hard nor too easy If the test is too easy the testing process becomes an exercise in tuning existing algorithms If the test is too hard the test is beyond the ability of existing algorithmic techniques The results from the test are poor and do not allow for an accurate assessment of algorithmic capabilities\nThe solution to the three bears problem is through the selection of images in the test set and the testing protocol Tests are administered using a testing protocol that states the mechanics of the tests and the manner in which the test will be scored In face recognition the protocol states the number of images of each person in the test how the output from the algorithm is recorded and how the performance results are reported\nThe characteristics and quality of the images are major factors in determining the di culty of the problem being evaluated For example if faces are in a predetermined position in the images the problem is di erent from that for images in which the faces can be located anywhere in the image In the FERET database variability was introduced by the inclusion of images taken at di erent dates and locations see section This resulted in changes in lighting scale and background\nThe testing protocol is based on a set of design principles Stating the design principle allows one to assess how appropriate the FERET test is for a particular face recognition algorithm Also design principles assist in determining if an evaluation methodology for testing algorithm s for a particular application is appropriate Before discussing the\ndesign principles we state the evaluation protocol In the testing protocol an algorithm is given two sets of images the target set and the query set We introduce this terminology to distinguish these sets from the gallery and probe sets that are used in computing performance statistics The target set is given to the algorithm as the set of known facial images The images in the query set consists of unknown facial images to be identi ed For each image qi in the query set Q an algorithm reports a similarity si k between qi and each image tk in the target set T The testing protocol is designed so that each algorithm can use a di erent similarity measure and we do not compare similarity measures from di erent algorithms The key property of the new protocol which allows for greater exibility in scoring is that for any two images qi and tk we know si k\nThis exibility allows the evaluation methodology to be robust and comprehensive it is achieved by computing scores for virtual galleries and probe sets A gallery G is a virtual gallery if G is a subset of the target set i e G T Similarly P is a virtual probe set if P Q For a given gallery G and probe set P the performance scores are computed by examination of similarity measures si k such that qi P and tk G\nThe virtual gallery and probe set technique allows us to characterize algorithm per formance by di erent categories of images The di erent categories include rotated images duplicates taken within a week of the gallery image duplicates where the time between the images is at least one year galleries containing one image per person and galleries containing duplicate images of the same person We can create a gallery of people and estimate an algorithm s performance by recognizing people in this gallery Using this as a starting point we can then create virtual galleries of people and determine how performance changes as the size of the gallery increases An other avenue of investigation is to create n di erent galleries of size and calculate the variation in algorithm performance with the di erent galleries\nTo take full advantage of virtual galleries and probe sets we selected multiple images of the same person and placed them into the target and query sets If such images were marked as the same person the algorithms being tested could use the information in the evaluation process To prevent this from happenning we require that each image in the target set be treated as an unique face In practice this condition is enforced by giving every image in the target and query set a unique random identi cation This is the rst design principle\nThe second design principle is that training is completed prior to the start of the test This forces each algorithm to have a general representation for faces not a representation tuned to a speci c gallery Without this condition virtual galleries would not be possible\nFor algorithms to have a general representation for faces they must be gallery class insensitive Examples are algorithms based on normalized correlation or principal com ponent analysis PCA An algorithm is class sensitive if the representation is tuned to a speci c gallery Examples are straight forward implementation of Fisher discriminant analysis Fisher discriminant algorithms were adapted to class insensitive testing methodologies by Zhao et al with performance results of these extensions being reported in this paper\nThe third design rule is that all algorithms tested compute a similarity measure be tween two facial images this similarity measure was computed for all pairs of images between the target and query sets Knowing the similarity score between all pairs of\nFace Recognition Algorithm\n(run at Testee\u2019s site)\nOutput File\nScoring Code\nGovernment Site)\n(run at\nResults\nMMMM\n0001\n0002\n0002\n0001\nNNNN\nProbe Image Name\nGallery Image Name\n(One Image/Person) Gallery images\nList of Probes\nFigure Schematic of the FERET testing procedure\nimages from the target and query sets allows for the construction of virtual galleries and probe sets\nTest Details\nIn the Sep FERET test the target set contained images and the query set images All the images in the target set were frontal images The query set consisted of all the images in the target set plus rotated images and digitally modi ed images We designed the digitally modi ed images to test the e ects of illumination and scale Results from the rotated and digitally modi ed images are not reported here For each query image qi an algorithm outputs the similarity measure si k for all images tk in the target set For a given query image qi the target images tk are sorted by the similarity scores si Since the target set is a subset of the query set the test output contains the similarity score between all images in the target set\nThere were two versions of the Sep test The target and query sets were the same for each version The rst version tested partially automatic algorithms by providing them with a list of images in the target and query sets and the coordinates of the center of the eyes for images in the target and query sets In the second version of the test the coordinates of the eyes were not provided By comparing the performance between the two versions we estimate performance of the face locating portion of a fully automatic algorithm at the system level\nThe test was administered at each group s site under the supervision of one of the au thors Each group had three days to complete the test on less than UNIX workstations this limit was not reached We did not record the time or number of workstations because execution times can vary according to the type of machines used machine and network con guration and the amount of time that the developers spent optimizing their code we wanted to encourage algorithm development not code optimization We imposed the\ntime limit to encourage the development of algorithms that could be incorporated into operational eldable systems\nThe images contained in the gallery and probe sets consisted of images from both the developmental and sequestered portions of the FERET database Only images from the FERET database were included in the test however algorithm developers were not prohibited from using images outside the FERET database to develop or tune parameters in their algorithms\nThe FERET test is designed to measure laboratory performance The test is not concerned with speed of the implementation real time implementation issues and speed and accuracy trade o s These issues and others need to be addressed in an operational elded system were beyond the scope of the Sep FERET test\nFigure presents a schematic of the testing procedure To ensure that matching was not done by le name we gave the images random names The nominal pose of each face was provided to the testee\nDecision Theory and Performance Evaluation\nThe basic models for evaluating the performance of an algorithm are the closed and open universes In the closed universe every probe is in the gallery In an open universe some probes are not in the gallery Both models re ect di erent and important aspects of face recognition algorithms and report di erent performance statistics The open universe models veri cation applications The FERET scoring procedures for veri cation is given in Rizvi et al\nThe closed universe model allows one to ask how good an algorithm is at identifying a probe image the question is not always is the top match correct but is the correct answer in the top n matches This lets one know how many images have to be examined to get a desired level of performance The performance statistics are reported as cumula tive match scores The rank is plotted along the horizontal axis and the vertical axis is the percentage of correct matches The cumulative match score can be calculated for any subset of the probe set We calculated this score to evaluate an algorithm s performance on di erent categories of probes i e rotated or scaled probes\nThe computation of an identi cation score is quite simple Let P be a probe set and jPj the size of P We score probe set P against gallery G where G fg gMg and P fp pNg by comparing the similarity scores si such that pi P and gk G For each probe image pi P we sort si for all gallery images gk G We assume that a smaller similarity score implies a closer match If gk and pi are the same image then si k The function id i gives the index of the gallery image of the person in probe pi i e pi is an image of the person in gid i A probe pi is correctly identi ed if si id i is the smallest scores for gk G A probe pi is in the top k if si id i is one of the k th smallest score si for gallery G Let Rk denote the number of probes in the top k We reported Rk jPj the fraction of probes in the top k As an example let k R and jPj Based on the formula the performance score for R is\nIn reporting identi cation performance results we state the size of the gallery and the number of probes scored The size of the gallery is the number of di erent faces people contained in the images that are in the gallery For all results that we report there is one image per person in the gallery thus the size of the gallery is also the number of images\nin the gallery The number of probes scored also size of the probe set is jPj The probe set may contain more than one image of a person and the probe set may not contain an image of everyone in the gallery Every image in the probe set has a corresponding image in the gallery\nLatest Test Results\nThe Sep FERET test was designed to measure algorithm performance for identi cation and veri cation tasks Both tasks are evaluated on the same sets of images We report the results for algorithms that includes partially automatic algorithms and fully automatic algorithms The test was administered in September and March see table for details of when the test was administered to which groups and which version of the test was taken Two of these algorithms were developed at the MIT Media Laboratory The rst was the same algorithm that was tested in March This algorithm was retested so that improvement since March could be measured The second algorithm was based on more recent work Algorithms were also tested from Excalibur Corp Carlsbad CA Michigan State University MSU Rutgers University University of Southern California USC and two from University of Maryland UMD The rst algorithm from UMD was tested in September and a second version of the algorithm was tested in March For the fully automatic version of test algorithms from MIT and USC were evaluated\nThe nal two algorithms were our implementation of normalized correlation and a principal components analysis PCA based algorithm These algorithms provide a performance baseline In our implementation of the PCA based algorithm all images were translated rotated and scaled so that the center of the eyes were placed on speci c pixels faces were masked to remove background and hair and the non masked facial pixels were processed by a histogram equalization algorithm The training set consisted of faces Faces were represented by their projection onto the rst eigenvectors and were identi ed by a nearest neighbor classi er using the L metric For normalized correlation the images were translated rotated and scaled so that the center of the eyes were placed on speci c pixels and faces were masked to remove background and hair\nPartially automatic algorithms\nWe report identi cation scores for four categories of probes The rst probe category was the FB probes g For each set of images there were two frontal images One of the images was randomly placed in the gallery and the other image was placed in the FB probe set This category is denoted by FB to di erentiate it from the fb images in the FERET database The second probe category contained all duplicate frontal images in the FERET database for the gallery images We refer to this category as the duplicate I probes The third category was the fc images taken the same day but with a di erent camera and lighting The fourth consisted of duplicates where there is at least one year between the acquisition of the probe image and corresponding gallery image We refer to this category as the duplicate II probes For this category the gallery images were acquired before January and the probe images were acquired after January\nTable List of groups that took the Sept test broken out by versions taken and dates administered The by MIT indicates that two algorithms were tested\nTest Date\nSeptember March\nVersion of test Group Baseline\nFully Automatic MIT Media Lab U of So California USC\nEye Coordinates Given Baseline PCA Baseline Correlation Excalibur Corp MIT Media Lab\nMichigan State U Rutgers U\nU Maryland USC\nThe gallery for the FB duplicate I and fc probes was the same and consisted of frontal images with one image person in the gallery thus the gallery contained individuals Also none of the faces in the gallery images wore glasses The gallery for duplicate II probes was a subset of images from the gallery for the other categories\nThe results for identi cation are reported as cumulative match scores Table shows the categories corresponding to the gures presenting the results type of results and size of the gallery and probe sets gs to\nIn gures and we compare the di culty of di erent probe sets Whereas gure reports identi cation performance for each algorithm gure shows a single curve that is an average of the identi cation performance of all algorithms for each probe category For example the rst ranked score for duplicate I probe sets is computed from an average of the rst ranked score for all algorithms in gure In gure we presented current upper bound for performance on partially automatic algorithms for each probe category For each category of probe gure plots the algorithm with the highest top rank score R Figures and reports performance of four categories of probes FB duplicate I fc duplicate II\nTable Figures reporting results for partially automatic algorithms Performance is broken out by probe category\nFigure no Probe Category Gallery size Probe set size\nFB\nduplicate I\nfc\nduplicate II\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 MSU UMD 96 MIT 95\nBaseline Cor Baseline EF\nExcalibur Rutgers\na\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUMD 97 USC\nUMD 96 Baseline Cor Baseline EF\nb\nFigure Identi cation performance against FB probes a Partially automatic algo rithms tested in September b Partially automatic algorithms tested in March\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 Excalibur Baseline EF Baseline Cor\nMIT 95 MSU\nUMD 96 Rutgers\na\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC UMD 97\nBaseline EF Baseline Cor\nUMD 96\nb\nFigure Identi cation performance against all duplicate I probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 UMD 96\nMSU Excalibur Baseline EF Rutgers MIT 95 Baseline Cor\na\n0\n0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC UMD 97 UMD 96\nBaseline EF Baseline Cor\nb\nFigure Identi cation performance against fc probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 Baseline EF\nExcalibur Rutgers MIT 95 Baseline Cor UMD 96\nMSU\na\n0\n0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC Baseline EF\nUMD 97 Baseline Cor\nUMD 96\nb\nFigure Identi cation performance against duplicate II probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nFB probes Duplicate I probes\nfc probes Duplicate II probes\nFigure Average identi cation performance of partially automatic algorithms on each probe category\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nFB probes fc probes\nDuplicate I probes Duplicate II probes\nFigure Current upper bound identi cation performance of partially automatic algo rithm for each probe category\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\n0.92\n0.94\n0.96\n0.98\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nUSC partially automatic MIT partially automatic\nUSC fully automatic MIT fully automatic\nFigure Identi cation performance of fully automatic algorithms against partially auto matic algorithms for FB probes\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nUSC partially automatic USC fully automatic MIT partially automatic MIT fully automatic\nFigure Identi cation performance of fully automatic algorithms against partially au tomatic algorithms for duplicate I probes\nFully Automatic Performance\nIn this subsection we report performance for the fully automatic algorithms of the MIT Media Lab and USC To allow for a comparison between the partially and fully automatic algorithms we plot the results for the partially and fully automatic algorithms Figure shows performance for FB probes and gure shows performance for duplicate I probes The gallery and probe sets are the same as in subsection\nVariation in Performance\nFrom a statistical point of view a face recognition algorithm estimates the identity of a face Consistent with this view we can ask about the variance in performance of an algorithm For a given category of images how does performance change if the algorithm is given a di erent gallery and probe set In tables and we show how algorithm performance varies if the people in the galleries change For this experiment we constructed six galleries of approximately individuals in which an individual was in only one gallery the number of people contained within each gallery versus the number of probes scored is given in tables and Results are reported for the partially automatic algorithms For the results in this section we order algorithms by their top rank score on each gallery for example in table the UMD Mar algorithm scored highest on gallery and the baseline PCA and correlation tied for th place Also included in this table is average performance for all algorithms Table reports results for FB probes Table is organized in the same manner as table except that duplicate I probes are scored Tables and report results for the same gallery The galleries were constructed by placing images within the galleries by chronological order in which the images were collected the rst gallery contains the rst images collected and the th gallery contains the most recent images collected In table mean age refers to the average time between collection of images contained in the gallery and the corresponding duplicate probes No scores are reported in table for gallery because there are no duplicates for this gallery\nDiscussion and Conclusion\nIn this paper we presented the Sep FERET evaluation protocol for face recognition algorithms The protocol makes it possible to independently evaluate algorithms The protocol was designed to evaluate algorithms on di erent galleries and probe sets for dif ferent scenarios Using this protocol we computed performance on identi cation and veri cation tasks The veri cation results are presented in Rizvi et al and all veri cation results mentioned in this section are from that paper In this paper we presented detailed identi cation results Because of the Sep FERET evaluation protocol s ability to test algorithms performance on di erent tasks for multiple galleries and probe sets it is the de facto standard for measuring performance of face recognition algorithms These results show that factors e ecting performance include scenario date tested and probe category\nThe Sep test was the latest FERET test the others were the Aug and Mar tests One of the main goals of the FERET tests has been to improve the performance of face recognition algorithms and is seen in the Sep FERET test The rst case is the improvement in performance of the MIT Media Lab September algorithm over\nthe March algorithm the second is the improvement of the UMD algorithm between September and March\nBy looking at progress over the series of FERET tests one sees that substantial progress has been made in face recognition The most direct method is to compare the performance of fully automatic algorithms on fb probes the two earlier FERET tests only evaluated fully automatic algorithms The best top rank score for fb probes on the Aug test was on a gallery of individuals and for Mar the top score was on a gallery of individuals This compares to in September and in March gallery of individuals This method shows that over the course of the FERET tests the absolute scores increased as the size of the database increased The March score was from one of the MIT Media Lab algorithms and represents an increase from in March\nOn duplicate I probes MIT Media Lab improved from March to September USC s performance remained approximately the same at be tween March and March This improvement in performance was achieved while the gallery size increased and the number of duplicate I probes increased from to While increasing the number of probes does not necessarily increase the di culty of iden ti cation tasks we argue that the Sep duplicate I probe set was more di cult to process then the Mar set The Sep duplicate I probe set contained the duplicate II probes and the Mar duplicate I probe set did not contain a similar class of probes Overall the duplicate II probe set was the most di cult probe set\nAnother goal of the FERET tests is to identify areas of strengths and weaknesses in the eld of face recognition We addressed this issue by computing algorithm per formance for multiple galleries and probe sets From this evaluation we concluded that algorithm performance is dependent on the gallery and probe sets We observed variation in performance due to changing the gallery and probe set within a probe category and by changing probe categories The e ect of changing the gallery while keeping the probe category constant is shown in tables and For fb probes the range for performance is to for duplicate I probes the range is to Equally important tables and shows the variability in relative performance levels For example in table UMD Sep duplicate performance varies between number three and nine Similar results were found in Moon and Phillips in their study of principal component analysis based face recognition algorithms This shows that an area of future research could measure the ef fect of changing galleries and probe sets and statistical measures that characterize these variations\nFigures and shows probe categories characterized by di culty These gures show that fb probes are the easiest and duplicate II probes are the most di cult On average duplicate I probes are easier to identify than fc probes However the best performance on fc probes is signi cantly better than the best performance on duplicate I and II probes This comparative analysis shows that future areas of research could address processing of duplicate II probes and developing methods to compensate for changes in illumination\nThe scenario being tested contributes to algorithm performance For identi cation the MIT Media Lab algorithm was clearly the best algorithm tested in September However for veri cation there was not an algorithm that was a top performer for all probe categories Also for the algorithms tested in March the USC algorithm performed overall better than the UMD algorithm for identi cation however for veri cation UMD\noverall performed better This shows that performance on one task is not predictive of performance on another task\nThe September FERET test shows that de nite progress is being made in face recognition and that the upper bound in performance has not been reached The im provement in performance documented in this paper shows directly that the FERET series of tests have made a signi cant contribution to face recognition This conclusion is indi rectly supported by the improvement in performance between the algorithms tested in September and March the number of papers that use FERET images and report experimental results using FERET images and the number of groups that participated in the Sep test"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "(Carlsbad, CA), Michigan State University (MSU) [9, 14], Rutgers University [11], University of Southern California (USC) [12], and two from University of Maryland (UMD) [1, 13, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recognition using transform coding of gray scale projection projections and the neural tree network"
            },
            "venue": {
                "fragments": [],
                "text": "Artiical Neural Networks with Applications in Speech and Vision"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 147
                            }
                        ],
                        "text": "\u2026from still images This was accomplished by evaluating algorithms ability on di erent scenarios categories of images and versions of algorithms Performance was computed for identi cation and veri cation scenarios In an identi cation application an algorithm is presented with a face that it must\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1994,
                                "start": 1991
                            }
                        ],
                        "text": "Two of the most critical requirements in support of producing reliable face recognition systems are a large database of facial images and a testing procedure to evaluate systems The Face Recognition Technology FERET program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests To date images from individuals are included in the FERET database which is divided into development and sequestered portions of the database In September the FERET program administered the third in a series of FERET face recognition tests The primary objectives of the third test were to assess the state of the art identify future areas of research and measure algorithm performance\nIntroduction\nOver the last decade face recognition has become an active area of research in com puter vision neuroscience and psychology Progress has advanced to the point that face recognition systems are being demonstrated in real world settings The rapid de velopment of face recognition is due to a combination of factors active development of\nThe work reported here is part of the Face Recognition Technology FERET program which is sponsored by the U S Department of Defense Counterdrug Technology Development Program Portions of this work was done while Jonathon Phillips was at the U S Army Research Laboratory ARL Jonathon Phillips acknowledges the support of the National Institute of Justice\nalgorithms the availability of a large database of facial images and a method for evaluat ing the performance of face recognition algorithms The FERET database and evaluation methodology address the latter two points and are de facto standards There have been three FERET evaluations with the most recent being the Sep FERET test\nThe Sep FERET test provides a comprehensive picture of the state of the art in face recognition from still images This was accomplished by evaluating algorithms ability on di erent scenarios categories of images and versions of algorithms Performance was computed for identi cation and veri cation scenarios In an identi cation application an algorithm is presented with a face that it must identify the face whereas in a veri cation application an algorithm is presented with a face and a claimed identity and the algorithm must accept or reject the claim In this paper we describe the FERET database the Sep FERET evaluation protocol and present identi cation results Veri cation results are presented in Rizvi et al\nTo obtain a robust assessment of performance algorithms are evaluated against dif ferent categories of images The categories are broken out by lighting changes people wearing glasses and the time between the acquisition date of the database image and the image presented to the algorithm By breaking out performance into these categories a better understanding of the face recognition eld in general as well as the strengths and weakness of individual algorithms is obtained This detailed analysis helps to assess which applications can be successfully addressed\nAll face recognition algorithms known to the authors consist of two parts face detection and normalization and face identi cation Algorithms that consist of both parts are referred to as fully automatic algorithms and those that consist of only the second part are partially automatic algorithms The Sep test evaluated both fully and partially automatic algorithms Partially automatic algorithms are given a facial image and the coordinates of the center of the eyes Fully automatic algorithms are only given facial images\nThe availability of the FERET database and evaluation methodology has made a signi cant di erence in the progress of development of face recognition algorithms Be fore the FERET database was created a large number of papers reported outstanding recognition results usually correct recognition on limited size databases usually individuals In fact this is still true Only a few of these algorithms reported results on images utilizing a common database let alone met the desirable goal of being evaluated on a standard testing protocol that included separate training and testing sets As a consequence there was no method to make informed comparisons among various algorithms\nThe FERET database has made it possible for researchers to develop algorithms on a common database and to report results in the literature using this database Results reported in the literature do not provide a direct comparison among algorithms because each researcher reported results using di erent assumptions scoring methods and images The independently administered FERET test allows for a direct quantitative assessment of the relative strengths and weaknesses of di erent approaches\nMore importantly the FERET database and tests clarify the current state of the art in face recognition and point out general directions for future research The FERET tests allow the computer vision community to assess overall strengths and weaknesses in the eld not only on the basis of the performance of an individual algorithm but\nin addition on the aggregate performance of all algorithms tested Through this type of assessment the community learns in an unbiased and open manner of the important technical problems to be addressed and how the community is progressing toward solving these problems\nBackground\nThe rst FERET tests took place in August and March for details of these tests and the FERET database and program see Phillips et al and Rauss et al The FERET database collection began in September along with the FERET program\nThe August test established for the rst time a performance baseline for face recognition algorithms This test was designed to measure performance on algorithms that could automatically locate normalize and identify faces from a database The test consisted of three subtests each with a di erent gallery and probe set The gallery contains the set of known individuals An image of an unknown face presented to the algorithm is called a probe and the collection of probes is called the probe set The rst subtest examined the ability of algorithms to recognize faces from a gallery of individuals The second was the false alarm test which measured how well an algorithm rejects faces not in the gallery The third baselined the e ects of pose changes on performance\nThe second FERET test that took place in March measured progress since August and evaluated algorithms on larger galleries The March evaluation consisted of a single test with a gallery of known individuals One emphasis of the test was on probe sets that contained duplicate images A duplicate is de ned as an image of a person whose corresponding gallery image was taken on a di erent date\nThe FERET database is designed to advance the state of the art in face recognition with the images collected directly supporting both algorithm development and the FERET evaluation tests The database is divided into a development set provided to researchers and a set of sequestered images for testing The images in the development set are representative of the sequestered images\nThe facial images were collected in sessions between August and July Collection sessions lasted one or two days In an e ort to maintain a degree of consistency throughout the database the same physical setup and location was used in each photog raphy session However because the equipment had to be reassembled for each session there was variation from session to session gure\nImages of an individual were acquired in sets of to images collected under rel atively unconstrained conditions Two frontal views were taken fa and fb a di erent facial expression was requested for the second frontal image For sets of images a third frontal image was taken with a di erent camera and di erent lighting this is referred to as the fc image The remaining images were collected at various aspects between right and left pro le To add simple variations to the database photographers sometimes took a second set of images for which the subjects were asked to put on their glasses and or pull their hair back Sometimes a second set of images of a person was taken on a later date such a set of images is referred to as a duplicate set Such duplicates sets result in variations in scale pose expression and illumination of the face\nBy July sets of images were in the database consisting of total images The database contains individuals and duplicate sets of images For\nfa fb duplicate I fc duplicate II\nFigure Examples of di erent categories of probes image The duplicate I image was taken within one year of the fa image and the duplicate II and fa images were taken at least one year apart\nsome people over two years elapsed between their rst and most recent sittings with some subjects being photographed multiple times gure The development portion of the database consisted of sets of images and was released to researchers The remaining images were sequestered by the Government\nTest Design\nTest Design Principles\nThe FERET Sep evaluation protocol was designed to assess the state of the art advance the state of the art and point to future directions of research To succeed at this the test design must solve the three bears problem The test cannot be neither too hard nor too easy If the test is too easy the testing process becomes an exercise in tuning existing algorithms If the test is too hard the test is beyond the ability of existing algorithmic techniques The results from the test are poor and do not allow for an accurate assessment of algorithmic capabilities\nThe solution to the three bears problem is through the selection of images in the test set and the testing protocol Tests are administered using a testing protocol that states the mechanics of the tests and the manner in which the test will be scored In face recognition the protocol states the number of images of each person in the test how the output from the algorithm is recorded and how the performance results are reported\nThe characteristics and quality of the images are major factors in determining the di culty of the problem being evaluated For example if faces are in a predetermined position in the images the problem is di erent from that for images in which the faces can be located anywhere in the image In the FERET database variability was introduced by the inclusion of images taken at di erent dates and locations see section This resulted in changes in lighting scale and background\nThe testing protocol is based on a set of design principles Stating the design principle allows one to assess how appropriate the FERET test is for a particular face recognition algorithm Also design principles assist in determining if an evaluation methodology for testing algorithm s for a particular application is appropriate Before discussing the\ndesign principles we state the evaluation protocol In the testing protocol an algorithm is given two sets of images the target set and the query set We introduce this terminology to distinguish these sets from the gallery and probe sets that are used in computing performance statistics The target set is given to the algorithm as the set of known facial images The images in the query set consists of unknown facial images to be identi ed For each image qi in the query set Q an algorithm reports a similarity si k between qi and each image tk in the target set T The testing protocol is designed so that each algorithm can use a di erent similarity measure and we do not compare similarity measures from di erent algorithms The key property of the new protocol which allows for greater exibility in scoring is that for any two images qi and tk we know si k\nThis exibility allows the evaluation methodology to be robust and comprehensive it is achieved by computing scores for virtual galleries and probe sets A gallery G is a virtual gallery if G is a subset of the target set i e G T Similarly P is a virtual probe set if P Q For a given gallery G and probe set P the performance scores are computed by examination of similarity measures si k such that qi P and tk G\nThe virtual gallery and probe set technique allows us to characterize algorithm per formance by di erent categories of images The di erent categories include rotated images duplicates taken within a week of the gallery image duplicates where the time between the images is at least one year galleries containing one image per person and galleries containing duplicate images of the same person We can create a gallery of people and estimate an algorithm s performance by recognizing people in this gallery Using this as a starting point we can then create virtual galleries of people and determine how performance changes as the size of the gallery increases An other avenue of investigation is to create n di erent galleries of size and calculate the variation in algorithm performance with the di erent galleries\nTo take full advantage of virtual galleries and probe sets we selected multiple images of the same person and placed them into the target and query sets If such images were marked as the same person the algorithms being tested could use the information in the evaluation process To prevent this from happenning we require that each image in the target set be treated as an unique face In practice this condition is enforced by giving every image in the target and query set a unique random identi cation This is the rst design principle\nThe second design principle is that training is completed prior to the start of the test This forces each algorithm to have a general representation for faces not a representation tuned to a speci c gallery Without this condition virtual galleries would not be possible\nFor algorithms to have a general representation for faces they must be gallery class insensitive Examples are algorithms based on normalized correlation or principal com ponent analysis PCA An algorithm is class sensitive if the representation is tuned to a speci c gallery Examples are straight forward implementation of Fisher discriminant analysis Fisher discriminant algorithms were adapted to class insensitive testing methodologies by Zhao et al with performance results of these extensions being reported in this paper\nThe third design rule is that all algorithms tested compute a similarity measure be tween two facial images this similarity measure was computed for all pairs of images between the target and query sets Knowing the similarity score between all pairs of\nFace Recognition Algorithm\n(run at Testee\u2019s site)\nOutput File\nScoring Code\nGovernment Site)\n(run at\nResults\nMMMM\n0001\n0002\n0002\n0001\nNNNN\nProbe Image Name\nGallery Image Name\n(One Image/Person) Gallery images\nList of Probes\nFigure Schematic of the FERET testing procedure\nimages from the target and query sets allows for the construction of virtual galleries and probe sets\nTest Details\nIn the Sep FERET test the target set contained images and the query set images All the images in the target set were frontal images The query set consisted of all the images in the target set plus rotated images and digitally modi ed images We designed the digitally modi ed images to test the e ects of illumination and scale Results from the rotated and digitally modi ed images are not reported here For each query image qi an algorithm outputs the similarity measure si k for all images tk in the target set For a given query image qi the target images tk are sorted by the similarity scores si Since the target set is a subset of the query set the test output contains the similarity score between all images in the target set\nThere were two versions of the Sep test The target and query sets were the same for each version The rst version tested partially automatic algorithms by providing them with a list of images in the target and query sets and the coordinates of the center of the eyes for images in the target and query sets In the second version of the test the coordinates of the eyes were not provided By comparing the performance between the two versions we estimate performance of the face locating portion of a fully automatic algorithm at the system level\nThe test was administered at each group s site under the supervision of one of the au thors Each group had three days to complete the test on less than UNIX workstations this limit was not reached We did not record the time or number of workstations because execution times can vary according to the type of machines used machine and network con guration and the amount of time that the developers spent optimizing their code we wanted to encourage algorithm development not code optimization We imposed the\ntime limit to encourage the development of algorithms that could be incorporated into operational eldable systems\nThe images contained in the gallery and probe sets consisted of images from both the developmental and sequestered portions of the FERET database Only images from the FERET database were included in the test however algorithm developers were not prohibited from using images outside the FERET database to develop or tune parameters in their algorithms\nThe FERET test is designed to measure laboratory performance The test is not concerned with speed of the implementation real time implementation issues and speed and accuracy trade o s These issues and others need to be addressed in an operational elded system were beyond the scope of the Sep FERET test\nFigure presents a schematic of the testing procedure To ensure that matching was not done by le name we gave the images random names The nominal pose of each face was provided to the testee\nDecision Theory and Performance Evaluation\nThe basic models for evaluating the performance of an algorithm are the closed and open universes In the closed universe every probe is in the gallery In an open universe some probes are not in the gallery Both models re ect di erent and important aspects of face recognition algorithms and report di erent performance statistics The open universe models veri cation applications The FERET scoring procedures for veri cation is given in Rizvi et al\nThe closed universe model allows one to ask how good an algorithm is at identifying a probe image the question is not always is the top match correct but is the correct answer in the top n matches This lets one know how many images have to be examined to get a desired level of performance The performance statistics are reported as cumula tive match scores The rank is plotted along the horizontal axis and the vertical axis is the percentage of correct matches The cumulative match score can be calculated for any subset of the probe set We calculated this score to evaluate an algorithm s performance on di erent categories of probes i e rotated or scaled probes\nThe computation of an identi cation score is quite simple Let P be a probe set and jPj the size of P We score probe set P against gallery G where G fg gMg and P fp pNg by comparing the similarity scores si such that pi P and gk G For each probe image pi P we sort si for all gallery images gk G We assume that a smaller similarity score implies a closer match If gk and pi are the same image then si k The function id i gives the index of the gallery image of the person in probe pi i e pi is an image of the person in gid i A probe pi is correctly identi ed if si id i is the smallest scores for gk G A probe pi is in the top k if si id i is one of the k th smallest score si for gallery G Let Rk denote the number of probes in the top k We reported Rk jPj the fraction of probes in the top k As an example let k R and jPj Based on the formula the performance score for R is\nIn reporting identi cation performance results we state the size of the gallery and the number of probes scored The size of the gallery is the number of di erent faces people contained in the images that are in the gallery For all results that we report there is one image per person in the gallery thus the size of the gallery is also the number of images\nin the gallery The number of probes scored also size of the probe set is jPj The probe set may contain more than one image of a person and the probe set may not contain an image of everyone in the gallery Every image in the probe set has a corresponding image in the gallery\nLatest Test Results\nThe Sep FERET test was designed to measure algorithm performance for identi cation and veri cation tasks Both tasks are evaluated on the same sets of images We report the results for algorithms that includes partially automatic algorithms and fully automatic algorithms The test was administered in September and March see table for details of when the test was administered to which groups and which version of the test was taken Two of these algorithms were developed at the MIT Media Laboratory The rst was the same algorithm that was tested in March This algorithm was retested so that improvement since March could be measured The second algorithm was based on more recent work Algorithms were also tested from Excalibur Corp Carlsbad CA Michigan State University MSU Rutgers University University of Southern California USC and two from University of Maryland UMD The rst algorithm from UMD was tested in September and a second version of the algorithm was tested in March For the fully automatic version of test algorithms from MIT and USC were evaluated\nThe nal two algorithms were our implementation of normalized correlation and a principal components analysis PCA based algorithm These algorithms provide a performance baseline In our implementation of the PCA based algorithm all images were translated rotated and scaled so that the center of the eyes were placed on speci c pixels faces were masked to remove background and hair and the non masked facial pixels were processed by a histogram equalization algorithm The training set consisted of faces Faces were represented by their projection onto the rst eigenvectors and were identi ed by a nearest neighbor classi er using the L metric For normalized correlation the images were translated rotated and scaled so that the center of the eyes were placed on speci c pixels and faces were masked to remove background and hair\nPartially automatic algorithms\nWe report identi cation scores for four categories of probes The rst probe category was the FB probes g For each set of images there were two frontal images One of the images was randomly placed in the gallery and the other image was placed in the FB probe set This category is denoted by FB to di erentiate it from the fb images in the FERET database The second probe category contained all duplicate frontal images in the FERET database for the gallery images We refer to this category as the duplicate I probes The third category was the fc images taken the same day but with a di erent camera and lighting The fourth consisted of duplicates where there is at least one year between the acquisition of the probe image and corresponding gallery image We refer to this category as the duplicate II probes For this category the gallery images were acquired before January and the probe images were acquired after January\nTable List of groups that took the Sept test broken out by versions taken and dates administered The by MIT indicates that two algorithms were tested\nTest Date\nSeptember March\nVersion of test Group Baseline\nFully Automatic MIT Media Lab U of So California USC\nEye Coordinates Given Baseline PCA Baseline Correlation Excalibur Corp MIT Media Lab\nMichigan State U Rutgers U\nU Maryland USC\nThe gallery for the FB duplicate I and fc probes was the same and consisted of frontal images with one image person in the gallery thus the gallery contained individuals Also none of the faces in the gallery images wore glasses The gallery for duplicate II probes was a subset of images from the gallery for the other categories\nThe results for identi cation are reported as cumulative match scores Table shows the categories corresponding to the gures presenting the results type of results and size of the gallery and probe sets gs to\nIn gures and we compare the di culty of di erent probe sets Whereas gure reports identi cation performance for each algorithm gure shows a single curve that is an average of the identi cation performance of all algorithms for each probe category For example the rst ranked score for duplicate I probe sets is computed from an average of the rst ranked score for all algorithms in gure In gure we presented current upper bound for performance on partially automatic algorithms for each probe category For each category of probe gure plots the algorithm with the highest top rank score R Figures and reports performance of four categories of probes FB duplicate I fc duplicate II\nTable Figures reporting results for partially automatic algorithms Performance is broken out by probe category\nFigure no Probe Category Gallery size Probe set size\nFB\nduplicate I\nfc\nduplicate II\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 MSU UMD 96 MIT 95\nBaseline Cor Baseline EF\nExcalibur Rutgers\na\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUMD 97 USC\nUMD 96 Baseline Cor Baseline EF\nb\nFigure Identi cation performance against FB probes a Partially automatic algo rithms tested in September b Partially automatic algorithms tested in March\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 Excalibur Baseline EF Baseline Cor\nMIT 95 MSU\nUMD 96 Rutgers\na\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC UMD 97\nBaseline EF Baseline Cor\nUMD 96\nb\nFigure Identi cation performance against all duplicate I probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 UMD 96\nMSU Excalibur Baseline EF Rutgers MIT 95 Baseline Cor\na\n0\n0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC UMD 97 UMD 96\nBaseline EF Baseline Cor\nb\nFigure Identi cation performance against fc probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 Baseline EF\nExcalibur Rutgers MIT 95 Baseline Cor UMD 96\nMSU\na\n0\n0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC Baseline EF\nUMD 97 Baseline Cor\nUMD 96\nb\nFigure Identi cation performance against duplicate II probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nFB probes Duplicate I probes\nfc probes Duplicate II probes\nFigure Average identi cation performance of partially automatic algorithms on each probe category\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nFB probes fc probes\nDuplicate I probes Duplicate II probes\nFigure Current upper bound identi cation performance of partially automatic algo rithm for each probe category\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\n0.92\n0.94\n0.96\n0.98\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nUSC partially automatic MIT partially automatic\nUSC fully automatic MIT fully automatic\nFigure Identi cation performance of fully automatic algorithms against partially auto matic algorithms for FB probes\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nUSC partially automatic USC fully automatic MIT partially automatic MIT fully automatic\nFigure Identi cation performance of fully automatic algorithms against partially au tomatic algorithms for duplicate I probes\nFully Automatic Performance\nIn this subsection we report performance for the fully automatic algorithms of the MIT Media Lab and USC To allow for a comparison between the partially and fully automatic algorithms we plot the results for the partially and fully automatic algorithms Figure shows performance for FB probes and gure shows performance for duplicate I probes The gallery and probe sets are the same as in subsection\nVariation in Performance\nFrom a statistical point of view a face recognition algorithm estimates the identity of a face Consistent with this view we can ask about the variance in performance of an algorithm For a given category of images how does performance change if the algorithm is given a di erent gallery and probe set In tables and we show how algorithm performance varies if the people in the galleries change For this experiment we constructed six galleries of approximately individuals in which an individual was in only one gallery the number of people contained within each gallery versus the number of probes scored is given in tables and Results are reported for the partially automatic algorithms For the results in this section we order algorithms by their top rank score on each gallery for example in table the UMD Mar algorithm scored highest on gallery and the baseline PCA and correlation tied for th place Also included in this table is average performance for all algorithms Table reports results for FB probes Table is organized in the same manner as table except that duplicate I probes are scored Tables and report results for the same gallery The galleries were constructed by placing images within the galleries by chronological order in which the images were collected the rst gallery contains the rst images collected and the th gallery contains the most recent images collected In table mean age refers to the average time between collection of images contained in the gallery and the corresponding duplicate probes No scores are reported in table for gallery because there are no duplicates for this gallery\nDiscussion and Conclusion\nIn this paper we presented the Sep FERET evaluation protocol for face recognition algorithms The protocol makes it possible to independently evaluate algorithms The protocol was designed to evaluate algorithms on di erent galleries and probe sets for dif ferent scenarios Using this protocol we computed performance on identi cation and veri cation tasks The veri cation results are presented in Rizvi et al and all veri cation results mentioned in this section are from that paper In this paper we presented detailed identi cation results Because of the Sep FERET evaluation protocol s ability to test algorithms performance on di erent tasks for multiple galleries and probe sets it is the de facto standard for measuring performance of face recognition algorithms These results show that factors e ecting performance include scenario date tested and probe category\nThe Sep test was the latest FERET test the others were the Aug and Mar tests One of the main goals of the FERET tests has been to improve the performance of face recognition algorithms and is seen in the Sep FERET test The rst case is the improvement in performance of the MIT Media Lab September algorithm over\nthe March algorithm the second is the improvement of the UMD algorithm between September and March\nBy looking at progress over the series of FERET tests one sees that substantial progress has been made in face recognition The most direct method is to compare the performance of fully automatic algorithms on fb probes the two earlier FERET tests only evaluated fully automatic algorithms The best top rank score for fb probes on the Aug test was on a gallery of individuals and for Mar the top score was on a gallery of individuals This compares to in September and in March gallery of individuals This method shows that over the course of the FERET tests the absolute scores increased as the size of the database increased The March score was from one of the MIT Media Lab algorithms and represents an increase from in March\nOn duplicate I probes MIT Media Lab improved from March to September USC s performance remained approximately the same at be tween March and March This improvement in performance was achieved while the gallery size increased and the number of duplicate I probes increased from to While increasing the number of probes does not necessarily increase the di culty of iden ti cation tasks we argue that the Sep duplicate I probe set was more di cult to process then the Mar set The Sep duplicate I probe set contained the duplicate II probes and the Mar duplicate I probe set did not contain a similar class of probes Overall the duplicate II probe set was the most di cult probe set\nAnother goal of the FERET tests is to identify areas of strengths and weaknesses in the eld of face recognition We addressed this issue by computing algorithm per formance for multiple galleries and probe sets From this evaluation we concluded that algorithm performance is dependent on the gallery and probe sets We observed variation in performance due to changing the gallery and probe set within a probe category and by changing probe categories The e ect of changing the gallery while keeping the probe category constant is shown in tables and For fb probes the range for performance is to for duplicate I probes the range is to Equally important tables and shows the variability in relative performance levels For example in table UMD Sep duplicate performance varies between number three and nine Similar results were found in Moon and Phillips in their study of principal component analysis based face recognition algorithms This shows that an area of future research could measure the ef fect of changing galleries and probe sets and statistical measures that characterize these variations\nFigures and shows probe categories characterized by di culty These gures show that fb probes are the easiest and duplicate II probes are the most di cult On average duplicate I probes are easier to identify than fc probes However the best performance on fc probes is signi cantly better than the best performance on duplicate I and II probes This comparative analysis shows that future areas of research could address processing of duplicate II probes and developing methods to compensate for changes in illumination\nThe scenario being tested contributes to algorithm performance For identi cation the MIT Media Lab algorithm was clearly the best algorithm tested in September However for veri cation there was not an algorithm that was a top performer for all probe categories Also for the algorithms tested in March the USC algorithm performed overall better than the UMD algorithm for identi cation however for veri cation UMD\noverall performed better This shows that performance on one task is not predictive of performance on another task\nThe September FERET test shows that de nite progress is being made in face recognition and that the upper bound in performance has not been reached The im provement in performance documented in this paper shows directly that the FERET series of tests have made a signi cant contribution to face recognition This conclusion is indi rectly supported by the improvement in performance between the algorithms tested in September and March the number of papers that use FERET images and report experimental results using FERET images and the number of groups that participated in the Sep test"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaBest Practice Recommendation for the Capture of Mugshots Version,\u00ba 1997"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaBest Practice Recommendation for the Capture of Mugshots Version,\u00ba 1997"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5468,
                                "start": 5454
                            }
                        ],
                        "text": "Two of the most critical requirements in support of producing reliable face recognition systems are a large database of facial images and a testing procedure to evaluate systems The Face Recognition Technology FERET program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests To date images from individuals are included in the FERET database which is divided into development and sequestered portions of the database In September the FERET program administered the third in a series of FERET face recognition tests The primary objectives of the third test were to assess the state of the art identify future areas of research and measure algorithm performance\nIntroduction\nOver the last decade face recognition has become an active area of research in com puter vision neuroscience and psychology Progress has advanced to the point that face recognition systems are being demonstrated in real world settings The rapid de velopment of face recognition is due to a combination of factors active development of\nThe work reported here is part of the Face Recognition Technology FERET program which is sponsored by the U S Department of Defense Counterdrug Technology Development Program Portions of this work was done while Jonathon Phillips was at the U S Army Research Laboratory ARL Jonathon Phillips acknowledges the support of the National Institute of Justice\nalgorithms the availability of a large database of facial images and a method for evaluat ing the performance of face recognition algorithms The FERET database and evaluation methodology address the latter two points and are de facto standards There have been three FERET evaluations with the most recent being the Sep FERET test\nThe Sep FERET test provides a comprehensive picture of the state of the art in face recognition from still images This was accomplished by evaluating algorithms ability on di erent scenarios categories of images and versions of algorithms Performance was computed for identi cation and veri cation scenarios In an identi cation application an algorithm is presented with a face that it must identify the face whereas in a veri cation application an algorithm is presented with a face and a claimed identity and the algorithm must accept or reject the claim In this paper we describe the FERET database the Sep FERET evaluation protocol and present identi cation results Veri cation results are presented in Rizvi et al\nTo obtain a robust assessment of performance algorithms are evaluated against dif ferent categories of images The categories are broken out by lighting changes people wearing glasses and the time between the acquisition date of the database image and the image presented to the algorithm By breaking out performance into these categories a better understanding of the face recognition eld in general as well as the strengths and weakness of individual algorithms is obtained This detailed analysis helps to assess which applications can be successfully addressed\nAll face recognition algorithms known to the authors consist of two parts face detection and normalization and face identi cation Algorithms that consist of both parts are referred to as fully automatic algorithms and those that consist of only the second part are partially automatic algorithms The Sep test evaluated both fully and partially automatic algorithms Partially automatic algorithms are given a facial image and the coordinates of the center of the eyes Fully automatic algorithms are only given facial images\nThe availability of the FERET database and evaluation methodology has made a signi cant di erence in the progress of development of face recognition algorithms Be fore the FERET database was created a large number of papers reported outstanding recognition results usually correct recognition on limited size databases usually individuals In fact this is still true Only a few of these algorithms reported results on images utilizing a common database let alone met the desirable goal of being evaluated on a standard testing protocol that included separate training and testing sets As a consequence there was no method to make informed comparisons among various algorithms\nThe FERET database has made it possible for researchers to develop algorithms on a common database and to report results in the literature using this database Results reported in the literature do not provide a direct comparison among algorithms because each researcher reported results using di erent assumptions scoring methods and images The independently administered FERET test allows for a direct quantitative assessment of the relative strengths and weaknesses of di erent approaches\nMore importantly the FERET database and tests clarify the current state of the art in face recognition and point out general directions for future research The FERET tests allow the computer vision community to assess overall strengths and weaknesses in the eld not only on the basis of the performance of an individual algorithm but\nin addition on the aggregate performance of all algorithms tested Through this type of assessment the community learns in an unbiased and open manner of the important technical problems to be addressed and how the community is progressing toward solving these problems\nBackground\nThe rst FERET tests took place in August and March for details of these tests and the FERET database and program see Phillips et al and Rauss et al The FERET database collection began in September along with the FERET program\nThe August test established for the rst time a performance baseline for face recognition algorithms This test was designed to measure performance on algorithms that could automatically locate normalize and identify faces from a database The test consisted of three subtests each with a di erent gallery and probe set The gallery contains the set of known individuals An image of an unknown face presented to the algorithm is called a probe and the collection of probes is called the probe set The rst subtest examined the ability of algorithms to recognize faces from a gallery of individuals The second was the false alarm test which measured how well an algorithm rejects faces not in the gallery The third baselined the e ects of pose changes on performance\nThe second FERET test that took place in March measured progress since August and evaluated algorithms on larger galleries The March evaluation consisted of a single test with a gallery of known individuals One emphasis of the test was on probe sets that contained duplicate images A duplicate is de ned as an image of a person whose corresponding gallery image was taken on a di erent date\nThe FERET database is designed to advance the state of the art in face recognition with the images collected directly supporting both algorithm development and the FERET evaluation tests The database is divided into a development set provided to researchers and a set of sequestered images for testing The images in the development set are representative of the sequestered images\nThe facial images were collected in sessions between August and July Collection sessions lasted one or two days In an e ort to maintain a degree of consistency throughout the database the same physical setup and location was used in each photog raphy session However because the equipment had to be reassembled for each session there was variation from session to session gure\nImages of an individual were acquired in sets of to images collected under rel atively unconstrained conditions Two frontal views were taken fa and fb a di erent facial expression was requested for the second frontal image For sets of images a third frontal image was taken with a di erent camera and di erent lighting this is referred to as the fc image The remaining images were collected at various aspects between right and left pro le To add simple variations to the database photographers sometimes took a second set of images for which the subjects were asked to put on their glasses and or pull their hair back Sometimes a second set of images of a person was taken on a later date such a set of images is referred to as a duplicate set Such duplicates sets result in variations in scale pose expression and illumination of the face\nBy July sets of images were in the database consisting of total images The database contains individuals and duplicate sets of images For\nfa fb duplicate I fc duplicate II\nFigure Examples of di erent categories of probes image The duplicate I image was taken within one year of the fa image and the duplicate II and fa images were taken at least one year apart\nsome people over two years elapsed between their rst and most recent sittings with some subjects being photographed multiple times gure The development portion of the database consisted of sets of images and was released to researchers The remaining images were sequestered by the Government\nTest Design\nTest Design Principles\nThe FERET Sep evaluation protocol was designed to assess the state of the art advance the state of the art and point to future directions of research To succeed at this the test design must solve the three bears problem The test cannot be neither too hard nor too easy If the test is too easy the testing process becomes an exercise in tuning existing algorithms If the test is too hard the test is beyond the ability of existing algorithmic techniques The results from the test are poor and do not allow for an accurate assessment of algorithmic capabilities\nThe solution to the three bears problem is through the selection of images in the test set and the testing protocol Tests are administered using a testing protocol that states the mechanics of the tests and the manner in which the test will be scored In face recognition the protocol states the number of images of each person in the test how the output from the algorithm is recorded and how the performance results are reported\nThe characteristics and quality of the images are major factors in determining the di culty of the problem being evaluated For example if faces are in a predetermined position in the images the problem is di erent from that for images in which the faces can be located anywhere in the image In the FERET database variability was introduced by the inclusion of images taken at di erent dates and locations see section This resulted in changes in lighting scale and background\nThe testing protocol is based on a set of design principles Stating the design principle allows one to assess how appropriate the FERET test is for a particular face recognition algorithm Also design principles assist in determining if an evaluation methodology for testing algorithm s for a particular application is appropriate Before discussing the\ndesign principles we state the evaluation protocol In the testing protocol an algorithm is given two sets of images the target set and the query set We introduce this terminology to distinguish these sets from the gallery and probe sets that are used in computing performance statistics The target set is given to the algorithm as the set of known facial images The images in the query set consists of unknown facial images to be identi ed For each image qi in the query set Q an algorithm reports a similarity si k between qi and each image tk in the target set T The testing protocol is designed so that each algorithm can use a di erent similarity measure and we do not compare similarity measures from di erent algorithms The key property of the new protocol which allows for greater exibility in scoring is that for any two images qi and tk we know si k\nThis exibility allows the evaluation methodology to be robust and comprehensive it is achieved by computing scores for virtual galleries and probe sets A gallery G is a virtual gallery if G is a subset of the target set i e G T Similarly P is a virtual probe set if P Q For a given gallery G and probe set P the performance scores are computed by examination of similarity measures si k such that qi P and tk G\nThe virtual gallery and probe set technique allows us to characterize algorithm per formance by di erent categories of images The di erent categories include rotated images duplicates taken within a week of the gallery image duplicates where the time between the images is at least one year galleries containing one image per person and galleries containing duplicate images of the same person We can create a gallery of people and estimate an algorithm s performance by recognizing people in this gallery Using this as a starting point we can then create virtual galleries of people and determine how performance changes as the size of the gallery increases An other avenue of investigation is to create n di erent galleries of size and calculate the variation in algorithm performance with the di erent galleries\nTo take full advantage of virtual galleries and probe sets we selected multiple images of the same person and placed them into the target and query sets If such images were marked as the same person the algorithms being tested could use the information in the evaluation process To prevent this from happenning we require that each image in the target set be treated as an unique face In practice this condition is enforced by giving every image in the target and query set a unique random identi cation This is the rst design principle\nThe second design principle is that training is completed prior to the start of the test This forces each algorithm to have a general representation for faces not a representation tuned to a speci c gallery Without this condition virtual galleries would not be possible\nFor algorithms to have a general representation for faces they must be gallery class insensitive Examples are algorithms based on normalized correlation or principal com ponent analysis PCA An algorithm is class sensitive if the representation is tuned to a speci c gallery Examples are straight forward implementation of Fisher discriminant analysis Fisher discriminant algorithms were adapted to class insensitive testing methodologies by Zhao et al with performance results of these extensions being reported in this paper\nThe third design rule is that all algorithms tested compute a similarity measure be tween two facial images this similarity measure was computed for all pairs of images between the target and query sets Knowing the similarity score between all pairs of\nFace Recognition Algorithm\n(run at Testee\u2019s site)\nOutput File\nScoring Code\nGovernment Site)\n(run at\nResults\nMMMM\n0001\n0002\n0002\n0001\nNNNN\nProbe Image Name\nGallery Image Name\n(One Image/Person) Gallery images\nList of Probes\nFigure Schematic of the FERET testing procedure\nimages from the target and query sets allows for the construction of virtual galleries and probe sets\nTest Details\nIn the Sep FERET test the target set contained images and the query set images All the images in the target set were frontal images The query set consisted of all the images in the target set plus rotated images and digitally modi ed images We designed the digitally modi ed images to test the e ects of illumination and scale Results from the rotated and digitally modi ed images are not reported here For each query image qi an algorithm outputs the similarity measure si k for all images tk in the target set For a given query image qi the target images tk are sorted by the similarity scores si Since the target set is a subset of the query set the test output contains the similarity score between all images in the target set\nThere were two versions of the Sep test The target and query sets were the same for each version The rst version tested partially automatic algorithms by providing them with a list of images in the target and query sets and the coordinates of the center of the eyes for images in the target and query sets In the second version of the test the coordinates of the eyes were not provided By comparing the performance between the two versions we estimate performance of the face locating portion of a fully automatic algorithm at the system level\nThe test was administered at each group s site under the supervision of one of the au thors Each group had three days to complete the test on less than UNIX workstations this limit was not reached We did not record the time or number of workstations because execution times can vary according to the type of machines used machine and network con guration and the amount of time that the developers spent optimizing their code we wanted to encourage algorithm development not code optimization We imposed the\ntime limit to encourage the development of algorithms that could be incorporated into operational eldable systems\nThe images contained in the gallery and probe sets consisted of images from both the developmental and sequestered portions of the FERET database Only images from the FERET database were included in the test however algorithm developers were not prohibited from using images outside the FERET database to develop or tune parameters in their algorithms\nThe FERET test is designed to measure laboratory performance The test is not concerned with speed of the implementation real time implementation issues and speed and accuracy trade o s These issues and others need to be addressed in an operational elded system were beyond the scope of the Sep FERET test\nFigure presents a schematic of the testing procedure To ensure that matching was not done by le name we gave the images random names The nominal pose of each face was provided to the testee\nDecision Theory and Performance Evaluation\nThe basic models for evaluating the performance of an algorithm are the closed and open universes In the closed universe every probe is in the gallery In an open universe some probes are not in the gallery Both models re ect di erent and important aspects of face recognition algorithms and report di erent performance statistics The open universe models veri cation applications The FERET scoring procedures for veri cation is given in Rizvi et al\nThe closed universe model allows one to ask how good an algorithm is at identifying a probe image the question is not always is the top match correct but is the correct answer in the top n matches This lets one know how many images have to be examined to get a desired level of performance The performance statistics are reported as cumula tive match scores The rank is plotted along the horizontal axis and the vertical axis is the percentage of correct matches The cumulative match score can be calculated for any subset of the probe set We calculated this score to evaluate an algorithm s performance on di erent categories of probes i e rotated or scaled probes\nThe computation of an identi cation score is quite simple Let P be a probe set and jPj the size of P We score probe set P against gallery G where G fg gMg and P fp pNg by comparing the similarity scores si such that pi P and gk G For each probe image pi P we sort si for all gallery images gk G We assume that a smaller similarity score implies a closer match If gk and pi are the same image then si k The function id i gives the index of the gallery image of the person in probe pi i e pi is an image of the person in gid i A probe pi is correctly identi ed if si id i is the smallest scores for gk G A probe pi is in the top k if si id i is one of the k th smallest score si for gallery G Let Rk denote the number of probes in the top k We reported Rk jPj the fraction of probes in the top k As an example let k R and jPj Based on the formula the performance score for R is\nIn reporting identi cation performance results we state the size of the gallery and the number of probes scored The size of the gallery is the number of di erent faces people contained in the images that are in the gallery For all results that we report there is one image per person in the gallery thus the size of the gallery is also the number of images\nin the gallery The number of probes scored also size of the probe set is jPj The probe set may contain more than one image of a person and the probe set may not contain an image of everyone in the gallery Every image in the probe set has a corresponding image in the gallery\nLatest Test Results\nThe Sep FERET test was designed to measure algorithm performance for identi cation and veri cation tasks Both tasks are evaluated on the same sets of images We report the results for algorithms that includes partially automatic algorithms and fully automatic algorithms The test was administered in September and March see table for details of when the test was administered to which groups and which version of the test was taken Two of these algorithms were developed at the MIT Media Laboratory The rst was the same algorithm that was tested in March This algorithm was retested so that improvement since March could be measured The second algorithm was based on more recent work Algorithms were also tested from Excalibur Corp Carlsbad CA Michigan State University MSU Rutgers University University of Southern California USC and two from University of Maryland UMD The rst algorithm from UMD was tested in September and a second version of the algorithm was tested in March For the fully automatic version of test algorithms from MIT and USC were evaluated\nThe nal two algorithms were our implementation of normalized correlation and a principal components analysis PCA based algorithm These algorithms provide a performance baseline In our implementation of the PCA based algorithm all images were translated rotated and scaled so that the center of the eyes were placed on speci c pixels faces were masked to remove background and hair and the non masked facial pixels were processed by a histogram equalization algorithm The training set consisted of faces Faces were represented by their projection onto the rst eigenvectors and were identi ed by a nearest neighbor classi er using the L metric For normalized correlation the images were translated rotated and scaled so that the center of the eyes were placed on speci c pixels and faces were masked to remove background and hair\nPartially automatic algorithms\nWe report identi cation scores for four categories of probes The rst probe category was the FB probes g For each set of images there were two frontal images One of the images was randomly placed in the gallery and the other image was placed in the FB probe set This category is denoted by FB to di erentiate it from the fb images in the FERET database The second probe category contained all duplicate frontal images in the FERET database for the gallery images We refer to this category as the duplicate I probes The third category was the fc images taken the same day but with a di erent camera and lighting The fourth consisted of duplicates where there is at least one year between the acquisition of the probe image and corresponding gallery image We refer to this category as the duplicate II probes For this category the gallery images were acquired before January and the probe images were acquired after January\nTable List of groups that took the Sept test broken out by versions taken and dates administered The by MIT indicates that two algorithms were tested\nTest Date\nSeptember March\nVersion of test Group Baseline\nFully Automatic MIT Media Lab U of So California USC\nEye Coordinates Given Baseline PCA Baseline Correlation Excalibur Corp MIT Media Lab\nMichigan State U Rutgers U\nU Maryland USC\nThe gallery for the FB duplicate I and fc probes was the same and consisted of frontal images with one image person in the gallery thus the gallery contained individuals Also none of the faces in the gallery images wore glasses The gallery for duplicate II probes was a subset of images from the gallery for the other categories\nThe results for identi cation are reported as cumulative match scores Table shows the categories corresponding to the gures presenting the results type of results and size of the gallery and probe sets gs to\nIn gures and we compare the di culty of di erent probe sets Whereas gure reports identi cation performance for each algorithm gure shows a single curve that is an average of the identi cation performance of all algorithms for each probe category For example the rst ranked score for duplicate I probe sets is computed from an average of the rst ranked score for all algorithms in gure In gure we presented current upper bound for performance on partially automatic algorithms for each probe category For each category of probe gure plots the algorithm with the highest top rank score R Figures and reports performance of four categories of probes FB duplicate I fc duplicate II\nTable Figures reporting results for partially automatic algorithms Performance is broken out by probe category\nFigure no Probe Category Gallery size Probe set size\nFB\nduplicate I\nfc\nduplicate II\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 MSU UMD 96 MIT 95\nBaseline Cor Baseline EF\nExcalibur Rutgers\na\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUMD 97 USC\nUMD 96 Baseline Cor Baseline EF\nb\nFigure Identi cation performance against FB probes a Partially automatic algo rithms tested in September b Partially automatic algorithms tested in March\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 Excalibur Baseline EF Baseline Cor\nMIT 95 MSU\nUMD 96 Rutgers\na\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC UMD 97\nBaseline EF Baseline Cor\nUMD 96\nb\nFigure Identi cation performance against all duplicate I probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 UMD 96\nMSU Excalibur Baseline EF Rutgers MIT 95 Baseline Cor\na\n0\n0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC UMD 97 UMD 96\nBaseline EF Baseline Cor\nb\nFigure Identi cation performance against fc probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 Baseline EF\nExcalibur Rutgers MIT 95 Baseline Cor UMD 96\nMSU\na\n0\n0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC Baseline EF\nUMD 97 Baseline Cor\nUMD 96\nb\nFigure Identi cation performance against duplicate II probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nFB probes Duplicate I probes\nfc probes Duplicate II probes\nFigure Average identi cation performance of partially automatic algorithms on each probe category\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nFB probes fc probes\nDuplicate I probes Duplicate II probes\nFigure Current upper bound identi cation performance of partially automatic algo rithm for each probe category\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\n0.92\n0.94\n0.96\n0.98\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nUSC partially automatic MIT partially automatic\nUSC fully automatic MIT fully automatic\nFigure Identi cation performance of fully automatic algorithms against partially auto matic algorithms for FB probes\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nUSC partially automatic USC fully automatic MIT partially automatic MIT fully automatic\nFigure Identi cation performance of fully automatic algorithms against partially au tomatic algorithms for duplicate I probes\nFully Automatic Performance\nIn this subsection we report performance for the fully automatic algorithms of the MIT Media Lab and USC To allow for a comparison between the partially and fully automatic algorithms we plot the results for the partially and fully automatic algorithms Figure shows performance for FB probes and gure shows performance for duplicate I probes The gallery and probe sets are the same as in subsection\nVariation in Performance\nFrom a statistical point of view a face recognition algorithm estimates the identity of a face Consistent with this view we can ask about the variance in performance of an algorithm For a given category of images how does performance change if the algorithm is given a di erent gallery and probe set In tables and we show how algorithm performance varies if the people in the galleries change For this experiment we constructed six galleries of approximately individuals in which an individual was in only one gallery the number of people contained within each gallery versus the number of probes scored is given in tables and Results are reported for the partially automatic algorithms For the results in this section we order algorithms by their top rank score on each gallery for example in table the UMD Mar algorithm scored highest on gallery and the baseline PCA and correlation tied for th place Also included in this table is average performance for all algorithms Table reports results for FB probes Table is organized in the same manner as table except that duplicate I probes are scored Tables and report results for the same gallery The galleries were constructed by placing images within the galleries by chronological order in which the images were collected the rst gallery contains the rst images collected and the th gallery contains the most recent images collected In table mean age refers to the average time between collection of images contained in the gallery and the corresponding duplicate probes No scores are reported in table for gallery because there are no duplicates for this gallery\nDiscussion and Conclusion\nIn this paper we presented the Sep FERET evaluation protocol for face recognition algorithms The protocol makes it possible to independently evaluate algorithms The protocol was designed to evaluate algorithms on di erent galleries and probe sets for dif ferent scenarios Using this protocol we computed performance on identi cation and veri cation tasks The veri cation results are presented in Rizvi et al and all veri cation results mentioned in this section are from that paper In this paper we presented detailed identi cation results Because of the Sep FERET evaluation protocol s ability to test algorithms performance on di erent tasks for multiple galleries and probe sets it is the de facto standard for measuring performance of face recognition algorithms These results show that factors e ecting performance include scenario date tested and probe category\nThe Sep test was the latest FERET test the others were the Aug and Mar tests One of the main goals of the FERET tests has been to improve the performance of face recognition algorithms and is seen in the Sep FERET test The rst case is the improvement in performance of the MIT Media Lab September algorithm over\nthe March algorithm the second is the improvement of the UMD algorithm between September and March\nBy looking at progress over the series of FERET tests one sees that substantial progress has been made in face recognition The most direct method is to compare the performance of fully automatic algorithms on fb probes the two earlier FERET tests only evaluated fully automatic algorithms The best top rank score for fb probes on the Aug test was on a gallery of individuals and for Mar the top score was on a gallery of individuals This compares to in September and in March gallery of individuals This method shows that over the course of the FERET tests the absolute scores increased as the size of the database increased The March score was from one of the MIT Media Lab algorithms and represents an increase from in March\nOn duplicate I probes MIT Media Lab improved from March to September USC s performance remained approximately the same at be tween March and March This improvement in performance was achieved while the gallery size increased and the number of duplicate I probes increased from to While increasing the number of probes does not necessarily increase the di culty of iden ti cation tasks we argue that the Sep duplicate I probe set was more di cult to process then the Mar set The Sep duplicate I probe set contained the duplicate II probes and the Mar duplicate I probe set did not contain a similar class of probes Overall the duplicate II probe set was the most di cult probe set\nAnother goal of the FERET tests is to identify areas of strengths and weaknesses in the eld of face recognition We addressed this issue by computing algorithm per formance for multiple galleries and probe sets From this evaluation we concluded that algorithm performance is dependent on the gallery and probe sets We observed variation in performance due to changing the gallery and probe set within a probe category and by changing probe categories The e ect of changing the gallery while keeping the probe category constant is shown in tables and For fb probes the range for performance is to for duplicate I probes the range is to Equally important tables and shows the variability in relative performance levels For example in table UMD Sep duplicate performance varies between number three and nine Similar results were found in Moon and Phillips in their study of principal component analysis based face recognition algorithms This shows that an area of future research could measure the ef fect of changing galleries and probe sets and statistical measures that characterize these variations\nFigures and shows probe categories characterized by di culty These gures show that fb probes are the easiest and duplicate II probes are the most di cult On average duplicate I probes are easier to identify than fc probes However the best performance on fc probes is signi cantly better than the best performance on duplicate I and II probes This comparative analysis shows that future areas of research could address processing of duplicate II probes and developing methods to compensate for changes in illumination\nThe scenario being tested contributes to algorithm performance For identi cation the MIT Media Lab algorithm was clearly the best algorithm tested in September However for veri cation there was not an algorithm that was a top performer for all probe categories Also for the algorithms tested in March the USC algorithm performed overall better than the UMD algorithm for identi cation however for veri cation UMD\noverall performed better This shows that performance on one task is not predictive of performance on another task\nThe September FERET test shows that de nite progress is being made in face recognition and that the upper bound in performance has not been reached The im provement in performance documented in this paper shows directly that the FERET series of tests have made a signi cant contribution to face recognition This conclusion is indi rectly supported by the improvement in performance between the algorithms tested in September and March the number of papers that use FERET images and report experimental results using FERET images and the number of groups that participated in the Sep test"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "Progress has advanced to the point that face-recognition systems are being demonstrated in real-world settings [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 144
                            }
                        ],
                        "text": "The rst FERET tests took place in August 1994 and March 1995 (for details of these tests and the FERET database and program, see Phillips et al [5, 6] and Rauss et al [7]); The FERET database collection began in September 1993 along with the FERET program."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The face recognition technology (FERET) program"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Oce of National Drug Control Policy, CTAC International Technology Symposium, pages 8{11 | 8{20"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5484,
                                "start": 5473
                            }
                        ],
                        "text": "Two of the most critical requirements in support of producing reliable face recognition systems are a large database of facial images and a testing procedure to evaluate systems The Face Recognition Technology FERET program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests To date images from individuals are included in the FERET database which is divided into development and sequestered portions of the database In September the FERET program administered the third in a series of FERET face recognition tests The primary objectives of the third test were to assess the state of the art identify future areas of research and measure algorithm performance\nIntroduction\nOver the last decade face recognition has become an active area of research in com puter vision neuroscience and psychology Progress has advanced to the point that face recognition systems are being demonstrated in real world settings The rapid de velopment of face recognition is due to a combination of factors active development of\nThe work reported here is part of the Face Recognition Technology FERET program which is sponsored by the U S Department of Defense Counterdrug Technology Development Program Portions of this work was done while Jonathon Phillips was at the U S Army Research Laboratory ARL Jonathon Phillips acknowledges the support of the National Institute of Justice\nalgorithms the availability of a large database of facial images and a method for evaluat ing the performance of face recognition algorithms The FERET database and evaluation methodology address the latter two points and are de facto standards There have been three FERET evaluations with the most recent being the Sep FERET test\nThe Sep FERET test provides a comprehensive picture of the state of the art in face recognition from still images This was accomplished by evaluating algorithms ability on di erent scenarios categories of images and versions of algorithms Performance was computed for identi cation and veri cation scenarios In an identi cation application an algorithm is presented with a face that it must identify the face whereas in a veri cation application an algorithm is presented with a face and a claimed identity and the algorithm must accept or reject the claim In this paper we describe the FERET database the Sep FERET evaluation protocol and present identi cation results Veri cation results are presented in Rizvi et al\nTo obtain a robust assessment of performance algorithms are evaluated against dif ferent categories of images The categories are broken out by lighting changes people wearing glasses and the time between the acquisition date of the database image and the image presented to the algorithm By breaking out performance into these categories a better understanding of the face recognition eld in general as well as the strengths and weakness of individual algorithms is obtained This detailed analysis helps to assess which applications can be successfully addressed\nAll face recognition algorithms known to the authors consist of two parts face detection and normalization and face identi cation Algorithms that consist of both parts are referred to as fully automatic algorithms and those that consist of only the second part are partially automatic algorithms The Sep test evaluated both fully and partially automatic algorithms Partially automatic algorithms are given a facial image and the coordinates of the center of the eyes Fully automatic algorithms are only given facial images\nThe availability of the FERET database and evaluation methodology has made a signi cant di erence in the progress of development of face recognition algorithms Be fore the FERET database was created a large number of papers reported outstanding recognition results usually correct recognition on limited size databases usually individuals In fact this is still true Only a few of these algorithms reported results on images utilizing a common database let alone met the desirable goal of being evaluated on a standard testing protocol that included separate training and testing sets As a consequence there was no method to make informed comparisons among various algorithms\nThe FERET database has made it possible for researchers to develop algorithms on a common database and to report results in the literature using this database Results reported in the literature do not provide a direct comparison among algorithms because each researcher reported results using di erent assumptions scoring methods and images The independently administered FERET test allows for a direct quantitative assessment of the relative strengths and weaknesses of di erent approaches\nMore importantly the FERET database and tests clarify the current state of the art in face recognition and point out general directions for future research The FERET tests allow the computer vision community to assess overall strengths and weaknesses in the eld not only on the basis of the performance of an individual algorithm but\nin addition on the aggregate performance of all algorithms tested Through this type of assessment the community learns in an unbiased and open manner of the important technical problems to be addressed and how the community is progressing toward solving these problems\nBackground\nThe rst FERET tests took place in August and March for details of these tests and the FERET database and program see Phillips et al and Rauss et al The FERET database collection began in September along with the FERET program\nThe August test established for the rst time a performance baseline for face recognition algorithms This test was designed to measure performance on algorithms that could automatically locate normalize and identify faces from a database The test consisted of three subtests each with a di erent gallery and probe set The gallery contains the set of known individuals An image of an unknown face presented to the algorithm is called a probe and the collection of probes is called the probe set The rst subtest examined the ability of algorithms to recognize faces from a gallery of individuals The second was the false alarm test which measured how well an algorithm rejects faces not in the gallery The third baselined the e ects of pose changes on performance\nThe second FERET test that took place in March measured progress since August and evaluated algorithms on larger galleries The March evaluation consisted of a single test with a gallery of known individuals One emphasis of the test was on probe sets that contained duplicate images A duplicate is de ned as an image of a person whose corresponding gallery image was taken on a di erent date\nThe FERET database is designed to advance the state of the art in face recognition with the images collected directly supporting both algorithm development and the FERET evaluation tests The database is divided into a development set provided to researchers and a set of sequestered images for testing The images in the development set are representative of the sequestered images\nThe facial images were collected in sessions between August and July Collection sessions lasted one or two days In an e ort to maintain a degree of consistency throughout the database the same physical setup and location was used in each photog raphy session However because the equipment had to be reassembled for each session there was variation from session to session gure\nImages of an individual were acquired in sets of to images collected under rel atively unconstrained conditions Two frontal views were taken fa and fb a di erent facial expression was requested for the second frontal image For sets of images a third frontal image was taken with a di erent camera and di erent lighting this is referred to as the fc image The remaining images were collected at various aspects between right and left pro le To add simple variations to the database photographers sometimes took a second set of images for which the subjects were asked to put on their glasses and or pull their hair back Sometimes a second set of images of a person was taken on a later date such a set of images is referred to as a duplicate set Such duplicates sets result in variations in scale pose expression and illumination of the face\nBy July sets of images were in the database consisting of total images The database contains individuals and duplicate sets of images For\nfa fb duplicate I fc duplicate II\nFigure Examples of di erent categories of probes image The duplicate I image was taken within one year of the fa image and the duplicate II and fa images were taken at least one year apart\nsome people over two years elapsed between their rst and most recent sittings with some subjects being photographed multiple times gure The development portion of the database consisted of sets of images and was released to researchers The remaining images were sequestered by the Government\nTest Design\nTest Design Principles\nThe FERET Sep evaluation protocol was designed to assess the state of the art advance the state of the art and point to future directions of research To succeed at this the test design must solve the three bears problem The test cannot be neither too hard nor too easy If the test is too easy the testing process becomes an exercise in tuning existing algorithms If the test is too hard the test is beyond the ability of existing algorithmic techniques The results from the test are poor and do not allow for an accurate assessment of algorithmic capabilities\nThe solution to the three bears problem is through the selection of images in the test set and the testing protocol Tests are administered using a testing protocol that states the mechanics of the tests and the manner in which the test will be scored In face recognition the protocol states the number of images of each person in the test how the output from the algorithm is recorded and how the performance results are reported\nThe characteristics and quality of the images are major factors in determining the di culty of the problem being evaluated For example if faces are in a predetermined position in the images the problem is di erent from that for images in which the faces can be located anywhere in the image In the FERET database variability was introduced by the inclusion of images taken at di erent dates and locations see section This resulted in changes in lighting scale and background\nThe testing protocol is based on a set of design principles Stating the design principle allows one to assess how appropriate the FERET test is for a particular face recognition algorithm Also design principles assist in determining if an evaluation methodology for testing algorithm s for a particular application is appropriate Before discussing the\ndesign principles we state the evaluation protocol In the testing protocol an algorithm is given two sets of images the target set and the query set We introduce this terminology to distinguish these sets from the gallery and probe sets that are used in computing performance statistics The target set is given to the algorithm as the set of known facial images The images in the query set consists of unknown facial images to be identi ed For each image qi in the query set Q an algorithm reports a similarity si k between qi and each image tk in the target set T The testing protocol is designed so that each algorithm can use a di erent similarity measure and we do not compare similarity measures from di erent algorithms The key property of the new protocol which allows for greater exibility in scoring is that for any two images qi and tk we know si k\nThis exibility allows the evaluation methodology to be robust and comprehensive it is achieved by computing scores for virtual galleries and probe sets A gallery G is a virtual gallery if G is a subset of the target set i e G T Similarly P is a virtual probe set if P Q For a given gallery G and probe set P the performance scores are computed by examination of similarity measures si k such that qi P and tk G\nThe virtual gallery and probe set technique allows us to characterize algorithm per formance by di erent categories of images The di erent categories include rotated images duplicates taken within a week of the gallery image duplicates where the time between the images is at least one year galleries containing one image per person and galleries containing duplicate images of the same person We can create a gallery of people and estimate an algorithm s performance by recognizing people in this gallery Using this as a starting point we can then create virtual galleries of people and determine how performance changes as the size of the gallery increases An other avenue of investigation is to create n di erent galleries of size and calculate the variation in algorithm performance with the di erent galleries\nTo take full advantage of virtual galleries and probe sets we selected multiple images of the same person and placed them into the target and query sets If such images were marked as the same person the algorithms being tested could use the information in the evaluation process To prevent this from happenning we require that each image in the target set be treated as an unique face In practice this condition is enforced by giving every image in the target and query set a unique random identi cation This is the rst design principle\nThe second design principle is that training is completed prior to the start of the test This forces each algorithm to have a general representation for faces not a representation tuned to a speci c gallery Without this condition virtual galleries would not be possible\nFor algorithms to have a general representation for faces they must be gallery class insensitive Examples are algorithms based on normalized correlation or principal com ponent analysis PCA An algorithm is class sensitive if the representation is tuned to a speci c gallery Examples are straight forward implementation of Fisher discriminant analysis Fisher discriminant algorithms were adapted to class insensitive testing methodologies by Zhao et al with performance results of these extensions being reported in this paper\nThe third design rule is that all algorithms tested compute a similarity measure be tween two facial images this similarity measure was computed for all pairs of images between the target and query sets Knowing the similarity score between all pairs of\nFace Recognition Algorithm\n(run at Testee\u2019s site)\nOutput File\nScoring Code\nGovernment Site)\n(run at\nResults\nMMMM\n0001\n0002\n0002\n0001\nNNNN\nProbe Image Name\nGallery Image Name\n(One Image/Person) Gallery images\nList of Probes\nFigure Schematic of the FERET testing procedure\nimages from the target and query sets allows for the construction of virtual galleries and probe sets\nTest Details\nIn the Sep FERET test the target set contained images and the query set images All the images in the target set were frontal images The query set consisted of all the images in the target set plus rotated images and digitally modi ed images We designed the digitally modi ed images to test the e ects of illumination and scale Results from the rotated and digitally modi ed images are not reported here For each query image qi an algorithm outputs the similarity measure si k for all images tk in the target set For a given query image qi the target images tk are sorted by the similarity scores si Since the target set is a subset of the query set the test output contains the similarity score between all images in the target set\nThere were two versions of the Sep test The target and query sets were the same for each version The rst version tested partially automatic algorithms by providing them with a list of images in the target and query sets and the coordinates of the center of the eyes for images in the target and query sets In the second version of the test the coordinates of the eyes were not provided By comparing the performance between the two versions we estimate performance of the face locating portion of a fully automatic algorithm at the system level\nThe test was administered at each group s site under the supervision of one of the au thors Each group had three days to complete the test on less than UNIX workstations this limit was not reached We did not record the time or number of workstations because execution times can vary according to the type of machines used machine and network con guration and the amount of time that the developers spent optimizing their code we wanted to encourage algorithm development not code optimization We imposed the\ntime limit to encourage the development of algorithms that could be incorporated into operational eldable systems\nThe images contained in the gallery and probe sets consisted of images from both the developmental and sequestered portions of the FERET database Only images from the FERET database were included in the test however algorithm developers were not prohibited from using images outside the FERET database to develop or tune parameters in their algorithms\nThe FERET test is designed to measure laboratory performance The test is not concerned with speed of the implementation real time implementation issues and speed and accuracy trade o s These issues and others need to be addressed in an operational elded system were beyond the scope of the Sep FERET test\nFigure presents a schematic of the testing procedure To ensure that matching was not done by le name we gave the images random names The nominal pose of each face was provided to the testee\nDecision Theory and Performance Evaluation\nThe basic models for evaluating the performance of an algorithm are the closed and open universes In the closed universe every probe is in the gallery In an open universe some probes are not in the gallery Both models re ect di erent and important aspects of face recognition algorithms and report di erent performance statistics The open universe models veri cation applications The FERET scoring procedures for veri cation is given in Rizvi et al\nThe closed universe model allows one to ask how good an algorithm is at identifying a probe image the question is not always is the top match correct but is the correct answer in the top n matches This lets one know how many images have to be examined to get a desired level of performance The performance statistics are reported as cumula tive match scores The rank is plotted along the horizontal axis and the vertical axis is the percentage of correct matches The cumulative match score can be calculated for any subset of the probe set We calculated this score to evaluate an algorithm s performance on di erent categories of probes i e rotated or scaled probes\nThe computation of an identi cation score is quite simple Let P be a probe set and jPj the size of P We score probe set P against gallery G where G fg gMg and P fp pNg by comparing the similarity scores si such that pi P and gk G For each probe image pi P we sort si for all gallery images gk G We assume that a smaller similarity score implies a closer match If gk and pi are the same image then si k The function id i gives the index of the gallery image of the person in probe pi i e pi is an image of the person in gid i A probe pi is correctly identi ed if si id i is the smallest scores for gk G A probe pi is in the top k if si id i is one of the k th smallest score si for gallery G Let Rk denote the number of probes in the top k We reported Rk jPj the fraction of probes in the top k As an example let k R and jPj Based on the formula the performance score for R is\nIn reporting identi cation performance results we state the size of the gallery and the number of probes scored The size of the gallery is the number of di erent faces people contained in the images that are in the gallery For all results that we report there is one image per person in the gallery thus the size of the gallery is also the number of images\nin the gallery The number of probes scored also size of the probe set is jPj The probe set may contain more than one image of a person and the probe set may not contain an image of everyone in the gallery Every image in the probe set has a corresponding image in the gallery\nLatest Test Results\nThe Sep FERET test was designed to measure algorithm performance for identi cation and veri cation tasks Both tasks are evaluated on the same sets of images We report the results for algorithms that includes partially automatic algorithms and fully automatic algorithms The test was administered in September and March see table for details of when the test was administered to which groups and which version of the test was taken Two of these algorithms were developed at the MIT Media Laboratory The rst was the same algorithm that was tested in March This algorithm was retested so that improvement since March could be measured The second algorithm was based on more recent work Algorithms were also tested from Excalibur Corp Carlsbad CA Michigan State University MSU Rutgers University University of Southern California USC and two from University of Maryland UMD The rst algorithm from UMD was tested in September and a second version of the algorithm was tested in March For the fully automatic version of test algorithms from MIT and USC were evaluated\nThe nal two algorithms were our implementation of normalized correlation and a principal components analysis PCA based algorithm These algorithms provide a performance baseline In our implementation of the PCA based algorithm all images were translated rotated and scaled so that the center of the eyes were placed on speci c pixels faces were masked to remove background and hair and the non masked facial pixels were processed by a histogram equalization algorithm The training set consisted of faces Faces were represented by their projection onto the rst eigenvectors and were identi ed by a nearest neighbor classi er using the L metric For normalized correlation the images were translated rotated and scaled so that the center of the eyes were placed on speci c pixels and faces were masked to remove background and hair\nPartially automatic algorithms\nWe report identi cation scores for four categories of probes The rst probe category was the FB probes g For each set of images there were two frontal images One of the images was randomly placed in the gallery and the other image was placed in the FB probe set This category is denoted by FB to di erentiate it from the fb images in the FERET database The second probe category contained all duplicate frontal images in the FERET database for the gallery images We refer to this category as the duplicate I probes The third category was the fc images taken the same day but with a di erent camera and lighting The fourth consisted of duplicates where there is at least one year between the acquisition of the probe image and corresponding gallery image We refer to this category as the duplicate II probes For this category the gallery images were acquired before January and the probe images were acquired after January\nTable List of groups that took the Sept test broken out by versions taken and dates administered The by MIT indicates that two algorithms were tested\nTest Date\nSeptember March\nVersion of test Group Baseline\nFully Automatic MIT Media Lab U of So California USC\nEye Coordinates Given Baseline PCA Baseline Correlation Excalibur Corp MIT Media Lab\nMichigan State U Rutgers U\nU Maryland USC\nThe gallery for the FB duplicate I and fc probes was the same and consisted of frontal images with one image person in the gallery thus the gallery contained individuals Also none of the faces in the gallery images wore glasses The gallery for duplicate II probes was a subset of images from the gallery for the other categories\nThe results for identi cation are reported as cumulative match scores Table shows the categories corresponding to the gures presenting the results type of results and size of the gallery and probe sets gs to\nIn gures and we compare the di culty of di erent probe sets Whereas gure reports identi cation performance for each algorithm gure shows a single curve that is an average of the identi cation performance of all algorithms for each probe category For example the rst ranked score for duplicate I probe sets is computed from an average of the rst ranked score for all algorithms in gure In gure we presented current upper bound for performance on partially automatic algorithms for each probe category For each category of probe gure plots the algorithm with the highest top rank score R Figures and reports performance of four categories of probes FB duplicate I fc duplicate II\nTable Figures reporting results for partially automatic algorithms Performance is broken out by probe category\nFigure no Probe Category Gallery size Probe set size\nFB\nduplicate I\nfc\nduplicate II\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 MSU UMD 96 MIT 95\nBaseline Cor Baseline EF\nExcalibur Rutgers\na\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUMD 97 USC\nUMD 96 Baseline Cor Baseline EF\nb\nFigure Identi cation performance against FB probes a Partially automatic algo rithms tested in September b Partially automatic algorithms tested in March\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 Excalibur Baseline EF Baseline Cor\nMIT 95 MSU\nUMD 96 Rutgers\na\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC UMD 97\nBaseline EF Baseline Cor\nUMD 96\nb\nFigure Identi cation performance against all duplicate I probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 UMD 96\nMSU Excalibur Baseline EF Rutgers MIT 95 Baseline Cor\na\n0\n0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC UMD 97 UMD 96\nBaseline EF Baseline Cor\nb\nFigure Identi cation performance against fc probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nMIT 96 Baseline EF\nExcalibur Rutgers MIT 95 Baseline Cor UMD 96\nMSU\na\n0\n0.2\n0.4\n0.6\n0.8\n1\n5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e m a t c h s c o r e\nRank\nUSC Baseline EF\nUMD 97 Baseline Cor\nUMD 96\nb\nFigure Identi cation performance against duplicate II probes a Partially automatic algorithms tested in September b Partially automatic algorithms tested in March\n0 0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nFB probes Duplicate I probes\nfc probes Duplicate II probes\nFigure Average identi cation performance of partially automatic algorithms on each probe category\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nFB probes fc probes\nDuplicate I probes Duplicate II probes\nFigure Current upper bound identi cation performance of partially automatic algo rithm for each probe category\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\n0.92\n0.94\n0.96\n0.98\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nUSC partially automatic MIT partially automatic\nUSC fully automatic MIT fully automatic\nFigure Identi cation performance of fully automatic algorithms against partially auto matic algorithms for FB probes\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 5 10 15 20 25 30 35 40 45 50\nC u m u l a t i v e M a t c h S c o r e\nRank\nUSC partially automatic USC fully automatic MIT partially automatic MIT fully automatic\nFigure Identi cation performance of fully automatic algorithms against partially au tomatic algorithms for duplicate I probes\nFully Automatic Performance\nIn this subsection we report performance for the fully automatic algorithms of the MIT Media Lab and USC To allow for a comparison between the partially and fully automatic algorithms we plot the results for the partially and fully automatic algorithms Figure shows performance for FB probes and gure shows performance for duplicate I probes The gallery and probe sets are the same as in subsection\nVariation in Performance\nFrom a statistical point of view a face recognition algorithm estimates the identity of a face Consistent with this view we can ask about the variance in performance of an algorithm For a given category of images how does performance change if the algorithm is given a di erent gallery and probe set In tables and we show how algorithm performance varies if the people in the galleries change For this experiment we constructed six galleries of approximately individuals in which an individual was in only one gallery the number of people contained within each gallery versus the number of probes scored is given in tables and Results are reported for the partially automatic algorithms For the results in this section we order algorithms by their top rank score on each gallery for example in table the UMD Mar algorithm scored highest on gallery and the baseline PCA and correlation tied for th place Also included in this table is average performance for all algorithms Table reports results for FB probes Table is organized in the same manner as table except that duplicate I probes are scored Tables and report results for the same gallery The galleries were constructed by placing images within the galleries by chronological order in which the images were collected the rst gallery contains the rst images collected and the th gallery contains the most recent images collected In table mean age refers to the average time between collection of images contained in the gallery and the corresponding duplicate probes No scores are reported in table for gallery because there are no duplicates for this gallery\nDiscussion and Conclusion\nIn this paper we presented the Sep FERET evaluation protocol for face recognition algorithms The protocol makes it possible to independently evaluate algorithms The protocol was designed to evaluate algorithms on di erent galleries and probe sets for dif ferent scenarios Using this protocol we computed performance on identi cation and veri cation tasks The veri cation results are presented in Rizvi et al and all veri cation results mentioned in this section are from that paper In this paper we presented detailed identi cation results Because of the Sep FERET evaluation protocol s ability to test algorithms performance on di erent tasks for multiple galleries and probe sets it is the de facto standard for measuring performance of face recognition algorithms These results show that factors e ecting performance include scenario date tested and probe category\nThe Sep test was the latest FERET test the others were the Aug and Mar tests One of the main goals of the FERET tests has been to improve the performance of face recognition algorithms and is seen in the Sep FERET test The rst case is the improvement in performance of the MIT Media Lab September algorithm over\nthe March algorithm the second is the improvement of the UMD algorithm between September and March\nBy looking at progress over the series of FERET tests one sees that substantial progress has been made in face recognition The most direct method is to compare the performance of fully automatic algorithms on fb probes the two earlier FERET tests only evaluated fully automatic algorithms The best top rank score for fb probes on the Aug test was on a gallery of individuals and for Mar the top score was on a gallery of individuals This compares to in September and in March gallery of individuals This method shows that over the course of the FERET tests the absolute scores increased as the size of the database increased The March score was from one of the MIT Media Lab algorithms and represents an increase from in March\nOn duplicate I probes MIT Media Lab improved from March to September USC s performance remained approximately the same at be tween March and March This improvement in performance was achieved while the gallery size increased and the number of duplicate I probes increased from to While increasing the number of probes does not necessarily increase the di culty of iden ti cation tasks we argue that the Sep duplicate I probe set was more di cult to process then the Mar set The Sep duplicate I probe set contained the duplicate II probes and the Mar duplicate I probe set did not contain a similar class of probes Overall the duplicate II probe set was the most di cult probe set\nAnother goal of the FERET tests is to identify areas of strengths and weaknesses in the eld of face recognition We addressed this issue by computing algorithm per formance for multiple galleries and probe sets From this evaluation we concluded that algorithm performance is dependent on the gallery and probe sets We observed variation in performance due to changing the gallery and probe set within a probe category and by changing probe categories The e ect of changing the gallery while keeping the probe category constant is shown in tables and For fb probes the range for performance is to for duplicate I probes the range is to Equally important tables and shows the variability in relative performance levels For example in table UMD Sep duplicate performance varies between number three and nine Similar results were found in Moon and Phillips in their study of principal component analysis based face recognition algorithms This shows that an area of future research could measure the ef fect of changing galleries and probe sets and statistical measures that characterize these variations\nFigures and shows probe categories characterized by di culty These gures show that fb probes are the easiest and duplicate II probes are the most di cult On average duplicate I probes are easier to identify than fc probes However the best performance on fc probes is signi cantly better than the best performance on duplicate I and II probes This comparative analysis shows that future areas of research could address processing of duplicate II probes and developing methods to compensate for changes in illumination\nThe scenario being tested contributes to algorithm performance For identi cation the MIT Media Lab algorithm was clearly the best algorithm tested in September However for veri cation there was not an algorithm that was a top performer for all probe categories Also for the algorithms tested in March the USC algorithm performed overall better than the UMD algorithm for identi cation however for veri cation UMD\noverall performed better This shows that performance on one task is not predictive of performance on another task\nThe September FERET test shows that de nite progress is being made in face recognition and that the upper bound in performance has not been reached The im provement in performance documented in this paper shows directly that the FERET series of tests have made a signi cant contribution to face recognition This conclusion is indi rectly supported by the improvement in performance between the algorithms tested in September and March the number of papers that use FERET images and report experimental results using FERET images and the number of groups that participated in the Sep test"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 180
                            }
                        ],
                        "text": "2 Background The rst FERET tests took place in August 1994 and March 1995 (for details of these tests and the FERET database and program, see Phillips et al [5, 6] and Rauss et al [7]); The FERET database collection began in September 1993 along with the FERET program."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The FERET (Face  Recognition Technology) program. In Surveillance and Assessment Technology for  Law Enforcement, volume"
            },
            "venue": {
                "fragments": [],
                "text": "SPIE Vol"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2035356983"
                        ],
                        "name": "P. Ranss",
                        "slug": "P.-Ranss",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Ranss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ranss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 228304130,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0f19e05e246bd58911438a0bb1b9674ce94d90c",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-recognition-technology-program-overview-and-Ranss",
            "title": {
                "fragments": [],
                "text": "Face recognition technology program overview and results"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736245"
                        ],
                        "name": "Laurenz Wiskott",
                        "slug": "Laurenz-Wiskott",
                        "structuredName": {
                            "firstName": "Laurenz",
                            "lastName": "Wiskott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurenz Wiskott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145893752"
                        ],
                        "name": "J. Fellous",
                        "slug": "J.-Fellous",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Fellous",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fellous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721722"
                        ],
                        "name": "N. Kr\u00fcger",
                        "slug": "N.-Kr\u00fcger",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Kr\u00fcger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kr\u00fcger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10223132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c27487c3e0894b65e976a287e6f8c9aa40f089c",
            "isKey": false,
            "numCitedBy": 2135,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system for recognizing human faces from single images out of a large database containing one image per person. Faces are represented by labeled graphs, based on a Gabor wavelet transform. Image graphs of new faces are extracted by an elastic graph matching process and can be compared by a simple similarity function. The system differs from Lades et al. (1993) in three respects. Phase information is used for accurate node positioning. Object-adapted graphs are used to handle large rotations in depth. Image graph extraction is based on a novel data structure, the bunch graph, which is constructed from a small set of sample image graphs."
            },
            "slug": "Face-recognition-by-elastic-bunch-graph-matching-Wiskott-Fellous",
            "title": {
                "fragments": [],
                "text": "Face recognition by elastic bunch graph matching"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A system for recognizing human faces from single images out of a large database containing one image per person, based on a Gabor wavelet transform, which differs from Lades et al. (1993) in three respects."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Image Processing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 483975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74b312560b79929540734067e58de46966b96130",
            "isKey": false,
            "numCitedBy": 1684,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised technique for visual learning, which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for unimodal distributions) and a mixture-of-Gaussians model (for multimodal distributions). Those probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition and coding. Our learning technique is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and nonrigid objects, such as hands."
            },
            "slug": "Probabilistic-Visual-Learning-for-Object-Moghaddam-Pentland",
            "title": {
                "fragments": [],
                "text": "Probabilistic Visual Learning for Object Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An unsupervised technique for visual learning is presented, which is based on density estimation in high-dimensional spaces using an eigenspace decomposition and is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and nonrigid objects."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The FERET ( FaceRecognition Technology ) program"
            },
            "venue": {
                "fragments": [],
                "text": "Surveillance and Assessment Technology forLaw Enforcement , volume SPIE"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaThe Face Recognition Technology (FERET) Program,\u00ba Proc. Office of Nat'l Drug Control Policy, CTAC Int'l Technology Symp"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaThe Face Recognition Technology (FERET) Program,\u00ba Proc. Office of Nat'l Drug Control Policy, CTAC Int'l Technology Symp"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The FERET (Face Recognition Technology) program. In Surveillance and Assessment Technology for Law Enforcement"
            },
            "venue": {
                "fragments": [],
                "text": "The FERET (Face Recognition Technology) program. In Surveillance and Assessment Technology for Law Enforcement"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phillips, \u00aaAnalysis of PCA-Based Face Recognition Algorithms,\u00ba Empirical Evaluation Techniques in Computer"
            },
            "venue": {
                "fragments": [],
                "text": "Phillips, \u00aaAnalysis of PCA-Based Face Recognition Algorithms,\u00ba Empirical Evaluation Techniques in Computer"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recognition using transform coding of gray scale projection projections and the neural tree network. In Artcjical neural networks with applications in speech and vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face Recognition Technology) Program,\u00ba Surveillance and Assessment Technology for Law Enforcement"
            },
            "venue": {
                "fragments": [],
                "text": "Face Recognition Technology) Program,\u00ba Surveillance and Assessment Technology for Law Enforcement"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic visual leaming for object detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaFace Recognition Using Transform Coding of Grayscale Projection Projections and the Neural Tree Network,\u00ba Artifical Neural Networks with Applications in Speech and Vision"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaFace Recognition Using Transform Coding of Grayscale Projection Projections and the Neural Tree Network,\u00ba Artifical Neural Networks with Applications in Speech and Vision"
            },
            "year": 1994
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/The-FERET-evaluation-methodology-for-algorithms-Phillips-Moon/791e530f6a4098bb39696d1476032821a7a1c569?sort=total-citations"
}