{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21169546"
                        ],
                        "name": "Donald Hindle",
                        "slug": "Donald-Hindle",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hindle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Hindle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416786"
                        ],
                        "name": "Mats Rooth",
                        "slug": "Mats-Rooth",
                        "structuredName": {
                            "firstName": "Mats",
                            "lastName": "Rooth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mats Rooth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Earlier work [11] on PP-attachment for verb phrases (whether the PP attaches to the preceding noun phrase or to the verb phrase) used statistics on co-occurences of two bigrams: the main verb (V ) and preposition (P ) bigram and the main noun in the object noun phrase (N ) and preposition bigram."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5410054,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "bdaf232c561f1f50e88b1d24097e214890b37e8b",
            "isKey": false,
            "numCitedBy": 638,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning."
            },
            "slug": "Structural-Ambiguity-and-Lexical-Relations-Hindle-Rooth",
            "title": {
                "fragments": [],
                "text": "Structural Ambiguity and Lexical Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "It is proposed that many ambiguous prepositional phrase attachments can be resolved on the based of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "We use a small set of rules, called a Tree Head Table, to obtain the head word of a constituent [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 626195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e78155b28b1f4db52a7c9076c89e81ac4b7d8ce",
            "isKey": false,
            "numCitedBy": 284,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules. \nIn this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules. \nIn experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%."
            },
            "slug": "Natural-Language-Parsing-as-Statistical-Pattern-Magerman",
            "title": {
                "fragments": [],
                "text": "Natural Language Parsing as Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853032"
                        ],
                        "name": "J. Lai",
                        "slug": "J.-Lai",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Lai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Mutual Information Bits Mutual information clustering, as described in [10], creates a a class \u201ctree\u201d for a given vocabulary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 198
                            }
                        ],
                        "text": "A scheme in which the word classes are built from the observed attachment preferences of words ought to outperform the mutual information clustering method, which uses only word bigram distributions[10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10986188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3de5d40b60742e3dfa86b19e7f660962298492af",
            "isKey": false,
            "numCitedBy": 3318,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics."
            },
            "slug": "Class-Based-n-gram-Models-of-Natural-Language-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "Class-Based n-gram Models of Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work addresses the problem of predicting a word from previous words in a sample of text and discusses n-gram models based on classes of words, finding that these models are able to extract classes that have the flavor of either syntactically based groupings or semanticallybased groupings, depending on the nature of the underlying statistics."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5598810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
            },
            "slug": "Towards-History-based-Grammars:-Using-Richer-Models-Black-Jelinek",
            "title": {
                "fragments": [],
                "text": "Towards History-based Grammars: Using Richer Models for Probabilistic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way and significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "Selection Order Feature (1) Preposition == \"of\" (2) Bit 2 of Head Noun == 0 (3) Preposition is \"to\" (4) Bit 12 of Head Noun == 1"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14289498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eadf7d20852caa92310d0cb582269b94226b1e58",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present an algorithm to adapt a n-gram language model to a document as it is dictated. The observed partial document is used to estimate a unigram distribution for the words that already occurred. Then, they find the closest n-gram distribution to the static n-gram distribution (using the discrimination information distance measure) that satisfies the marginal constraints derived from the document. The resulting minimum discrimination information model results in a perplexity of 208 instead of 290 for the static trigram model on a document of 321 words.<<ETX>>"
            },
            "slug": "Adaptive-Language-Modeling-Using-Minimum-Estimation-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Adaptive Language Modeling Using Minimum Discriminant Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "The authors present an algorithm to adapt a n-gram language model to a document as it is dictated that results in a perplexity of 208 instead of 290 for the static trigram model on a document of 321 words."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153733790"
                        ],
                        "name": "R. Garside",
                        "slug": "R.-Garside",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Garside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Garside"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143998726"
                        ],
                        "name": "G. Leech",
                        "slug": "G.-Leech",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Leech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Leech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "112868461"
                        ],
                        "name": "E. Eyes",
                        "slug": "E.-Eyes",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Eyes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Eyes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17387777,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32889f93ab4f08e6b8d9e8bf4719fef18ec986d3",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book is about building computer programs that parse (analyze, or diagram) sentences of a real-world English. The English we are concerned with might be a corpus of everyday, naturally-occurring prose, such as the entire text of this morning's newspaper. Most programs that now exist for this purpose are not very successful at finding the correct analysis for everyday sentences. In contrast, the programs described here make use of a more successful statistically-driven approach. Our book is, first, a record of a five-year research collaboration between IBM and Lancaster University. Large numbers of real-world sentences were fed into the memory of a program for grammatical analysis (including a detailed grammar of English) and processed by statistical methods. The idea is to single out the correct parse, among all those offered by the grammar, on the basis of probabilities. Second, this is a how-to book, showing how to build and implement a statistically-driven broad-coverage grammar of English. We even supply our own grammar, with the necessary statistical algorithms, and with the knowledge needed to prepare a very large set (or corpus) of sentences so that it can be used to guide the statistical processing of the grammar's rules."
            },
            "slug": "Statistically-driven-computer-grammars-of-English-:-Black-Garside",
            "title": {
                "fragments": [],
                "text": "Statistically-driven computer grammars of English : the IBM/LANCASTER approach"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This book is about building computer programs that parse (analyze, or diagram) sentences of a real-world English, which might be a corpus of everyday, naturally-occurring prose, such as the entire text of this morning's newspaper."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17870175,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "08b67692bc037eada8d3d7ce76cc70994e7c8116",
            "isKey": false,
            "numCitedBy": 10876,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Treatment of the predictive aspect of statistical mechanics as a form of statistical inference is extended to the density-matrix formalism and applied to a discussion of the relation between irreversibility and information loss. A principle of \"statistical complementarity\" is pointed out, according to which the empirically verifiable probabilities of statistical mechanics necessarily correspond to incomplete predictions. A preliminary discussion is given of the second law of thermodynamics and of a certain class of irreversible processes, in an approximation equivalent to that of the semiclassical theory of radiation."
            },
            "slug": "Information-Theory-and-Statistical-Mechanics-Jaynes",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistical Mechanics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1411057829"
                        ],
                        "name": "Geo. R. Lawrence Co.",
                        "slug": "Geo.-R.-Lawrence-Co.",
                        "structuredName": {
                            "firstName": "Geo.",
                            "lastName": "Co.",
                            "middleNames": [
                                "R.",
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geo. R. Lawrence Co."
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222435709,
            "fieldsOfStudy": [
                "Environmental Science",
                "Law"
            ],
            "id": "43844d57cb9617c0d3e8586425ac200fc7e0a4fa",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "H[...] U.S. Copyright Office\nCopyright claimant's address: Chicago.\nCopyright deposit; Geo. R. Lawrence; August 17, 1906."
            },
            "slug": "Pacific-Grove,-California-Co.",
            "title": {
                "fragments": [],
                "text": "Pacific Grove, California"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1906
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49223598"
                        ],
                        "name": "J. Darroch",
                        "slug": "J.-Darroch",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Darroch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Darroch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12360582"
                        ],
                        "name": "D. Ratcliff",
                        "slug": "D.-Ratcliff",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Ratcliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ratcliff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "For example, one can use the Generalized Iterative Scaling algorithm of Darroch and Ratcliff [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120862597,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37c931cbaa9217b829596dd196520a838562a109",
            "isKey": false,
            "numCitedBy": 1329,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalized-Iterative-Scaling-for-Log-Linear-Models-Darroch-Ratcliff",
            "title": {
                "fragments": [],
                "text": "Generalized Iterative Scaling for Log-Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2390150"
                        ],
                        "name": "Dekai Wu",
                        "slug": "Dekai-Wu",
                        "structuredName": {
                            "firstName": "Dekai",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekai Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60595687,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "b664689eadc2a2210b05a28c57fdf84fe0e1967e",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Review-of-Statistically-driven-computer-grammars-of-Wu",
            "title": {
                "fragments": [],
                "text": "Review of Statistically-driven computer grammars of English: the IBM/lancaster approach by Ezra Black, Roger Garside, and Geoffrey Leech. Editions Rodopi 1993."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statisticallydriven Computer Grammars of English: The IBM"
            },
            "venue": {
                "fragments": [],
                "text": "Statisticallydriven Computer Grammars of English: The IBM"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy Methods in Machine Translation"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum Entropy Methods in Machine Translation"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy Methods and Their Applications to Maximum Likelihood Parameter Estimation of Conditional Exponential Models"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum Entropy Methods and Their Applications to Maximum Likelihood Parameter Estimation of Conditional Exponential Models"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 1,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 13,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Maximum-Entropy-Model-for-Prepositional-Phrase-Ratnaparkhi-Reynar/00f20179b9087fbf24b6656008a9380c590d9ec9?sort=total-citations"
}