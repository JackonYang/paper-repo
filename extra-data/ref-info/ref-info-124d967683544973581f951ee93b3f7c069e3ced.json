{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747918"
                        ],
                        "name": "S. Bileschi",
                        "slug": "S.-Bileschi",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Bileschi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bileschi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996960"
                        ],
                        "name": "M. Riesenhuber",
                        "slug": "M.-Riesenhuber",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Riesenhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riesenhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Learning sparse spatio-temporal motion S2 features In the work [61], a Gaussian-like function is used to compute the responses of dense S2 features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Using motion-direction sensitive S1 units In the work [61], a still gray-value input image is first analyzed by an array of Gabor filters (S1 units) at multiple orientations for all positions and scales."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "The motion pathway can therefore be modeled as a feedforward hierarchical architecture [17, 33, 54, 21, 61]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 126
                            }
                        ],
                        "text": "(c)The first 100-200 ms visual information processing is feedforward (d) plasticity and learning probably occur at all stages [62, 61]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "(b) Adding the scale-invariance by using space-time-oriented S1 units with multiple filter sizes, as used in [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 89
                            }
                        ],
                        "text": "and successfully applied to the multiple object recognition tasks in real world scenario [62, 61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 283
                            }
                        ],
                        "text": "The increasing selectivity (from moving edges to complex flow-field patterns) and invariance (position invariance of V1 complex cells, form invariance of MT cells, and from/position invariance of MST cells) observed in the dorsal stream have also been observed in the ventral stream [21, 61], indicating similar organizations of the two streams, and thus supporting the extension from model of object recognition to action recognition."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "Our approach builds on recent work on object recognition [62, 61] based on hierarchical feedforward architectures and extends a neurobiological model of motion processing in the visual cortex [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "Our approach is closely related to the feedforward hierarchical architectures with alternating template matching and maximum-pooling, used for the recognition of objects in still images [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2179592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71e3d9fc53ba14c2feeb7390f0dc99076553b05a",
            "isKey": false,
            "numCitedBy": 1714,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex"
            },
            "slug": "Robust-Object-Recognition-with-Cortex-Like-Serre-Wolf",
            "title": {
                "fragments": [],
                "text": "Robust Object Recognition with Cortex-Like Mechanisms"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727853"
                        ],
                        "name": "John K. Tsotsos",
                        "slug": "John-K.-Tsotsos",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsotsos",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John K. Tsotsos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145578393"
                        ],
                        "name": "Yueju Liu",
                        "slug": "Yueju-Liu",
                        "structuredName": {
                            "firstName": "Yueju",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yueju Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398979864"
                        ],
                        "name": "J. Martinez-Trujillo",
                        "slug": "J.-Martinez-Trujillo",
                        "structuredName": {
                            "firstName": "Julio",
                            "lastName": "Martinez-Trujillo",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Martinez-Trujillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739588"
                        ],
                        "name": "M. Pomplun",
                        "slug": "M.-Pomplun",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Pomplun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pomplun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718692"
                        ],
                        "name": "E. Simine",
                        "slug": "E.-Simine",
                        "structuredName": {
                            "firstName": "Evgueni",
                            "lastName": "Simine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Simine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1814839"
                        ],
                        "name": "Kunhao Zhou",
                        "slug": "Kunhao-Zhou",
                        "structuredName": {
                            "firstName": "Kunhao",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kunhao Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 143
                            }
                        ],
                        "text": "We can achieve visual attention and thus foreground segmentation by taking into account the backprojections known to be numerous in the cortex [70, 64]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 779582,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "6d877e2a0b5b97b49253a694e4e587ffd2137f78",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 135,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Attending-to-visual-motion-Tsotsos-Liu",
            "title": {
                "fragments": [],
                "text": "Attending to visual motion"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 126
                            }
                        ],
                        "text": "Motivated by the recent success of biologically inspired approaches for the recognition of objects in real-world applications [25, 16, 20], we here extend a neurobiological"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 6
                            }
                        ],
                        "text": "As in [25, 16], we used n \u00d7 n templates (with n = 4, 8, 12, 16)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 35
                            }
                        ],
                        "text": "C2 shape features In previous work [25, 16], a still grayvalue input image is first analyzed by an array of Gabor filters (S1 units) at multiple orientations for all positions and scales."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Beyond the S1 stage our system follows closely the approach by [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In [25, 16] for the recognition of static objects, Gabor filters at multiple orientations were used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "The approach builds on recent work on object recognition based on hierarchical feedforward architectures [25, 16, 20] and extends a neurobiological model of motion processing in the visual cortex [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 88
                            }
                        ],
                        "text": "Here we follow the more recent framework using scale and position invariant C2 features [25, 16] that originated with the work of Riesenhuber & Poggio [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 116
                            }
                        ],
                        "text": "Space-time oriented S1 units: These units constitute the most direct extension to the object recognition systems by [25, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "An illustration of the dense C2 features [25] (a) vs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 141
                            }
                        ],
                        "text": "As recent work in object recognition has indicated, models of cortical processing are starting to suggest new algorithms for computer vision [25, 16, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 260426,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "040c23e5a409fbdedd5032263dfcb1a4d7dfd200",
            "isKey": false,
            "numCitedBy": 969,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge-detectors over neighboring positions and multiple orientations. Our system's architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories. We also demonstrate that our system is able to learn from very few examples. The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex."
            },
            "slug": "Object-recognition-with-features-inspired-by-visual-Serre-Wolf",
            "title": {
                "fragments": [],
                "text": "Object recognition with features inspired by visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex and exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2491353"
                        ],
                        "name": "R. Sigala",
                        "slug": "R.-Sigala",
                        "structuredName": {
                            "firstName": "Rodrigo",
                            "lastName": "Sigala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sigala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33365402"
                        ],
                        "name": "M. Giese",
                        "slug": "M.-Giese",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Giese",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Giese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 73
                            }
                        ],
                        "text": "Interestingly, we find that the optical flow features previously used in [10, 4, 28] lead to worse performance than the gradient-based features and the space-time oriented filters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "Indeed none of the existing neurobiological models of motion processing have been used on real-world data [10, 13, 4, 28, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 68
                            }
                        ],
                        "text": "The model has only been applied so far to simple artificial stimuli [10, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 188
                            }
                        ],
                        "text": "While this model has been successful in explaining a host of physiological and psychophysical data, it has only been tested on simple artificial stimuli such as point-light motion stimuli [10, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9036072,
            "fieldsOfStudy": [
                "Biology",
                "Psychology",
                "Computer Science"
            ],
            "id": "13a614eacfe304979f6e8db852bb511aa593f34b",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans can recognize biological motion from strongly impoverished stimuli, like point-light displays. Although the neural mechanism underlying this robust perceptual process have not yet been clarified, one possible explanation is that the visual system extracts specific motion features that are suitable for the robust recognition of both normal and degraded stimuli. We present a neural model for biological motion recognition that learns robust mid-level motion features in an unsupervised way using a neurally plausible memory-trace learning rule. Optimal mid-level features were learnt from image motion sequences containing a walker with, or without background motion clutter. After learning of the motion features, the detection performance of the model substantially increases, in particular in presence of clutter. The learned mid-level motion features are characterized by horizontal opponent motion, where this feature type arises more frequently for the training stimuli without motion clutter. The learned features are consistent with recent psychophysical data that indicates that opponent motion might be critical for the detection of point light walkers."
            },
            "slug": "Learning-Features-of-Intermediate-Complexity-for-of-Sigala-Serre",
            "title": {
                "fragments": [],
                "text": "Learning Features of Intermediate Complexity for the Recognition of Biological Motion"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A neural model for biological motion recognition that learns robust mid-level motion features in an unsupervised way using a neurally plausible memory-trace learning rule is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3315611"
                        ],
                        "name": "A. Casile",
                        "slug": "A.-Casile",
                        "structuredName": {
                            "firstName": "Antonino",
                            "lastName": "Casile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Casile"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33365402"
                        ],
                        "name": "M. Giese",
                        "slug": "M.-Giese",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Giese",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Giese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 73
                            }
                        ],
                        "text": "Interestingly, we find that the optical flow features previously used in [10, 4, 28] lead to worse performance than the gradient-based features and the space-time oriented filters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "Indeed none of the existing neurobiological models of motion processing have been used on real-world data [10, 13, 4, 28, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 85
                            }
                        ],
                        "text": "The constant q, which controls the width of the tuning curve, was set to q = 2 as in [10, 4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17658324,
            "fieldsOfStudy": [
                "Biology",
                "Physics"
            ],
            "id": "37830662a9088903e32192c7b18cb0d3465d6991",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans can perceive the motion of living beings from very impoverished stimuli like point-light displays. How the visual system achieves the robust generalization from normal to point-light stimuli remains an unresolved question. We present evidence on multiple levels demonstrating that this generalization might be accomplished by an extraction of simple mid-level optic flow features within coarse spatial arrangement, potentially exploiting relatively simple neural circuits: (1) A statistical analysis of the most informative mid-level features reveals that normal and point-light walkers share very similar dominant local optic flow features. (2) We devise a novel point-light stimulus (critical features stimulus) that contains these features, and which is perceived as a human walker even though it is inconsistent with the skeleton of the human body. (3) A neural model that extracts only these critical features accounts for substantial recognition rates for strongly degraded stimuli. We conclude that recognition of biological motion might be accomplished by detecting mid-level optic flow features with relatively coarse spatial localization. The computationally challenging reconstruction of precise position information from degraded stimuli might not be required."
            },
            "slug": "Critical-features-for-the-recognition-of-biological-Casile-Giese",
            "title": {
                "fragments": [],
                "text": "Critical features for the recognition of biological motion."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is concluded that recognition of biological motion might be accomplished by detecting mid-level optic flow features with relatively coarse spatial localization, and the computationally challenging reconstruction of precise position information from degraded stimuli might not be required."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of vision"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2243801"
                        ],
                        "name": "Jim Mutch",
                        "slug": "Jim-Mutch",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Mutch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jim Mutch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1427294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e65fcb0e04174577f211d702d3f837e3624c5b",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply a biologically inspired model of visual object recognition to the multiclass object categorization problem. Our model modifies that of Serre, Wolf, and Poggio. As in that work, we first apply Gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. We refine the approach in several biologically plausible ways, using simple versions of sparsification and lateral inhibition. We demonstrate the value of retaining some position and scale information above the intermediate feature level. Using feature selection we arrive at a model that performs better with fewer features. Our final model is tested on the Caltech 101 object categories and the UIUC car localization task, in both cases achieving state-of-the-art performance. The results strengthen the case for using this class of model in computer vision."
            },
            "slug": "Multiclass-Object-Recognition-with-Sparse,-Features-Mutch-Lowe",
            "title": {
                "fragments": [],
                "text": "Multiclass Object Recognition with Sparse, Localized Features"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A biologically inspired model of visual object recognition to the multiclass object categorization problem, modifies that of Serre, Wolf, and Poggio, and demonstrates the value of retaining some position and scale information above the intermediate feature level."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33365402"
                        ],
                        "name": "M. Giese",
                        "slug": "M.-Giese",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Giese",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Giese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "As in [ 10 ], S1 unit responses were obtained by applying the following equation:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Several specific implementations of such motiondirection sensitive cells have been proposed (see [ 10 ] for references)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It has also been suggested [ 10 ] that another part of the visual system, the ventral stream of the visual cortex, involved with the analysis of shape may also be important for the recognition of motion (consistent with recent work in computer vision [17] which has shown the benefit of using shape features in addition to motion features for the recognition of actions)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "While this model has been successful in explaining a host of physiological and psychophysical data, it has only been tested on simple artificial stimuli such as point-light motion stimuli [ 10 , 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Indeed none of the existing neurobiological models of motion processing have been used on real-world data [ 10 , 13, 4, 28, 11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The analysis of motion information proceeds in MT and MST where neurons have substantial position and scale invariance and are tuned to optical flow patterns, see [ 10 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "model [ 10 ] of motion processing in the dorsal stream of the visual cortex."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In order to convert the neuroscience model of [ 10 ] into a real computer vision system, we altered it in two significant ways: We propose a new set of motion-sensitive units which is shown to perform significantly better and we describe new tuning functions and feature selection techniques which build on recent work on object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Their organization is hierarchical; aiming, in a series of processing stages, to gradually increase both the selectivity of neurons along with their invariance to 2D transformations (see [ 10 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It extends an earlier neurobiological model of motion processing in the dorsal stream of the visual cortex by Giese & Poggio [ 10 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The model has only been applied so far to simple artificial stimuli [ 10 , 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The approach builds on recent work on object recognition based on hierarchical feedforward architectures [25, 16, 20] and extends a neurobiological model of motion processing in the visual cortex [ 10 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, the model of [ 10 ] is too simple to deal with real videos."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "speed ( p = 3) and a higher speed ( p = 6). The constant q, which controls the width of the tuning curve, was set to q = 2 as in [ 10 , 4]. Taking all the possible combinations of p and p, we obtained 8 types of S1 units."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Interestingly, we find that the optical flow features previously used in [ 10 , 4, 28] lead to worse performance than the gradient-based features and the space-time oriented filters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15558063,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "ef4df0277debc4fe468b3920da795d102f0325b6",
            "isKey": false,
            "numCitedBy": 863,
            "numCiting": 178,
            "paperAbstract": {
                "fragments": [],
                "text": "The visual recognition of complex movements and actions is crucial for the survival of many species. It is important not only for communication and recognition at a distance, but also for the learning of complex motor actions by imitation. Movement recognition has been studied in psychophysical, neurophysiological and imaging experiments, and several cortical areas involved in it have been identified. We use a neurophysiologically plausible and quantitative model as a tool for organizing and making sense of the experimental data, despite their growing size and complexity. We review the main experimental findings and discuss possible neural mechanisms, and show that a learning-based, feedforward model provides a neurophysiologically plausible and consistent summary of many key experimental results."
            },
            "slug": "Cognitive-neuroscience:-Neural-mechanisms-for-the-Giese-Poggio",
            "title": {
                "fragments": [],
                "text": "Cognitive neuroscience: Neural mechanisms for the recognition of biological movements"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A learning-based, feedforward model provides a neurophysiologically plausible and consistent summary of many key experimental results, and is used as a tool for organizing and making sense of the experimental data, despite their growing size and complexity."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Reviews Neuroscience"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116003860"
                        ],
                        "name": "J. Bergen",
                        "slug": "J.-Bergen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 184
                            }
                        ],
                        "text": "The energy model was built from two space-time separable filters whose spatial responses are 2D Gabor functions and temporal responses are based on psychophysical experimental results [1, 56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5248006,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "33ce6c2f2d5128be710fb3ddd8f1117758b9b4a9",
            "isKey": false,
            "numCitedBy": 3532,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "A motion sequence may be represented as a single pattern in x-y-t space; a velocity of motion corresponds to a three-dimensional orientation in this space. Motion sinformation can be extracted by a system that responds to the oriented spatiotemporal energy. We discuss a class of models for human motion mechanisms in which the first stage consists of linear filters that are oriented in space-time and tuned in spatial frequency. The outputs of quadrature pairs of such filters are squared and summed to give a measure of motion energy. These responses are then fed into an opponent stage. Energy models can be built from elements that are consistent with known physiology and psychophysics, and they permit a qualitative understanding of a variety of motion phenomena."
            },
            "slug": "Spatiotemporal-energy-models-for-the-perception-of-Adelson-Bergen",
            "title": {
                "fragments": [],
                "text": "Spatiotemporal energy models for the perception of motion."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A class of models for human motion mechanisms in which the first stage consists of linear filters that are oriented in space-time and tuned in spatial frequency that permit a qualitative understanding of a variety of motion phenomena."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9200530"
                        ],
                        "name": "Juan Carlos Niebles",
                        "slug": "Juan-Carlos-Niebles",
                        "structuredName": {
                            "firstName": "Juan Carlos",
                            "lastName": "Niebles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Carlos Niebles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 249
                            }
                        ],
                        "text": "It has also been suggested [10] that another part of the visual system, the ventral stream of the visual cortex, involved with the analysis of shape may also be important for the recognition of motion (consistent with recent work in computer vision [17] which has shown the benefit of using shape features in addition to motion features for the recognition of actions)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9213242,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a5fe226444b2ca367ac9ac54b5b4113023fd404",
            "isKey": false,
            "numCitedBy": 485,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel model for human action categorization. A video sequence is represented as a collection of spatial and spatial-temporal features by extracting static and dynamic interest points. We propose a hierarchical model that can be characterized as a constellation of bags-of-features and that is able to combine both spatial and spatial-temporal features. Given a novel video sequence, the model is able to categorize human actions in a frame-by-frame basis. We test the model on a publicly available human action dataset [2] and show that our new method performs well on the classification task. We also conducted control experiments to show that the use of the proposed mixture of hierarchical models improves the classification performance over bag of feature models. An additional experiment shows that using both dynamic and static features provides a richer representation of human actions when compared to the use of a single feature type, as demonstrated by our evaluation in the classification task."
            },
            "slug": "A-Hierarchical-Model-of-Shape-and-Appearance-for-Niebles-Fei-Fei",
            "title": {
                "fragments": [],
                "text": "A Hierarchical Model of Shape and Appearance for Human Action Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A hierarchical model that can be characterized as a constellation of bags-of-features and that is able to combine both spatial and spatial-temporal features is proposed and shown to improve the classification performance over bag of feature models."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4855179"
                        ],
                        "name": "B. J. Geesaman",
                        "slug": "B.-J.-Geesaman",
                        "structuredName": {
                            "firstName": "Bard",
                            "lastName": "Geesaman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. J. Geesaman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075438986"
                        ],
                        "name": "R. Andersen",
                        "slug": "R.-Andersen",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Andersen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Andersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "substantial position and scale invariance and [22,  20 ], and respond to large flow field"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2478282,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "4a37ce9757a1216bdb81c7cdb3dd9621a6721e89",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Several groups have proposed that area MSTd of the macaque monkey has a role in processing optical flow information used in the analysis of self motion, based on its neurons\u2019 selectivity for large-field motion patterns such as expansion, contraction, and rotation. It has also been suggested that this cortical region may be important in analyzing the complex motions of objects. More generally, MSTd could be involved in the generic function of complex motion pattern representation, with its cells responsible for integrating local motion signals sent forward from area MT into a more unified representation. If MSTd is extracting generic motion pattern signals, it would be important that the preferred tuning of MSTd neurons not depend on the particular features and cues that allow these motions to be represented. To test this idea, we examined the diversity of stimulus features and cues over which MSTd cells can extract information about motion patterns such as expansion, contraction, rotation, and spirals. The different classes of stimuli included: coherently moving random dot patterns, solid squares, outlines of squares, a square aperture moving in front of an underlying stationary pattern of random dots, a square composed entirely of flicker, and a square of nonFourier motion. When a unit was tuned with respect to motion patterns across these stimulus classes, the motion pattern producing the most vigorous response in a neuron was nearly the same for each class. Although preferred tuning was invariant, the magnitude and width of the tuning curves often varied between classes. Thus, MSTd is form/cue invariant for complex motions, making it an appropriate candidate for analysis of object motion as well as motion introduced by observer translation."
            },
            "slug": "The-Analysis-of-Complex-Motion-Patterns-by-Form/Cue-Geesaman-Andersen",
            "title": {
                "fragments": [],
                "text": "The Analysis of Complex Motion Patterns by Form/Cue Invariant MSTd Neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work examined the diversity of stimulus features and cues over which MSTd cells can extract information about motion patterns such as expansion, contraction, rotation, and spirals and found that form/cue invariant for complex motions is an appropriate candidate for analysis of object motion."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2160697"
                        ],
                        "name": "R. C. Emerson",
                        "slug": "R.-C.-Emerson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Emerson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Emerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116003860"
                        ],
                        "name": "J. Bergen",
                        "slug": "J.-Bergen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 151
                            }
                        ],
                        "text": "The pooling mechanism has been widely used to model V1 complex cells: some work computed the V1 complex cells as a linear summation of V1 simple cells [43, 66, 15], and others computed the V1 complex cells as a local maximum of V1 simple cells [21, 62]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "Following the previous finding that some aspects of complex cells\u2019 responses can be obtained by combining subunits distributed over a localized spatial region [15], they computed the responses of V1 complex cells as a weighted sum of simple cells with the same space-time orientation over a local spatial region."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 22386719,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "20edc1d449e47a937c71e35df845a5816e14d19a",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Directionally-selective-complex-cells-and-the-of-in-Emerson-Bergen",
            "title": {
                "fragments": [],
                "text": "Directionally selective complex cells and the computation of motion energy in cat visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080929103"
                        ],
                        "name": "Claudio Fanti",
                        "slug": "Claudio-Fanti",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Fanti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claudio Fanti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398327241"
                        ],
                        "name": "Lihi Zelnik-Manor",
                        "slug": "Lihi-Zelnik-Manor",
                        "structuredName": {
                            "firstName": "Lihi",
                            "lastName": "Zelnik-Manor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lihi Zelnik-Manor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 196
                            }
                        ],
                        "text": "The other common class of approaches is based on the processing of spatio-temporal features, either global as in the case of low-resolution videos [33, 6, 2] or local for higher resolution images [24, 5, 7, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1765587,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ae55802a906fadd51203ed41793e480ce4c5d23",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic models have been previously shown to be efficient and effective for modeling and recognition of human motion. In particular we focus on methods which represent the human motion model as a triangulated graph. Previous approaches learned models based just on positions and velocities of the body parts while ignoring their appearance. Moreover, a heuristic approach was commonly used to obtain translation invariance. In this paper we suggest an improved approach for learning such models and using them for human motion recognition. The suggested approach combines multiple cues, i.e., positions, velocities and appearance into both the learning and detection phases. Furthermore, we introduce global variables in the model, which can represent global properties such as translation, scale or view-point. The model is learned in an unsupervised manner from unlabelled data. We show that the suggested hybrid probabilistic model (which combines global variables, like translation, with local variables, like relative positions and appearances of body parts), leads to: (i) faster convergence of learning phase, (it) robustness to occlusions, and, (Hi) higher recognition rate."
            },
            "slug": "Hybrid-models-for-human-motion-recognition-Fanti-Zelnik-Manor",
            "title": {
                "fragments": [],
                "text": "Hybrid models for human motion recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper focuses on methods which represent the human motion model as a triangulated graph and introduces global variables in the model, which can represent global properties such as translation, scale or view-point, and shows that the suggested hybrid probabilistic model leads to faster convergence of learning phase and higher recognition rate."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2361660"
                        ],
                        "name": "J. Lange",
                        "slug": "J.-Lange",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Lange",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lange"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805100"
                        ],
                        "name": "M. Lappe",
                        "slug": "M.-Lappe",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Lappe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lappe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "Indeed none of the existing neurobiological models of motion processing have been used on real-world data [10, 13, 4, 28, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10229227,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "0094e110d06884a58f6d7cf5df102a40b230c47c",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Biological motion perception is the compelling ability of the visual system to perceive complex human movements effortlessly and within a fraction of a second. Recent neuroimaging and neurophysiological studies have revealed that the visual perception of biological motion activates a widespread network of brain areas. The superior temporal sulcus has a crucial role within this network. The roles of other areas are less clear. We present a computational model based on neurally plausible assumptions to elucidate the contributions of motion and form signals to biological motion perception and the computations in the underlying brain network. The model simulates receptive fields for images of the static human body, as found by neuroimaging studies, and temporally integrates their responses by leaky integrator neurons. The model reveals a high correlation to data obtained by neurophysiological, neuroimaging, and psychophysical studies."
            },
            "slug": "A-Model-of-Biological-Motion-Perception-from-Form-Lange-Lappe",
            "title": {
                "fragments": [],
                "text": "A Model of Biological Motion Perception from Configural Form Cues"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A computational model based on neurally plausible assumptions is presented to elucidate the contributions of motion and form signals to biological motion perception and the computations in the underlying brain network."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360881"
                        ],
                        "name": "D. Heeger",
                        "slug": "D.-Heeger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heeger",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heeger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "A set of three dimensional Gabor filters were used to extract image flow [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43141585,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8c67519c225112349f6c0e6e45c98cced5f02197",
            "isKey": false,
            "numCitedBy": 670,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A model is presented, consonant with current views regarding the neurophysiology and psychophysics of motion perception, that combines the outputs of a set of spatiotemporal motion-energy filters to extract optical flow. The output velocity is encoded as the peak in a distribution of velocity-tuned units that behave much like cells of the middle temporal area of the primate brain. The model appears to deal with the aperture problem as well as the human visual system since it extracts the correct velocity for patterns that have large differences in contrast at different spatial orientations, and it simulates psychophysical data on the coherence of sine-grating plaid patterns."
            },
            "slug": "Model-for-the-extraction-of-image-flow.-Heeger",
            "title": {
                "fragments": [],
                "text": "Model for the extraction of image flow."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The model appears to deal with the aperture problem as well as the human visual system since it extracts the correct velocity for patterns that have large differences in contrast at different spatial orientations, and it simulates psychophysical data on the coherence of sine-grating plaid patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "While these approaches have been successful for the recognition of actions from articulated objects such as humans (see [9] for a review), they are not expected to be useful in the case of less articulated objects such as rodents [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7788290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4e4b41b6010ac1e6c90791168f57bcd75b696ab",
            "isKey": false,
            "numCitedBy": 2210,
            "numCiting": 145,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to recognize humans and their activities by vision is key for a machine to interact intelligently and effortlessly with a human-inhabited environment. Because of many potentially important applications, \u201clooking at people\u201d is currently one of the most active application domains in computer vision. This survey identifies a number of promising applications and provides an overview of recent developments in this domain. The scope of this survey is limited to work on whole-body or hand motion; it does not include work on human faces. The emphasis is on discussing the various methodologies; they are grouped in 2-D approaches with or without explicit shape models and 3-D approaches. Where appropriate, systems are reviewed. We conclude with some thoughts about future directions."
            },
            "slug": "The-Visual-Analysis-of-Human-Movement:-A-Survey-Gavrila",
            "title": {
                "fragments": [],
                "text": "The Visual Analysis of Human Movement: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A number of promising applications are identified and an overview of recent developments in this domain is provided, including work on whole-body or hand motion and the various methodologies."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996960"
                        ],
                        "name": "M. Riesenhuber",
                        "slug": "M.-Riesenhuber",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Riesenhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riesenhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 118
                            }
                        ],
                        "text": "It extends an earlier neurobiological model of motion processing in the dorsal stream of the visual cortex by Giese & Poggio [10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Here we follow the more recent framework using scale and position invariant C2 features [25, 16] that originated with the work of Riesenhuber & Poggio [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8920227,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "85abadb689897997f1e37baa7b5fc6f7d497518b",
            "isKey": false,
            "numCitedBy": 3318,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function."
            },
            "slug": "Hierarchical-models-of-object-recognition-in-cortex-Riesenhuber-Poggio",
            "title": {
                "fragments": [],
                "text": "Hierarchical models of object recognition in cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions is described."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2034026"
                        ],
                        "name": "N. Majaj",
                        "slug": "N.-Majaj",
                        "structuredName": {
                            "firstName": "Najib",
                            "lastName": "Majaj",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Majaj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3172042"
                        ],
                        "name": "M. Carandini",
                        "slug": "M.-Carandini",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Carandini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Carandini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145707107"
                        ],
                        "name": "J. Movshon",
                        "slug": "J.-Movshon",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Movshon",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movshon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 157
                            }
                        ],
                        "text": ", who presented a plaid containing two gratings with different orientations and moving independently along the direction perpendicular to their orientations [41, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 874177,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "65eb3754da42be6311aa438791667501e74186c6",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Direction-selective neurons in primary visual cortex have small receptive fields that encode the motions of local features. These motions often differ from the motion of the object to which they belong and must therefore be integrated elsewhere. A candidate site for this integration is visual cortical area MT (V5), in which cells with large receptive fields compute the motion of patterns. Previous studies of motion integration in MT have used stimuli that fill the receptive field, and thus do not test whether motion information is really integrated across this whole area. For each MT neuron, we identified two regions (\u201cpatches\u201d) within the receptive field that were approximately equally effective in driving responses. We then measured responses to plaids whose component gratings overlapped within a patch, and compared them with responses to the same component gratings presented in separate patches. Cells that were selective for the direction of motion of the whole pattern when the gratings overlapped lost this selectivity when the gratings were separated and became selective instead for the direction of motion of the individual components. If MT cells simply pooled all of the inputs that endow them with a receptive field, they would encode all of the motions in the receptive field as belonging to a single object. Our results indicate instead that critical elements of the computations underlying pattern-direction selectivity in MT are done locally, on a scale smaller than the whole receptive field."
            },
            "slug": "Motion-Integration-by-Neurons-in-Macaque-MT-Is-Not-Majaj-Carandini",
            "title": {
                "fragments": [],
                "text": "Motion Integration by Neurons in Macaque MT Is Local, Not Global"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Critical elements of the computations underlying pattern-direction selectivity in MT are done locally, on a scale smaller than the whole receptive field, indicating that motion information is really integrated across this whole area."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152138711"
                        ],
                        "name": "J. McLean",
                        "slug": "J.-McLean",
                        "structuredName": {
                            "firstName": "Judith",
                            "lastName": "McLean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McLean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073840837"
                        ],
                        "name": "S. Raab",
                        "slug": "S.-Raab",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Raab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Raab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3459632"
                        ],
                        "name": "L. Palmer",
                        "slug": "L.-Palmer",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Palmer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Palmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "analyzed the three-dimensional first-order properties of simple cells in cat and found two classes of cells [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37831629,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "eaecf8b6471751f8dff2af615f82c901dc6c36b8",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "A reverse correlation technique, which permits estimation of three-dimensional first-order properties of receptive fields (RFs), was applied to simple cells in areas 17 and 18 of cat. Two classes of simple cells were found. For one class, the spatial and temporal RF characteristics were separable, i.e. they could be synthesized as the product of spatial and temporal weighting functions. RFs in the other class were inseparable, i.e. bright and dark subregions comprising each field were obliquely oriented in space-time. Based on a linear superposition model, these observations led to testable hypotheses: (1) simple cells with separable space-time characteristics should be speed but not direction selective and (2) simple cells with inseparable space-time characteristics should be direction selective and the optimal velocity of moving stimuli should be predictable from the slope of the oriented subregions. These hypotheses were tested by comparing responses to moving bars with those predicted by application of the convolution integral. Linear predictions accounted for waveforms of responses to moving bars in detail. For cells with oriented space-time characteristics, the preferred direction was always predicted correctly and the optimal speed was predicted quite well. Most cells with separable space-time characteristics were not direction selective as predicted. The major discrepancies between measured and predicted behavior were twofold. First, 8/32 cells with separable space-time RFs were direction selective. Second, predicted directional indices were weakly correlated with actual measurements. These conclusions hold for simple cells in both areas 17 and 18. The major difference between simple RFs in these areas is the coarser spatial scale seen in area 18. These results demonstrate a significant linear contribution to the speed and direction selectivity of simple cells in areas 17 and 18. Where additional, nonlinear mechanisms are inferred, they appear to act synergistically with the linear mechanism."
            },
            "slug": "Contribution-of-linear-mechanisms-to-the-of-local-McLean-Raab",
            "title": {
                "fragments": [],
                "text": "Contribution of linear mechanisms to the specification of local motion by simple cells in areas 17 and 18 of the cat."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A significant linear contribution to the speed and direction selectivity of simple cells in areas 17 and 18 is demonstrated, where additional, nonlinear mechanisms are inferred and appear to act synergistically with the linear mechanism."
            },
            "venue": {
                "fragments": [],
                "text": "Visual neuroscience"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360881"
                        ],
                        "name": "D. Heeger",
                        "slug": "D.-Heeger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heeger",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 258
                            }
                        ],
                        "text": "\u2026of [10] into a real computer vision system, we altered it in two significant ways: We propose a new set of motion-sensitive units which is shown to perform significantly better and we describe new tuning functions and feature selection techniques which build on recent work on object recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 152
                            }
                        ],
                        "text": "Processing is then hierarchical: feature complexity and position/scale invariance are gradually increased by alternating between a template matching and a max pooling operation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It lacks translation invariance and uses a limited handcrafted dictionary of features in intermediate stages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6873077,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4df17f8bbaf8e84f9ebe88c59f88b24babfac9b3",
            "isKey": true,
            "numCitedBy": 949,
            "numCiting": 166,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-model-of-neuronal-responses-in-visual-area-MT-Simoncelli-Heeger",
            "title": {
                "fragments": [],
                "text": "A model of neuronal responses in visual area MT"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 147
                            }
                        ],
                        "text": "The other common class of approaches is based on the processing of spatio-temporal features, either global as in the case of low-resolution videos [33, 6, 2] or local for higher resolution images [24, 5, 7, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 62
                            }
                        ],
                        "text": "which has been compared favorably to several other approaches [33, 6, 18] on the KTH human and UCSD mice datasets described earlier."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1350374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804d86dd7ab3498266922244e73a88c1add5a6ab",
            "isKey": false,
            "numCitedBy": 1470,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Our goal is to recognize human action at a distance, at resolutions where a whole person may be, say, 30 pixels tall. We introduce a novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure, and an associated similarity measure to be used in a nearest-neighbor framework. Making use of noisy optical flow measurements is the key challenge, which is addressed by treating optical flow not as precise pixel displacements, but rather as a spatial pattern of noisy measurements which are carefully smoothed and aggregated to form our spatiotemporal motion descriptor. To classify the action being performed by a human figure in a query sequence, we retrieve nearest neighbor(s) from a database of stored, annotated video sequences. We can also use these retrieved exemplars to transfer 2D/3D skeletons onto the figures in the query sequence, as well as two forms of data-based action synthesis \"do as I do\" and \"do as I say\". Results are demonstrated on ballet, tennis as well as football datasets."
            },
            "slug": "Recognizing-action-at-a-distance-Efros-Berg",
            "title": {
                "fragments": [],
                "text": "Recognizing action at a distance"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure is introduced, and an associated similarity measure to be used in a nearest-neighbor framework is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913957"
                        ],
                        "name": "N. Rust",
                        "slug": "N.-Rust",
                        "structuredName": {
                            "firstName": "Nicole",
                            "lastName": "Rust",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Rust"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2182796"
                        ],
                        "name": "Valerio Mante",
                        "slug": "Valerio-Mante",
                        "structuredName": {
                            "firstName": "Valerio",
                            "lastName": "Mante",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valerio Mante"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145707107"
                        ],
                        "name": "J. Movshon",
                        "slug": "J.-Movshon",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Movshon",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movshon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 91
                            }
                        ],
                        "text": "The directional tuning curve of V1 neurons is modeled as a circular-Gaussian-like function [6, 21, 57]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 448010,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9a6e9a26e63ae3facc6776c874c05467d28a4c20",
            "isKey": false,
            "numCitedBy": 490,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Neurons in area MT (V5) are selective for the direction of visual motion. In addition, many are selective for the motion of complex patterns independent of the orientation of their components, a behavior not seen in earlier visual areas. We show that the responses of MT cells can be captured by a linear-nonlinear model that operates not on the visual stimulus, but on the afferent responses of a population of nonlinear V1 cells. We fit this cascade model to responses of individual MT neurons and show that it robustly predicts the separately measured responses to gratings and plaids. The model captures the full range of pattern motion selectivity found in MT. Cells that signal pattern motion are distinguished by having convergent excitatory input from V1 cells with a wide range of preferred directions, strong motion opponent suppression and a tuned normalization that may reflect suppressive input from the surround of V1 cells."
            },
            "slug": "How-MT-cells-analyze-the-motion-of-visual-patterns-Rust-Mante",
            "title": {
                "fragments": [],
                "text": "How MT cells analyze the motion of visual patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work shows that the responses of MT cells can be captured by a linear-nonlinear model that operates not on the visual stimulus, but on the afferent responses of a population of nonlinear V1 cells, and robustly predicts the separately measured responses to gratings and plaids."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50170517"
                        ],
                        "name": "M. Blank",
                        "slug": "M.-Blank",
                        "structuredName": {
                            "firstName": "Moshe",
                            "lastName": "Blank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Blank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089071"
                        ],
                        "name": "Lena Gorelick",
                        "slug": "Lena-Gorelick",
                        "structuredName": {
                            "firstName": "Lena",
                            "lastName": "Gorelick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lena Gorelick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760994"
                        ],
                        "name": "R. Basri",
                        "slug": "R.-Basri",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Basri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Basri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 147
                            }
                        ],
                        "text": "The other common class of approaches is based on the processing of spatio-temporal features, either global as in the case of low-resolution videos [33, 6, 2] or local for higher resolution images [24, 5, 7, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "Weizmann human: The Weizmann human action dataset [2] contains eighty-one low resolution (180 \u00d7 144 pixels) video sequences with nine subjects performing nine actions: running, walking, jumping-jack, jumping forward on two legs, jumping in place on two legs, galloping-sideways, waving two hands, waving one hand, and bending."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 175905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a9eb04b9b07d4a58aa78eb9f68a77ade0199fab",
            "isKey": false,
            "numCitedBy": 1704,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Human action in video sequences can be seen as silhouettes of a moving torso and protruding limbs undergoing articulated motion. We regard human actions as three-dimensional shapes induced by the silhouettes in the space-time volume. We adopt a recent approach by Gorelick et al. (2004) for analyzing 2D shapes and generalize it to deal with volumetric space-time action shapes. Our method utilizes properties of the solution to the Poisson equation to extract space-time features such as local space-time saliency, action dynamics, shape structure and orientation. We show that these features are useful for action recognition, detection and clustering. The method is fast, does not require video alignment and is applicable in (but not limited to) many scenarios where the background is known. Moreover, we demonstrate the robustness of our method to partial occlusions, non-rigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action and low quality video"
            },
            "slug": "Actions-as-space-time-shapes-Blank-Gorelick",
            "title": {
                "fragments": [],
                "text": "Actions as space-time shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The method is fast, does not require video alignment and is applicable in many scenarios where the background is known, and the robustness of the method is demonstrated to partial occlusions, non-rigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action and low quality video."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4129050"
                        ],
                        "name": "Yoshihisa Ninokura",
                        "slug": "Yoshihisa-Ninokura",
                        "structuredName": {
                            "firstName": "Yoshihisa",
                            "lastName": "Ninokura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihisa Ninokura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47024242"
                        ],
                        "name": "H. Mushiake",
                        "slug": "H.-Mushiake",
                        "structuredName": {
                            "firstName": "Hajime",
                            "lastName": "Mushiake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mushiake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2436887"
                        ],
                        "name": "J. Tanji",
                        "slug": "J.-Tanji",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Tanji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tanji"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "might be found at dierent locations in the visual cortex such as STS, temporal cortex and prefrontal cortex [24, 69,  47 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2253990,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "d6ee7797fd0213f6fab82e9978db27b75e2bc70b",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Generation of information about the temporal order of events is essential for the control of memory-based behavioral tasks. We studied cellular activity in the lateral prefrontal cortex (LPFC) in two monkeys that were required to remember the temporal order in which visual objects were presented. In this report, we focus on cellular activity in response to the sequential appearance of three different objects. We identified cells that responded selectively to physical properties (color and shape) of objects (23%) in the ventral part of the LPFC and cells for which activity was selective for the numerical position (rank order) of objects (44%) in the dorsal part of the LPFC. We also identified cells for which activity was selective for both the physical properties and rank order of objects (30%). The third type of cells, distributed in the ventral LPFC, seems of importance in integrating the two categories of information, i.e., physical and temporal information about the occurrences of objects to construct sequential order information. Furthermore, we identified a distinct group of cells that exhibited selectivity for the sequence of presentation of the three objects. Our findings suggest that LPFC cells are involved in encoding temporal sequences of events when such information is required for planning forthcoming motor behavior."
            },
            "slug": "Integration-of-temporal-order-and-object-in-the-Ninokura-Mushiake",
            "title": {
                "fragments": [],
                "text": "Integration of temporal order and object information in the monkey lateral prefrontal cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The findings suggest that LPFC cells are involved in encoding temporal sequences of events when such information is required for planning forthcoming motor behavior."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39009279"
                        ],
                        "name": "J. Maunsell",
                        "slug": "J.-Maunsell",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Maunsell",
                            "middleNames": [
                                "H.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Maunsell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1392063491"
                        ],
                        "name": "D. van Essen",
                        "slug": "D.-van-Essen",
                        "structuredName": {
                            "firstName": "D. C.",
                            "lastName": "van Essen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. van Essen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 69
                            }
                        ],
                        "text": "The neurons in MT and MST are tuned to speed and direction of motion [37, 2, 31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 125
                            }
                        ],
                        "text": "The cells in this area inherit the direction and speed tuning properties from their direct afferent inputs, V1 complex cells [42, 3, 37, 39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8708245,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "0bb3df8cfca9f04bc5ad21cd9851603a7a1fb31f",
            "isKey": false,
            "numCitedBy": 1502,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Recordings were made from single units in the middle temporal visual area (MT) of anesthetized, paralyzed macaque monkeys. A computer-driven stimulator was used to make quantitative tests of selectivity for stimulus direction, speed, and orientation. The data were taken from 168 units that were histologically identified as being in MT. 2. The results confirm previous reports of a high degree of direction selectivity in MT. The response above background to stimuli moving in a unit's preferred direction was, an average, 10.9 times that to stimuli moving in the opposite direction. There was a marked tendency for nearby units to have similar preferred directions. 3. Most units were also sharply tuned for the speed of stimulus motion. For some cells the response fell to less than half-maximal at speeds only a factor of two from the optimum; on average, responses were greater than half-maximal only over a 7.7-fold range of speed. The distribution of preferred speeds for different units was unimodal, with a peak near 32 degrees/s; the total range of preferred speeds extended from 2 to 256 degrees/s. Nearby units generally responded best to similar speeds of motion. 4. Most units in MT showed selectivity for stimulus orientation when tested with stationary, flashed bars. However, stationary stimuli generally elicited only brief responses; when averaged over the duration of the stimulus, the responses were much less than those to moving stimuli. The preferred orientation was usually, but not always, perpendicular to the preferred direction of movement. 5. A comparison of the results of the present study with a previous quantitative investigation in the owl monkey shows a striking similarity in response properties in MT of the two species. 6. The presence of both direction and speed selectivity in MT of the macaque suggests that this area is more specialized for the analysis of visual motion than has been previously recognized."
            },
            "slug": "Functional-properties-of-neurons-in-middle-temporal-Maunsell-Essen",
            "title": {
                "fragments": [],
                "text": "Functional properties of neurons in middle temporal visual area of the macaque monkey. I. Selectivity for stimulus direction, speed, and orientation."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The presence of both direction and speed selectivity in MT of the macaque suggests that this area is more specialized for the analysis of visual motion than has been previously recognized."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053178525"
                        ],
                        "name": "G. Johansson",
                        "slug": "G.-Johansson",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "Johansson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Johansson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "been tested on simple artificial stimuli such as point-light motion stimuli [ 27 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 54046837,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "58ea2fa0580b2117618be6e1cc9658a5c9531dba",
            "isKey": false,
            "numCitedBy": 4094,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports the first phase of a research program on visual perception of motion patterns characteristic of living organisms in locomotion. Such motion patterns in animals and men are termed here as biological motion. They are characterized by a far higher degree of complexity than the patterns of simple mechanical motions usually studied in our laboratories. In everyday perceptions, the visual information from biological motion and from the corresponding figurative contour patterns (the shape of the body) are intermingled. A method for studying information from the motion pattern per se without interference with the form aspect was devised. In short, the motion of the living body was represented by a few bright spots describing the motions of the main joints. It is found that 10\u201312 such elements in adequate motion combinations in proximal stimulus evoke a compelling impression of human walking, running, dancing, etc. The kinetic-geometric model for visual vector analysis originally developed in the study of perception of motion combinations of the mechanical type was applied to these biological motion patterns. The validity of this model in the present context was experimentally tested and the results turned out to be highly positive."
            },
            "slug": "Visual-perception-of-biological-motion-and-a-model-Johansson",
            "title": {
                "fragments": [],
                "text": "Visual perception of biological motion and a model for its analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The kinetic-geometric model for visual vector analysis originally developed in the study of perception of motion combinations of the mechanical type was applied to biological motion patterns and the results turned out to be highly positive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1857939"
                        ],
                        "name": "G. DeAngelis",
                        "slug": "G.-DeAngelis",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "DeAngelis",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. DeAngelis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821490"
                        ],
                        "name": "I. Ohzawa",
                        "slug": "I.-Ohzawa",
                        "structuredName": {
                            "firstName": "Izumi",
                            "lastName": "Ohzawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Ohzawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2374159"
                        ],
                        "name": "R. Freeman",
                        "slug": "R.-Freeman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Freeman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freeman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 89
                            }
                        ],
                        "text": "Both simple and complex cells are sensitive to direction of motion and spatial frequency [9, 39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23042178,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b386d1034abc0d6d8ea892151aa398c6a9814e2e",
            "isKey": false,
            "numCitedBy": 462,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "1. We have tested the hypothesis that simple cells in the cat's visual cortex perform a linear spatiotemporal filtering of the visual image. To conduct this study we note that a visual neuron behaves linearly if the responses to small, brief flashes of light are mathematically related, via the Fourier transform, to the responses elicited by sinusoidal grating stimuli. 2. We have evaluated the linearity of temporal and spatial summation for 118 simple cells recorded from the striate cortex (area 17) of adult cats and kittens at ages 4 and 8 wk postnatal. These neurons represent a subset of the population of cells for which we have described the postnatal development of spatiotemporal receptive-field structure in the preceding paper. Spatiotemporal receptive-field profiles are constructed, with the use of a reverse correlation technique, from the responses to random sequences of small bar stimuli that are brighter or darker than the background. Fourier analysis of spatiotemporal receptive-field profiles yields linear predictions of the cells' spatial and temporal frequency tuning. These predicted responses are compared with spatial and temporal frequency tuning curves measured by the use of drifting, sinusoidal-luminance grating stimuli. 3. For most simple cells, there is good agreement between spatial and temporal frequency tuning curves predicted from the receptive-field profile and those measured by the use of sinusoidal gratings. These results suggest that both spatial and temporal summation within simple cells are approximately linear. There is a tendency for predicted tuning curves to be slightly broader than measured tuning curves, a finding that is consistent with the effects of a threshold nonlinearity at the output of these neurons. In some cases, however, predicted tuning curves deviate from measured responses only at low spatial and temporal frequencies. This cannot be explained by a simple threshold nonlinearity. 4. If linearity is assumed, it should be possible to predict the direction selectivity of simple cells from the structure of their spatiotemporal receptive-field profiles. For virtually all cells, linear predictions correctly determine the preferred direction of motion of a visual stimulus. However, the strength of the directional bias is typically underestimated by a factor of about two on the basis of linear predictions. Consideration of the expansive exponential nonlinearity revealed in the contrast-response function permits a reconciliation of the discrepancy between measured and predicted direction selectivity indexes. 5. Overall, these findings show that spatiotemporal receptive-field profiles obtained with the use of reverse correlation may be used to predict a variety of response properties for simple cells.(ABSTRACT TRUNCATED AT 400 WORDS)"
            },
            "slug": "Spatiotemporal-organization-of-simple-cell-fields-DeAngelis-Ohzawa",
            "title": {
                "fragments": [],
                "text": "Spatiotemporal organization of simple-cell receptive fields in the cat's striate cortex. II. Linearity of temporal and spatial summation."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Results suggest that both spatial and temporal summation within simple cells are approximately linear, and show that spatiotemporal receptive-field profiles obtained with the use of reverse correlation may be used to predict a variety of response properties for simple cells."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4012237"
                        ],
                        "name": "E. Grossman",
                        "slug": "E.-Grossman",
                        "structuredName": {
                            "firstName": "Emily",
                            "lastName": "Grossman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Grossman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144043541"
                        ],
                        "name": "R. Blake",
                        "slug": "R.-Blake",
                        "structuredName": {
                            "firstName": "Randolph",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Blake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "might be found at dierent locations in the visual cortex such as STS, temporal cortex and prefrontal cortex [ 24 , 69, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14169352,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "dcbad05503e8345d0770268880f5965bd1f8f7a1",
            "isKey": false,
            "numCitedBy": 643,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Brain-Areas-Active-during-Visual-Perception-of-Grossman-Blake",
            "title": {
                "fragments": [],
                "text": "Brain Areas Active during Visual Perception of Biological Motion"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "These systems have been around for quiet some time now, starting with the work of Fukushima [8] and LeCun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 82
                            }
                        ],
                        "text": "These systems have been around for quiet some time now, starting with the work of Fukushima [8] and LeCun et al. [12]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206775608,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "69e68bfaadf2dccff800158749f5a50fe82d173b",
            "isKey": false,
            "numCitedBy": 3718,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern."
            },
            "slug": "Neocognitron:-A-self-organizing-neural-network-for-Fukushima",
            "title": {
                "fragments": [],
                "text": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A neural network model for a mechanism of visual pattern recognition that is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity of their shapes without affected by their positions."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70878937"
                        ],
                        "name": "E. Mingolla",
                        "slug": "E.-Mingolla",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Mingolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Mingolla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2654928"
                        ],
                        "name": "C. Pack",
                        "slug": "C.-Pack",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Pack",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 312,
                                "start": 308
                            }
                        ],
                        "text": "proposed a V1-MT-MST neural model to explain the flow-field pattern sensitivity of MST cells by combining well-known neural mechanisms: log polar cortical magnification, Gaussian motion-sensitive receptive fields, spatial pooling of motion-sensitive signals and subtractive extraretinal eye movement signals [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "7A in [22]), presumably from the particular combination of multiple MT afferent cells [58, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "in [23]) This formulation transforms the spiral motion in a cartesian coordinate into a oblique linear motion in a log-polar coordinate in the cortex, therefore MST cells\u2019 flow-field selectivity simply results from local spatial summation of MT cells with the same directional preferences, rather than from complex and specialized interactions as the template model in [58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 222
                            }
                        ],
                        "text": "The difference is that S2 units respond to classdependent prototypes extracted from the training data, while MST neurons respond to patterns with general structures such as circular, radial, spiral or translational motion [23, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 466742,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "663621a2dd2e5edff11b84570bbdf1245596bfd9",
            "isKey": true,
            "numCitedBy": 88,
            "numCiting": 209,
            "paperAbstract": {
                "fragments": [],
                "text": "Cells in the dorsal medial superior temporal cortex (MSTd) process optic flow generated by self-motion during visually guided navigation. A neural model shows how interactions between well-known neural mechanisms (log polar cortical magnification, Gaussian motion-sensitive receptive fields, spatial pooling of motion-sensitive signals and subtractive extraretinal eye movement signals) lead to emergent properties that quantitatively simulate neurophysiological data about MSTd cell properties and psychophysical data about human navigation. Model cells match MSTd neuron responses to optic flow stimuli placed in different parts of the visual field, including position invariance, tuning curves, preferred spiral directions, direction reversals, average response curves and preferred locations for stimulus motion centers. The model shows how the preferred motion direction of the most active MSTd cells can explain human judgments of self-motion direction (heading), without using complex heading templates. The model explains when extraretinal eye movement signals are needed for accurate heading perception, and when retinal input is sufficient, and how heading judgments depend on scene layouts and rotation rates."
            },
            "slug": "A-neural-model-of-motion-processing-and-visual-by-Grossberg-Mingolla",
            "title": {
                "fragments": [],
                "text": "A neural model of motion processing and visual navigation by cortical area MST."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A neural model shows how the preferred motion direction of the most active MSTd cells can explain human judgments of self-motion direction (heading), without using complex heading templates, and explains how heading judgments depend on scene layouts and rotation rates."
            },
            "venue": {
                "fragments": [],
                "text": "Cerebral cortex"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1857939"
                        ],
                        "name": "G. DeAngelis",
                        "slug": "G.-DeAngelis",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "DeAngelis",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. DeAngelis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821490"
                        ],
                        "name": "I. Ohzawa",
                        "slug": "I.-Ohzawa",
                        "structuredName": {
                            "firstName": "Izumi",
                            "lastName": "Ohzawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Ohzawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2374159"
                        ],
                        "name": "R. Freeman",
                        "slug": "R.-Freeman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Freeman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "receptive field are tilted in space-time domain (the two classes were also reported in [10])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12827601,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "32a1f849d426a39d4d97fe44a78e0797916048e0",
            "isKey": false,
            "numCitedBy": 462,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Receptive-field-dynamics-in-the-central-visual-DeAngelis-Ohzawa",
            "title": {
                "fragments": [],
                "text": "Receptive-field dynamics in the central visual pathways"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Neurosciences"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89655629"
                        ],
                        "name": "H. Saito",
                        "slug": "H.-Saito",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Saito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Saito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4892169"
                        ],
                        "name": "M. Yukie",
                        "slug": "M.-Yukie",
                        "structuredName": {
                            "firstName": "Masao",
                            "lastName": "Yukie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yukie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157875742"
                        ],
                        "name": "K. Tanaka",
                        "slug": "K.-Tanaka",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48900541"
                        ],
                        "name": "K. Hikosaka",
                        "slug": "K.-Hikosaka",
                        "structuredName": {
                            "firstName": "Kazuo",
                            "lastName": "Hikosaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hikosaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46562774"
                        ],
                        "name": "Y. Fukada",
                        "slug": "Y.-Fukada",
                        "structuredName": {
                            "firstName": "Yoshiro",
                            "lastName": "Fukada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Fukada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4121674"
                        ],
                        "name": "E. Iwai",
                        "slug": "E.-Iwai",
                        "structuredName": {
                            "firstName": "Eiichi",
                            "lastName": "Iwai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Iwai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "7A in [22]), presumably from the particular combination of multiple MT afferent cells [58, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 373,
                                "start": 369
                            }
                        ],
                        "text": "in [23]) This formulation transforms the spiral motion in a cartesian coordinate into a oblique linear motion in a log-polar coordinate in the cortex, therefore MST cells\u2019 flow-field selectivity simply results from local spatial summation of MT cells with the same directional preferences, rather than from complex and specialized interactions as the template model in [58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14986154,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "379d916a7fa2320f3dfe4e0da277eb05487475a1",
            "isKey": false,
            "numCitedBy": 694,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "Using anesthetized and paralyzed monkeys, we have studied the visual response properties of neurons in the cortical area surrounding the middle temporal area (MT) in the superior temporal sulcus (STS). Systematic electrode penetrations revealed that there is a functionally distinct region where three classes of directionally selective cells with large receptive fields cluster. This region is anteriorly adjoined to the dorsal two-thirds of MT, has a width of 4\u20135 mm mediolaterally, and therefore may correspond to the dorsal part of the medial superior temporal area (MST), which was previously defined as a MT-recipient zone. One class of cells responded to a straight movement of patterns in the frontoparallel plane with directional selectivity (D cells: 217/422, 51.4%). The second class of cells selectively responded to an expanding or contracting size change of patterns (S cells: 66/422, 15.7%). These cells responded neither to a change in width of a slit of any orientation or any length, nor to a change in brightness. The third class of cells responded only to a rotation of patterns in one direction (R cells: 58/422, 13.7%). A majority of these cells (41/58) responded to the clockwise or counterclockwise rotation of patterns in the frontoparallel plane (Rf cells), while the rest responded to a rotation of patterns in depth (Rd cells). We will suggest that these cells acquire the ability to discover whole events of visual motion-- i.e., unidirectional straight movement, size change (radial movement), and rotation--by integrating elemental motion information extracted by MT cells. The receptive fields of D, S, and Rf cells can be constructed by converging signals of MT cells, the preferred directions of which are arranged in parallel (D cells), radially (S cells), and circularly (Rf cells). The receptive fields of Rd cells can be constructed, in turn, by the convergence of signals of S cells."
            },
            "slug": "Integration-of-direction-signals-of-image-motion-in-Saito-Yukie",
            "title": {
                "fragments": [],
                "text": "Integration of direction signals of image motion in the superior temporal sulcus of the macaque monkey"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is suggested that cells in the cortical area surrounding the middle temporal area (MT) in the superior temporal sulcus (STS) acquire the ability to discover whole events of visual motion by integrating elemental motion information extracted by MT cells."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3235030"
                        ],
                        "name": "J. Decety",
                        "slug": "J.-Decety",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Decety",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Decety"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2582927"
                        ],
                        "name": "J. Gr\u00e8zes",
                        "slug": "J.-Gr\u00e8zes",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Gr\u00e8zes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gr\u00e8zes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 135
                            }
                        ],
                        "text": "Several electrophysiological or fMRI studies have shown that there exist neurons in STS that respond selectively to biological motions [24, 11, 47, 55]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6824074,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "5f6dcba11ef31ddf35b573916ff10a8a097541dd",
            "isKey": false,
            "numCitedBy": 816,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-mechanisms-subserving-the-perception-of-Decety-Gr\u00e8zes",
            "title": {
                "fragments": [],
                "text": "Neural mechanisms subserving the perception of human actions"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9200530"
                        ],
                        "name": "Juan Carlos Niebles",
                        "slug": "Juan-Carlos-Niebles",
                        "structuredName": {
                            "firstName": "Juan Carlos",
                            "lastName": "Niebles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Carlos Niebles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3254319"
                        ],
                        "name": "Hongcheng Wang",
                        "slug": "Hongcheng-Wang",
                        "structuredName": {
                            "firstName": "Hongcheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongcheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 196
                            }
                        ],
                        "text": "The other common class of approaches is based on the processing of spatio-temporal features, either global as in the case of low-resolution videos [33, 6, 2] or local for higher resolution images [24, 5, 7, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 62
                            }
                        ],
                        "text": "which has been compared favorably to several other approaches [33, 6, 18] on the KTH human and UCSD mice datasets described earlier."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 40046466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e25d2a9aa691e63657fef30f5850799d757f69e6",
            "isKey": false,
            "numCitedBy": 981,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel unsupervised learning method for human action categories. A video sequence is represented as a collection of spatial-temporal words by extracting space-time interest points. The algorithm automatically learns the probability distributions of the spatial-temporal words and the intermediate topics corresponding to human action categories. This is achieved by using latent topic models such as the probabilistic Latent Semantic Analysis (pLSA) model and Latent Dirichlet Allocation (LDA). Our approach can handle noisy feature points arisen from dynamic background and moving cameras due to the application of the probabilistic models. Given a novel video sequence, the algorithm can categorize and localize the human action(s) contained in the video. We test our algorithm on three challenging datasets: the KTH human motion dataset, the Weizmann human action dataset, and a recent dataset of figure skating actions. Our results reflect the promise of such a simple approach. In addition, our algorithm can recognize and localize multiple actions in long and complex video sequences containing multiple motions."
            },
            "slug": "Unsupervised-Learning-of-Human-Action-Categories-Niebles-Wang",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Human Action Categories Using Spatial-Temporal Words"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A novel unsupervised learning method for human action categories that can recognize and localize multiple actions in long and complex video sequences containing multiple motions."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49685714"
                        ],
                        "name": "Jason Lee",
                        "slug": "Jason-Lee",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389262"
                        ],
                        "name": "W. Wong",
                        "slug": "W.-Wong",
                        "structuredName": {
                            "firstName": "Willy",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Wong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "Indeed none of the existing neurobiological models of motion processing have been used on real-world data [10, 13, 4, 28, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3151487,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9c830ba987c8cc75ed2edae3253b5afa72bcaddf",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "A computational model is presented for the detection of coherent motion based on template matching and hidden Markov models. The premise of this approach is that the growth in detection sensitivity is greater for coherent motion of structured forms than for random coherent motion. In this preliminary study, a recent experiment was simulated with the model and the results are shown to be in agreement with the above premise. This model can be extended to be part of a more complex and elaborate computational visual system."
            },
            "slug": "A-stochastic-model-for-the-detection-of-coherent-Lee-Wong",
            "title": {
                "fragments": [],
                "text": "A stochastic model for the detection of coherent motion"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A computational model is presented for the detection of coherent motion based on template matching and hidden Markov models that shows that the growth in detection sensitivity is greater for coherent motion of structured forms than for random coherent motion."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330398"
                        ],
                        "name": "N. Priebe",
                        "slug": "N.-Priebe",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Priebe",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Priebe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852557"
                        ],
                        "name": "C. Cassanello",
                        "slug": "C.-Cassanello",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Cassanello",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cassanello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2486944"
                        ],
                        "name": "S. Lisberger",
                        "slug": "S.-Lisberger",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Lisberger",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lisberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "Using more complex moving patterns, pattern-sensitive cells are shown to be insensitive to the exact shape of the moving stimulus [50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14709207,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "be5b4fb3dcff78d6f0e673bd407bbbbc989f556b",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Tuning for speed is one key feature of motion-selective neurons in the middle temporal visual area of the macaque cortex (MT, or V5). The present paper asks whether speed is coded in a way that is invariant to the shape of the moving stimulus, and if so, how. When tested with single sine-wave gratings of different spatial and temporal frequencies, MT neurons show a continuum in the degree to which preferred speed depends on spatial frequency. There is some dependence in 75% of MT neurons, and the other 25% maintain speed tuning despite changes in spatial frequency. When tested with stimuli constructed by adding two superimposed sine-wave gratings, the preferred speed of MT neurons becomes less dependent on spatial frequency. Analysis of these responses reveals a speed-tuning nonlinearity that selectively enhances the responses of the neuron when multiple spatial frequencies are present and moving at the same speed. Consistent with the presence of the nonlinearity, MT neurons show speed tuning that is close to form-invariant when the moving stimuli comprise square-wave gratings, which contain multiple spatial frequencies moving at the same speed. We conclude that the neural circuitry in and before MT makes no explicit attempt to render MT neurons speed-tuned for sine-wave gratings, which do not occur in natural scenes. Instead, MT neurons derive form-invariant speed tuning in a way that takes advantage of the multiple spatial frequencies that comprise moving objects in natural scenes."
            },
            "slug": "The-Neural-Representation-of-Speed-in-Macaque-Area-Priebe-Cassanello",
            "title": {
                "fragments": [],
                "text": "The Neural Representation of Speed in Macaque Area MT/V5"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is concluded that the neural circuitry in and before MT makes no explicit attempt to render MT neurons speed-tuned for sine-wave gratings, which do not occur in natural scenes, and MT neurons derive form-invariant speed tuning in a way that takes advantage of the multiple spatial frequencies that comprise moving objects in natural Scenes."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90841478"
                        ],
                        "name": "Y-Lan Boureau",
                        "slug": "Y-Lan-Boureau",
                        "structuredName": {
                            "firstName": "Y-Lan",
                            "lastName": "Boureau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y-Lan Boureau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "In addition, they also showed that applying a simple feature selection technique to the C2 feature responses can lead to an efficient system which can perform better with less features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "Motivated by the recent success of biologically inspired approaches for the recognition of objects in real-world applications [25, 16, 20], we here extend a neurobiological\n978-1-4244-1631-8/07/$25.00 \u00a92007 IEEE\nmodel [10] of motion processing in the dorsal stream of the visual cortex."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Conversely applying biological models to realworld scenarios should help constrain plausible algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11398758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccd52aff02b0f902f4ce7247c4fee7273014c41c",
            "isKey": true,
            "numCitedBy": 1089,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples."
            },
            "slug": "Unsupervised-Learning-of-Invariant-Feature-with-to-Ranzato-Huang",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions that alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152286443"
                        ],
                        "name": "Andrew T. Smith",
                        "slug": "Andrew-T.-Smith",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Smith",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew T. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5678528"
                        ],
                        "name": "R. Snowden",
                        "slug": "R.-Snowden",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Snowden",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Snowden"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 45
                            }
                        ],
                        "text": "Area MST receives its input from the MT area [71, 67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118368782,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "47ecce28fc7987ffea272926387f5ac70287bc22",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 Introduction: motion detection - an overview. Part 2 Principles of local motion detection: computational and neural constraints for the measurement of local visual motion motion processing in the primate cerebral cortex the psychophysics of motion perception. Part 3 Inputs to local motion detectors: motion detector models - psychophysical evidence the detection of second-order motion physiological studies of motion inputs. Part 4 Integration of motion signals: models of two-dimensional motion perception visual motion integration - a neurophysiological and psychophysical perspective spatial integration of local motion signals. Part 5 High-order interpretation of motion: optic and retinal flow motion-in-depth structure from motion. Part 5 motion and detection and eye movements: visual motion caused by movements of the eye, head, and body the visual drive for smooth eye movements."
            },
            "slug": "Visual-detection-of-motion-Smith-Snowden",
            "title": {
                "fragments": [],
                "text": "Visual detection of motion"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work focuses on the detection and analysis of visual motion caused by movements of the eye, head, and body the visual drive for smooth eye movements and the integration of local motion signals."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716904"
                        ],
                        "name": "J. Koenderink",
                        "slug": "J.-Koenderink",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Koenderink",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koenderink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7977369"
                        ],
                        "name": "A. Doorn",
                        "slug": "A.-Doorn",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Doorn",
                            "middleNames": [
                                "J.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doorn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 211
                            }
                        ],
                        "text": "Space-time-oriented S1 units: Most studies focus on the spatial structure of receptive fields: V1 simple cells\u2019 receptive field profiles were modeled by twodimensional Gabors or Gaussian-derivative functions in [28, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 24284500,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "00aa5220d49f3fcf357c1b64ac14f24cd8afb76d",
            "isKey": false,
            "numCitedBy": 626,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that a convolution with certain reasonable receptive field (RF) profiles yields the exact partial derivatives of the retinal illuminance blurred to a specified degree. Arbitrary concatenations of such RF profiles yield again similar ones of higher order and for a greater degree of blurring.By replacing the illuminance with its third order jet extension we obtain position dependent geometries. It is shown how such a representation can function as the substrate for \u201cpoint processors\u201d computing geometrical features such as edge curvature. We obtain a clear dichotomy between local and multilocal visual routines. The terms of the truncated Taylor series representing the jets are partial derivatives whose corresponding RF profiles closely mimic the well known units in the primary visual cortex. Hence this description provides a novel means to understand and classify these units.Taking the receptive field outputs as the basic input data one may devise visual routines that compute geometric features on the basis of standard differential geometry exploiting the equivalence with the local jets (partial derivatives with respect to the space coordinates)."
            },
            "slug": "Representation-of-local-geometry-in-the-visual-Koenderink-Doorn",
            "title": {
                "fragments": [],
                "text": "Representation of local geometry in the visual system"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "It is shown that a convolution with certain reasonable receptive field (RF) profiles yields the exact partial derivatives of the retinal illuminance blurred to a specified degree and how this representation can function as the substrate for \u201cpoint processors\u201d computing geometrical features such as edge curvature."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762649"
                        ],
                        "name": "V. Rabaud",
                        "slug": "V.-Rabaud",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Rabaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Rabaud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524603"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 230
                            }
                        ],
                        "text": "While these approaches have been successful for the recognition of actions from articulated objects such as humans (see [9] for a review), they are not expected to be useful in the case of less articulated objects such as rodents [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "The authors would like to thank Piotr Dollar for providing the code for the benchmark system [5] in Section 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "UCSD mice: The UCSD mice behavior dataset [5] contains seven subsets, each being recorded at different points in a day such that multiple occurrences of actions within each subset vary substantially."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 196
                            }
                        ],
                        "text": "The other common class of approaches is based on the processing of spatio-temporal features, either global as in the case of low-resolution videos [33, 6, 2] or local for higher resolution images [24, 5, 7, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "The code for [5] was graciously provided by Piotr Dollar."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1956774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f1707caad72573633c2307fa26ec093e8f4bb03",
            "isKey": true,
            "numCitedBy": 2717,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A common trend in object recognition is to detect and leverage the use of sparse, informative feature points. The use of such features makes the problem more manageable while providing increased robustness to noise and pose variation. In this work we develop an extension of these ideas to the spatio-temporal case. For this purpose, we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and we propose an alternative. Anchoring off of these interest points, we devise a recognition algorithm based on spatio-temporally windowed data. We present recognition results on a variety of datasets including both human and rodent behavior."
            },
            "slug": "Behavior-recognition-via-sparse-spatio-temporal-Doll\u00e1r-Rabaud",
            "title": {
                "fragments": [],
                "text": "Behavior recognition via sparse spatio-temporal features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and an alternative is proposed, and a recognition algorithm based on spatio-temporally windowed data is devised."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145707107"
                        ],
                        "name": "J. Movshon",
                        "slug": "J.-Movshon",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Movshon",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movshon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1935795"
                        ],
                        "name": "I. Thompson",
                        "slug": "I.-Thompson",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Thompson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Thompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34941108"
                        ],
                        "name": "D. Tolhurst",
                        "slug": "D.-Tolhurst",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tolhurst",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tolhurst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 151
                            }
                        ],
                        "text": "The pooling mechanism has been widely used to model V1 complex cells: some work computed the V1 complex cells as a linear summation of V1 simple cells [43, 66, 15], and others computed the V1 complex cells as a local maximum of V1 simple cells [21, 62]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5659259,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d435143b4a7b2d7c56a696504e87db39157c3449",
            "isKey": false,
            "numCitedBy": 963,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "1. We have examined the responses of simple cells in the cat's atriate cortex to visual patterns that were designed to reveal the extent to which these cells may be considered to sum light\u2010evoked influences linearly across their receptive fields. We used one\u2010dimensional luminance\u2010modulated bars and grating as stimuli; their orientation was always the same as the preferred orientation of the neurone under study. The stimuli were presented on an oscilloscope screen by a digital computer, which also accumulated neuronal responses and controlled a randomized sequence of stimulus presentations. 2. The majority of simple cells respond to sinusoidal gratings that are moving or whose contrast is modulated in time in a manner consistent with the hypothesis that they have linear spatial summation. Their responses to moving gratings of all spatial frequencies are modulated in synchrony with the passage of the gratings' bars across their receptive fields, and they do not produce unmodulated responses even at the highest spatial frequencies. Many of these cells respond to temporally modulated stationary gratings simply by changing their response amplitude sinusoidally as the spatial phase of the grating the grating is varied. Nonetheless, their behavior appears to indicate linear spatial summation, since we show in an Appendix that the absence of a 'null' phase in a visual neurone need not indicate non\u2010linear spatial summation, and further that a linear neurone lacking a 'null' phase should give responses of the form that we have observed in this type of simple cell. 3. A minority of simple cells appears to have significant non\u2010linearities of spatial summation. These neurones respond to moving gratings of high spatial frequency with a partially or totally unmodulated elevation of firing rate. They have no 'null' phases when tested with stationary gratings, and reveal their non\u2010linearity by giving responses to gratings of some spatial phases that are composed partly or wholly of even harmonics of the stimulus frequency ('on\u2010off' responses). 4. We compared simple receptive fields with their sensitivity to sinusoidal gratings of different spatial frequencies. Qualitatively, the most sensitive subregions of simple cells' receptive fields are roughly the same width as the individual bars of the gratings to which they are most sensitive. Quantitatively, their receptive field profiles measured with thin stationary lines, agree well with predicted profiles derived by Fourier synthesis of their spatial frequency tuning curves."
            },
            "slug": "Spatial-summation-in-the-receptive-fields-of-simple-Movshon-Thompson",
            "title": {
                "fragments": [],
                "text": "Spatial summation in the receptive fields of simple cells in the cat's striate cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The responses of simple cells in the cat's atriate cortex to visual patterns that were designed to reveal the extent to which these cells may be considered to sum light\u2010evoked influences linearly across their receptive fields are examined."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2712738"
                        ],
                        "name": "C. Sch\u00fcldt",
                        "slug": "C.-Sch\u00fcldt",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Sch\u00fcldt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sch\u00fcldt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3033284"
                        ],
                        "name": "B. Caputo",
                        "slug": "B.-Caputo",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Caputo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caputo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "KTH human : The KTH human action dataset [24] contains six types of human actions: walking, jogging, running, boxing, hand waving and hand clapping."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 196
                            }
                        ],
                        "text": "The other common class of approaches is based on the processing of spatio-temporal features, either global as in the case of low-resolution videos [33, 6, 2] or local for higher resolution images [24, 5, 7, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8777811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b786e478cf0be6fcfaeb7812e25da85523236855",
            "isKey": false,
            "numCitedBy": 3080,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Local space-time features capture local events in video and can be adapted to the size, the frequency and the velocity of moving patterns. In this paper, we demonstrate how such features can be used for recognizing complex motion patterns. We construct video representations in terms of local space-time features and integrate such representations with SVM classification schemes for recognition. For the purpose of evaluation we introduce a new video database containing 2391 sequences of six human actions performed by 25 people in four different scenarios. The presented results of action recognition justify the proposed method and demonstrate its advantage compared to other relative approaches for action recognition."
            },
            "slug": "Recognizing-human-actions:-a-local-SVM-approach-Sch\u00fcldt-Laptev",
            "title": {
                "fragments": [],
                "text": "Recognizing human actions: a local SVM approach"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper construct video representations in terms of local space-time features and integrate such representations with SVM classification schemes for recognition and presents the presented results of action recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3821632"
                        ],
                        "name": "J. Perrone",
                        "slug": "J.-Perrone",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Perrone",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Perrone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Perrone proposed a mechanism to explain the speed tuning of MT cells by investigating their properties in the frequency domain [49]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1513097,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "efc0f3d85063dff8f61aba471bf0d2d987e66e4e",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A recent study by Priebe et al., (2006) has shown that a small proportion (27%) of primate directionally selective, complex V1 neurons are tuned for the speed of image motion. In this study, I show that the weighted intersection mechanism (WIM) model, which was previously proposed to explain speed tuning in middle temporal neurons, can also explain the tuning found in complex V1 neurons. With the addition of a contrast gain mechanism, this model is able to replicate the effects of contrast on V1 speed tuning, a phenomenon that was recently discovered by Priebe et al., (2006). The WIM model simulations also indicate that V1 neuron spatiotemporal frequency response maps may be asymmetrical in shape and hence poorly characterized by the symmetrical two-dimensional Gaussian fitting function used by Priebe et al., (2006) to classify their cells. Therefore, the actual proportion of speed tuning among directional complex V1 cells may be higher than the 27% estimate suggested by these authors."
            },
            "slug": "A-Single-Mechanism-Can-Explain-the-Speed-Tuning-of-Perrone",
            "title": {
                "fragments": [],
                "text": "A Single Mechanism Can Explain the Speed Tuning Properties of MT and V1 Complex Neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The weighted intersection mechanism (WIM) model, which was previously proposed to explain speed tuning in middle temporal neurons, can also explain the tuning found in complex V1 neurons and is able to replicate the effects of contrast on V1 speed tuning."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31660891"
                        ],
                        "name": "A. Mikami",
                        "slug": "A.-Mikami",
                        "structuredName": {
                            "firstName": "Akichika",
                            "lastName": "Mikami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mikami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067110828"
                        ],
                        "name": "W. Newsome",
                        "slug": "W.-Newsome",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Newsome",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Newsome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735519269"
                        ],
                        "name": "R. Wurtz",
                        "slug": "R.-Wurtz",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Wurtz",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wurtz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7754284,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d7f5c37ca39d73cc1042387d5c7bdf64b2bc9b96",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We measured the spatial and temporal limits of directional interactions for 105 directionally selective middle temporal (MT) neurons and 26 directionally selective striate (V1) neurons. Directional interactions were measured using sequentially flashed stimuli in which the spatial and temporal intervals between stimuli were systematically varied over a broad range. A direction index was employed to determine the strength of directional interactions for each combination of spatial and temporal intervals tested. The maximum spatial interval for which directional interactions occurred in a particular neuron was positively correlated with receptive-field size and with retinal eccentricity in both MT and V1. The maximum spatial interval was, on average, three times as large in MT as in V1. The maximum temporal interval for which we obtained directional interactions was similar in MT and V1 and did not vary with receptive-field size or eccentricity. The maximum spatial interval for directional interactions as measured with flashed stimuli was positively correlated with the maximum speed of smooth motion that yielded directional responses. MT neurons were directionally selective for higher speeds than were V1 neurons. These observations indicate that the large receptive fields found in MT permit directional interactions over longer distances than do the more limited receptive fields of V1 neurons. A functional advantage is thereby conferred on MT neurons because they detect directional differences for higher speeds than do V1 neurons. Recent psychophysical studies have measured the spatial and temporal limits for the perception of apparent motion in sequentially flashed visual displays. A comparison of the psychophysical results with our physiological data indicates that the spatiotemporal limits for perception are similar to the limits for direction selectivity in MT neurons but differ markedly from those for V1 neurons. These observations suggest a correspondence between neuronal responses in MT and the short-range process of apparent motion."
            },
            "slug": "Motion-selectivity-in-macaque-visual-cortex.-II.-of-Mikami-Newsome",
            "title": {
                "fragments": [],
                "text": "Motion selectivity in macaque visual cortex. II. Spatiotemporal range of directional interactions in MT and V1."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The spatiotemporal limits for perception are similar to the limits for direction selectivity in MT neurons but differ markedly from those for V1 neurons, suggesting a correspondence between neuronal responses in MT and the short-range process of apparent motion."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772264"
                        ],
                        "name": "E. Schwartz",
                        "slug": "E.-Schwartz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Schwartz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "The property can be modeled by transforming the visual information in a cartesian coordinate in the retina into a log-polar coordinate in V1 [60]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11324692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "98948396c3fcdc4f681af95d4d26413047a0b60b",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In previous work, it was suggested that the sequence regularity property of cortical neurons could be accounted for if the local geometric structure of the cortex were a recapituation of the global complex logarithmic structure of the retinotopic mapping. This model is developed in detail: the excitatory and inhibitory structure of cortical receptive fields may be approximated by a complex logarithmic local geometry, coupled with an intra-cortical lateral inhibition operator which may flow unidirectionally yet still create \u201crotating\u201d receptive field structure. The direction of intra-cortical lateral inhibition follows the borders of cortical ocular dominance columns, which are the approximate images under the global complex logarithmic mapping, of exponentially spaced, horizontal straight lines in the visual field. Two different topological structures are discussed for the local cortical manifold. The binocular trigger features of cortical neurons follow from the same geometric model, and the ratio of binocular to monocular cortical cells is related to the size and shape of cortical dendritic tree's by an application of integral geometry. Recent results in optical pattern recognition are cited to suggest that the rotation and size invariant properties of the cortical map are essential to any cross-correlational basis for stereopsis. Finally, a meromorphic function is presented which is both locally and globally complex logarithmic in its structure, and therefore represents the model presented in this and previous papers in a concise mathematical form. This function is closely related to the description of a Karman vortex pattern, in fluid mechanics, and leads to the suggestion that the boundary conditions of layer IV of the cortex (i.e. periodic ocular dominance columns) are causally related to the existence of sequence regularity in the cortex. The developmental implications of this statement are that the specification of neural connections in the cortex may follow directly, both locally and globally, from the detailed nature of the cortical boundary conditions (i.e. anatomy), coupled with general physico-mathematical considerations of continuity and differentiability in the neural fiber flow."
            },
            "slug": "Afferent-geometry-in-the-primate-visual-cortex-and-Schwartz",
            "title": {
                "fragments": [],
                "text": "Afferent geometry in the primate visual cortex and the generation of neuronal trigger features"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This model is developed in detail: the excitatory and inhibitory structure of cortical receptive fields may be approximated by a complex logarithmic local geometry, coupled with an intra-cortical lateral inhibition operator which may flow unidirectionally yet still create \u201crotating\u201d receptive field structure."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1867246"
                        ],
                        "name": "L. Lagae",
                        "slug": "L.-Lagae",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Lagae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lagae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48472440"
                        ],
                        "name": "H. Maes",
                        "slug": "H.-Maes",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Maes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Maes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5178732"
                        ],
                        "name": "S. Raiguel",
                        "slug": "S.-Raiguel",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Raiguel",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Raiguel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93240872"
                        ],
                        "name": "D. Xiao",
                        "slug": "D.-Xiao",
                        "structuredName": {
                            "firstName": "D",
                            "lastName": "Xiao",
                            "middleNames": [
                                "K"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1826285"
                        ],
                        "name": "G. Orban",
                        "slug": "G.-Orban",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Orban",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orban"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 211
                            }
                        ],
                        "text": "Similarly to MT pattern cells, MST cells respond to the moving stimulus regardless of the form [20], but opposed to the MT cells that respond to the position of moving stimulus, MST cells are position invariant [30, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 25760566,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a8496e629846d9e0f4de89802679faedc05f0491",
            "isKey": false,
            "numCitedBy": 302,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. We recorded and tested quantitatively 65 middle temporal (MT) and 82 middle superior temporal (MST) cells in paralyzed and anesthetized monkeys. 2. Responses to the three elementary optic flow components (EFCs)--rotation, deformation, and expansion/contraction--and to translation (in the display) were compared after optimization of stimulus direction, speed, size, and position. As a control responses to flicker were measured. 3. Response windows were adapted in correspondence with our finding that latencies of MT and MST cells decrease with increasing speed for all types of motion. 4. There was a response continuum in MT as well as in MST cells. Compared with translation, MST cells responded significantly more to rotation but less to flicker than MT cells. MST cells were significantly more direction selective for expansion/contraction than MT cells. 5. MST cells generally responded to fewer motion types than MT cells. 6. Position invariance of EFC direction selectivity was tested over a region of the visual field centered on the translation receptive field (RF). Direction selectivity for an EFC was not position invariant in MT cells but it was invariant in 40% of the MST cells tested. These cells were considered EFC selective. 7. Most EFC-selective MST cells were selective for a single EFC, possibly combined with translation. Few of them were selective for deformation. 8. EFC selectivity was also speed invariant and EFC-selective MST cells usually had RFs summating inputs over wide portions of the visual field. 9. EFC-selective MST cells with similar selectivities were clustered."
            },
            "slug": "Responses-of-macaque-STS-neurons-to-optic-flow-a-of-Lagae-Maes",
            "title": {
                "fragments": [],
                "text": "Responses of macaque STS neurons to optic flow components: a comparison of areas MT and MST."
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Responses to the three elementary optic flow components (EFCs)--rotation, deformation, and expansion/contraction--and to translation were compared after optimization of stimulus direction, speed, size, and position."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107584303"
                        ],
                        "name": "J. P. Jones",
                        "slug": "J.-P.-Jones",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Jones",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. P. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3459632"
                        ],
                        "name": "L. Palmer",
                        "slug": "L.-Palmer",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Palmer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Palmer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 211
                            }
                        ],
                        "text": "Space-time-oriented S1 units: Most studies focus on the spatial structure of receptive fields: V1 simple cells\u2019 receptive field profiles were modeled by twodimensional Gabors or Gaussian-derivative functions in [28, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16809045,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "0dbf797d5b34f40d16eeadfa7a5b4543c2af2c11",
            "isKey": false,
            "numCitedBy": 1709,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Using the two-dimensional (2D) spatial and spectral response profiles described in the previous two reports, we test Daugman's generalization of Marcelja's hypothesis that simple receptive fields belong to a class of linear spatial filters analogous to those described by Gabor and referred to here as 2D Gabor filters. 2. In the space domain, we found 2D Gabor filters that fit the 2D spatial response profile of each simple cell in the least-squared error sense (with a simplex algorithm), and we show that the residual error is devoid of spatial structure and statistically indistinguishable from random error. 3. Although a rigorous statistical approach was not possible with our spectral data, we also found a Gabor function that fit the 2D spectral response profile of each simple cell and observed that the residual errors are everywhere small and unstructured. 4. As an assay of spatial linearity in two dimensions, on which the applicability of Gabor theory is dependent, we compare the filter parameters estimated from the independent 2D spatial and spectral measurements described above. Estimates of most parameters from the two domains are highly correlated, indicating that assumptions about spatial linearity are valid. 5. Finally, we show that the functional form of the 2D Gabor filter provides a concise mathematical expression, which incorporates the important spatial characteristics of simple receptive fields demonstrated in the previous two reports. Prominent here are 1) Cartesian separable spatial response profiles, 2) spatial receptive fields with staggered subregion placement, 3) Cartesian separable spectral response profiles, 4) spectral response profiles with axes of symmetry not including the origin, and 5) the uniform distribution of spatial phase angles. 6. We conclude that the Gabor function provides a useful and reasonably accurate description of most spatial aspects of simple receptive fields. Thus it seems that an optimal strategy has evolved for sampling images simultaneously in the 2D spatial and spatial frequency domains."
            },
            "slug": "An-evaluation-of-the-two-dimensional-Gabor-filter-Jones-Palmer",
            "title": {
                "fragments": [],
                "text": "An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It seems that an optimal strategy has evolved for sampling images simultaneously in the 2D spatial and spatial frequency domains and the Gabor function provides a useful and reasonably accurate description of most spatial aspects of simple receptive fields."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330398"
                        ],
                        "name": "N. Priebe",
                        "slug": "N.-Priebe",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Priebe",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Priebe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2486944"
                        ],
                        "name": "S. Lisberger",
                        "slug": "S.-Lisberger",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Lisberger",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lisberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145707107"
                        ],
                        "name": "J. Movshon",
                        "slug": "J.-Movshon",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Movshon",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movshon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "In addition, complex cells were recently found to be sensitive to the speed of the moving stimulus [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 853080,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e3c8e8e3d9ba857307642171cba98107c55cb967",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We recorded the responses of direction-selective simple and complex cells in the primary visual cortex (V1) of anesthetized, paralyzed macaque monkeys. When studied with sine-wave gratings, almost all simple cells in V1 had responses that were separable for spatial and temporal frequency: the preferred temporal frequency did not change and preferred speed decreased as a function of the spatial frequency of the grating. As in previous recordings from the middle temporal visual area (MT), approximately one-quarter of V1 complex cells had separable responses to spatial and temporal frequency, and one-quarter were \u201cspeed tuned\u201d in the sense that preferred speed did not change as a function of spatial frequency. Half fell between these two extremes. Reducing the contrast of the gratings caused the population of V1 complex cells to become more separable in their tuning for spatial and temporal frequency. Contrast dependence is explained by the contrast gain of the neurons, which was relatively higher for gratings that were either both of high or both of low temporal and spatial frequency. For stimuli that comprised two spatially superimposed sine-wave gratings, the preferred speeds and tuning bandwidths of V1 neurons could be predicted from the sum of the responses to the component gratings presented alone, unlike neurons in MT that showed nonlinear interactions. We conclude that spatiotemporal modulation of contrast gain creates speed tuning from separable inputs in V1 complex cells. Speed tuning in MT could be primarily inherited from V1, but processing that occurs after V1 and possibly within MT computes selective combinations of speed-tuned signals of special relevance for downstream perceptual and motor mechanisms."
            },
            "slug": "Tuning-for-Spatiotemporal-Frequency-and-Speed-in-of-Priebe-Lisberger",
            "title": {
                "fragments": [],
                "text": "Tuning for Spatiotemporal Frequency and Speed in Directionally Selective Neurons of Macaque Striate Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Spatiotemporal modulation of contrast gain creates speed tuning from separable inputs in V1 complex cells, which could be primarily inherited from V1, but processing that occurs after V1 and possibly within MT computes selective combinations of speed-tuned signals of special relevance for downstream perceptual and motor mechanisms."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31660891"
                        ],
                        "name": "A. Mikami",
                        "slug": "A.-Mikami",
                        "structuredName": {
                            "firstName": "Akichika",
                            "lastName": "Mikami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mikami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067110828"
                        ],
                        "name": "W. Newsome",
                        "slug": "W.-Newsome",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Newsome",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Newsome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735519269"
                        ],
                        "name": "R. Wurtz",
                        "slug": "R.-Wurtz",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Wurtz",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wurtz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 89
                            }
                        ],
                        "text": "Both simple and complex cells are sensitive to direction of motion and spatial frequency [9, 39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 125
                            }
                        ],
                        "text": "The cells in this area inherit the direction and speed tuning properties from their direct afferent inputs, V1 complex cells [42, 3, 37, 39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14982773,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "ec1715cce1df5e7c9d4a19a5123ecc33ad9a4f6c",
            "isKey": false,
            "numCitedBy": 422,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Mechanisms of direction selectivity and speed selectivity were studied in single neurons of the middle temporal visual area (MT) of behaving macaque monkeys. Visual stimuli were presented in both smooth and stroboscopic motion within a neuron's receptive field as the monkey fixated a stationary point of light. Direction selectivity, speed selectivity, and the spontaneous discharge characteristics of MT neurons in behaving monkeys were similar to those reported in previous studies in anesthetized monkeys. Stroboscopic motion stimuli were sequences of flashes characterized by the spatial and temporal intervals between each flash. The spatial and temporal intervals were systematically varied so that suppressive and facilitatory interactions could be studied in both the preferred and null directions. Suppression and facilitation were measured by subtracting the peak discharge rate elicited by a single flash from the peak discharge rate elicited by a stroboscopic train of flashes. The dominant mechanism of direction selectivity in MT was a pronounced suppression of discharge for motion in the null direction which we interpreted as inhibition. The inhibition was sufficiently potent to abolish the responses to single flashed stimuli when they were embedded in a series of flashes in the null direction, and it frequently reduced the neuronal discharge to a level below the spontaneous firing rate. Facilitation in the preferred direction was a prominent feature of the responses of some, but not all, MT neurons. The peak discharge rate for stroboscopic motion in the preferred direction was more than twice the peak rate to a single flash for approximately 50% of the neurons in our sample. The direction selectivity of most MT neurons showed the effects of both inhibitory and facilitatory mechanisms, and it was not possible to segregate MT neurons into distinct groups on the basis of these measures. Suppressive mechanisms contributed to speed tuning as well as direction tuning. The low-speed cutoff for motion in the preferred direction resulted from suppression in 82% of the neurons tested. The high-speed cutoff resulted from suppression in 32% of the neurons tested. The latter mechanism appeared to be distinct from the inhibitory mechanism which acted in the null direction in that large spatial intervals were required for its activation."
            },
            "slug": "Motion-selectivity-in-macaque-visual-cortex.-I.-of-Mikami-Newsome",
            "title": {
                "fragments": [],
                "text": "Motion selectivity in macaque visual cortex. I. Mechanisms of direction and speed selectivity in extrastriate area MT."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The direction selectivity of most MT neurons showed the effects of both inhibitory and facilitatory mechanisms, and it was not possible to segregate MT neurons into distinct groups on the basis of these measures."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88900034"
                        ],
                        "name": "P. Daniel",
                        "slug": "P.-Daniel",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Daniel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Daniel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2837738"
                        ],
                        "name": "D. Whitteridge",
                        "slug": "D.-Whitteridge",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Whitteridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Whitteridge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "The mapping of visual information from retina to V1 obeys a cortical magnification, meaning the cortical resolution gradually increases from periphery to fovea [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29375114,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "831efb7246ac22a24c41570c7741ed1204fe2919",
            "isKey": false,
            "numCitedBy": 1107,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "On the basis of his extensive and elegant anatomical investigations on the visual cortex, Poliak (1932) suggested that a mathematical projection of the retina on the cerebral cortex must exist. Talbot & Marshall (1941) used physiological methods to map the central part of the visual field on to the posterolateral surface ofthe cortex in the monkey. They devised an index of cortical representation expressed as the increment of the angle, measured radially from the centre of gaze, which is represented on each millimetre of cortex. We have confirmed their observations and have extended the mapping to the buried visual cortex in the horizontal and vertical calcarine fissures. We have preferred to use the reciprocal of their index and to call it the cortical magnification, M. When this is measured along radii and at right angles to them, it provides the empirical quantitative relation which Polyak wanted. It also defines the shape and size of the visual receptive field. We have made such a surface, folded it and compared it with the calcarine cortex of the monkey. A preliminary account of this work has been published (Daniel & Whitteridge, 1959)."
            },
            "slug": "The-representation-of-the-visual-field-on-the-in-Daniel-Whitteridge",
            "title": {
                "fragments": [],
                "text": "The representation of the visual field on the cerebral cortex in monkeys"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "On the basis of his extensive and elegant anatomical investigations on the visual cortex, Poliak (1932) suggested that a mathematical projection of the retina on the cerebral cortex must exist and this work has made such a surface, folded it and compared it with the calcarine cortex of the monkey."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1867246"
                        ],
                        "name": "L. Lagae",
                        "slug": "L.-Lagae",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Lagae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lagae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87144753"
                        ],
                        "name": "S. Raiguel",
                        "slug": "S.-Raiguel",
                        "structuredName": {
                            "firstName": "S",
                            "lastName": "Raiguel",
                            "middleNames": [
                                "E"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Raiguel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1826285"
                        ],
                        "name": "G. Orban",
                        "slug": "G.-Orban",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Orban",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orban"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The neurons in MT and MST are tuned to speed and direction of motion [37, 2,  31 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29754866,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ef48c4119b207a24e0de076d66524b0d1240a969",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. We tested quantitatively the responses of 147 middle temporal (MT) cells to light and dark bars moving at different speeds ranging over a 1,000-fold range (0.5-512 deg/s). 2. We derived the following quantities from the speed-response (SR) curves obtained for opposite directions of motion. Speed selectivity was characterized by the maximum response, optimum speed, upper cutoff speed, response to slow movement, and tuning width. Direction selectivity was characterized by the direction index (DI) averaged over speeds yielding significant responses (MDI) and by the direction index at optimal speed (PDI). 3. There was an excellent correlation between speed characteristics for light and dark bars. These correlations were stronger than the correlations between direction indexes. The strongest correlations were obtained for maximum response and upper cutoff. 4. SR curves were classified into three groups: low pass (25%), tuned (43%), and broadband (28%), leaving 4% unclassified. 5. In the majority (75%) of MT cells, there was an agreement between the typology of speed selectivity for light and dark bars. Cells were classified as tuned (33%), low pass (22%), broadband (19%), and mixed (22%), leaving 4% unclassified. In addition to differences in speed characteristics, these groups also differed in response level, direction selectivity, and distribution of preferred directions. 6. For tuned cells, there was a very tight correlation of most speed characteristics for light and dark bars. 7. Direction selectivity depended on stimulus speed in most neurons, yielding a tuned average speed-DI curve. 8. Speed characteristics, proportions of speed selectivity types, and direction selectivity indexes showed little dependence on laminar position. 9. Speed characteristics and direction selectivity indexes were not dependent on eccentricity. Proportion of speed selectivity types however, changed dramatically with eccentricity: low-pass cells dominated foveally, tuned cells parafoveally, and broadband cells peripherally. 10. There were also small eccentricity effects on the range of optimal speeds shown by tuned cells and on the speed at which direction selectivity decreases in the slow speed range."
            },
            "slug": "Speed-and-direction-selectivity-of-macaque-middle-Lagae-Raiguel",
            "title": {
                "fragments": [],
                "text": "Speed and direction selectivity of macaque middle temporal neurons."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "There were small eccentricity effects on the range of optimal speeds shown by tuned cells and on the speed at which direction selectivity decreases in the slow speed range, which was not dependent on eccentricity."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061913309"
                        ],
                        "name": "Katherine Pullen",
                        "slug": "Katherine-Pullen",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Pullen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katherine Pullen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 63
                            }
                        ],
                        "text": "One class of approaches relies on the tracking of object parts [32, 19, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5247338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a42bb12955ca6e544e7d563d2cded64a86dad343",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences. We introduce the use and integration of a mathematical technique, the product of exponential maps and twist motions, into a differential motion estimation. This results in solving simple linear systems, and enables us to recover robustly the kinematic degrees-of-freedom in noise and complex self occluded configurations. A new factorization technique lets us also recover the kinematic chain model itself. We are able to track several human walk cycles, several wallaby hop cycles, and two walk cycels of the famous movements of Eadweard Muybridge's motion studies from the last century. To the best of our knowledge, this is the first computer vision based system that is able to process such challenging footage."
            },
            "slug": "Twist-Based-Acquisition-and-Tracking-of-Animal-and-Bregler-Malik",
            "title": {
                "fragments": [],
                "text": "Twist Based Acquisition and Tracking of Animal and Human Kinematics"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences, and is the first computer vision based system able to process such challenging footage."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554248"
                        ],
                        "name": "T. Albright",
                        "slug": "T.-Albright",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Albright",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Albright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 69
                            }
                        ],
                        "text": "The neurons in MT and MST are tuned to speed and direction of motion [37, 2, 31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18480250,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4854f115e124c5b95ea16afc522918a59f2bf803",
            "isKey": false,
            "numCitedBy": 1069,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "We recorded from single neurons in the middle temporal visual area (MT) of the macaque monkey and studied their direction and orientation selectivity. We also recorded from single striate cortex (V1) neurons in order to make direct comparisons with our observations in area MT. All animals were immobilized and anesthetized with nitrous oxide. Direction selectivity of 110 MT neurons was studied with three types of moving stimuli: slits, single spots, and random-dot fields. All of the MT neurons were found to be directionally selective using one or more of these stimuli. MT neurons exhibited a broad range of direction-tuning bandwidths to all stimuli (minimum = 32 degrees, maximum = 186 degrees, mean = 95 degrees). On average, responses were strongly unidirectional and of similar magnitude for all three stimulus types. Orientation selectivity of 89 MT neurons was studied with stationary flashed slits. Eighty-three percent were found to be orientation selective. Overall, orientation-tuning bandwidths were significantly narrower (mean = 64 degrees) than direction-tuning bandwidths for moving stimuli. Moreover, responses to stationary-oriented stimuli were generally smaller than those to moving stimuli. Direction selectivity of 55 V1 neurons was studied with moving slits; orientation selectivity of 52 V1 neurons was studied with stationary flashed slits. In V1, compared with MT, direction-tuning bandwidths were narrower (mean = 68 degrees). Moreover, V1 responses to moving stimuli were weaker, and bidirectional tuning was more common. The mean orientation-tuning bandwidth in V1 was also significantly narrower than that in MT (mean = 52 degrees), but the responses to stationary-oriented stimuli were of similar magnitude in the two areas. We examined the relationship between optimal direction and optimal orientation for MT neurons and found that 61% had an orientation preference nearly perpendicular to the preferred direction of motion, as is the case for all V1 neurons. However, another 29% of MT neurons had an orientation preference roughly parallel to the preferred direction. These observations, when considered together with recent reports claiming sensitivity of some MT neurons to moving visual patterns (39), suggest specific neural mechanisms underlying pattern-motion sensitivity in area MT. These results support the notion that area MT represents a further specialization over area V1 for stimulus motion processing. Furthermore, the marked similarities between direction and orientation tuning in area MT in macaque and owl monkey support the suggestion that these areas are homologues."
            },
            "slug": "Direction-and-orientation-selectivity-of-neurons-in-Albright",
            "title": {
                "fragments": [],
                "text": "Direction and orientation selectivity of neurons in visual area MT of the macaque."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The notion that area MT represents a further specialization over area V1 for stimulus motion processing is supported and the marked similarities between direction and orientation tuning in area MT in macaque and owl monkey support the suggestion that these areas are homologues."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40098508"
                        ],
                        "name": "M. Graziano",
                        "slug": "M.-Graziano",
                        "structuredName": {
                            "firstName": "Mart\u00edn",
                            "lastName": "Graziano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Graziano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075438986"
                        ],
                        "name": "R. Andersen",
                        "slug": "R.-Andersen",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Andersen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Andersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067036984"
                        ],
                        "name": "R. Snowden",
                        "slug": "R.-Snowden",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Snowden",
                            "middleNames": [
                                "J"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Snowden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "substantial position and scale invariance and [ 22 , 20], and respond to large flow field"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15882396,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c97c12adc6abebc9bb6381b689adddbd9e7013fc",
            "isKey": false,
            "numCitedBy": 524,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Cells in the dorsal division of the medial superior temporal area (MSTd) have large receptive fields and respond to expansion/contraction, rotation, and translation motions. These same motions are generated as we move through the environment, leading investigators to suggest that area MSTd analyzes the optical flow. One influential idea suggests that navigation is achieved by decomposing the optical flow into the separate and discrete channels mentioned above, that is, expansion/contraction, rotation, and translation. We directly tested whether MSTd neurons perform such a decomposition by examining whether there are cells that are preferentially tuned to intermediate spiral motions, which combine both expansion/contraction and rotation components. The finding that many cells in MSTd are preferentially selective for spiral motions indicates that this simple three-channel decomposition hypothesis for MSTd does not appear to be correct. Instead, there is a continuum of patterns to which MSTd cells are selective. In addition, we find that MSTd cells maintain their selectivity when stimuli are moved to different locations in their large receptive fields. This position invariance indicates that MSTd cells selective for expansion cannot give precise information about the retinal location of the focus of expansion. Thus, individual MSTd neurons cannot code, in a precise fashion, the direction of heading by using the location of the focus of expansion. The only way this navigational information could be accurately derived from MSTd is through the use of a coarse, population encoding. Positional invariance and selectivity for a wide array of stimuli suggest that MSTd neurons encode patterns of motion per se, regardless of whether these motions are generated by moving objects or by motion induced by observer locomotion."
            },
            "slug": "Tuning-of-MST-neurons-to-spiral-motions-Graziano-Andersen",
            "title": {
                "fragments": [],
                "text": "Tuning of MST neurons to spiral motions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Positional invariance and selectivity for a wide array of stimuli suggest that MSTd neurons encode patterns of motion per se, regardless of whether these motions are generated by moving objects or by motion induced by observer locomotion."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3205375"
                        ],
                        "name": "T. Lindeberg",
                        "slug": "T.-Lindeberg",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Lindeberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lindeberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "C2 shape features In previous work [25, 16], a still grayvalue input image is first analyzed by an array of Gabor filters (S1 units) at multiple orientations for all positions and scales."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2619278,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f90d79809325d2b78e35a79ecb372407f81b3993",
            "isKey": false,
            "numCitedBy": 2381,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Local image features or interest points provide compact and abstract representations of patterns in an image. We propose to extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for its interpretation. To detect spatio-temporal events, we build on the idea of the Harris and Forstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We then estimate the spatio-temporal extents of the detected events and compute their scale-invariant spatio-temporal descriptors. Using such descriptors, we classify events and construct video representation in terms of labeled space-time points. For the problem of human motion analysis, we illustrate how the proposed method allows for detection of walking people in scenes with occlusions and dynamic backgrounds."
            },
            "slug": "Space-time-interest-points-Laptev-Lindeberg",
            "title": {
                "fragments": [],
                "text": "Space-time interest points"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work builds on the idea of the Harris and Forstner interest point operators and detects local structures in space-time where the image values have significant local variations in both space and time to detect spatio-temporal events."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144043541"
                        ],
                        "name": "R. Blake",
                        "slug": "R.-Blake",
                        "structuredName": {
                            "firstName": "Randolph",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1995854"
                        ],
                        "name": "M. Shiffrar",
                        "slug": "M.-Shiffrar",
                        "structuredName": {
                            "firstName": "Maggie",
                            "lastName": "Shiffrar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shiffrar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "At the same time, our understanding of the brain mechanisms responsible for the recognition of actions has progressed over the past decades (see [1] for a recent review) and a body of experimental data is growing rapidly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5867069,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ccdddae016dcc15643c9cbc2e26da9585737604b",
            "isKey": false,
            "numCitedBy": 783,
            "numCiting": 222,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans, being highly social creatures, rely heavily on the ability to perceive what others are doing and to infer from gestures and expressions what others may be intending to do. These perceptual skills are easily mastered by most, but not all, people, in large part because human action readily communicates intentions and feelings. In recent years, remarkable advances have been made in our understanding of the visual, motoric, and affective influences on perception of human action, as well as in the elucidation of the neural concomitants of perception of human action. This article reviews those advances and, where possible, draws links among those findings."
            },
            "slug": "Perception-of-human-motion.-Blake-Shiffrar",
            "title": {
                "fragments": [],
                "text": "Perception of human motion."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In recent years, remarkable advances have been made in the understanding of the visual, motoric, and affective influences on perception of human action, as well as in the elucidation of the neural concomitants of perception ofhuman action."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of psychology"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1995854"
                        ],
                        "name": "M. Shiffrar",
                        "slug": "M.-Shiffrar",
                        "structuredName": {
                            "firstName": "Maggie",
                            "lastName": "Shiffrar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shiffrar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2683931"
                        ],
                        "name": "J. Freyd",
                        "slug": "J.-Freyd",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Freyd",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Freyd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[27]), to the present feedforward architecture."
                    },
                    "intents": []
                }
            ],
            "corpusId": 145567436,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "34aa2366d7f0f130856ca06f6b38826a8bc793b0",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Observers viewed pairs of alternating photographs of a human body in different positions. Shortest-path motion solutions were pitted against anatomically possible movements. With short stimulus onset asynchronies (SOAs), observers tended to report the shortest path despite violations of anatomical constraints. However, with longer SOAs observers became increasingly likely to report the anatomically possible, but longer, paths. This finding, in conjunction with those from a second study, challenges the accepted wisdom that apparent motion paths are independent of the object. Instead, our findings suggest that when given enough time and appropriate stimuli, the visual system prefers at least some object-appropriate apparent motion paths."
            },
            "slug": "Apparent-Motion-of-the-Human-Body-Shiffrar-Freyd",
            "title": {
                "fragments": [],
                "text": "Apparent Motion of the Human Body"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2385092"
                        ],
                        "name": "C. Gross",
                        "slug": "C.-Gross",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gross",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gross"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "Interestingly, the organization of these two pathways is very similar [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 84940489,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "adeb5268e1be296baeb2c45e8c118b673ea8fc0d",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Information about perception and memory is accumulating rapidly in both basic and clinical neuroscience, and this progress has been made using a variety of approaches while drawing jointly on the traditions of neuroanatomy, neurophysiology, and neuropsychology. In order to disseminate research occurring in leading laboratories around the world, an international symposium on \"Brain Mechanisms of Perception and Memory: From Neuron to Behavior\" was held in Toyama, Japan, in October 1991. Planned in conjunction with this important meeting, this volume presents the work of over 40 eminent scientists from around the world. Their research covers many topics, including such core issues as the perception of form, perception of motion, memory and the limbic system, the neocortex, and neural plasticity. A prominent area of discussion at the symposium, and one which figures prominently in this volume, is work with nonhuman primates, especially useful in the study of perception and memory. The breadth of coverage of this volume in conjunction with its extensive studies of nonhuman primates makes this book a necessary reference for those interested in current perspectives on brain mechanisms of perception and memory. Neuroscientists, neuropsychologists, cognitive and physiological psychologists will find this authoritative, state-of-the-art review important and informative reading."
            },
            "slug": "Brain-Mechanisms-of-Perception-and-Memory:-From-to-Gross",
            "title": {
                "fragments": [],
                "text": "Brain Mechanisms of Perception and Memory: From Neuron to Behavior.Taketoshi Ono , Larry R. Squire , Marcus E. Raichle , David I. Perrett , Masaji Fukuda"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The breadth of coverage of this volume in conjunction with its extensive studies of nonhuman primates makes this book a necessary reference for those interested in current perspectives on brain mechanisms of perception and memory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 63
                            }
                        ],
                        "text": "One class of approaches relies on the tracking of object parts [32, 19, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 702738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a03a9ad60525db2fe6afb29883174bb8ce937360",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a system that can annotate a video sequence with: a description of the appearance of each actor; when the actor is in view; and a representation of the actor's activity while in view. The system does not require a fixed background, and is automatic. The system works by (1) tracking people in 2D and then, using an annotated motion capture dataset, (2) synthesizing an annotated 3D motion sequence matching the 2D tracks. The 3D motion capture data is manually annotated off-line using a class structure that describes everyday motions and allows motion annotations to be composed \u2014 one may jump while running, for example. Descriptions computed from video of real motions show that the method is accurate."
            },
            "slug": "Automatic-Annotation-of-Everyday-Movements-Ramanan-Forsyth",
            "title": {
                "fragments": [],
                "text": "Automatic Annotation of Everyday Movements"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A system that can annotate a video sequence with a description of the appearance of each actor; when the actor is in view; and a representation of the actor's activity while in view is described."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143706693"
                        ],
                        "name": "M. Mishkin",
                        "slug": "M.-Mishkin",
                        "structuredName": {
                            "firstName": "Mortimer",
                            "lastName": "Mishkin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mishkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830863"
                        ],
                        "name": "Leslie G. Ungerleider",
                        "slug": "Leslie-G.-Ungerleider",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Ungerleider",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leslie G. Ungerleider"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6680922"
                        ],
                        "name": "K. Macko",
                        "slug": "K.-Macko",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "Macko",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Macko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "among objects, as well as for the analysis of motion information [72,  40 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Researchers have largely explored the properties of dierent cortical areas and con- nections among them [72,  40 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15565609,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a1430265eb509e214d2bbbb04adc8e87f3589863",
            "isKey": false,
            "numCitedBy": 2573,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Object-vision-and-spatial-vision:-two-cortical-Mishkin-Ungerleider",
            "title": {
                "fragments": [],
                "text": "Object vision and spatial vision: two cortical pathways"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Neurosciences"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Motivated by optical-flow algorithms that are based on the constant-brightness assumption and by recent work on video matching [26], we consider two types of S1 units: |It/(Ix + 1)| and |It/(Iy + 1)|."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6891864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4811a11937448ea5d40a0af36c974e08ab12c08c",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a behavior-based similarity measure which tells us whether two different space-time intensity patterns of two different video segments could have resulted from a similar underlying motion field. This is done directly from the intensity information, without explicitly computing the underlying motions. Such a measure allows us to detect similarity between video segments of differently dressed people performing the same type of activity. It requires no foreground/background segmentation, no prior learning of activities, and no motion estimation or tracking. Using this behavior-based similarity measure, we extend the notion of 2-dimensional image correlation into the 3-dimensional space-time volume, thus allowing to correlate dynamic behaviors and actions. Small space-time video segments (small video clips) are \"correlated\" against entire video sequences in all three dimensions (x,y, and t). Peak correlation values correspond to video locations with similar dynamic behaviors. Our approach can detect very complex behaviors in video sequences (e.g., ballet movements, pool dives, running water), even when multiple complex activities occur simultaneously within the field-of-view of the camera."
            },
            "slug": "Space-time-behavior-based-correlation-Shechtman-Irani",
            "title": {
                "fragments": [],
                "text": "Space-time behavior based correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A behavior-based similarity measure is introduced which tells us whether two different space-time intensity patterns of two different video segments could have resulted from a similar underlying motion field, thus allowing to correlate dynamic behaviors and actions."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460061"
                        ],
                        "name": "G. Rizzolatti",
                        "slug": "G.-Rizzolatti",
                        "structuredName": {
                            "firstName": "Giacomo",
                            "lastName": "Rizzolatti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Rizzolatti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2419400"
                        ],
                        "name": "L. Fogassi",
                        "slug": "L.-Fogassi",
                        "structuredName": {
                            "firstName": "Leonardo",
                            "lastName": "Fogassi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fogassi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2914469"
                        ],
                        "name": "V. Gallese",
                        "slug": "V.-Gallese",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Gallese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Gallese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Finally in higher polysensory areas, one can find neurons which are tuned to short chunks of actions (see [22] for a review)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6792943,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "b43a37ae510fa93c5b3a9253904248450faf8136",
            "isKey": false,
            "numCitedBy": 3034,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "What are the neural bases of action understanding? Although this capacity could merely involve visual analysis of the action, it has been argued that we actually map this visual information onto its motor representation in our nervous system. Here we discuss evidence for the existence of a system, the 'mirror system', that seems to serve this mapping function in primates and humans, and explore its implications for the understanding and imitation of action."
            },
            "slug": "Neurophysiological-mechanisms-underlying-the-and-of-Rizzolatti-Fogassi",
            "title": {
                "fragments": [],
                "text": "Neurophysiological mechanisms underlying the understanding and imitation of action"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Evidence for the existence of a system, the 'mirror system', that seems to serve this mapping function in primates and humans is discussed, and its implications for the understanding and imitation of action are explored."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Reviews Neuroscience"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36155815"
                        ],
                        "name": "J. Robson",
                        "slug": "J.-Robson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Robson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Robson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122257265,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "75fa9076ed59eeaf552e6fe448072d0d5062cd7a",
            "isKey": false,
            "numCitedBy": 852,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "T HE dependence of the form of the spatial contrast-sensitivity function for a square-wave test grating upon the duration of exposure of the target has been investigated by Schober and Hilz. 1 Kelly 2 has pointed out an analogous dependence of the form of the temporal contrast (modulation) sensitivity function upon the angular extent of the test target. The reciprocal nature of these spatio-temporal interactions can be particularly clearly ap\u00ad preciated if the threshold contrast is determined for a grating target whose luminance perpendicular to the bars is given by where m is the contrast, v the spatial frequency, and \u0192 the temporal frequency of the target. FIG. 2. Temporal contrast-sensitivity (reciprocal of threshold contrast) functions for different spatial frequencies. The points are the means of four measurements and the curves (two with dashed low-frequency sections) differ only in their positions along the contrast-sensitivity scale, O 0.5 cycle per degree, \u25cf 4, \u2206 16, \u25b2 22 cycles per degree. FIG. 1. Spatial contrast-sensitivity (reciprocal of threshold contrast) functions for different temporal frequencies. The points are the means of four measurements and the curves (one with a dashed low-frequency section) differ only in their positions along the contrast-sensitivity scale O 1 cycle per second, \u25cf 6, \u2206 16, \u25b2 22 cycles per second. Such a pattern was set up as a display on a cathode-ray oscil\u00ad loscope and Figs. 1 and 2 show the results of threshold-contrast measurements made by the author (a well-corrected myope). Viewing was binocular at a distance of 2 m. The grating pattern subtended 2.5\u00b0\u00d72.5\u00b0 in the center of a 10\u00b0\u00d7 10\u00b0 screen illuminated to the same mean luminance of 20 cd/m 2. The general similarity of the two sets of contrast-sensitivity functions is immediately evident but two features are particularly remarkable. First, the form of the fall-off in sensitivity at high"
            },
            "slug": "Spatial-and-Temporal-Contrast-Sensitivity-Functions-Robson",
            "title": {
                "fragments": [],
                "text": "Spatial and Temporal Contrast-Sensitivity Functions of the Visual System"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4349046"
                        ],
                        "name": "C. Distler",
                        "slug": "C.-Distler",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Distler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Distler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3112410"
                        ],
                        "name": "D. Boussaoud",
                        "slug": "D.-Boussaoud",
                        "structuredName": {
                            "firstName": "Driss",
                            "lastName": "Boussaoud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boussaoud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144375727"
                        ],
                        "name": "R. Desimone",
                        "slug": "R.-Desimone",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Desimone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Desimone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830863"
                        ],
                        "name": "Leslie G. Ungerleider",
                        "slug": "Leslie-G.-Ungerleider",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Ungerleider",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leslie G. Ungerleider"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16542004,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e19680e4e98b19b86550d2133dd685eb3a0fc33b",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "In macaque monkeys, lesions involving the posterior portion of the inferior temporal cortex, cytoarchitectonic area TEO, produce a severe impairment in visual pattern discrimination. Recently, this area has been shown to contain a complete, though coarse, representation of the contralateral visual field (Boussaoud, Desimone, and Ungerleider: J. Comp. Neurol. 306:554\u2013575, '91). Because the inputs and outputs of area TEO have not yet been fully described, we injected a variety of retrograde and anterograde tracers into 11 physiologically identified sites within TEO of seven rhesus monkeys and analyzed the areal and laminar distribution of its cortical connections."
            },
            "slug": "Cortical-connections-of-inferior-temporal-area-TEO-Distler-Boussaoud",
            "title": {
                "fragments": [],
                "text": "Cortical connections of inferior temporal area TEO in macaque monkeys"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "In macaque monkeys, lesions involving the posterior portion of the inferior temporal cortex, cytoarchitectonic area TEO, produce a severe impairment in visual pattern discrimination and the areal and laminar distribution of its cortical connections were analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of comparative neurology"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 63
                            }
                        ],
                        "text": "One class of approaches relies on the tracking of object parts [32, 19, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 438781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef02336545db21d6d994c637f31887cd2de6d1bc",
            "isKey": false,
            "numCitedBy": 463,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A framework for modeling and recognition of temporal activities is proposed. The modeling of sets of exemplar activities is achieved by parameterizing their representation in the form of principal components. Recognition of spatio-temporal variants of modeled activities is achieved by parameterizing the search in the space of admissible transformations that the activities can undergo. Experiments on recognition of articulated and deformable object motion from image motion parameters are presented."
            },
            "slug": "Parameterized-modeling-and-recognition-of-Yacoob-Black",
            "title": {
                "fragments": [],
                "text": "Parameterized Modeling and Recognition of Activities"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments on recognition of articulated and deformable object motion from image motion parameters are presented, and a framework for modeling and recognition of temporal activities is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202245"
                        ],
                        "name": "R. Gattass",
                        "slug": "R.-Gattass",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Gattass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gattass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2385092"
                        ],
                        "name": "C. Gross",
                        "slug": "C.-Gross",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gross",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gross"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "2 Middle Temporal Area (MT) Area MT lies along the posterior bank of the superior temporal sulcus [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17048826,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "eff1a9c334e25da6e74548fdfe6415eb5f013ff5",
            "isKey": false,
            "numCitedBy": 397,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY AND CONCLUSIONS 1. The representation of the visual field in the striate projection zone in the posterior portion of the superior temporal sulcus of the macaque (MT) was mapped with multiunit electrodes. The animals were immobilized and anesthetized and in each animal 25-35 electrode penetrations were typically made over several recording sessions. 2. MT contains a representation of virtually the entire contralateral visual field. The representation of the vertical meridian forms its ventrolateral border and lies near the bottom of the lower bank of the superior temporal sulcus (STS). The representation of the horizontal meridian runs across the floor of STS. The upper field is located ventral and anterior and the lower field dorsal and posterior. The medial border lies at the junction of the floor of STS and its upper bank. 3. MT is similar to striate cortex in being a first-order transformation of the visual field. In both areas, receptive-field size and cortical magnification increase with eccentricity. MT is much smaller than striate cortex and has much larger receptive fields at a given eccentricity and a cruder topography. 4. The results further support the suggestion that MT in the macaque is homologous to visual area MT in New World primates."
            },
            "slug": "Visual-topography-of-striate-projection-zone-(MT)-Gattass-Gross",
            "title": {
                "fragments": [],
                "text": "Visual topography of striate projection zone (MT) in posterior superior temporal sulcus of the macaque."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results further support the suggestion that MT in the macaque is homologous to visual area MT in New World primates and is similar to striate cortex in being a first-order transformation of the visual field."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398327241"
                        ],
                        "name": "Lihi Zelnik-Manor",
                        "slug": "Lihi-Zelnik-Manor",
                        "structuredName": {
                            "firstName": "Lihi",
                            "lastName": "Zelnik-Manor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lihi Zelnik-Manor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 147
                            }
                        ],
                        "text": "The other common class of approaches is based on the processing of spatio-temporal features, either global as in the case of low-resolution videos [33, 6, 2] or local for higher resolution images [24, 5, 7, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 62
                            }
                        ],
                        "text": "which has been compared favorably to several other approaches [33, 6, 18] on the KTH human and UCSD mice datasets described earlier."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "Space-time gradient-based S1 units: These features are based on space and time gradients, which were used for instance, in the system by Zelnik-Manor & Irani [33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59697192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ed88100dad66fe48e19577a04a630552592b685",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Dynamic events can be regarded as long-term temporal objects, which are characterized by spatio-temporal features at multiple temporal scales. Based on this, we design a simple statistical distance measure between video sequences (possibly of different lengths) based on their behavioral content. This measure is non-parametric and can thus handle a wide range of dynamic events. We use this measure for isolating and clustering events within long continuous video sequences. This is done without prior knowledge of the types of events, their models, or their temporal extent. An outcome of such a clustering process is a temporal segmentation of long video sequences into event-consistent sub-sequences, and their grouping into event-consistent clusters. Our event representation and associated distance measure can also be used for event-based indexing into long video sequences, even when only one short example-clip is available. However, when multiple example-clips of the same event are available (either as a result of the clustering process, or given manually), these can be used to refine the event representation, the associated distance measure, and accordingly the quality of the detection and clustering process."
            },
            "slug": "Event-Based-Video-Analysis,-Zelnik-Manor-Irani",
            "title": {
                "fragments": [],
                "text": "Event-Based Video Analysis,"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple statistical distance measure between video sequences based on their behavioral content that can handle a wide range of dynamic events and be used for isolating and clustering events within long continuous video sequences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398327241"
                        ],
                        "name": "Lihi Zelnik-Manor",
                        "slug": "Lihi-Zelnik-Manor",
                        "structuredName": {
                            "firstName": "Lihi",
                            "lastName": "Zelnik-Manor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lihi Zelnik-Manor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "Our approach is closely related to feedforward hierarchical template matching architectures that have been used for the recognition of objects in still images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 86361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cef1a7aab17a1e4c7e7abdc027c7706287d6edad",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Dynamic events can be regarded as long-term temporal objects, which are characterized by spatio-temporal features at multiple temporal scales. Based on this, we design a simple statistical distance measure between video sequences (possibly of different lengths) based on their behavioral content. This measure is non-parametric and can thus handle a wide range of dynamic events. We use this measure for isolating and clustering events within long continuous video sequences. This is done without prior knowledge of the types of events, their models, or their temporal extent. An outcome of such a clustering process is a temporal segmentation of long video sequences into event-consistent sub-sequences, and their grouping into event-consistent clusters. Our event representation and associated distance measure can also be used for event-based indexing into long video sequences, even when only one short example-clip is available. However, when multiple example-clips of the same event are available (either as a result of the clustering process, or given manually), these can be used to refine the event representation, the associated distance measure, and accordingly the quality of the detection and clustering process."
            },
            "slug": "Event-based-analysis-of-video-Zelnik-Manor-Irani",
            "title": {
                "fragments": [],
                "text": "Event-based analysis of video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple statistical distance measure between video sequences based on their behavioral content that can handle a wide range of dynamic events and be used for isolating and clustering events within long continuous video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48535018"
                        ],
                        "name": "J. C. Anderson",
                        "slug": "J.-C.-Anderson",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Anderson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3083749"
                        ],
                        "name": "T. Binzegger",
                        "slug": "T.-Binzegger",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Binzegger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Binzegger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145942574"
                        ],
                        "name": "K. Martin",
                        "slug": "K.-Martin",
                        "structuredName": {
                            "firstName": "Kevan",
                            "lastName": "Martin",
                            "middleNames": [
                                "A.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7025163"
                        ],
                        "name": "K. Rockland",
                        "slug": "K.-Rockland",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "Rockland",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rockland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 125
                            }
                        ],
                        "text": "The cells in this area inherit the direction and speed tuning properties from their direct afferent inputs, V1 complex cells [42, 3, 37, 39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14933558,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c3d6894542d72ac6db93baf89b916e23e3636736",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "Area V5 (middle temporal) in the superior temporal sulcus of macaque receives a direct projection from the primary visual cortex (V1). By injecting anterograde tracers (biotinylated dextran andPhaseolus vulgaris lectin) into V1, we have examined the synaptic boutons that they form in V5 in the electron microscope. Nearly 80% of the target cells in V5 were spiny (excitatory). The boutons formed asymmetric (Gray\u2019s type 1) synapses with spines (54%), dendrites (33%), and somata (13%). All somatic targets and some (26%) of the target dendritic shafts showed features characteristic of smooth (inhibitory) cells. Each bouton formed, on average, 1.7 synapses. The larger boutons formed multiple synapses with the same neuron and completely enveloped the entire spine head. On most dendritic shafts and all somata the postsynaptic density en face was disk-shaped but in about half the cases the reconstructed postsynaptic densities of synapses on spines appeared as complete or partial annuli. Even in the zones of densest innervation only 3% of the asymmetric synapses were formed by the labeled boutons. Although the V1 projection forms only a small minority of synapses in V5, its affect could be considerably amplified by local circuits in V5, in a way analogous to the amplification of the small thalamic input to area V1."
            },
            "slug": "The-Connection-from-Cortical-Area-V1-to-V5:-A-Light-Anderson-Binzegger",
            "title": {
                "fragments": [],
                "text": "The Connection from Cortical Area V1 to V5: A Light and Electron Microscopic Study"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Although the V1 projection forms only a small minority of synapses in V5, its affect could be considerably amplified by local circuits in V 5, in a way analogous to the amplification of the small thalamic input to area V1."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "These systems have been around for quiet some time now, starting with the work of Fukushima [8] and LeCun et al. [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64294544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f42b865e20e61a954239f421b42007236e671f19",
            "isKey": false,
            "numCitedBy": 3516,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), allows such multi-module systems to be trained globally using Gradient-Based methods so as to minimize an overall performance measure. Two systems for on-line handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of Graph Transformer Networks. A Graph Transformer Network for reading bank check is also described. It uses Convolutional Neural Network character recognizers combined with global training techniques to provides record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day."
            },
            "slug": "GradientBased-Learning-Applied-to-Document-Haykin-Kosko",
            "title": {
                "fragments": [],
                "text": "GradientBased Learning Applied to Document Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Various methods applied to handwritten character recognition are reviewed and compared and Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34712076"
                        ],
                        "name": "C. Stauffer",
                        "slug": "C.-Stauffer",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stauffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stauffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719838"
                        ],
                        "name": "W. Grimson",
                        "slug": "W.-Grimson",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Grimson",
                            "middleNames": [
                                "Eric",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Grimson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "We have also experimented with a standard background subtraction technique [68]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "Preprocessing We preprocessed the datasets to speed up our experiments: for the KTH human and UCSD mice datasets we used the openCV GMM background subtraction technique based on [68]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8195115,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eac7287d7ef69252358c1fbddedf123e11012370",
            "isKey": false,
            "numCitedBy": 7637,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A common method for real-time segmentation of moving regions in image sequences involves \"background subtraction\", or thresholding the error between an estimate of the image without moving objects and the current image. The numerous approaches to this problem differ in the type of background model used and the procedure used to update the model. This paper discusses modeling each pixel as a mixture of Gaussians and using an on-line approximation to update the model. The Gaussian, distributions of the adaptive mixture model are then evaluated to determine which are most likely to result from a background process. Each pixel is classified based on whether the Gaussian distribution which represents it most effectively is considered part of the background model. This results in a stable, real-time outdoor tracker which reliably deals with lighting changes, repetitive motions from clutter, and long-term scene changes. This system has been run almost continuously for 16 months, 24 hours a day, through rain and snow."
            },
            "slug": "Adaptive-background-mixture-models-for-real-time-Stauffer-Grimson",
            "title": {
                "fragments": [],
                "text": "Adaptive background mixture models for real-time tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper discusses modeling each pixel as a mixture of Gaussians and using an on-line approximation to update the model, resulting in a stable, real-time outdoor tracker which reliably deals with lighting changes, repetitive motions from clutter, and long-term scene changes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[12]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35270,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5488845"
                        ],
                        "name": "P. Grobstein",
                        "slug": "P.-Grobstein",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Grobstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Grobstein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 104
                            }
                        ],
                        "text": "Researchers have largely explored the properties of different cortical areas and connections among them [72, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 327
                            }
                        ],
                        "text": "The visual cortex appears to be organized into two functionally specialized pathways: a ventral stream that is crucial for the processing of shape information and object vision, and a dorsal stream that is crucual for the processing of the spatial relationships among objects, as well as for the analysis of motion information [72, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53163820,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "2295c833b7bd8094a0162d911210ed7944748d70",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-Visual-Behavior,-David-J.-Ingle,-Melvyn-Grobstein",
            "title": {
                "fragments": [],
                "text": "Analysis of Visual Behavior, David J. Ingle, Melvyn A. Goodale, Richard J.W. Mansfield (Eds.). MIT press, Cambridge, MA and London (1982), 834"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830863"
                        ],
                        "name": "Leslie G. Ungerleider",
                        "slug": "Leslie-G.-Ungerleider",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Ungerleider",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leslie G. Ungerleider"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144375727"
                        ],
                        "name": "R. Desimone",
                        "slug": "R.-Desimone",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Desimone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Desimone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 45
                            }
                        ],
                        "text": "Area MST receives its input from the MT area [71, 67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1876622,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "9e254bfa05198bf32ae77ebdd39593c4e88bc2f6",
            "isKey": false,
            "numCitedBy": 813,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "We have identified the cortical connections of area MT and determined their topographic organization and relationship to myeloarchitectural fields. Efferents of MT were examined in seven macaques that had received injections of tritiated amino acids, and afferents were examined in one macaque that had received injections of two fluorescent dyes. The injection sites formed an orderly sequence from the representation of central to that of peripheral vision in the upper and lower visual fields."
            },
            "slug": "Cortical-connections-of-visual-area-MT-in-the-Ungerleider-Desimone",
            "title": {
                "fragments": [],
                "text": "Cortical connections of visual area MT in the macaque"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The cortical connections of area MT were identified and their topographic organization and relationship to myeloarchitectural fields were determined and macaques that had received injections of tritiated amino acids and fluorescent dyes were examined."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of comparative neurology"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40588702"
                        ],
                        "name": "B. D. Lucas",
                        "slug": "B.-D.-Lucas",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Lucas",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. D. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "Optical flow based S1 units: A second set of S1 units was obtained by computing the optical flow of the input image sequence using Lucas & Kanade\u2019s algorithm [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2121536,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "a06547951c97b2a32f23a6c2b5f79c8c75c9b9bd",
            "isKey": false,
            "numCitedBy": 13329,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is taster because it examines far fewer potential matches between the images than existing techniques Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show how our technique can be adapted tor use in a stereo vision system."
            },
            "slug": "An-Iterative-Image-Registration-Technique-with-an-Lucas-Kanade",
            "title": {
                "fragments": [],
                "text": "An Iterative Image Registration Technique with an Application to Stereo Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration, and can be generalized to handle rotation, scaling and shearing."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828659"
                        ],
                        "name": "A. Watson",
                        "slug": "A.-Watson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Watson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Watson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066156"
                        ],
                        "name": "A. Ahumada",
                        "slug": "A.-Ahumada",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Ahumada",
                            "middleNames": [
                                "J."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ahumada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Considering one-dimensional motion, an object moving in a constant speed has a spectrum that lies on a line in the spatio-temporal frequency domain [73]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17168999,
            "fieldsOfStudy": [
                "History"
            ],
            "id": "922df390de7140904f7c7f484a513009450a099b",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This document has been reproduced from the best copy furnished by the organizational source. It is being released in the interest of making available as much information as possible. This document may contain data, which exceeds the sheet parameters. It was furnished in this condition by the organizational source and is the best copy available. This document may contain tone-on-tone or color graphs, charts and/or pictures, which have been reproduced in black and white. This document is paginated as submitted by the original source. Portions of this document are not fully legible due to the historical nature of some of the material. However, it is the best reproduction available from the original submission."
            },
            "slug": "A-look-at-motion-in-the-frequency-domain-Watson-Ahumada",
            "title": {
                "fragments": [],
                "text": "A look at motion in the frequency domain"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Portions of this document are not fully legible due to the historical nature of some of the material, however, it is the best reproduction available from the original submission."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "As in [16], we have performed experiments with the zeronorm SVM [31] of Weston et al. Instead of regularizing the norm of the hyperplane ||w||2, the classifier tries to optimize the objective function:\n||w||0 + C N\u2211\ni=1\n\u03b6i, such that (wT xi + b) > 1\u2212 \u03b6i (4)\nThe zero norm ||w||0 here indicates the count of the feature used."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "Motivated by these findings, we experiment with the zero-norm SVM [31] feature selection technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "At each round an SVM classifier is trained on the pool of C2 features and the training set is re-weighted using the weights of the trained SVM. Typically this leads to sparser SVM weights at each stage."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "These feature vectors are finally passed to a linear SVM classifier to obtain a classification label."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "As in [16], we have performed experiments with the zeronorm SVM [31] of Weston et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "The final classification stage is a linear multi-class SVM classifier trained using the all-pairs method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "Finally, a vector description is obtained by computing the histogram of cuboid-types of each video, and a SVM classifier is used for classification."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8993541,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "797928abfe0d189383325fe6322ced2226bcd457",
            "isKey": true,
            "numCitedBy": 808,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A blow-molded thermoplastic can has front and side walls and a nozzle integral therewith. The nozzle leads into a quarter-moon-shaped, force-absorbing protuberance in the wall of the can directed away from the can handle, which, under force, snaps inward into the can, thereby altering the position of the geometrical longitudinal axis of the nozzle."
            },
            "slug": "Use-of-the-Zero-Norm-with-Linear-Models-and-Kernel-Weston-Elisseeff",
            "title": {
                "fragments": [],
                "text": "Use of the Zero-Norm with Linear Models and Kernel Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A blow-molded thermoplastic can has front and side walls and a nozzle integral therewith, which leads into a quarter-moon-shaped, force-absorbing protuberance in the wall of the can directed away from the can handle, which snaps inward into the can, thereby altering the position of the geometrical longitudinal axis of the nozzle."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "At the C1 stage, down-sampling is performed for each S1 type by computing a local max over an 8 \u00d7 8 grid of S1 units with the same selectivity (e.g. same preferred motion direction and speed in the case of the space-time oriented S1 units)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "As a result some tolerance to small shifts is gained during the transition from the S1 to the C1 stages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "In [25, 16] for the recognition of static objects, Gabor filters at multiple orientations were used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Details about the datasets are given below."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "Here we follow the more recent framework using scale and position invariant C2 features [25, 16] that originated with the work of Riesenhuber & Poggio [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "In this stage, a global max across all positions is taken for each S2 feature maps."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "In these approaches, S1 units corresponded to 2D Gabor filters at multiple orientations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "Learning sparse spatio-temporal motion C2 features Recently, Mutch & Lowe showed that C2 features can be sparsified leading to a significant gain in performance [16] on standard object recognition databases (see also [20])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "As recent work in object recognition has indicated, models of cortical processing are starting to suggest new algorithms for computer vision [25, 16, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "Motivated by the recent success of biologically inspired approaches for the recognition of objects in real-world applications [25, 16, 20], we here extend a neurobiological\n978-1-4244-1631-8/07/$25.00 \u00a92007 IEEE\nmodel [10] of motion processing in the dorsal stream of the visual cortex."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiclass object recognition using sparse, localized hmax features"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "In particular, the model of [10] is too simple to deal with real videos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "model [10] of motion processing in the dorsal stream of the visual cortex."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "The analysis of motion information proceeds in MT and MST where neurons have substantial position and scale invariance and are tuned to optical flow patterns, see [10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "The approach builds on recent work on object recognition based on hierarchical feedforward architectures [25, 16, 20] and extends a neurobiological model of motion processing in the visual cortex [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 144
                            }
                        ],
                        "text": "Here we follow the more recent framework using scale and position invariant C2 features [25, 16] that originated with the work of Riesenhuber & Poggio [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "Indeed none of the existing neurobiological models of motion processing have been used on real-world data [10, 13, 4, 28, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 68
                            }
                        ],
                        "text": "The model has only been applied so far to simple artificial stimuli [10, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 73
                            }
                        ],
                        "text": "Interestingly, we find that the optical flow features previously used in [10, 4, 28] lead to worse performance than the gradient-based features and the space-time oriented filters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "Their organization is hierarchical; aiming, in a series of processing stages, to gradually increase both the selectivity of neurons along with their invariance to 2D transformations (see [10])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 188
                            }
                        ],
                        "text": "While this model has been successful in explaining a host of physiological and psychophysical data, it has only been tested on simple artificial stimuli such as point-light motion stimuli [10, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "It extends an earlier neurobiological model of motion processing in the dorsal stream of the visual cortex by Giese & Poggio [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural mechanisms for the recognition of biological movements and action"
            },
            "venue": {
                "fragments": [],
                "text": "Nat. Rev. Neurosci., 4:179\u2013192"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1861222864"
                        ],
                        "name": "Miss A.O. Penney",
                        "slug": "Miss-A.O.-Penney",
                        "structuredName": {
                            "firstName": "Miss",
                            "lastName": "Penney",
                            "middleNames": [
                                "A.O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miss A.O. Penney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "We perform feature selection on the C2 features [74]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "Feature selection on training C2 vectors The feature selection algorithm we used is AROM (approximation of the zero-norm Minimization) [74]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "Motivated by these findings, we experiment with the AROM feature selection technique [74] in the S2 stage to select relevant motion prototypes, and thus facilitating the template matching."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221039574,
            "fieldsOfStudy": [],
            "id": "45fd483402290ad4cae059a4e20cd586c019c3da",
            "isKey": true,
            "numCitedBy": 151814,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "(b)-Penney",
            "title": {
                "fragments": [],
                "text": "(b)"
            },
            "venue": {
                "fragments": [],
                "text": "The New Yale Book of Quotations"
            },
            "year": 2021
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "This idea was inspired by Hubel and Wiesel [26] and subsequently the architecture was constructed by Fukushima and applied on handwritten-digits recognition [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 239
                            }
                        ],
                        "text": "Such direction-sensitive cells were first discovered in the mammalian visual cortex by Hubel and Wiesel, who projected moving bars of light across the receptive fields of cells in the primary visual cortex of anesthetized cats and monkeys [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "It is widely accepted that complex cells combine multiple simple cells to gain position invariance and thus non-linearity [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Brain and Visual Perception"
            },
            "venue": {
                "fragments": [],
                "text": "chapter 10,14. Oxford Press"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 126
                            }
                        ],
                        "text": "Motivated by the recent success of biologically inspired approaches for the recognition of objects in real-world applications [25, 16, 20], we here extend a neurobiological"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "As in [16], we find that sparse features in intermediate stages outperform dense ones and that using a simple feature selection approach leads to an efficient system that performs better with far fewer features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 6
                            }
                        ],
                        "text": "As in [25, 16], we used n \u00d7 n templates (with n = 4, 8, 12, 16)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 35
                            }
                        ],
                        "text": "C2 shape features In previous work [25, 16], a still grayvalue input image is first analyzed by an array of Gabor filters (S1 units) at multiple orientations for all positions and scales."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "In [25, 16] for the recognition of static objects, Gabor filters at multiple orientations were used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "The approach builds on recent work on object recognition based on hierarchical feedforward architectures [25, 16, 20] and extends a neurobiological model of motion processing in the visual cortex [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 88
                            }
                        ],
                        "text": "Here we follow the more recent framework using scale and position invariant C2 features [25, 16] that originated with the work of Riesenhuber & Poggio [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 116
                            }
                        ],
                        "text": "Space-time oriented S1 units: These units constitute the most direct extension to the object recognition systems by [25, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "Learning sparse spatio-temporal motion C2 features Recently, Mutch & Lowe showed that C2 features can be sparsified leading to a significant gain in performance [16] on standard object recognition databases (see also [20])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 141
                            }
                        ],
                        "text": "As recent work in object recognition has indicated, models of cortical processing are starting to suggest new algorithms for computer vision [25, 16, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiclass object recognition using sparse"
            },
            "venue": {
                "fragments": [],
                "text": "localized hmax features. In CVPR"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47289728"
                        ],
                        "name": "R. K\u00fchn",
                        "slug": "R.-K\u00fchn",
                        "structuredName": {
                            "firstName": "Reimer",
                            "lastName": "K\u00fchn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. K\u00fchn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145292153"
                        ],
                        "name": "J. Hemmen",
                        "slug": "J.-Hemmen",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hemmen",
                            "middleNames": [
                                "Leo",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hemmen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64623816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a3b9645c353cd1b834c20f979427b91e25d4777",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Temporal-association-K\u00fchn-Hemmen",
            "title": {
                "fragments": [],
                "text": "Temporal association"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46818787"
                        ],
                        "name": "B. K\u0229dzia",
                        "slug": "B.-K\u0229dzia",
                        "structuredName": {
                            "firstName": "Boles\u0142aw",
                            "lastName": "K\u0229dzia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. K\u0229dzia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 99
                            }
                        ],
                        "text": "The size of the filters is chosen to match that of the receptive field of a typical V1 simple cell [62, 56]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 184
                            }
                        ],
                        "text": "The energy model was built from two space-time separable filters whose spatial responses are 2D Gabor functions and temporal responses are based on psychophysical experimental results [1, 56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46366664,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "528637e7acb252af7c3343960dfe88fa4c143d00",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "[Contrast-sensitivity-function-of-the-visual-K\u0229dzia",
            "title": {
                "fragments": [],
                "text": "[Contrast sensitivity function of the visual system]."
            },
            "venue": {
                "fragments": [],
                "text": "Klinika oczna"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2321909"
                        ],
                        "name": "M. Stryker",
                        "slug": "M.-Stryker",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stryker",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stryker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Neurons in the temporal cortex can learn to associate pairs of arbitrary geometrical stimuli [69]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 166
                            }
                        ],
                        "text": "We assume that such spatio-temporal seneitivity neurons might be found at different locations in the visual cortex such as STS, temporal cortex and prefrontal cortex [24, 69, 47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4239068,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "dda1c7807d51296bac44f0f24950193682dc70e5",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Temporal-associations-Stryker",
            "title": {
                "fragments": [],
                "text": "Temporal associations"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145707107"
                        ],
                        "name": "J. Movshon",
                        "slug": "J.-Movshon",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Movshon",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movshon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40377829"
                        ],
                        "name": "M. Gizzi",
                        "slug": "M.-Gizzi",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Gizzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gizzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2885922"
                        ],
                        "name": "W. Newsome",
                        "slug": "W.-Newsome",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Newsome",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Newsome"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 157
                            }
                        ],
                        "text": ", who presented a plaid containing two gratings with different orientations and moving independently along the direction perpendicular to their orientations [41, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11380229,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "18da0dcadf40334bdd9d686176f0140b7c721762",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-analysis-of-moving-visual-patterns-Movshon-Adelson",
            "title": {
                "fragments": [],
                "text": "The analysis of moving visual patterns"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6916627,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "563e821bb5ea825efb56b77484f5287f08cf3753",
            "isKey": false,
            "numCitedBy": 4091,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Convolutional-networks-for-images,-speech,-and-time-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Convolutional networks for images, speech, and time series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "The other common class of approaches is based on the processing of spatio-temporal features, either global as in the case of low-resolution videos [76, 14, 4] or local for higher resolution images [59, 13, 16, 46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "Weizmann Human The Weizmann human action dataset [4] contains eighty-one low resolution (180\u00d7 144 pixels) video sequences with nine subjects performing nine actions: running, walking, jumping-jack, jumping forward on two legs, jumping in place on two legs, galloping-sideways, waving two hands, waving one hand, and bend-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Actions as spacetime shapes"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Computer Vision, pages 1395\u20131402"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 125
                            }
                        ],
                        "text": "The cells in this area inherit the direction and speed tuning properties from their direct afferent inputs, V1 complex cells [42, 3, 37, 39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Functional characteristics of striate cortical neurons projecting to MT in the macaque"
            },
            "venue": {
                "fragments": [],
                "text": "Soc. Neurosci . Abstr."
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Integration of form and motion in the anterior temporal polysensory area (STPa) of the macaque monkey"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Neurophysiology, 76:109\u2013129"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "developed the convolutional network [33], also a feedforward hierarchical architecture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "The motion pathway can therefore be modeled as a feedforward hierarchical architecture [17, 33, 54, 21, 61]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convolutional networks for images"
            },
            "venue": {
                "fragments": [],
                "text": "speech, and timeseries. The Handbook of Brain Theory and Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis of Visual Behavior, chapter Two cortical visual systems"
            },
            "venue": {
                "fragments": [],
                "text": "Analysis of Visual Behavior, chapter Two cortical visual systems"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "This equation borrows its form of directional-tuning from [7], but the speed-tuning part in [7] is a rectangular function, which gives the middle speed tuning characteristic."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural mechanisms for the recognition of biological movements and actions"
            },
            "venue": {
                "fragments": [],
                "text": "Nature Reviews Neuroscience,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convolutional networks for images, speech, and timeseries . The Handbook of Brain Theory and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Convolutional networks for images, speech, and timeseries . The Handbook of Brain Theory and Neural Networks"
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 63,
            "methodology": 34,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 93,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Biologically-Inspired-System-for-Action-Jhuang-Serre/124d967683544973581f951ee93b3f7c069e3ced?sort=total-citations"
}