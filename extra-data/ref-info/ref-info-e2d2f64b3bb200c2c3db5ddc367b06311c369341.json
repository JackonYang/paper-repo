{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34979516"
                        ],
                        "name": "D. Hewlett",
                        "slug": "D.-Hewlett",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Hewlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hewlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8651990"
                        ],
                        "name": "Alexandre Lacoste",
                        "slug": "Alexandre-Lacoste",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Lacoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandre Lacoste"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3240386"
                        ],
                        "name": "Andrew Fandrianto",
                        "slug": "Andrew-Fandrianto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fandrianto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Fandrianto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111716473"
                        ],
                        "name": "Jay Han",
                        "slug": "Jay-Han",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jay Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554321"
                        ],
                        "name": "Matthew Kelcey",
                        "slug": "Matthew-Kelcey",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Kelcey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Kelcey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39835551"
                        ],
                        "name": "David Berthelot",
                        "slug": "David-Berthelot",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Berthelot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Berthelot"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 116
                            }
                        ],
                        "text": "The most similar dataset to our approach regarding the NLP field is the WikiReading dataset and related challenges (Hewlett et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15870937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "832fc9327695f7425d8759c6aaeec0fa2d7b0a90",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNNbased architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%."
            },
            "slug": "WikiReading:-A-Novel-Large-scale-Language-Task-over-Hewlett-Lacoste",
            "title": {
                "fragments": [],
                "text": "WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work presents WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances, and compares various state-of-the-art DNNbased architectures for document classification, information extraction, and question answering."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108812052"
                        ],
                        "name": "Xiaohui Zhao",
                        "slug": "Xiaohui-Zhao",
                        "structuredName": {
                            "firstName": "Xiaohui",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohui Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109569771"
                        ],
                        "name": "Zhuo Wu",
                        "slug": "Zhuo-Wu",
                        "structuredName": {
                            "firstName": "Zhuo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108004518"
                        ],
                        "name": "Xiaoguang Wang",
                        "slug": "Xiaoguang-Wang",
                        "structuredName": {
                            "firstName": "Xiaoguang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoguang Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 179
                            }
                        ],
                        "text": "To further increase training efficiency, we can use the documents of a previously defined layout, so that the models could learn how to extract a particular piece of information (Zhao et al., 2019; Denk and Reisswig, 2019; Liu et al., 2019a; Sarkhel and Nandi, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 88518818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "024aea24cc4d0949d6fe1482591d2429a9d8bbeb",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting key information from documents, such as receipts or invoices, and preserving the interested texts to structured data is crucial in the document-intensive streamline processes of office automation in areas that includes but not limited to accounting, financial, and taxation areas. To avoid designing expert rules for each specific type of document, some published works attempt to tackle the problem by learning a model to explore the semantic context in text sequences based on the Named Entity Recognition (NER) method in the NLP field. In this paper, we propose to harness the effective information from both semantic meaning and spatial distribution of texts in documents. Specifically, our proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations. We further explore the effect of employing different structures of convolutional neural network and propose a fast and portable structure. We demonstrate the effectiveness of the proposed method on a dataset with up to $4,484$ labelled receipts, without any pre-training or post-processing, achieving state of the art performance that is much better than the NER based methods in terms of either speed and accuracy. Experimental results also demonstrate that the proposed CUTIE model being able to achieve good performance with a much smaller amount of training data."
            },
            "slug": "CUTIE:-Learning-to-Understand-Documents-with-Text-Zhao-Wu",
            "title": {
                "fragments": [],
                "text": "CUTIE: Learning to Understand Documents with Convolutional Universal Text Information Extractor"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations and aims to harness the effective information from both semantic meaning and spatial distribution of texts in documents."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110965354"
                        ],
                        "name": "Xiaojing Liu",
                        "slug": "Xiaojing-Liu",
                        "structuredName": {
                            "firstName": "Xiaojing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112256"
                        ],
                        "name": "Feiyu Gao",
                        "slug": "Feiyu-Gao",
                        "structuredName": {
                            "firstName": "Feiyu",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feiyu Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112207346"
                        ],
                        "name": "Qiong Zhang",
                        "slug": "Qiong-Zhang",
                        "structuredName": {
                            "firstName": "Qiong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36530509"
                        ],
                        "name": "Huasha Zhao",
                        "slug": "Huasha-Zhao",
                        "structuredName": {
                            "firstName": "Huasha",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huasha Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 223
                            }
                        ],
                        "text": "To further increase training efficiency, we can use the documents of a previously defined layout, so that the models could learn how to extract a particular piece of information (Zhao et al., 2019; Denk and Reisswig, 2019; Liu et al., 2019a; Sarkhel and Nandi, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 109
                            }
                        ],
                        "text": "Contextual String Embeddings (\u201cFlair\u201d) (Akbik et al., 2018), BERT-base (Devlin et al., 2018) and RoBERTa-base (Liu et al., 2019b) models are used for this."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 85528598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04df8c70257b5280b9d303502c9d7ddf946f181b",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model."
            },
            "slug": "Graph-Convolution-for-Multimodal-Information-from-Liu-Gao",
            "title": {
                "fragments": [],
                "text": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper introduces a graph convolution based model to combine textual and visual information presented in VRDs and outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3245784"
                        ],
                        "name": "Ritesh Sarkhel",
                        "slug": "Ritesh-Sarkhel",
                        "structuredName": {
                            "firstName": "Ritesh",
                            "lastName": "Sarkhel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ritesh Sarkhel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145781033"
                        ],
                        "name": "Arnab Nandi",
                        "slug": "Arnab-Nandi",
                        "structuredName": {
                            "firstName": "Arnab",
                            "lastName": "Nandi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arnab Nandi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 178
                            }
                        ],
                        "text": "To further increase training efficiency, we can use the documents of a previously defined layout, so that the models could learn how to extract a particular piece of information (Zhao et al., 2019; Denk and Reisswig, 2019; Liu et al., 2019a; Sarkhel and Nandi, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195259530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "875759b92cb1de0d4b547556b22db54f70043a9a",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Physical and digital documents often contain visually rich information. With such information, there is no strict ordering or positioning in the document where the data values must appear. Along with textual cues, these documents often also rely on salient visual features to define distinct semantic boundaries and augment the information they disseminate. When performing information extraction (IE), traditional techniques fall short, as they use a text-only representation and do not consider the visual cues inherent to the layout of these documents. We propose VS2, a generalized approach for information extraction from heterogeneous visually rich documents. There are two major contributions of this work. First, we propose a robust segmentation algorithm that decomposes a visually rich document into a bag of visually isolated but semantically coherent areas, called logical blocks. Document type agnostic low-level visual and semantic features are used in this process. Our second contribution is a distantly supervised search-and-select method for identifying the named entities within these documents by utilizing the context boundaries defined by these logical blocks. Experimental results on three heterogeneous datasets suggest that the proposed approach significantly outperforms its text-only counterparts on all datasets. Comparing it against the state-of-the-art methods also reveal that VS2 performs comparably or better on all datasets."
            },
            "slug": "Visual-Segmentation-for-Information-Extraction-from-Sarkhel-Nandi",
            "title": {
                "fragments": [],
                "text": "Visual Segmentation for Information Extraction from Heterogeneous Visually Rich Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes VS2, a generalized approach for information extraction from heterogeneous visually rich documents, and proposes a robust segmentation algorithm that decomposes a visually rich document into a bag of visually isolated but semantically coherent areas, called logical blocks."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD Conference"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153368526"
                        ],
                        "name": "Timo I. Denk",
                        "slug": "Timo-I.-Denk",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Denk",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timo I. Denk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9992847"
                        ],
                        "name": "C. Reisswig",
                        "slug": "C.-Reisswig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Reisswig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Reisswig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 178
                            }
                        ],
                        "text": "To further increase training efficiency, we can use the documents of a previously defined layout, so that the models could learn how to extract a particular piece of information (Zhao et al., 2019; Denk and Reisswig, 2019; Liu et al., 2019a; Sarkhel and Nandi, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 202558968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fbda89395f993040b7665730c64182ade3be195",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "For understanding generic documents, information like font sizes, column layout, and generally the positioning of words may carry semantic information that is crucial for solving a downstream document intelligence task. Our novel BERTgrid, which is based on Chargrid by Katti et al. (2018), represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network. The contextualized embedding vectors are retrieved from a BERT language model. We use BERTgrid in combination with a fully convolutional network on a semantic instance segmentation task for extracting fields from invoices. We demonstrate its performance on tabulated line item and document header field extraction."
            },
            "slug": "BERTgrid:-Contextualized-Embedding-for-2D-Document-Denk-Reisswig",
            "title": {
                "fragments": [],
                "text": "BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The novel BERTgrid, which is based on Chargrid, represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35685584"
                        ],
                        "name": "Guillaume Jaume",
                        "slug": "Guillaume-Jaume",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Jaume",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Jaume"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025777"
                        ],
                        "name": "H. K. Ekenel",
                        "slug": "H.-K.-Ekenel",
                        "structuredName": {
                            "firstName": "Hazim",
                            "lastName": "Ekenel",
                            "middleNames": [
                                "Kemal"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. K. Ekenel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710257"
                        ],
                        "name": "J. Thiran",
                        "slug": "J.-Thiran",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Thiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Thiran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 173188931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58c793e278cdbf669a615b2c2479cd69ff785d63",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset for form understanding in noisy scanned documents (FUNSD) that aims at extracting and structuring the textual content of forms. The dataset comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking. To the best of our knowledge, this is the first publicly available dataset with comprehensive annotations to address FoUn task. We also present a set of baselines and introduce metrics to evaluate performance on the FUNSD dataset, which can be downloaded at https://guillaumejaume.github.io/FUNSD."
            },
            "slug": "FUNSD:-A-Dataset-for-Form-Understanding-in-Noisy-Jaume-Ekenel",
            "title": {
                "fragments": [],
                "text": "FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This work presents a new dataset for form understanding in noisy scanned documents (FUNSD) that aims at extracting and structuring the textual content of forms, and is the first publicly available dataset with comprehensive annotations to address FoUn task."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39139825"
                        ],
                        "name": "Matthew E. Peters",
                        "slug": "Matthew-E.-Peters",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Peters",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50043859"
                        ],
                        "name": "Mark Neumann",
                        "slug": "Mark-Neumann",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136562"
                        ],
                        "name": "Mohit Iyyer",
                        "slug": "Mohit-Iyyer",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Iyyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Iyyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40642935"
                        ],
                        "name": "Matt Gardner",
                        "slug": "Matt-Gardner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Gardner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143997772"
                        ],
                        "name": "Christopher Clark",
                        "slug": "Christopher-Clark",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "However, as far as the state of the art is concerned, there are many machine learning models which must be trained for general named entities to be robust (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3626819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3febb2bed8865945e7fddc99efd791887bb7e14f",
            "isKey": false,
            "numCitedBy": 7987,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
            },
            "slug": "Deep-Contextualized-Word-Representations-Peters-Neumann",
            "title": {
                "fragments": [],
                "text": "Deep Contextualized Word Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new type of deep contextualized word representation is introduced that models both complex characteristics of word use and how these uses vary across linguistic contexts, allowing downstream models to mix different types of semi-supervision signals."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31622211"
                        ],
                        "name": "Vishal Sunder",
                        "slug": "Vishal-Sunder",
                        "structuredName": {
                            "firstName": "Vishal",
                            "lastName": "Sunder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vishal Sunder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143780768"
                        ],
                        "name": "A. Srinivasan",
                        "slug": "A.-Srinivasan",
                        "structuredName": {
                            "firstName": "Ashwin",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Srinivasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3213990"
                        ],
                        "name": "L. Vig",
                        "slug": "L.-Vig",
                        "structuredName": {
                            "firstName": "Lovekesh",
                            "lastName": "Vig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143725466"
                        ],
                        "name": "Gautam M. Shroff",
                        "slug": "Gautam-M.-Shroff",
                        "structuredName": {
                            "firstName": "Gautam",
                            "lastName": "Shroff",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gautam M. Shroff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35888656"
                        ],
                        "name": "R. Rahul",
                        "slug": "R.-Rahul",
                        "structuredName": {
                            "firstName": "Rohit",
                            "lastName": "Rahul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rahul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 275
                            }
                        ],
                        "text": "\u2026of business users, systems that automatically gather information about individuals, their roles, significant dates, addresses, and amounts from invoices, companies reports and contracts, would be useful (Holt and Chisholm, 2018; Katti et al., 2018; Wr\u00f3blewska et al., 2018; Sunder et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 317,
                                "start": 226
                            }
                        ],
                        "text": "From the point of view of business users, systems that automatically gather information about individuals, their roles, significant dates, addresses, and amounts from invoices, companies reports and contracts, would be useful (Holt and Chisholm, 2018; Katti et al., 2018; Wr\u00f3blewska et al., 2018; Sunder et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 174803467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee04f694177134074e18d14def58cd5252b5b45c",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Our interest in this paper is in meeting a rapidly growing industrial demand for information extraction from images of documents such as invoices, bills, receipts etc. In practice users are able to provide a very small number of example images labeled with the information that needs to be extracted. We adopt a novel two-level neuro-deductive, approach where (a) we use pre-trained deep neural networks to populate a relational database with facts about each document-image; and (b) we use a form of deductive reasoning, related to meta-interpretive learning of transition systems to learn extraction programs: Given task-specific transitions defined using the entities and relations identified by the neural detectors and a small number of instances (usually 1, sometimes 2) of images and the desired outputs, a resource-bounded meta-interpreter constructs proofs for the instance(s) via logical deduction; a set of logic programs that extract each desired entity is easily synthesized from such proofs. In most cases a single training example together with a noisy-clone of itself suffices to learn a program-set that generalizes well on test documents, at which time the value of each entity is determined by a majority vote across its program-set. We demonstrate our two-level neuro-deductive approach on publicly available datasets (\"Patent\" and \"Doctor's Bills\") and also describe its use in a real-life industrial problem."
            },
            "slug": "One-shot-Information-Extraction-from-Document-using-Sunder-Srinivasan",
            "title": {
                "fragments": [],
                "text": "One-shot Information Extraction from Document Images using Neuro-Deductive Program Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel two-level neuro-deductive approach that uses pre-trained deep neural networks to populate a relational database with facts about each document-image and uses a form of deductive reasoning related to meta-interpretive learning of transition systems to learn extraction programs."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 89
                            }
                        ],
                        "text": "The Flair model (based on stacked char-Bi-LSTM language model and GloVe word embeddings (Pennington et al., 2014)) is used as an encoder and Bi-LSTM with a CRF layer\u2014as an output decoder.9 Based on many experiments on the NDA and Charity development datasets, we found out the best setup for\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 114
                            }
                        ],
                        "text": "1 Pipeline based on Flair The Flair model (based on stacked char-Bi-LSTM language model and GloVe word embeddings (Pennington et al., 2014)) is used as an encoder and Bi-LSTM with a CRF layer\u2014as an output decoder."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1957433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "isKey": false,
            "numCitedBy": 22534,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."
            },
            "slug": "GloVe:-Global-Vectors-for-Word-Representation-Pennington-Socher",
            "title": {
                "fragments": [],
                "text": "GloVe: Global Vectors for Word Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods and produces a vector space with meaningful substructure."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403712"
                        ],
                        "name": "A. Akbik",
                        "slug": "A.-Akbik",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Akbik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Akbik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2725328"
                        ],
                        "name": "Duncan A. J. Blythe",
                        "slug": "Duncan-A.-J.-Blythe",
                        "structuredName": {
                            "firstName": "Duncan",
                            "lastName": "Blythe",
                            "middleNames": [
                                "A.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duncan A. J. Blythe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2742129"
                        ],
                        "name": "Roland Vollgraf",
                        "slug": "Roland-Vollgraf",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Vollgraf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roland Vollgraf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 39
                            }
                        ],
                        "text": "Contextual String Embeddings (\u201cFlair\u201d) (Akbik et al., 2018), BERT-base (Devlin et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 177
                            }
                        ],
                        "text": "However, as far as the state of the art is concerned, there are many machine learning models which must be trained for general named entities to be robust (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 38
                            }
                        ],
                        "text": "Contextual String Embeddings (\u201cFlair\u201d) (Akbik et al., 2018), BERT-base (Devlin et al., 2018) and RoBERTa-base (Liu et al., 2019b) models are used for this."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 48
                            }
                        ],
                        "text": "9We used implementation from the Flair library (Akbik et al., 2018) in version 0.4.5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 49
                            }
                        ],
                        "text": "(9)We used implementation from the Flair library (Akbik et al., 2018) in version 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52010710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "421fc2556836a6b441de806d7b393a35b6eaea58",
            "isKey": true,
            "numCitedBy": 862,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair"
            },
            "slug": "Contextual-String-Embeddings-for-Sequence-Labeling-Akbik-Blythe",
            "title": {
                "fragments": [],
                "text": "Contextual String Embeddings for Sequence Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes to leverage the internal states of a trained character language model to produce a novel type of word embedding which they refer to as contextual string embeddings, which are fundamentally model words as sequences of characters and are contextualized by their surrounding text."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8059409"
                        ],
                        "name": "Xavier R. Holt",
                        "slug": "Xavier-R.-Holt",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Holt",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier R. Holt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144083778"
                        ],
                        "name": "Andrew Chisholm",
                        "slug": "Andrew-Chisholm",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Chisholm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Chisholm"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 134
                            }
                        ],
                        "text": "Finally, there are datasets with the information extraction task based on invoices, which are not publicly available to the community (Holt and Chisholm, 2018; Katti et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 317,
                                "start": 226
                            }
                        ],
                        "text": "From the point of view of business users, systems that automatically gather information about individuals, their roles, significant dates, addresses, and amounts from invoices, companies reports and contracts, would be useful (Holt and Chisholm, 2018; Katti et al., 2018; Wr\u00f3blewska et al., 2018; Sunder et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67865941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8184785988752f2258038869158e1351a51563d",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Business documents encode a wealth of information in a format tailored to human consumption \u2013 i.e. aesthetically disbursed natural language text, graphics and tables. We address the task of extracting key fields (e.g. the amount due on an invoice) from a wide-variety of potentially unseen document formats. In contrast to traditional template driven extraction systems, we introduce a content-driven machine-learning approach which is both robust to noise and generalises to unseen document formats. In a comparison of our approach with alternative invoice extraction systems, we observe an absolute accuracy gain of 20\\% across compared fields, and a 25\\%\u201394\\% reduction in extraction latency."
            },
            "slug": "Extracting-structured-data-from-invoices-Holt-Chisholm",
            "title": {
                "fragments": [],
                "text": "Extracting structured data from invoices"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In contrast to traditional template driven extraction systems, this work introduces a content-driven machine-learning approach which is both robust to noise and generalises to unseen document formats."
            },
            "venue": {
                "fragments": [],
                "text": "ALTA"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719068"
                        ],
                        "name": "Anoop R. Katti",
                        "slug": "Anoop-R.-Katti",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Katti",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop R. Katti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9992847"
                        ],
                        "name": "C. Reisswig",
                        "slug": "C.-Reisswig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Reisswig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Reisswig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39387393"
                        ],
                        "name": "Cordula Guder",
                        "slug": "Cordula-Guder",
                        "structuredName": {
                            "firstName": "Cordula",
                            "lastName": "Guder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cordula Guder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14334250"
                        ],
                        "name": "Sebastian Brarda",
                        "slug": "Sebastian-Brarda",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Brarda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Brarda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704747"
                        ],
                        "name": "S. Bickel",
                        "slug": "S.-Bickel",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Bickel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bickel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2216963"
                        ],
                        "name": "J. H\u00f6hne",
                        "slug": "J.-H\u00f6hne",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "H\u00f6hne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00f6hne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803968"
                        ],
                        "name": "J. Faddoul",
                        "slug": "J.-Faddoul",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Faddoul",
                            "middleNames": [
                                "Baptiste"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Faddoul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 230
                            }
                        ],
                        "text": "\u2026of business users, systems that automatically gather information about individuals, their roles, significant dates, addresses, and amounts from invoices, companies reports and contracts, would be useful (Holt and Chisholm, 2018; Katti et al., 2018; Wr\u00f3blewska et al., 2018; Sunder et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 160
                            }
                        ],
                        "text": "Finally, there are datasets with the information extraction task based on invoices, which are not publicly available to the community (Holt and Chisholm, 2018; Katti et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 317,
                                "start": 226
                            }
                        ],
                        "text": "From the point of view of business users, systems that automatically gather information about individuals, their roles, significant dates, addresses, and amounts from invoices, companies reports and contracts, would be useful (Holt and Chisholm, 2018; Katti et al., 2018; Wr\u00f3blewska et al., 2018; Sunder et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52815006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15aae08159856cdbf0ce539357d473a04dcbb7f3",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images."
            },
            "slug": "Chargrid:-Towards-Understanding-2D-Documents-Katti-Reisswig",
            "title": {
                "fragments": [],
                "text": "Chargrid: Towards Understanding 2D Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel type of text representation is introduced that preserves the 2D layout of a document by encoding each document page as a two-dimensional grid of characters and it is shown that it significantly outperforms approaches based on sequential text or document images."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 197
                            }
                        ],
                        "text": "However, as far as the state of the art is concerned, there are many machine learning models which must be trained for general named entities to be robust (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 19
                            }
                        ],
                        "text": ", 2018), BERT-base (Devlin et al., 2018) and RoBERTa-base (Liu et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 70
                            }
                        ],
                        "text": "Contextual String Embeddings (\u201cFlair\u201d) (Akbik et al., 2018), BERT-base (Devlin et al., 2018) and RoBERTa-base (Liu et al., 2019b) models are used for this."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": false,
            "numCitedBy": 33744,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143897180"
                        ],
                        "name": "Anna Wr\u00f3blewska",
                        "slug": "Anna-Wr\u00f3blewska",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Wr\u00f3blewska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Wr\u00f3blewska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822665"
                        ],
                        "name": "Tomasz Stanislawek",
                        "slug": "Tomasz-Stanislawek",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Stanislawek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Stanislawek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1414123904"
                        ],
                        "name": "Bartlomiej Prus-Zajaczkowski",
                        "slug": "Bartlomiej-Prus-Zajaczkowski",
                        "structuredName": {
                            "firstName": "Bartlomiej",
                            "lastName": "Prus-Zajaczkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bartlomiej Prus-Zajaczkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79665491"
                        ],
                        "name": "Lukasz Garncarek",
                        "slug": "Lukasz-Garncarek",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Garncarek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Garncarek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 250
                            }
                        ],
                        "text": "\u2026of business users, systems that automatically gather information about individuals, their roles, significant dates, addresses, and amounts from invoices, companies reports and contracts, would be useful (Holt and Chisholm, 2018; Katti et al., 2018; Wr\u00f3blewska et al., 2018; Sunder et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 317,
                                "start": 226
                            }
                        ],
                        "text": "From the point of view of business users, systems that automatically gather information about individuals, their roles, significant dates, addresses, and amounts from invoices, companies reports and contracts, would be useful (Holt and Chisholm, 2018; Katti et al., 2018; Wr\u00f3blewska et al., 2018; Sunder et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52881550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb4cec661f4d5d0b83c49353b896f16ed7bdd55e",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present our work in progress on building an artificial intelligence system dedicated to tasks regarding the processing of formal documents used in various kinds of business procedures. The main challenge is to build machine learning (ML) models to improve the quality and efficiency of business processes involving image processing, optical character recognition (OCR), text mining and information extraction. In the paper we introduce the research and application field, some common techniques used in this area and our preliminary results and conclusions."
            },
            "slug": "Robotic-Process-Automation-of-Unstructured-Data-Wr\u00f3blewska-Stanislawek",
            "title": {
                "fragments": [],
                "text": "Robotic Process Automation of Unstructured Data with Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The main challenge is to build machine learning models to improve the quality and efficiency of business processes involving image processing, optical character recognition (OCR), text mining and information extraction."
            },
            "venue": {
                "fragments": [],
                "text": "FedCSIS"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323179"
                        ],
                        "name": "Yinhan Liu",
                        "slug": "Yinhan-Liu",
                        "structuredName": {
                            "firstName": "Yinhan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinhan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40511414"
                        ],
                        "name": "Myle Ott",
                        "slug": "Myle-Ott",
                        "structuredName": {
                            "firstName": "Myle",
                            "lastName": "Ott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Myle Ott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39589154"
                        ],
                        "name": "Naman Goyal",
                        "slug": "Naman-Goyal",
                        "structuredName": {
                            "firstName": "Naman",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naman Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3048577"
                        ],
                        "name": "Jingfei Du",
                        "slug": "Jingfei-Du",
                        "structuredName": {
                            "firstName": "Jingfei",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingfei Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144863691"
                        ],
                        "name": "Mandar Joshi",
                        "slug": "Mandar-Joshi",
                        "structuredName": {
                            "firstName": "Mandar",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mandar Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50536468"
                        ],
                        "name": "Danqi Chen",
                        "slug": "Danqi-Chen",
                        "structuredName": {
                            "firstName": "Danqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35084211"
                        ],
                        "name": "M. Lewis",
                        "slug": "M.-Lewis",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Lewis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759422"
                        ],
                        "name": "Veselin Stoyanov",
                        "slug": "Veselin-Stoyanov",
                        "structuredName": {
                            "firstName": "Veselin",
                            "lastName": "Stoyanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veselin Stoyanov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 223
                            }
                        ],
                        "text": "To further increase training efficiency, we can use the documents of a previously defined layout, so that the models could learn how to extract a particular piece of information (Zhao et al., 2019; Denk and Reisswig, 2019; Liu et al., 2019a; Sarkhel and Nandi, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 25
                            }
                        ],
                        "text": ", 2018) and RoBERTa-base (Liu et al., 2019b) models are used for this."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 109
                            }
                        ],
                        "text": "Contextual String Embeddings (\u201cFlair\u201d) (Akbik et al., 2018), BERT-base (Devlin et al., 2018) and RoBERTa-base (Liu et al., 2019b) models are used for this."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 198953378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "isKey": false,
            "numCitedBy": 7266,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
            },
            "slug": "RoBERTa:-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott",
            "title": {
                "fragments": [],
                "text": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is found that BERT was significantly undertrained, and can match or exceed the performance of every model published after it, and the best model achieves state-of-the-art results on GLUE, RACE and SQuAD."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 11
                            }
                        ],
                        "text": "Tesseract (Smith, 2020) in version 4.1.1-rc1-7-gb36c11."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tesseract Open Source OCR Engine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90288920"
                        ],
                        "name": "Adrian Waygood",
                        "slug": "Adrian-Waygood",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Waygood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrian Waygood"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 240081945,
            "fieldsOfStudy": [],
            "id": "47d79963ac69111d8dc82a228d26e6a746a4d087",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Transformers-Waygood",
            "title": {
                "fragments": [],
                "text": "Transformers"
            },
            "venue": {
                "fragments": [],
                "text": "An Introduction to Electrical Science"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hugging Face"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tesseract Open Source OCR Engine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 19,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Kleister:-A-novel-task-for-Information-Extraction-Grali'nski-Stanislawek/e2d2f64b3bb200c2c3db5ddc367b06311c369341?sort=total-citations"
}