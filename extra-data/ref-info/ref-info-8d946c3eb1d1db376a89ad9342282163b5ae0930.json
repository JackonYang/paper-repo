{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6519466"
                        ],
                        "name": "A. Hyvarinen",
                        "slug": "A.-Hyvarinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyvarinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyvarinen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "which was introduced in [17] using a more heuristic derivation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195863084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8cd29d495030c3b8306a375db1a861ed65247e70",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent component analysis (ICA) is a statistical signal processing technique whose main applications are blind source separation, blind deconvolution, and feature extraction. Estimation of ICA is usually performed by optimizing a 'contrast' function based on higher-order cumulants. It is shown how almost any error function can be used to construct a contrast function to perform the ICA estimation. In particular, this means that one can use contrast functions that are robust against outliers. As a practical method for finding the relevant extrema of such contrast functions, a fixed-point iteration scheme is then introduced. The resulting algorithms are quite simple and converge fast and reliably. These algorithms also enable estimation of the independent components one-by-one, using a simple deflation scheme."
            },
            "slug": "A-family-of-fixed-point-algorithms-for-independent-Hyvarinen",
            "title": {
                "fragments": [],
                "text": "A family of fixed-point algorithms for independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how almost any error function can be used to construct a contrast function to perform the ICA estimation, and this means that one can use contrast functions that are robust against outliers."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "Non-Gaussianity of the independent components is necessary for the identifiability of the model (2), see [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "[31] , \u201cThe nonlinear PCA learning rule in independent component analysis,\u201dNeurocomputing,vol. 17, no. 1, pp. 25\u201346, 1997."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "If it is singular or near-singular, the dimension of the data must be reduced, for example with PCA [7], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 123
                            }
                        ],
                        "text": "We treat in this paper the problem of estimating the transformation given by (linear) independent component analysis (ICA) [7], [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "For symmetric variables, this is a generalization of the cumulantbased approximation in [7], which is obtained by taking ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "Comon [7] showed how to obtain a more general formulation for ICA that does not need to assume an underlying data model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "Using the concept of differential entropy, one can define the mutual information between the (scalar) random variables [7], [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "Thus we define in this paper, following [7], the ICA of a random vector as an invertible transformation as in (1) where the matrix is determined so that the mutual information of the transformed componentsis minimized."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "This transformation is always possible; indeed, it can be accomplished by classical PCA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "Because negentropy is invariant for invertible linear transformations [7], it is now obvious from (5) that finding an invertible transformation that minimizes the mutual information is roughly equivalent to finding directions in which the negentropy is maximized ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "Negentropy can also be interpreted as a measure of nongaussianity [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "In [19] it was shown that these approximations are often considerably more accurate than the conventional, cumulant-based approximations in [1], [7], and [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": true,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6519466"
                        ],
                        "name": "A. Hyvarinen",
                        "slug": "A.-Hyvarinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyvarinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyvarinen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "In fact, it can be seen that the results in [18] are valid even in this case, and thus we have the following theorem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [18], evaluation of asymptotic variances was addressed using a related family of contrast functions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Again, we can adapt the results in [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195864127,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a0d51aefac793acf227997476d80ef76f98b9c6c",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The author (1997) introduced a large family of one-unit contrast functions to be used in independent component analysis (ICA). In this paper, the family is analyzed mathematically in the case of a finite sample. Two aspects of the estimators obtained using such contrast functions are considered: asymptotic variance, and robustness against outliers. An expression for the contrast function that minimizes the asymptotic variance is obtained as a function of the probability densities of the independent components. Combined with robustness considerations, these results provide strong arguments in favor of the use of contrast functions based on slowly growing functions, and against the use of kurtosis, which is the classical contrast function."
            },
            "slug": "One-unit-contrast-functions-for-independent-a-Hyvarinen",
            "title": {
                "fragments": [],
                "text": "One-unit contrast functions for independent component analysis: a statistical analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120692531"
                        ],
                        "name": "Liuyue Wang",
                        "slug": "Liuyue-Wang",
                        "structuredName": {
                            "firstName": "Liuyue",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liuyue Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210204"
                        ],
                        "name": "R. Vig\u00e1rio",
                        "slug": "R.-Vig\u00e1rio",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vig\u00e1rio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vig\u00e1rio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768650"
                        ],
                        "name": "J. Joutsensalo",
                        "slug": "J.-Joutsensalo",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Joutsensalo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Joutsensalo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "If it is singular or near-singular, the dimension of the data must be reduced, for example with PCA [7], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "In certain applications, however, it may be desired to use a symmetric decorrelation, in which no vectors are \u201cprivileged\u201d over others [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "the data [1], [3], [5], [6], [23], [24], [27], [28], [31]:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "A reduction of the step size in the stabilized version has a similar effect, as is well-known in stochastic approximation methods [24], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 310835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c93c897fd80f5246b839a2044798780cf2c5a77",
            "isKey": true,
            "numCitedBy": 452,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent component analysis (ICA) is a recently developed, useful extension of standard principal component analysis (PCA). The ICA model is utilized mainly in blind separation of unknown source signals from their linear mixtures. In this application only the source signals which correspond to the coefficients of the ICA expansion are of interest. In this paper, we propose neural structures related to multilayer feedforward networks for performing complete ICA. The basic ICA network consists of whitening, separation, and basis vector estimation layers. It can be used for both blind source separation and estimation of the basis vectors of ICA. We consider learning algorithms for each layer, and modify our previous nonlinear PCA type algorithms so that their separation capabilities are greatly improved. The proposed class of networks yields good results in test examples with both artificial and real-world data."
            },
            "slug": "A-class-of-neural-networks-for-independent-analysis-Karhunen-Oja",
            "title": {
                "fragments": [],
                "text": "A class of neural networks for independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes neural structures related to multilayer feedforward networks for performing complete independent component analysis (ICA) and modify the previous nonlinear PCA type algorithms so that their separation capabilities are greatly improved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144616256"
                        ],
                        "name": "D. Pham",
                        "slug": "D.-Pham",
                        "structuredName": {
                            "firstName": "Dinh",
                            "lastName": "Pham",
                            "middleNames": [
                                "Tuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46355559"
                        ],
                        "name": "Philippe Garat",
                        "slug": "Philippe-Garat",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Garat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philippe Garat"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32042337,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f93fed0bac99e37acf30ea0c1356725f74ec2b78",
            "isKey": false,
            "numCitedBy": 494,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two methods for separating mixture of independent sources without any precise knowledge of their probability distribution. They are obtained by considering a maximum likelihood (ML) solution corresponding to some given distributions of the sources and relaxing this assumption afterward. The first method is specially adapted to temporally independent non-Gaussian sources and is based on the use of nonlinear separating functions. The second method is specially adapted to correlated sources with distinct spectra and is based on the use of linear separating filters. A theoretical analysis of the performance of the methods has been made. A simple procedure for optimally choosing the separating functions is proposed. Further, in the second method, a simple implementation based on the simultaneous diagonalization of two symmetric matrices is provided. Finally, some numerical and simulation results are given, illustrating the performance of the method and the good agreement between the experiments and the theory."
            },
            "slug": "Blind-separation-of-mixture-of-independent-sources-Pham-Garat",
            "title": {
                "fragments": [],
                "text": "Blind separation of mixture of independent sources through a quasi-maximum likelihood approach"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Two methods for separating mixture of independent sources without any precise knowledge of their probability distribution are proposed by considering a maximum likelihood (ML) solution corresponding to some given distributions of the sources and relaxing this assumption afterward."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "the data [1], [3], [5], [6], [23], [24], [27], [28], [31]:"
                    },
                    "intents": []
                }
            ],
            "corpusId": 37365552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11656f389fabe0c8ab987ded90372d06c6591008",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-nonlinear-PCA-learning-rule-in-independent-Oja",
            "title": {
                "fragments": [],
                "text": "The nonlinear PCA learning rule in independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "that are closely related related to those introduced in [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This theorem can be considered a corollary of the theorem in [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, finding a single direction that maximizes negentropy is a form of projection pursuit, and could also be interpreted as estimation of a single independent component [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We show in the Appendix B how to modify the algorithms in [24] to minimize the contrast functions used in this paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One popular way of formulating the ICA problem is to consider the estimation of the following generative model for the data [1, 3, 5, 6, 23, 24, 27, 28, 31]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A reduction of the step size \u03bc in the stabilized version has a similar effect, as is well-known in stochastic approximation methods [24, 28]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8072151,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "bac464b9562ce85b1155883023f36b8e3e8ddba3",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis-by-general-nonlinear-Hyv\u00e4rinen-Oja",
            "title": {
                "fragments": [],
                "text": "Independent component analysis by general nonlinear Hebbian-like learning rules"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211061"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346683"
                        ],
                        "name": "J. Hurri",
                        "slug": "J.-Hurri",
                        "structuredName": {
                            "firstName": "Jarmo",
                            "lastName": "Hurri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hurri"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "These applications include artifact ca cellation in EEG and MEG [36], [37], decomposition of evoked fields in MEG [38], and feature extraction of image data [25], [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "In feature extraction[4], [25], is the coefficient of theth feature in the observed data vector ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Taking also into account the fact that most independent components encountered in practice are super-Gaussian [3], [25], one reaches the conclusion that as a general-purpose contrast function, one should choose a function that resembles rather"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17346685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0479efee5558edd13115f37dac41ccd9272138d4",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse coding is a method for finding a representation of data in which each of the components of the representation is only rarely significantly active. Such a representation is closely related to the techniques of independent component analysis and blind source separation. In this paper, we investigate the application of sparse coding for image feature extraction. We show how sparse coding can be used to extract wavelet-like features from natural image data. As an application of such a feature extraction scheme, we show how to apply a soft-thresholding operator on the components of sparse coding in order to reduce Gaussian noise. Methods based on sparse coding have the important benefit over wavelet methods that the features are determined solely by the statistical properties of the data, while the wavelet transformation relies heavily on certain abstract mathematical properties that may be only weakly related to the properties of the natural data."
            },
            "slug": "Image-feature-extraction-by-sparse-coding-and-Hyv\u00e4rinen-Oja",
            "title": {
                "fragments": [],
                "text": "Image feature extraction by sparse coding and independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown how sparse coding can be used to extract wavelet-like features from natural image data and how to apply a soft-thresholding operator on the components of sparse coding in order to reduce Gaussian noise."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716124"
                        ],
                        "name": "Beate H. Laheld",
                        "slug": "Beate-H.-Laheld",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Laheld",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beate H. Laheld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "the data [1], [3], [5], [6], [23], [24], [27], [28], [31]:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "Almost identical results have also been obtained in [5] for another algorithm."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "The convergence of the orthonormalization method in (26), which may be considered a variation of Potter\u2019s formula (see [5]), is proven in the Appendix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17839672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8637f042e3d2a2d45de41566b4203646987a8424",
            "isKey": false,
            "numCitedBy": 1501,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Source separation consists of recovering a set of independent signals when only mixtures with unknown coefficients are observed. This paper introduces a class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called equivariant adaptive separation via independence (EASI). The EASI algorithms are based on the idea of serial updating. This specific form of matrix updates systematically yields algorithms with a simple structure for both real and complex mixtures. Most importantly, the performance of an EASI algorithm does not depend on the mixing matrix. In particular, convergence rates, stability conditions, and interference rejection levels depend only on the (normalized) distributions of the source signals. Closed-form expressions of these quantities are given via an asymptotic performance analysis. The theme of equivariance is stressed throughout the paper. The source separation problem has an underlying multiplicative structure. The parameter space forms a (matrix) multiplicative group. We explore the (favorable) consequences of this fact on implementation, performance, and optimization of EASI algorithms."
            },
            "slug": "Equivariant-adaptive-source-separation-Cardoso-Laheld",
            "title": {
                "fragments": [],
                "text": "Equivariant adaptive source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called EASI, which yields algorithms with a simple structure for both real and complex mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "the data [1], [3], [5], [6], [23], [24], [27], [28], [31]:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "Taking also into account the fact that most independent components encountered in practice are super-Gaussian [3], [25], one reaches the conclusion that as a general-purpose contrast function, one should choose a function that resembles rather"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 132
                            }
                        ],
                        "text": "Thus the optimal contrast function is the same as the one obtained by the maximum likelihood approach [34], or the infomax approach [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8758,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [19] it was shown that these approximations are often considerably more accurate than the conventional, cumulant-based approximations in [1], [7], and [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "The novel approximations of negentropy introduced in [19] were then used for constructing novel contrast (objective) functions for ICA."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "We use here the new approximations developed in [19], based on the maximum entropy principle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11359216,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "04528b1e4303170535abccf1c968ae16b6c1e397",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a first-order approximation of the density of maximum entropy for a continuous 1-D random variable, given a number of simple constraints. This results in a density expansion which is somewhat similar to the classical polynomial density expansions by Gram-Charlier and Edgeworth. Using this approximation of density, an approximation of 1-D differential entropy is derived. The approximation of entropy is both more exact and more robust against outliers than the classical approximation based on the polynomial density expansions, without being computationally more expensive. The approximation has applications, for example, in independent component analysis and projection pursuit."
            },
            "slug": "New-Approximations-of-Differential-Entropy-for-and-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "New Approximations of Differential Entropy for Independent Component Analysis and Projection Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A first-order approximation of the density of maximum entropy for a continuous 1-D random variable is derived, which results in a density expansion which is somewhat similar to the classical polynomial density expansions by Gram-Charlier and Edgeworth."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2654304"
                        ],
                        "name": "R. Cristescu",
                        "slug": "R.-Cristescu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Cristescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cristescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "Some extensions of the methods introduced in this paper are presented in [20], in which the problem of noisy data is addressed, and in [22], which deals with the situation where there are more independent components than observed variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18618366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "335cedb67dba4809ab0651ca945a81e31eec8689",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a very fast method for estimating over-complete bases of independent components from image data. This is based on the concept of quasi-orthogonality, which means that in a very high-dimensional space, there can be a large, over-complete set of vectors that are almost orthogonal to each other. Thus we may estimate an over-complete basis by using one-unit ICA algorithms and forcing only partial decorrelation between the different independent components. The method can be implemented using a modification of the FastICA algorithm, which leads to a computationally highly efficient method."
            },
            "slug": "A-fast-algorithm-for-estimating-overcomplete-ICA-Hyv\u00e4rinen-Cristescu",
            "title": {
                "fragments": [],
                "text": "A fast algorithm for estimating overcomplete ICA bases for image windows"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A very fast method for estimating over-complete bases of independent components from image data based on the concept of quasi-orthogonality, which means that in a very high-dimensional space, there can be a large, over- complete set of vectors that are almost orthogonal to each other."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "Several principles and methods have been developed to find such a linear representation, including principal component analysis [30], factor analysis [15], projection pursuit [12], [16], independent component analysis [27], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40000333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de86c61defbb8c259583074f3cf63afe13571ce1",
            "isKey": false,
            "numCitedBy": 856,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principal-components,-minor-components,-and-linear-Oja",
            "title": {
                "fragments": [],
                "text": "Principal components, minor components, and linear neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "In feature extraction[4], [25], is the coefficient of theth feature in the observed data vector ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6219133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca1d23be869380ac9e900578c601c2d1febcc0c9",
            "isKey": false,
            "numCitedBy": 2373,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-\u201cindependent-components\u201d-of-natural-scenes-are-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "The \u201cindependent components\u201d of natural scenes are edge filters"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [19] it was shown that these approximations are often considerably more accurate than the conventional, cumulant-based approximations in [1], [7], and [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "the data [1], [3], [5], [6], [23], [24], [27], [28], [31]:"
                    },
                    "intents": []
                }
            ],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "We treat in this paper the problem of estimating the transformation given by (linear) independent component analysis (ICA) [7], [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "the data [1], [3], [5], [6], [23], [24], [27], [28], [31]:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "In blind source separation[27], the observed values of correspond to a realization of an -dimensional discrete-time signal , ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 218
                            }
                        ],
                        "text": "Several principles and methods have been developed to find such a linear representation, including principal component analysis [30], factor analysis [15], projection pursuit [12], [16], independent component analysis [27], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": true,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28110514"
                        ],
                        "name": "N. Delfosse",
                        "slug": "N.-Delfosse",
                        "structuredName": {
                            "firstName": "Nathalie",
                            "lastName": "Delfosse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Delfosse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144667255"
                        ],
                        "name": "P. Loubaton",
                        "slug": "P.-Loubaton",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Loubaton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Loubaton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "This resulted in a generalization of the kurtosis-based approach in [7] and [9], and also enabled estimation of the independent components one by one."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "In that case, it can also be proven that there are no spurious optima [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12894028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7a108b22336a90f728eb62cade867054c24481d",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-blind-separation-of-independent-sources:-A-Delfosse-Loubaton",
            "title": {
                "fragments": [],
                "text": "Adaptive blind separation of independent sources: A deflation approach"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144616256"
                        ],
                        "name": "D. Pham",
                        "slug": "D.-Pham",
                        "structuredName": {
                            "firstName": "Dinh",
                            "lastName": "Pham",
                            "middleNames": [
                                "Tuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94375334"
                        ],
                        "name": "P. Garrat",
                        "slug": "P.-Garrat",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Garrat",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Garrat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Thus the optimal contrast function is the same as the one obtained by the maximum likelihood approach [34], or the infomax approach [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15960752,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "04659300743b0a716154918340c6743d435de902",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Fig. 1. Absolute value of the crosstalk with respect to the number of samples (NS) used to estimate the cross cumulants. Each point is the average of 10 experiments. to estimate the cross-cumulants. Each point in Fig. 1 corresponds to the average over 10 experiments, in which the mixing matrix is randomly chosen: The matrix entries mC3 (i # j) are random numbers in the range [-1, +1]. With 500 samples, a residual crosstalk of about-20 dB is obtained. In the case of nonstationary signals, cross-cumulant estimation must be done on few samples and has a larger variance. Consequently, it can lead to more inaccurate estimation of the mixing matrix. We still obtained an interesting performance: a residual crosstalk of about-15 to-20 dB, with various signals (colored noise, speech) and statistics estimated over 500 samples. In this correspondence, we proved that the mixing matrix can be. estimated using fourth-ordercross-cumulants, for two mixtures of two non-Gaussian sources. Solutions are obtained by rooting a fourth-order polynomial equation. Using second-order cross-cumulants allows us to simplify the method; the solution is then obtained by rooting two second-order polynomial equations and gives the result if one source is Gaussian. The methods are then quite simple, but its roots are very sensitive to the accuracy of the estimated cumulants. In fact, this direct solution is less accurate than indirect methods, especially adaptive a l g o r i b s. Moreover, we restricted the study to the separation of two sources, and theoretical solutions for three sources or more seems not easily tractable. However, in the case of two mixtures of two sources, it may give a good starting point with a small computation cost for any adaptive algorithm. REFERENCES J.-F. Cardoso, \" Blind identification of independent signals, \" in Proc."
            },
            "slug": "Separation-of-a-mixture-of-independent-sources-a-Pham-Garrat",
            "title": {
                "fragments": [],
                "text": "Separation of a mixture of independent sources through a maximum likelihood approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proved that the mixing matrix can be estimated using fourth-ordercross-cumulants, for two mixtures of two non-Gaussian sources, and theoretical solutions for three sources or more seems not easily tractable."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "also applications inexploratory data analysisin the same way as the closely related method of projection pursuit [12], [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "This formulation of ICA also shows explicitly the connection between ICA and projection pursuit [11], [12], [16], [26]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "Several principles and methods have been developed to find such a linear representation, including principal component analysis [30], factor analysis [15], projection pursuit [12], [16], independent component analysis [27], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120727315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cce218b91cf634413ef9a71f702bd37b1a9ad2a6",
            "isKey": false,
            "numCitedBy": 590,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods. A number of practical issues concerning its application are addressed. A connection to multivariate density estimation is established, and its properties are investigated through simulation studies and application to real data. The goal of exploratory projection pursuit is to use the data to find low- (one-, two-, or three-) dimensional projections that provide the most revealing views of the full-dimensional data. With these views the human gift for pattern recognition can be applied to help discover effects that may not have been anticipated in advance. Since linear effects are directly captured by the covariance structure of the variable pairs (which are straightforward to estimate) the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables. Although arbitrary ..."
            },
            "slug": "Exploratory-Projection-Pursuit-Friedman",
            "title": {
                "fragments": [],
                "text": "Exploratory Projection Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods and the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210204"
                        ],
                        "name": "R. Vig\u00e1rio",
                        "slug": "R.-Vig\u00e1rio",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vig\u00e1rio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vig\u00e1rio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2190275"
                        ],
                        "name": "V. Jousm\u00e4ki",
                        "slug": "V.-Jousm\u00e4ki",
                        "structuredName": {
                            "firstName": "Veikko",
                            "lastName": "Jousm\u00e4ki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Jousm\u00e4ki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32405184"
                        ],
                        "name": "M. H\u00e4m\u00e4l\u00e4inen",
                        "slug": "M.-H\u00e4m\u00e4l\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "H\u00e4m\u00e4l\u00e4inen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H\u00e4m\u00e4l\u00e4inen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873866"
                        ],
                        "name": "R. Hari",
                        "slug": "R.-Hari",
                        "structuredName": {
                            "firstName": "Riitta",
                            "lastName": "Hari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "These applications include artifact ca cellation in EEG and MEG [36], [37], decomposition of evoked fields in MEG [38], and feature extraction of image data [25], [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17626526,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "104473d1e7eeae9a31f274d1194ce10408d217eb",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We have studied the application of an independent component analysis (ICA) approach to the identification and possible removal of artifacts from a magnetoencephalographic (MEG) recording. This statistical technique separates components according to the kurtosis of their amplitude distributions over time, thus distinguishing between strictly periodical signals, and regularly and irregularly occurring signals. Many artifacts belong to the last category. In order to assess the effectiveness of the method, controlled artifacts were produced, which included saccadic eye movements and blinks, increased muscular tension due to biting and the presence of a digital watch inside the magnetically shielded room. The results demonstrate the capability of the method to identify and clearly isolate the produced artifacts."
            },
            "slug": "Independent-Component-Analysis-for-Identification-Vig\u00e1rio-Jousm\u00e4ki",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The results demonstrate the capability of the independent component analysis (ICA) method to identify and clearly isolate the produced artifacts."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210204"
                        ],
                        "name": "R. Vig\u00e1rio",
                        "slug": "R.-Vig\u00e1rio",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vig\u00e1rio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vig\u00e1rio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2538598"
                        ],
                        "name": "J. S\u00e4rel\u00e4",
                        "slug": "J.-S\u00e4rel\u00e4",
                        "structuredName": {
                            "firstName": "Jaakko",
                            "lastName": "S\u00e4rel\u00e4",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. S\u00e4rel\u00e4"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "These applications include artifact ca cellation in EEG and MEG [36], [37], decomposition of evoked fields in MEG [38], and feature extraction of image data [25], [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "These applications inclu de artifact cancellation in EEG and MEG [36, 37], decomposition of evoked fields in MEG [38], and feature e xtraction of image data [35, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15342848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4013be99e08d5d981b6afec52ffda3990e6175d",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a novel approach to the problem of decomposition of auditory evoked fields (AEF), measured by magnetoencephalography (MEG), into basic components. This approach is based on independent component analysis (ICA), that separates components according to the kurtosis of their amplitude distribution over time. The fixed-point algorithm used extracts one independent component at a time, allowing the combination of a high resolution 122-channel whole-scalp neuromagnetometer, to a fast and very efficient implementation of ICA."
            },
            "slug": "Independent-Component-Analysis-in-Wave-of-Auditory-Vig\u00e1rio-S\u00e4rel\u00e4",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis in Wave Decomposition of Auditory Evoked Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A novel approach to the problem of decomposition of auditory evoked fields (AEF), measured by magnetoencephalography (MEG), into basic components is introduced, based on independent component analysis (ICA), that separates components according to the kurtosis of their amplitude distribution over time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 145
                            }
                        ],
                        "text": "The use of ICA for feature extraction is motivated by result s in neurosciences that suggest that the similar principle of redundancy reduc tion [2, 32] explains some aspects of the early processing of sensory data by the brain."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9526302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e309e441a38ccee6456bd02e0f1e894e44180d53",
            "isKey": false,
            "numCitedBy": 618,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural images contain characteristic statistical regularities that set them apart from purely random images. Understanding what these regularities are can enable natural images to be coded more efficiently. In this paper, we describe some of the forms of structure that are contained in natural images, and we show how these are related to the response properties of neurons at early stages of the visual system. Many of the important forms of structure require higher-order (i.e. more than linear, pairwise) statistics to characterize, which makes models based on linear Hebbian learning, or principal components analysis, inappropriate for finding efficient codes for natural images. We suggest that a good objective for an efficient coding of natural scenes is to maximize the sparseness of the representation, and we show that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex."
            },
            "slug": "Natural-image-statistics-and-efficient-coding.-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Natural image statistics and efficient coding."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that a good objective for an efficient coding of natural Scenes is to maximize the sparseness of the representation, and it is shown that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210204"
                        ],
                        "name": "R. Vig\u00e1rio",
                        "slug": "R.-Vig\u00e1rio",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vig\u00e1rio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vig\u00e1rio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17061385,
            "fieldsOfStudy": [
                "Computer Science",
                "Geology"
            ],
            "id": "133a230b7e5db7bde7752712123cf9160c1cbe90",
            "isKey": false,
            "numCitedBy": 593,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extraction-of-ocular-artefacts-from-EEG-using-Vig\u00e1rio",
            "title": {
                "fragments": [],
                "text": "Extraction of ocular artefacts from EEG using independent component analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Electroencephalography and clinical neurophysiology"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "of redundancy reduction [2], [32] explains some aspects of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474176"
                        ],
                        "name": "J. H. Hateren",
                        "slug": "J.-H.-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hateren",
                            "middleNames": [
                                "H.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46498936"
                        ],
                        "name": "A. Schaaf",
                        "slug": "A.-Schaaf",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "Schaaf",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schaaf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "These applications include artifact ca cellation in EEG and MEG [36], [37], decomposition of evoked fields in MEG [38], and feature extraction of image data [25], [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15666050,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1",
            "isKey": false,
            "numCitedBy": 802,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed."
            },
            "slug": "Independent-component-filters-of-natural-images-in-Hateren-Schaaf",
            "title": {
                "fragments": [],
                "text": "Independent component filters of natural images compared with simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis on a large set of natural images: there is no match, however, in calculated and measured distributions for the peak of the spatial frequency response."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37367995"
                        ],
                        "name": "X. Giannakopoulos",
                        "slug": "X.-Giannakopoulos",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Giannakopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Giannakopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "of our algorithm with other algorithms was performed in [13],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15288294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcca19c6d8f46158dcd1f508ed0a7f55d7ea17f5",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Several neural algorithms for Independent Component Analysis (ICA) have been introduced lately, but their computational properties have not yet been systematically studied. In this paper, we compare the accuracy, convergence speed, computational load, and other properties of five prominent neural or semi-neural ICA algorithms. The comparison reveals some interesting differences between the algorithms."
            },
            "slug": "An-Experimental-Comparison-of-Neural-ICA-Algorithms-Giannakopoulos-Karhunen",
            "title": {
                "fragments": [],
                "text": "An Experimental Comparison of Neural ICA Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper compares the accuracy, convergence speed, computational load, and other properties of five prominent neural or semi-neural ICA algorithms and reveals some interesting differences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "n [8, 7]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42795,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016914"
                        ],
                        "name": "J. Tukey",
                        "slug": "J.-Tukey",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tukey",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tukey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "This formulation of ICA also shows explicitly the connection between ICA and projection pursuit [11], [12], [16], [26]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7997450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e12d7b5498d251692d87abc3ee983c078fee7f5f",
            "isKey": false,
            "numCitedBy": 1652,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for the analysis of multivariate data is presented and is discussed in terms of specific examples. The algorithm seeks to find one-and two-dimensional linear projections of multivariate data that are relatively highly revealing."
            },
            "slug": "A-Projection-Pursuit-Algorithm-for-Exploratory-Data-Friedman-Tukey",
            "title": {
                "fragments": [],
                "text": "A Projection Pursuit Algorithm for Exploratory Data Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An algorithm for the analysis of multivariate data is presented and is discussed in terms of specific examples to find one-and two-dimensional linear projections of multivariable data that are relatively highly revealing."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064699986"
                        ],
                        "name": "D. Chakrabarti",
                        "slug": "D.-Chakrabarti",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Chakrabarti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Chakrabarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068766142"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "the data [1], [3], [5], [6], [23], [24], [27], [28], [31]:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "An earlier version (for kurtosis only) was derived as a fixedpoint iteration of a neural learning rule in [23], which is where its name comes from."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118274211,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "6ade6139ee56684cdf190f7f1212541fcb5ffb69",
            "isKey": false,
            "numCitedBy": 2269,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An apparatus for hydrolytic degradation of plastics in which plastic material is deposited into a tubular housing via a feed hopper. An elongated screw shaft has a first section in the form of a high pitch screw thread disposed below the feed hopper to receive and advance the material to a second section. The second section of the screw shaft is in the form of a lower pitch thread for compressing the plastic material and transferring it to a longer, third section in the form of kneading discs, from which material passes through an outlet nozzle section to a cyclone separator where trapped gases and liquid may be withdrawn. The tubular housing is vented upstream of the feed hopper and a water inlet pipe is disposed adjacent to the second section of the screw shaft, downstream of the feed hopper. The outlet nozzle section is provided with pressure measuring and regulating means and a liquid level measuring and regulating device."
            },
            "slug": "A-fast-fixed-point-algorithm-for-independent-Chakrabarti-Hoyer",
            "title": {
                "fragments": [],
                "text": "A fast fixed - point algorithm for independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2501863"
                        ],
                        "name": "D. Luenberger",
                        "slug": "D.-Luenberger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Luenberger",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Luenberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "According to the Kuhn-Tucker conditions [29], the optima of E{G(wT x)} under the constraint E{(wT x)2}= \u2016w\u20162 = 1 are obtained at points where E{xg(w x)}\u2212\u03b2w = 0 (17)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117941806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2bbea031af4e0aab292323c6dcd128050b26540",
            "isKey": false,
            "numCitedBy": 5461,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nEngineers must make decisions regarding the distribution of expensive resources in a manner that will be economically beneficial. This problem can be realistically formulated and logically analyzed with optimization theory. This book shows engineers how to use optimization theory to solve complex problems. Unifies the large field of optimization with a few geometric principles. Covers functional analysis with a minimum of mathematics. Contains problems that relate to the applications in the book."
            },
            "slug": "Optimization-by-Vector-Space-Methods-Luenberger",
            "title": {
                "fragments": [],
                "text": "Optimization by Vector Space Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book shows engineers how to use optimization theory to solve complex problems with a minimum of mathematics and unifies the large field of optimization with a few geometric principles."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11495976"
                        ],
                        "name": "H. Harman",
                        "slug": "H.-Harman",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Harman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Harman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Several principles and methods have been developed to find such a linear representation, including principal component analysis [30], factor analysis [15], projection pursuit [12, 16], independent component analysis [27], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61007780,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "ae316bff26f23b12a58e62942d1240b65f93999f",
            "isKey": false,
            "numCitedBy": 5141,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This thoroughly revised third edition of Harry H. Harman's authoritative text incorporates the many new advances made in computer science and technology over the last ten years. The author gives full coverage to both theoretical and applied aspects of factor analysis from its foundations through the most advanced techniques. This highly readable text will be welcomed by researchers and students working in psychology, statistics, economics, and related disciplines."
            },
            "slug": "Modern-factor-analysis-Harman",
            "title": {
                "fragments": [],
                "text": "Modern factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "We show in the Appendix B how to modify the algorithms in [24] to minimize the contrast functions used i n this paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "In f act, finding a single direction that maximizes negentropy is a form of projection pursuit, and could also be interpreted as estimation of a single independent component [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 133
                            }
                        ],
                        "text": "A reduc tion of the step size\u03bc in the stabilized version has a similar effect, as is well-known in stochastic approxi mat on methods [24, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "that are closely related related to those introduced in [24] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 124
                            }
                        ],
                        "text": "One popular way of formulating the ICA problem is to consider the estimation of the following generative model for the data [1, 3, 5, 6, 23, 24, 27, 28, 31]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysi  s by general nonlinear Hebbian-like learning rules.Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 101
                            }
                        ],
                        "text": "If it is singular or near-singular, t he dimension of the data must be reduced, for example with PCA [7, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "Because negentropy is invariant for invertible linear tran sformations [7], it is now obvious from (5) that finding an invertible transformation W that minimizes the mutual information is roughly equivalen t to finding directions in which the negentropy is maximized ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 141
                            }
                        ],
                        "text": "In [19] it was shown that these approximations are often cons iderably more accurate than the conventional, cumulant-based approximations in [7, 1, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "Non-Gaussianity of the independent components is necessary for the identifiability of the model (2), see [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "[31] , \u201cThe nonlinear PCA learning rule in independent component analysis,\u201dNeurocomputing,vol. 17, no. 1, pp. 25\u201346, 1997."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 10
                            }
                        ],
                        "text": "Following Comon [7], the ICA problem was formulated as the search for a linear transformation that minimizes the mutual information of the resulting components."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "If it is singular or near-singular, the dimension of the data must be reduced, for example with PCA [7], [28]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "[7] P. Comon, \u201cIndependent component analysis\u2014"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "For symmetric variables, this is a generaliz ation of the cumulant-based approximation in [7], which is obtained by takingG(yi) = y(4)i ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "Comon [7] showed how to obtain a more general formulation for ICA that does not need to assume an underlying data model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "Thus we define in this paper, following [7], the ICA of a random vector x as an invertible transformation s = Wx as in (1) where the matrix W is determined so that the mutual information of the transformed components s i i minimized."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 124
                            }
                        ],
                        "text": "We treat in this paper the problem of estimating the transfor mation given by (linear) independent component analysis (ICA) [7, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "This transformation is always possible; indeed, it can be accomplished by classical PCA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "Negentropy can also be interpreted as a measure of nongaussianity [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Following Comon [7], the ICA problem was formula ted as the search for a linear transformation that minimizes the mutual information of the resulting comp onents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis\u2014a new concept  ? Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "36:287\u2013314"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "A reduction of the step size in the stabilized version has a similar effect, as is well-known in stochastic approximation methods [24], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [24], it was shown how to add a bigradient feedback to the learning rule."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "This is equivalent to the learning rule in [24], except that the self-adaptation constant is different."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "This leads to neural (adaptive) algorithms that are closely related to those introduced in [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "This theorem can be considered a corollary of the theorem in [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "neuron, as given above (see also [24])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "In fact, finding a single direction that maximizes negentropy is a form of projection pursuit, and could also be interpreted as estimation of a single independent component [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "the data [1], [3], [5], [6], [23], [24], [27], [28], [31]:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "We show in the Appendix B how to modify the algorithms in [24] to minimize the contrast functions used in this paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis by general nonlinear Hebbianlike learning rules"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Processing, vol. 64, no. 3, pp. 301\u2013313, 1998."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145447865"
                        ],
                        "name": "A. Mansour",
                        "slug": "A.-Mansour",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mansour"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "In blind source separation[27], the observed values of x correspond to a realization of an m-dimensional discrete-time signal x(t), t = 1,2, ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 217
                            }
                        ],
                        "text": "Several principles and metho ds ave been developed to find such a linear representation, including principal component analysis [30] , factor analysis [15], projection pursuit [12, 16], independent component analysis [27], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 124
                            }
                        ],
                        "text": "We treat in this paper the problem of estimating the transfor mation given by (linear) independent component analysis (ICA) [7, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 124
                            }
                        ],
                        "text": "One popular way of formulating the ICA problem is to consider the estimation of the following generative model for the data [1, 3, 5, 6, 23, 24, 27, 28, 31]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208963683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3af7f1ecb210ad315aeae4ffae932d17f9f85d47",
            "isKey": true,
            "numCitedBy": 350,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-Separation-of-Sources-Mansour",
            "title": {
                "fragments": [],
                "text": "Blind Separation of Sources"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143929773"
                        ],
                        "name": "M. C. Jones",
                        "slug": "M.-C.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33734211"
                        ],
                        "name": "R. Sibson",
                        "slug": "R.-Sibson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Sibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sibson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "In [19] it was shown that these approximations are often considerably more accurate than the conventional, cumulant-based approximations in [1], [7], and [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "This formulation of ICA also shows explicitly the connection between ICA and projection pursuit [11], [12], [16], [26]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 125481163,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1ebb53a7e5cff86b2b42d1108a0fa81f571d8894",
            "isKey": false,
            "numCitedBy": 1404,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-projection-pursuit-Jones-Sibson",
            "title": {
                "fragments": [],
                "text": "What is projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856152"
                        ],
                        "name": "E. Parzen",
                        "slug": "E.-Parzen",
                        "structuredName": {
                            "firstName": "Emanuel",
                            "lastName": "Parzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Parzen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "First, we define the differential entropy of a random vector with density as follows [33]:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124462446,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7479833c1446aee007b91218f9bf2abddf30feb5",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "1.-Random-Variables-and-Stochastic-Processes-Parzen",
            "title": {
                "fragments": [],
                "text": "1. Random Variables and Stochastic Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145567905"
                        ],
                        "name": "I. Miller",
                        "slug": "I.-Miller",
                        "structuredName": {
                            "firstName": "Irwin",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "First, we define the differential entropy of a random vector with density as follows [33]:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122664770,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9e3329d7e7343bb34e6288627635a41917ba5339",
            "isKey": false,
            "numCitedBy": 4241,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probability,-Random-Variables,-and-Stochastic-Miller",
            "title": {
                "fragments": [],
                "text": "Probability, Random Variables, and Stochastic Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Several principles and methods have been developed to find such a linear representation, including principal component analysis [30], factor analysis [15], projection pursuit [12, 16], independent component analysis [27], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 181474578,
            "fieldsOfStudy": [],
            "id": "ae316bff26f23b12a58e62942d1240b65f93999f",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modern factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "These applications include artifact ca cellation in EEG and MEG [36], [37], decomposition of evoked fields in MEG [38], and feature extraction of image data [25], [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 65
                            }
                        ],
                        "text": "These applications inclu de artifact cancellation in EEG and MEG [36, 37], decomposition of evoked fields in MEG [38], and feature e xtraction of image data [35, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and E"
            },
            "venue": {
                "fragments": [],
                "text": "O  ja. Independent component analysis for identification of artifacts in magnetoencephalographic re  co dings. InAdvances in Neural Information Processing Systems  , volume 10, pages 229\u2013235. MIT Press"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 141
                            }
                        ],
                        "text": "In [19] it was shown that these approximations are often cons iderably more accurate than the conventional, cumulant-based approximations in [7, 1, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 96
                            }
                        ],
                        "text": "This formulation of ICA also shows explicitly the connection between ICA and projection pursuit [11, 12, 16, 26]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What is projection pursuit ?  J"
            },
            "venue": {
                "fragments": [],
                "text": "of the Royal Statistical Society, Ser. A  , 150:1\u201336"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "These applications include artifact ca cellation in EEG and MEG [36], [37], decomposition of evoked fields in MEG [38], and feature extraction of image data [25], [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 65
                            }
                        ],
                        "text": "These applications inclu de artifact cancellation in EEG and MEG [36, 37], decomposition of evoked fields in MEG [38], and feature e xtraction of image data [35, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and E"
            },
            "venue": {
                "fragments": [],
                "text": "O  ja. Independent component analysis for identification of artifacts in magnetoencephalographic re  co dings. InAdvances in Neural Information Processing Systems  , volume 10, pages 229\u2013235. MIT Press"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Projection pursuit. The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Projection pursuit. The Annals of Statistics"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 141
                            }
                        ],
                        "text": "In [19] it was shown that these approximations are often cons iderably more accurate than the conventional, cumulant-based approximations in [7, 1, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 124
                            }
                        ],
                        "text": "One popular way of formulating the ICA problem is to consider the estimation of the following generative model for the data [1, 3, 5, 6, 23, 24, 27, 28, 31]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new learning al  gorithm for blind source separation"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 8  , pages 757\u2013763. MIT Press"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 124
                            }
                        ],
                        "text": "One popular way of formulating the ICA problem is to consider the estimation of the following generative model for the data [1, 3, 5, 6, 23, 24, 27, 28, 31]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The nonlinear PCA learning rule in independent c  omponent analysis.Neurocomputing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Some extensions of the methods introduced in this paper are presented in [20], in wh ch t e problem of noisy data is addressed, and in [22], which deals with the situation where there are more ind pendent components than observed variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast independent component analysis wit  h noisy data using gaussian moments"
            },
            "venue": {
                "fragments": [],
                "text": " Proc. Int. Symp. on Circuits and Systems  , pages V57\u2013V61, Orlando, Florida"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Thus the optimal contrast function is the same as the one obtained by the maximum likelihood approach [34], or the inf omax approach [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Separation of a mix  ture of independent sources through a maximum likelihood approach"
            },
            "venue": {
                "fragments": [],
                "text": " Proc. EUSIPCO, pages 771\u2013774"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Several principles and metho ds ave been developed to find such a linear representation, including principal component analysis [30] , factor analysis [15], projection pursuit [12, 16], independent component analysis [27], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Harman.Modern Factor Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "University of Chicago Press,"
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "According to the Kuhn\u2013Tucker conditions [29], the optima of under the constraint are obtained at points where"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Luenberger,  Optimization by Vector Space Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "The vector is thus an estimator of a row of the matrix ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A NALYSIS OF ESTIMATORS AND CHOICE OF CONTRAST FUNCTION"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What is projection pursuit ? J. of the Royal Statistical Society"
            },
            "venue": {
                "fragments": [],
                "text": "What is projection pursuit ? J. of the Royal Statistical Society"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 157
                            }
                        ],
                        "text": "These applications inclu de artifact cancellation in EEG and MEG [36, 37], decomposition of evoked fields in MEG [38], and feature e xtraction of image data [35, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent comp onent filters of natural images compared with simple cells in primary visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "In [19] it was shown that these approximations are often considerably more accurate than the conventional, cumulant-based approximations in [1], [7], and [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "the data [1], [3], [5], [6], [23], [24], [27], [28], [31]:"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new learning algorithm for blind source separation"
            },
            "venue": {
                "fragments": [],
                "text": "inAdvances in Neural Inform. Processing 8. Cambridge: MIT Press, 1996, pp. 757\u2013763."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "These applications include artifact ca cellation in EEG and MEG [36], [37], decomposition of evoked fields in MEG [38], and feature extraction of image data [25], [35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extraction of ocular artifacts from EEG using independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": " Electroenceph. Clin. Neurophysiol., vol. 103, no. 3, pp. 395\u2013404, 1997."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The FastICA MATLAB package Available at http://www.cis.hut.fi/projects/ica/fastica"
            },
            "venue": {
                "fragments": [],
                "text": "The FastICA MATLAB package Available at http://www.cis.hut.fi/projects/ica/fastica"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "the data [1], [3], [5], [6], [23], [24], [27], [28], [31]:"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing and Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "New York: Wiley,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "These applications include artifact ca cellation in EEG and MEG [36], [37], decomposition of evoked fields in MEG [38], and feature extraction of image data [25], [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 65
                            }
                        ],
                        "text": "These applications inclu de artifact cancellation in EEG and MEG [36, 37], decomposition of evoked fields in MEG [38], and feature e xtraction of image data [35, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extraction of ocular artifacts from EEG usi  ng independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": " Electroenceph. Clin. Neurophysiol.  , 103(3):395\u2013404"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Several principles and methods have been developed to find such a linear representation, including principal component analysis [30], factor analysis [15], projection pursuit [12], [16], independent component analysis [27], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modern Factor Analysis, 2nd ed"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The FastICA MATLAB package"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast independent component analysis with noisy data using gaussian moments"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int. Symp. on Circuits and Systems"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "Almost identical results have also been obtained in [5] for another algorithm."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "The convergenc e of the orthonormalization method in (26), which may be considered a variation of Potter\u2019s formula (see [5]), is proven in the Appendix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 124
                            }
                        ],
                        "text": "One popular way of formulating the ICA problem is to consider the estimation of the following generative model for the data [1, 3, 5, 6, 23, 24, 27, 28, 31]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Equivariant adaptive s  ource separation.IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Trans. on Signal Processing,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 21
                            }
                        ],
                        "text": "In feature extraction[4, 25], si is the coefficient of thei-th feature in the observed data vector x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 112
                            }
                        ],
                        "text": "Tak ing also into account the fact that most independent components encountered in practice are super-Gaussia n [3, 25], one reaches the conclusion that as a general-purpose contrast function, one should choose a fun ctionG that resembles rather Gopt(u) = |u| \u03b1,where\u03b1 < 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 157
                            }
                        ],
                        "text": "These applications inclu de artifact cancellation in EEG and MEG [36, 37], decomposition of evoked fields in MEG [38], and feature e xtraction of image data [35, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image fea ture extraction by sparse coding and independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": " Proc. Int. Conf. on Pattern Recognition (ICPR\u201998)  , pages 1268\u20131273, Brisbane, Australia"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Some extensions of the methods introduced in this paper are presented in [20], in which the problem of noisy data is addressed, and in [22], which deals with the situation where there are more independent components than observed variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast independent component analysis with noisy data using gaussian moments"
            },
            "venue": {
                "fragments": [],
                "text": "inProc. Int. Symp. Circuits Syst.,  Orlando, FL, 1999, to be published."
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 96
                            }
                        ],
                        "text": "This formulation of ICA also shows explicitly the connection between ICA and projection pursuit [11, 12, 16, 26]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A projection pursuit algo  rithm for exploratory data analysis"
            },
            "venue": {
                "fragments": [],
                "text": " IEEE Trans. of Computers  , c-23(9):881\u2013890"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 112
                            }
                        ],
                        "text": "Tak ing also into account the fact that most independent components encountered in practice are super-Gaussia n [3, 25], one reaches the conclusion that as a general-purpose contrast function, one should choose a fun ctionG that resembles rather Gopt(u) = |u| \u03b1,where\u03b1 < 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "Thus the optimal contrast function is the same as the one obtained by the maximum likelihood approach [34], or the inf omax approach [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 124
                            }
                        ],
                        "text": "One popular way of formulating the ICA problem is to consider the estimation of the following generative model for the data [1, 3, 5, 6, 23, 24, 27, 28, 31]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information-maximizat ion approach to blind separation and blind deconvolution.Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 25,
            "methodology": 43,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 64,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Fast-and-robust-fixed-point-algorithms-for-analysis-Hyv\u00e4rinen/8d946c3eb1d1db376a89ad9342282163b5ae0930?sort=total-citations"
}