{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722627"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "This leads to the following results (see technical detail in [24]): term-III = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T r t =1 I i =1 I j=1 \u03be i, j,r,s r (t ) log a i, j (43) and term-IV = R r =1 T r t =1 I i =1 d(r, t \u2212 1, i ) I j=1 a i, j log a i, j (44) where a i, j = p(q r,t = j|q r,t \u22121 = i, s, ,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "(39) The simplification process for the second term in (37) is can be found in [24], which gives the final result of term-II = R r =1 T r t =1 I i =1 d(r, t, i ) \u03c7 r,t p(\u03c7 r,t |q r,t = i; ) \u00d7 log p(\u03c7 r,t |q r,t = i; ), (40) where d(r, t, i ) = s d (s)p(q r,t = i |s, , ) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14877890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "509eb97e89ac67c591dc4078c826815bc462a187",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "In this book, we introduce the background and mainstream methods of probabilistic modeling and discriminative parameter optimization for speech recognition. The specific models treated in depth include the widely used exponential-family distributions and the hidden Markov model. A detailed study is presented on unifying the common objective functions for discriminative learning in speech recognition, namely maximum mutual information (MMI), minimum classification error, and minimum phone/word error. The unification is presented, with rigorous mathematical analysis, in a common rational-function form. This common form enables the use of the growth transformation (or extended Baum\u2013Welch) optimization framework in discriminative learning of model parameters. In addition to all the necessary introduction of the background and tutorial material on the subject, we also included technical details on the derivation of the parameter optimization formulas for exponential-family distribut ons, discrete hidden Markov models (HMMs), and continuous-density HMMs in discriminative learning. Selected experimental results obtained by the authors in firsthand are presented to show that discriminative learning can lead to superior speech recognition performance over conventional parameter learning. Details on major algorithmic implementation issues with practical significance are provided to enable the practitioners to directly reproduce the theory in the earlier part of the book into engineering practice. Table of Contents: Introduction and Background / Statistical Speech Recognition: A Tutorial / Discriminative Learning: A Unified Objective Function / Discriminative Learning Algorithm for Exponential-Family Distributions / Discriminative Learning Algorithm for Hidden Markov Model / Practical Implementation of Discriminative Learning / Selected Experimental Results / Epilogue / Major Symbols Used in the Book and Their Descriptions / Mathematical Notation / Bibliography"
            },
            "slug": "Discriminative-Learning-for-Speech-Recognition-He-Deng",
            "title": {
                "fragments": [],
                "text": "Discriminative Learning for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This book introduces the background and mainstream methods of probabilistic modeling and discriminative parameter optimization for speech recognition and includes technical details on the derivation of the parameter optimization formulas for exponential-family distribut ons, discrete hidden Markov models (HMMs), and continuous-density HMMs in discriminating learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144490010"
                        ],
                        "name": "R. Schl\u00fcter",
                        "slug": "R.-Schl\u00fcter",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Schl\u00fcter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schl\u00fcter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3153147"
                        ],
                        "name": "Wolfgang Macherey",
                        "slug": "Wolfgang-Macherey",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Macherey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Macherey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067884890"
                        ],
                        "name": "Boris M\u00fcller",
                        "slug": "Boris-M\u00fcller",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "(10) Another popular form of g s r (X r ; ) and G S r (X r ; ) (the latter has similar effects to (10) and was used in [54]) is \u23a7 \u23a8 \u23a9 g S r (X r ; ) = log p \u03b7 (X r , S r"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16596710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2fa4beb417215d721f0a4aeee72d49347c966e3",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Comparison-of-discriminative-training-criteria-and-Schl\u00fcter-Macherey",
            "title": {
                "fragments": [],
                "text": "Comparison of discriminative training criteria and optimization methods for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144881562"
                        ],
                        "name": "P. Liu",
                        "slug": "P.-Liu",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108150800"
                        ],
                        "name": "Cong Liu",
                        "slug": "Cong-Liu",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36357862"
                        ],
                        "name": "Hui Jiang",
                        "slug": "Hui-Jiang",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705574"
                        ],
                        "name": "F. Soong",
                        "slug": "F.-Soong",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Soong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Soong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108875405"
                        ],
                        "name": "Ren-Hua Wang",
                        "slug": "Ren-Hua-Wang",
                        "structuredName": {
                            "firstName": "Ren-Hua",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ren-Hua Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6419503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5c06218fe7be70180736cc5253413713eab7c7a",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel optimization algorithm called constrained line search (CLS) for discriminative training (DT) of Gaussian mixture continuous density hidden Markov model (CDHMM) in speech recognition. The CLS method is formulated under a general framework for optimizing any discriminative objective functions including maximum mutual information (MMI), minimum classification error (MCE), minimum phone error (MPE)/minimum word error (MWE), etc. In this method, discriminative training of HMM is first cast as a constrained optimization problem, where Kullback-Leibler divergence (KLD) between models is explicitly imposed as a constraint during optimization. Based upon the idea of line search, we show that a simple formula of HMM parameters can be found by constraining the KLD between HMM of two successive iterations in an quadratic form. The proposed CLS method can be applied to optimize all model parameters in Gaussian mixture CDHMMs, including means, covariances, and mixture weights. We have investigated the proposed CLS approach on several benchmark speech recognition databases, including TIDIGITS, Resource Management (RM), and Switchboard. Experimental results show that the new CLS optimization method consistently outperforms the conventional EBW method in both recognition performance and convergence behavior."
            },
            "slug": "A-Constrained-Line-Search-Optimization-Method-for-Liu-Liu",
            "title": {
                "fragments": [],
                "text": "A Constrained Line Search Optimization Method for Discriminative Training of HMMs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the new CLS optimization method consistently outperforms the conventional EBW method in both recognition performance and convergence behavior."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145320076"
                        ],
                        "name": "W. Chou",
                        "slug": "W.-Chou",
                        "structuredName": {
                            "firstName": "Wu",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Chou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14496197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e93c3ea36a5bd49a11601e1a4985848476c3a8bf",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, various discriminative learning techniques for HMMs have consistently yielded significant benefits in speech recognition. In this paper, we present a novel optimization technique using the minimum classification error (MCE) criterion to optimize the HMM parameters. Unlike maximum mutual information training where an extended Baum-Welch (EBW) algorithm exists to optimize its objective function, for MCE training the original EBW algorithm cannot be directly applied. In this work, we extend the original EBW algorithm and derive a novel method for MCE-based model parameter estimation. Compared with conventional gradient descent methods for MCE learning, the proposed method gives a solid theoretical basis, stable convergence, and it is well suited for the large-scale batch-mode training process essential in large-scale speech recognition and other pattern recognition applications. Evaluation experiments, including model training and speech recognition, are reported on both a small vocabulary task (TI-digits) and a large vocabulary task (WSJ), where the effectiveness of the proposed method is demonstrated. We expect new future applications and success of this novel learning method in general pattern recognition and multimedia processing, in addition to speech and audio processing applications we present in this paper"
            },
            "slug": "A-Novel-Learning-Method-for-Hidden-Markov-Models-in-He-Deng",
            "title": {
                "fragments": [],
                "text": "A Novel Learning Method for Hidden Markov Models in Speech and Audio Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work extends the original EBW algorithm and derive a novel method for MCE-based model parameter estimation and gives a solid theoretical basis, stable convergence, and it is well suited for the large-scale batch-mode training process essential in large- scale speech recognition and other pattern recognition applications."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Workshop on Multimedia Signal Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3153147"
                        ],
                        "name": "Wolfgang Macherey",
                        "slug": "Wolfgang-Macherey",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Macherey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Macherey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2449367"
                        ],
                        "name": "L. Haferkamp",
                        "slug": "L.-Haferkamp",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Haferkamp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Haferkamp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144490010"
                        ],
                        "name": "R. Schl\u00fcter",
                        "slug": "R.-Schl\u00fcter",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Schl\u00fcter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schl\u00fcter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9909937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ab27d913b027cb6c621df529883ca341ced1e61",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminative training criteria have been shown to consistently outperform maximum likelihood trained speech recognition systems. In this paper we employ the Minimum Classification Error (MCE) criterion to optimize the parameters of the acoustic model of a large scale speech recognition system. The statistics for both the correct and the competing model are solely collected on word lattices without the use of N -best lists. Thus, particularly for long utterances, the number of sentence alternatives taken into account is significantly larger compared to N -best lists. The MCE criterion is embedded in an extended unifying approach for a class of discriminative training criteria which allows for direct comparison of the performance gain obtained with the improvements of other commonly used criteria such as Maximum Mutual Information (MMI) and Minimum Word Error (MWE). Experiments conducted on large vocabulary tasks show a consistent performance gain for MCE over MMI. Moreover, the improvements obtained with MCE turn out to be in the same order of magnitude as the performance gains obtained with the MWE criterion."
            },
            "slug": "Investigations-on-error-minimizing-training-for-in-Macherey-Haferkamp",
            "title": {
                "fragments": [],
                "text": "Investigations on error minimizing training criteria for discriminative training in automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The MCE criterion is embedded in an extended unifying approach for a class of discriminative training criteria which allows for direct comparison of the performance gain obtained with the improvements of other commonly used criteria such as Maximum Mutual Information and Minimum Word Error."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749064"
                        ],
                        "name": "D. Kanevsky",
                        "slug": "D.-Kanevsky",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Kanevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kanevsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 21096264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b18111ae3da2cdd6586f875aaffbeb2755e50ba2",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The well-known Baum-Eagon (1967) inequality provides an effective iterative scheme for homogeneous polynomials with positive coefficients over a domain of probability values /spl Delta/. The Baum-Eagon inequality was extended to rational functions over /spl Delta/ by Gopalakrishnan et. al. (see IEEE Trans. Inform. Theory, Jan. 1991) and a variant of this extended inequality was used by Merialdo (see Proc. ICASSP-88, 1988, and IEEE Trans. Acoust., Speech, Signal Processing, April 1994) for the maximum mutual information training of a connected digit recognizer. However, in many applications (e.g. corrective training) we are interested in maximizing an objective function over a domain D that is different from /spl Delta/ and may be defined by non-linear constraints. We show how to extend the basic inequality of Gopalakrishnan to (not necessary rational) functions that are defined on general manifolds. We describe an effective iterative scheme that is based on this inequality and its application to estimation problems via minimum information discrimination."
            },
            "slug": "A-generalization-of-the-Baum-algorithm-to-functions-Kanevsky",
            "title": {
                "fragments": [],
                "text": "A generalization of the Baum algorithm to functions on non-linear manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An effective iterative scheme is described that is based on the basic inequality of Gopalakrishnan to (not necessary rational) functions that are defined on general manifolds and its application to estimation problems via minimum information discrimination."
            },
            "venue": {
                "fragments": [],
                "text": "1995 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061634414"
                        ],
                        "name": "Wu Hou",
                        "slug": "Wu-Hou",
                        "structuredName": {
                            "firstName": "Wu",
                            "lastName": "Hou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wu Hou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9391905"
                        ],
                        "name": "Chin-Hui Lee",
                        "slug": "Chin-Hui-Lee",
                        "structuredName": {
                            "firstName": "Chin-Hui",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Hui Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "One popular definition takes the following form [29]: G S r (X r ; ) = log 1 N N i =1 p \u03b7 (X r , s r,i"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "\u2026framework of GT also alleviates the need for other heuristics, e.g., tuning the parameter-dependent learning rate as in some other methods [29], [52] , for the objective function (26), the GT-based optimization algorithm will construct an auxiliary function of the following form: , this\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "\u2026the loss function can be defined for each training token r, and it is usually defined through a sigmoid function as originally proposed in [28], [29] : IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 where \u03b1 > 0 is the slope of the\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Consequently, the objective function of MCE is usually optimized using the generalized probabilistic descent (GPD) [9], [28], [29] algorithm or other gradient-based methods [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "The MCE-based classifier design is a discriminant-function-based approach to pattern recognition [1], [28], [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "This flexibility in classifier design for sequential pattern recognition has been a fertile field of research, and many approaches have been developed [22], [29], [47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11313392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdc3542552842d826a661f21e87917b976b4f7ee",
            "isKey": true,
            "numCitedBy": 729,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A critical component in the pattern matching approach to speech recognition is the training algorithm, which aims at producing typical (reference) patterns or models for accurate pattern comparison. In this paper, we discuss the issue of speech recognizer training from a broad perspective with root in the classical Bayes decision theory. We differentiate the method of classifier design by way of distribution estimation and the discriminative method of minimizing classification error rate based on the fact that in many realistic applications, such as speech recognition, the real signal distribution form is rarely known precisely. We argue that traditional methods relying on distribution estimation are suboptimal when the assumed distribution form is not the true one, and that \"optimality\" in distribution estimation does not automatically translate into \"optimality\" in classifier design. We compare the two different methods in the context of hidden Markov modeling for speech recognition. We show the superiority of the minimum classification error (MCE) method over the distribution estimation method by providing the results of several key speech recognition experiments. In general, the MCE method provides a significant reduction of recognition error rate."
            },
            "slug": "Minimum-classification-error-rate-methods-for-Juang-Hou",
            "title": {
                "fragments": [],
                "text": "Minimum classification error rate methods for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The issue of speech recognizer training from a broad perspective with root in the classical Bayes decision theory is discussed, and the superiority of the minimum classification error (MCE) method over the distribution estimation method is shown by providing the results of several key speech recognition experiments."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153683235"
                        ],
                        "name": "Yi Li",
                        "slug": "Yi-Li",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809809"
                        ],
                        "name": "L. Shapiro",
                        "slug": "L.-Shapiro",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Shapiro",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shapiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748118"
                        ],
                        "name": "J. Bilmes",
                        "slug": "J.-Bilmes",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Bilmes",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bilmes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1188030,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1dc65ee55f3eb1940a2cbcaaef22338be4d19fa4",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a two-phase generative/discriminative learning procedure for the recognition of classes of objects and concepts in outdoor scenes. Our method uses both multiple types of object features and context within the image. The generative phase normalizes the description length of images, which can have an arbitrary number of extracted features of each type. In the discriminative phase, a classifier learns which images, as represented by this fixed-length description, contain the target object. We have tested the approach by comparing it to several other approaches in the literature and by experimenting with several different data sets and combinations of features. Our results, using color, texture, and structure features, show a significant improvement over previously published results in image retrieval. Using salient region features, we are competitive with recent results in object recognition"
            },
            "slug": "A-generative/discriminative-learning-algorithm-for-Li-Shapiro",
            "title": {
                "fragments": [],
                "text": "A generative/discriminative learning algorithm for image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work has developed a two-phase generative/discriminative learning procedure for the recognition of classes of objects and concepts in outdoor scenes using both multiple types of object features and context within the image."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740397"
                        ],
                        "name": "M. Gales",
                        "slug": "M.-Gales",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Gales",
                            "middleNames": [
                                "John",
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gales"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109232944"
                        ],
                        "name": "Do Yeong Kim",
                        "slug": "Do-Yeong-Kim",
                        "structuredName": {
                            "firstName": "Do",
                            "lastName": "Kim",
                            "middleNames": [
                                "Yeong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Do Yeong Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716393"
                        ],
                        "name": "P. Woodland",
                        "slug": "P.-Woodland",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Woodland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Woodland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1276336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c3f552d130f0cdbd86ab363b093f6c7e1deb25a",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the use of discriminative schemes based on themaximum mutual information (MMI) and minimum phone error (MPE) objective functions for both task and gender adaptation. A method for incorporating prior information into the discriminative training framework is described. If an appropriate form of prior distribution is used, then this may be implemented by simply altering the values of the counts used for parameter estimation. The prior distribution can be based around maximum likelihood parameter estimates, giving a technique known as I-smoothing, or for adaptation it can be based around a MAP estimate of the ML parameters, leading to MMI-MAP, or MPE-MAP.MMI-MAP isshown tobe effectivefor taskadaptation, where data from one task (Voicemail) is used to adapt a HMM set trained on another task (Switchboard). MPE-MAP is shown to be effective for generating gender-dependent models for Broadcast News transcription."
            },
            "slug": "MMI-MAP-and-MPE-MAP-for-acoustic-model-adaptation-Povey-Gales",
            "title": {
                "fragments": [],
                "text": "MMI-MAP and MPE-MAP for acoustic model adaptation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "MMI-MAP is shown to be effective for generating gender-dependent models for Broadcast News transcription, and MPE-MAP, a method for incorporating prior information into the discriminative training framework, is described."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718611"
                        ],
                        "name": "L. Mangu",
                        "slug": "L.-Mangu",
                        "structuredName": {
                            "firstName": "Lidia",
                            "lastName": "Mangu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mangu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698208"
                        ],
                        "name": "G. Saon",
                        "slug": "G.-Saon",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Saon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Saon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38940652"
                        ],
                        "name": "H. Soltau",
                        "slug": "H.-Soltau",
                        "structuredName": {
                            "firstName": "Hagen",
                            "lastName": "Soltau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Soltau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 197
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 75541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a70ac6680d319905d6bfca4cea0b4dc6c15f420",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "MPE (minimum phone error) is a previously introduced technique for discriminative training of HMM parameters. fMPE applies the same objective function to the features, transforming the data with a kernel-like method and training millions of parameters, comparable to the size of the acoustic model. Despite the large number of parameters, fMPE is robust to over-training. The method is to train a matrix projecting from posteriors of Gaussians to a normal size feature space, and then to add the projected features to normal features such as PLP. The matrix is trained from a zero start using a linear method. Sparsity of posteriors ensures speed in both training and test time. The technique gives similar improvements to MPE (around 10% relative). MPE on top of fMPE results in error rates up to 6.5% relative better than MPE alone, or more if multiple layers of transform are trained."
            },
            "slug": "fMPE:-discriminatively-trained-features-for-speech-Povey-Kingsbury",
            "title": {
                "fragments": [],
                "text": "fMPE: discriminatively trained features for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2452389"
                        ],
                        "name": "E. McDermott",
                        "slug": "E.-McDermott",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "McDermott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798550"
                        ],
                        "name": "Timothy J. Hazen",
                        "slug": "Timothy-J.-Hazen",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Hazen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy J. Hazen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9332945"
                        ],
                        "name": "Jonathan Le Roux",
                        "slug": "Jonathan-Le-Roux",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726970"
                        ],
                        "name": "A. Nakamura",
                        "slug": "A.-Nakamura",
                        "structuredName": {
                            "firstName": "Atsushi",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715709"
                        ],
                        "name": "S. Katagiri",
                        "slug": "S.-Katagiri",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Katagiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katagiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "Consequently, the objective function of MCE is usually optimized using the generalized probabilistic descent (GPD) [9], [28], [29] algorithm or other gradient-based methods [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1044834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7bdb332ce9a601cdd1d0f12685eab6d8b8374250",
            "isKey": false,
            "numCitedBy": 623,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "The minimum classification error (MCE) framework for discriminative training is a simple and general formalism for directly optimizing recognition accuracy in pattern recognition problems. The framework applies directly to the optimization of hidden Markov models (HMMs) used for speech recognition problems. However, few if any studies have reported results for the application of MCE training to large-vocabulary, continuous-speech recognition tasks. This article reports significant gains in recognition performance and model compactness as a result of discriminative training based on MCE training applied to HMMs, in the context of three challenging large-vocabulary (up to 100 k word) speech recognition tasks: the Corpus of Spontaneous Japanese lecture speech transcription task, a telephone-based name recognition task, and the MIT Jupiter telephone-based conversational weather information task. On these tasks, starting from maximum likelihood (ML) baselines, MCE training yielded relative reductions in word error ranging from 7% to 20%. Furthermore, this paper evaluates the use of different methods for optimizing the MCE criterion function, as well as the use of precomputed recognition lattices to speed up training. An overview of the MCE framework is given, with an emphasis on practical implementation issues"
            },
            "slug": "Discriminative-Training-for-Large-Vocabulary-Speech-McDermott-Hazen",
            "title": {
                "fragments": [],
                "text": "Discriminative Training for Large-Vocabulary Speech Recognition Using Minimum Classification Error"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This article reports significant gains in recognition performance and model compactness as a result of discriminative training based on MCE training applied to HMMs, in the context of three challenging large-vocabulary speech recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "The MCE-based classifier design is a discriminant-function-based approach to pattern recognition [1], [28], [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31220579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1339348aeef592802288d9d929a085cb3ae61c4b",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions."
            },
            "slug": "A-Theory-of-Adaptive-Pattern-Classifiers-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Adaptive Pattern Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions, and there is an important tradeoff between speed and accuracy of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 571789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "677160bfb207cdd4ac0328e276d1b91b81f6b918",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, we have developed a novel discriminative training method named large-margin minimum classification error (LM-MCE) training that incorporates the idea of discriminative margin into the conventional minimum classification error (MCE) training method. In our previous work, this novel approach was formulated specifically for the MCE training using the sigmoid loss function and its effectiveness was demonstrated on the TIDIGITS task alone. In this paper two additional contributions are made. First, we formulate LM-MCE as a Bayes risk minimization problem whose loss function not only includes empirical error rates but also a margin-bound risk. This new formulation allows us to extend the same technique to a wide variety of MCE based training. Second, we have successfully applied LM-MCE training approach to the Microsoft internal large vocabulary telephony speech recognition task (with 2000 hours of training data and 120K of vocabulary) and achieved significant recognition accuracy improvement across-the-board. To our best knowledge, this is the first time that the large-margin approach is demonstrated to be successful in large-scale speech recognition tasks."
            },
            "slug": "Large-Margin-Minimum-Classification-Error-Training-Yu-Deng",
            "title": {
                "fragments": [],
                "text": "Large-Margin Minimum Classification Error Training for Large-Scale Speech Recognition Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work has successfully applied LM-MCE training approach to the Microsoft internal large vocabulary telephony speech recognition task and achieved significant recognition accuracy improvement across-the-board."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768120"
                        ],
                        "name": "T. Jebara",
                        "slug": "T.-Jebara",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Jebara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jebara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34343512,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05fd5aa1926c99ce0b658d4a85803f9f7bda1eb3",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 191,
            "paperAbstract": {
                "fragments": [],
                "text": "I propose a common framework that combines three different paradigms in machine learning: generative, discriminative and imitative learning. A generative probabilistic distribution is a principled way to model many machine learning and machine perception problems. Therein, one provides domain specific knowledge in terms of structure and parameter priors over the joint space of variables. Bayesian networks and Bayesian statistics provide a rich and flexible language for specifying this knowledge and subsequently refining it with data and observations. The final result is a distribution that is a good generator of novel exemplars. Conversely, discriminative algorithms adjust a possibly non-distributional model to data optimizing for a specific task, such as classification or prediction. This typically leads to superior performance yet compromises the flexibility of generative modeling. I present Maximum Entropy Discrimination (MED) as a framework to combine both discriminative estimation and generative probability densities. Calculations involve distributions over parameters, margins, and priors and are provably and uniquely solvable for the exponential family. Extensions include regression, feature selection, and transduction. SVMs are also naturally subsumed and can be augmented with, for example, feature selection, to obtain substantial improvements. To extend to mixtures of exponential families, I derive a discriminative variant of the ExpectationMaximization (EM) algorithm for latent discriminative learning (or latent MED). While EM and Jensen lower bound log-likelihood, a dual upper bound is made possible via a novel reverse-Jensen inequality. The variational upper bound on latent log-likelihood has the same form as EM bounds, is computable efficiently and is globally guaranteed. It permits powerful discriminative learning with the wide range of contemporary probabilistic mixture models (mixtures of Gaussians, mixtures of multinomials and hidden Markov models). We provide empirical results on standardized data sets that demonstrate the viability of the hybrid discriminative-generative approaches of MED and reverse-Jensen bounds over state of the art discriminative techniques or generative approaches. Subsequently, imitative learning is presented as another variation on generative modeling which also learns from exemplars from an observed data source. However, the distinction is that the generative model is an agent that is interacting in a much more complex surrounding external world. It is not efficient to model the aggregate space in a generative setting. I demonstrate that imitative learning (under appropriate conditions) can be adequately addressed as a discriminative prediction task which outperforms the usual generative approach. This discriminative-imitative learning approach is applied with a generative perceptual system to synthesize a real-time agent that learns to engage in social interactive behavior. Thesis Supervisor: Alex Pentland Title: Toshiba Professor of Media Arts and Sciences, MIT Media Lab Discriminative, Generative and Imitative Learning"
            },
            "slug": "Discriminative,-generative-and-imitative-learning-Jebara-Pentland",
            "title": {
                "fragments": [],
                "text": "Discriminative, generative and imitative learning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is demonstrated that imitative learning can be adequately addressed as a discriminative prediction task which outperforms the usual generative approach and is applied with a generative perceptual system to synthesize a real-time agent that learns to engage in social interactive behavior."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145492758"
                        ],
                        "name": "S. Axelrod",
                        "slug": "S.-Axelrod",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Axelrod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Axelrod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782589"
                        ],
                        "name": "Vaibhava Goel",
                        "slug": "Vaibhava-Goel",
                        "structuredName": {
                            "firstName": "Vaibhava",
                            "lastName": "Goel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vaibhava Goel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687020"
                        ],
                        "name": "R. Gopinath",
                        "slug": "R.-Gopinath",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Gopinath",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gopinath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145321771"
                        ],
                        "name": "P. Olsen",
                        "slug": "P.-Olsen",
                        "structuredName": {
                            "firstName": "Peder",
                            "lastName": "Olsen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Olsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2484153"
                        ],
                        "name": "K. Visweswariah",
                        "slug": "K.-Visweswariah",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Visweswariah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Visweswariah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "It was later extended and applied to MMI-based discriminative training of continuous-density HMMs (CDHMMs) [2], [20], [41], [59], [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2515414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bb978aed6b2a5b750090fb13aa004e40fe8a898",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study discriminative training of acoustic models for speech recognition under two criteria: maximum mutual information (MMI) and a novel \"error-weighted\" training technique. We present a proof that the standard MMI training technique is valid for a very general class of acoustic models with any kind of parameter tying. We report experimental results for subspace constrained Gaussian mixture models (SCGMMs), where the exponential model weights of all Gaussians are required to belong to a common \"tied\" subspace, as well as for subspace precision and mean (SPAM) models which impose separate subspace constraints on the precision matrices (i.e., inverse covariance matrices) and means. It has been shown previously that SCGMMs and SPAM models generalize and yield significant error rate improvements over previously considered model classes such as diagonal models, models with semitied covariances, and extended maximum likelihood linear transformation (EMLLT) models. We show here that MMI and error-weighted training each individually result in over 20% relative reduction in word error rate on a digit task over maximum-likelihood (ML) training. We also show that a gain of as much as 28% relative can be achieved by combining these two discriminative estimation techniques"
            },
            "slug": "Discriminative-Estimation-of-Subspace-Constrained-Axelrod-Goel",
            "title": {
                "fragments": [],
                "text": "Discriminative Estimation of Subspace Constrained Gaussian Mixture Models for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is shown here that MMI and error-weighted training each individually result in over 20% relative reduction in word error rate on a digit task over maximum-likelihood (ML) training."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715709"
                        ],
                        "name": "S. Katagiri",
                        "slug": "S.-Katagiri",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Katagiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katagiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "Consequently, the objective function of MCE is usually optimized using the generalized probabilistic descent (GPD) [9], [28], [29] algorithm or other gradient-based methods [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "The MCE-based classifier design is a discriminant-function-based approach to pattern recognition [1], [28], [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "\u2026measure, the loss function can be defined for each training token r, and it is usually defined through a sigmoid function as originally proposed in [28], [29] : IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 where \u03b1 > 0 is the slope of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18399598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc34c3b927882a55fb2c5c39af265f96499f1a67",
            "isKey": true,
            "numCitedBy": 269,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, due to the advent of artificial neural networks and learning vector quantizers, there is a resurgent interest in reexamining the classical techniques of discriminant analysis to suit the new classifier structures. One of the particular problems of interest is minimum error classification in which the misclassification probability is to be minimized based on a given set of training samples. In this paper, we propose a new formulation for the minimum error classification problem, together with a fundamental technique for designing a classifier that approaches the objective of minimum classification error in a more direct manner than traditional methods. We contrast the new method to several traditional classifier designs in typical experiments to demonstrate the superiority of the new learning formulation. The method can be applied to other classifier structures as well. Experimental results pertaining to a speech recognition task are also provided to show the effectiveness of the new technique."
            },
            "slug": "Discriminative-Learning-for-Minimum-Error-Juang-Katagiri",
            "title": {
                "fragments": [],
                "text": "Discriminative Learning for Minimum Error Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes a new formulation for the minimum error classification problem, together with a fundamental technique for designing a classifier that approaches the objective of minimum classification error in a more direct manner than traditional methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115902507"
                        ],
                        "name": "Jian Wu",
                        "slug": "Jian-Wu",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755472"
                        ],
                        "name": "J. Droppo",
                        "slug": "J.-Droppo",
                        "structuredName": {
                            "firstName": "Jasha",
                            "lastName": "Droppo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Droppo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11684238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0079397aa82224ada0929889b448f045386a2526",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Two feature extraction and compensation algorithms, feature-space minimum phone error (fMPE), which contributed to the recent significant progress in conversational speech recognition, and stereo-based piecewise linear compensation for environments (SPLICE), which has been used successfully in noise-robust speech recognition, are analyzed and compared. These two algorithms have been developed by very different motivations and been applied to very different speech-recognition tasks as well. While the mathematical construction of the two algorithms is ostensibly different, in this report, we establish a direct link between them. We show that both algorithms in the run-time operation accomplish feature extraction/compensation by adding a posterior-based weighted sum of \"correction vectors,\" or equivalently the column vectors in the fMPE projection matrix, to the original, uncompensated features. Although the published fMPE algorithm empirically motivates such a feature extraction, operation as \"a reasonable starting point for training\" our analysis proves that it is a natural consequence of the rigorous minimum mean square error (MMSE) optimization rule as developed in SPLICE. Further, we review and compare related speech-recognition results with the use of fMPE and SPLICE algorithms. The results demonstrate the effectiveness of discriminative training on the feature extraction parameters (i.e., projection matrix in fMPE and equivalently correction vectors in SPLICE). The analysis and comparison of the two algorithms provide useful insight into the strong success of fMPE and point to further algorithm improvement and extension."
            },
            "slug": "Analysis-and-comparison-of-two-speech-feature-Deng-Wu",
            "title": {
                "fragments": [],
                "text": "Analysis and comparison of two speech feature extraction/compensation algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Two feature extraction and compensation algorithms, feature-space minimum phone error (fMPE) and SPLICE, and stereo-based piecewise linear compensation for environments (SPLICE), which have been used successfully in noise-robust speech recognition are analyzed and compared."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145320075"
                        ],
                        "name": "W. Chou",
                        "slug": "W.-Chou",
                        "structuredName": {
                            "firstName": "Wu",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Chou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40332866"
                        ],
                        "name": "Li Li",
                        "slug": "Li-Li",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Li"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35776181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424448675d8573f0992eeda042e4cce8a6b73c42",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present the theoretical framework of minimum classification error (MCE) training of generalized linear classifiers for text classification. We show that many important text classifiers, either probabilistic or non-probabilistic, can be unified under this framework, and the proposed MCE classifier training approach can be applied to improve the classifier performance. In addition, we describe an effective MCE classifier training algorithm that uses AdaBoost to generate alternative initial classifiers, as opposed to combining multiple classifiers as it is typically used. This method is applied to MCE classifier training to overcome local minimums in optimal classifier parameter search, utilizing the fact that the family of generalized linear classifiers is closed under AdaBoost. Moreover, we extend the loss function in MCE training to incorporate training sample prior distributions to compensate the imbalanced training data distribution in each category. Experimental studies are performed on the text classification tasks, and the significant classification error reductions of 25% - 55% are observed."
            },
            "slug": "A-minimum-classification-error-(MCE)-framework-for-Chou-Li",
            "title": {
                "fragments": [],
                "text": "A minimum classification error (MCE) framework for generalized linear classifier in machine learning for text categorization/retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that many important text classifiers, either probabilistic or non-probabilistic, can be unified under this framework, and the proposed MCE classifier training approach can be applied to improve the classifier performance."
            },
            "venue": {
                "fragments": [],
                "text": "2004 International Conference on Machine Learning and Applications, 2004. Proceedings."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716393"
                        ],
                        "name": "P. Woodland",
                        "slug": "P.-Woodland",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Woodland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Woodland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "In order to bypass this issue, a method of optimizing MPE/MWE objective functions based on a heuristic weak-sense auxiliary function (WSAF) was developed in [45] and [47]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 192
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "It was originally developed in [45] and [47] and has demonstrated quite effective performance improvement in speech recognition ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16095655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0509bf552a0d1fe895c019e4e8f1b1599c7112e4",
            "isKey": false,
            "numCitedBy": 798,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce the Minimum Phone Error (MPE) and Minimum Word Error (MWE) criteria for the discriminative training of HMM systems. The MPE/MWE criteria are smoothed approximations to the phone or word error rate respectively. We also discuss I-smoothing which is a novel technique for smoothing discriminative training criteria using statistics for maximum likelihood estimation (MLE). Experiments have been performed on the Switchboard/Call Home corpora of telephone conversations with up to 265 hours of training data. It is shown that for the maximum mutual information estimation (MMIE) criterion, I-smoothing reduces the word error rate (WER) by 0.4% absolute over the MMIE baseline. The combination of MPE and I-smoothing gives an improvement of 1 % over MMIE and a total reduction in WER of 4.8% absolute over the original MLE system."
            },
            "slug": "Minimum-Phone-Error-and-I-smoothing-for-improved-Povey-Woodland",
            "title": {
                "fragments": [],
                "text": "Minimum Phone Error and I-smoothing for improved discriminative training"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The Minimum Phone Error (MPE) and Minimum Word Error (MWE) criteria are smoothed approximations to the phone or word error rate respectively and I-smoothing which is a novel technique for smoothing discriminative training criteria using statistics for maximum likelihood estimation (MLE)."
            },
            "venue": {
                "fragments": [],
                "text": "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702029"
                        ],
                        "name": "R. Chengalvarayan",
                        "slug": "R.-Chengalvarayan",
                        "structuredName": {
                            "firstName": "Rathinavelu",
                            "lastName": "Chengalvarayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chengalvarayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18784282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a7cd1cb6f5da6c0ac547a2fd814bc68a7131094",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we extend the maximum likelihood (ML) training algorithm to the minimum classification error (MCE) training algorithm for discriminatively estimating the state-dependent polynomial coefficients in the stochastic trajectory model or the trended hidden Markov model (HMM) originally proposed in Deng (1992). The main motivation of this extension is the new model space for smoothness-constrained, state-bound speech trajectories associated with the trended HMM, contrasting the conventional, stationary-state HMM, which describes only the piecewise-constant \"degraded trajectories\" in the observation data. The discriminative training implemented for the trended HMM has the potential to utilize this new, constrained model space, thereby providing stronger power to disambiguate the observational trajectories generated from nonstationary sources corresponding to different speech classes. Phonetic classification results are reported which demonstrate consistent performance improvements with use of the MCE-trained trended HMM both over the regular ML-trained trended HMM and over the MCE-trained stationary-state HMM."
            },
            "slug": "Speech-trajectory-discrimination-using-the-minimum-Chengalvarayan-Deng",
            "title": {
                "fragments": [],
                "text": "Speech trajectory discrimination using the minimum classification error learning"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "The maximum likelihood training algorithm is extended to the minimum classification error training algorithm for discriminatively estimating the state-dependent polynomial coefficients in the stochastic trajectory model or the trended hidden Markov model, originally proposed in Deng (1992)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801692"
                        ],
                        "name": "Y. Normandin",
                        "slug": "Y.-Normandin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Normandin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Normandin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "It was later extended and applied to MMI-based discriminative training of continuous-density HMMs (CDHMMs) [2], [20], [41], [59], [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60900969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a27fc84fb14188f7149bd5cae9b24b7eaed3e588",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models (HMMs) are one of the most powerful speech recognition tools available today. Even so, the inadequacies of HMMs as a \"correct\" modeling framework for speech are well known. In that context, we argue that the maximum mutual information estimation (MMIE) formulation for training is more appropriate vis-a-vis maximum likelihood estimation (MLE) for reducing the error rate. We also show how MMIE paves the way for new training possibilities. \nWe introduce Corrective MMIE training, a very efficient new training algorithm which uses a modified version of a discrete reestimation formula recently proposed by Gopalakrishnan et al. We propose reestimation formulas for the case of diagonal Gaussian densities, experimentally demonstrate their convergence properties, and integrate them into our training algorithm. In a connected digit recognition task, MMIE consistently improves the recognition performance of our recognizer."
            },
            "slug": "Hidden-Markov-models,-maximum-mutual-information-Normandin",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models, maximum mutual information estimation, and the speech recognition problem"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work argues that the maximum mutual information estimation (MMIE) formulation for training is more appropriate vis-a-vis maximum likelihood estimation ( MLE) for reducing the error rate and proposes reestimation formulas for the case of diagonal Gaussian densities, experimentally demonstrate their convergence properties, and integrate them into the training algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 775373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bece46ed303f8eaef2affae2cba4e0aef51fe636",
            "isKey": false,
            "numCitedBy": 1551,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ\u2019s."
            },
            "slug": "Maximum-Entropy-Markov-Models-for-Information-and-McCallum-Freitag",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy Markov Models for Information Extraction and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A new Markovian sequence model is presented that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716393"
                        ],
                        "name": "P. Woodland",
                        "slug": "P.-Woodland",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Woodland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Woodland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "It was later extended and applied to MMI-based discriminative training of continuous-density HMMs (CDHMMs) [2], [20], [41], [59], [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10502210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa321cbc2482116f32eaa54a2f393229afa48398",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes, and evaluates on a large scale, the lattice based framework for discriminative training of large vocabulary speech recognition systems based on Gaussian mixture hidden Markov models (HMMs). The paper concentrates on the maximum mutual information estimation (MMIE) criterion which has been used to train HMM systems for conversational telephone speech transcription using up to 265 hours of training data. These experiments represent the largest-scale application of discriminative training techniques for speech recognition of which the authors are aware, and have led to significant reductions in word error rate for both triphone and quinphone HMMs compared to our best models trained using maximum likelihood estimation. The MMIE latticebased implementation used; techniques for ensuring improved generalisation; and interactions with maximum likelihood based adaptation are all discussed. Furthermore several variations to the MMIE training scheme are introduced with the aim of reducing over-training."
            },
            "slug": "Large-scale-discriminative-training-for-speech-Woodland-Povey",
            "title": {
                "fragments": [],
                "text": "Large scale discriminative training for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "These experiments represent the largest-scale application of discriminative training techniques for speech recognition, and have led to significant reductions in word error rate for both triphone and quinphone HMMs compared to the best models trained using maximum likelihood estimation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801698"
                        ],
                        "name": "A. Gunawardana",
                        "slug": "A.-Gunawardana",
                        "structuredName": {
                            "firstName": "Asela",
                            "lastName": "Gunawardana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gunawardana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143817088"
                        ],
                        "name": "M. Mahajan",
                        "slug": "M.-Mahajan",
                        "structuredName": {
                            "firstName": "Milind",
                            "lastName": "Mahajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mahajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [21] SEPTEMBER 2008 In (25), X 1 . . ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 216
                            }
                        ],
                        "text": "\" ) Given the misclassification measure, the loss function can be defined for each training token r, and it is usually defined through a sigmoid function as originally proposed in [28], [29] : IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 where \u03b1 > 0 is the slope of the sigmoid function, often determined empirically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 204
                            }
                        ],
                        "text": "Although a sufficiently large D i guarantees monotone convergence of the GT-based iterative estimation formulas, i.e., (52), (57) for the discrete HMM and (70), (71) for the CDHMM, IEEE SIGNAL PROCESSING MAGAZINE [28] SEPTEMBER 2008 \u03bc i = R r =1 T r t =1 \u03b3 (i, r, t)x t + D i \u03bc i R r =1 T r t=1 \u03b3 (i, r, t) + D i , ( 70) i = R r=1 T r t=1 \u03b3 (i, r, t)(x t \u2212 \u03bc i )(x t \u2212 \u03bc i ) T + D i i + D i (\u03bc i \u2212 \u03bc i )(\u03bc i \u2212 \u03bc i ) T R r=1 T r t=1 \u03b3 (i, r, t) + D i (71) IEEE SIGNAL PROCESSING MAGAZINE [29] SEPTEMBER 2008 the value of D i from the monotone convergence proof is a very loose upper bound and it can be too large for a reasonable convergence speed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 157
                            }
                        ],
                        "text": ") is given by Therefore, parameter optimization of MMI-based discriminative learning is to maximize the following objective function: IEEE SIGNAL PROCESSING MAGAZINE [17] SEPTEMBER 2008 where P(s r ) is the language model probability of pattern sequence s r ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 58
                            }
                        ],
                        "text": "From (50), (41), and (36), we have IEEE SIGNAL PROCESSING MAGAZINE [26] SEPTEMBER 2008 b i (k) = R r=1 T r t=1 s.t. x r,t =k s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t) + b i (k) R r =1 T r t =1 d(r, t, i) R r=1 T r t=1 s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t) + R r=1 T r t=1 d(r, t, i) (49) a i, j = R r =1 T r t =1 s p(s |X, , )(C(s) \u2212 O(( ))\u03be i, j,r,s r (t ) + a i, j R r =1 T r t =1 d(r, t \u2212 1, i ) R r =1 T r t =1 s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t ) + R r=1 T r t =1 d(r, t \u2212 1, i ) (54) IEEE SIGNAL PROCESSING MAGAZINE [27] SEPTEMBER 2008 The theoretical basis for setting D i to ensure that (52) and (57) are growth transformations is the requirement described in (32) that d(s) of (58) be sufficiently large so that In practice, D i and\u02dcD i given by (59) and (60) have often been found to be over conservative and unnecessarily large, causing slower convergence than those obtained through some empirical methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [25] SEPTEMBER 2008 term-I = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 q p(q |X, s, , ) R r =1 T r t =1 log p(x r,t |q r,t , ,) = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T r t =1 I i =1 q,q r,t =i p(q |X, s, , ) \u00d7 log p(x r,t |q r,t = i, ,) = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T r t =1 I i =1 \u03b3 i,r,s r (t ) log p(x r,t |q r,t = i, )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [20] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [20] SEPTEMBER 2008 O MCE (() = R r =1 s r p(X r , s r |)\u03b4(s r , S r ) s r p(X r , s r"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1744924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad4cfdb0c0bb62bf1fa99cb3d0519cadd9cbc160",
            "isKey": true,
            "numCitedBy": 359,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we show the novel application of hidden conditional random fields (HCRFs) \u2013 conditional random fields with hidden state sequences \u2013 for modeling speech. Hidden state sequences are critical for modeling the non-stationarity of speech signals. We show that HCRFs can easily be trained using the simple direct optimization technique of stochastic gradient descent. We present the results on the TIMIT phone classification task and show that HCRFs outperforms comparable ML and CML/MMI trained HMMs. In fact, HCRF results on this task are the best single classifier results known to us. We note that the HCRF framework is easily extensible to recognition since it is a state and label sequence modeling technique. We also note that HCRFs have the ability to handle complex features without any change in training procedure."
            },
            "slug": "Hidden-conditional-random-fields-for-phone-Gunawardana-Mahajan",
            "title": {
                "fragments": [],
                "text": "Hidden conditional random fields for phone classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents the results on the TIMIT phone classification task and shows that HCRFs outperforms comparable ML and CML/MMI trained HMMs and has the ability to handle complex features without any change in training procedure."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112720470"
                        ],
                        "name": "Yan Xiong",
                        "slug": "Yan-Xiong",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316043"
                        ],
                        "name": "Qiang Huo",
                        "slug": "Qiang-Huo",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Huo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Huo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39497278"
                        ],
                        "name": "Chorkin Chan",
                        "slug": "Chorkin-Chan",
                        "structuredName": {
                            "firstName": "Chorkin",
                            "lastName": "Chan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chorkin Chan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 25360106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e717397bb24f9a637a925442419102dbf7f2809",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We study a discrete contextual stochastic (CS) model for complex and variant patterns like handwritten Chinese characters. Three fundamental problems of using CS models for character recognition are discussed, and several practical techniques for solving these problems are investigated. A formulation for discriminative training of CS model parameters is also introduced and its practical usage investigated. To illustrate the characteristics of the various algorithms, comparative experiments are performed on a recognition task with a vocabulary consisting of 50 pairs of highly similar handwritten Chinese characters. The experimental results confirm the effectiveness of the discriminative training for improving recognition performance."
            },
            "slug": "A-Discrete-Contextual-Stochastic-Model-for-the-of-Xiong-Huo",
            "title": {
                "fragments": [],
                "text": "A Discrete Contextual Stochastic Model for the Offline Recognition of Handwritten Chinese Characters"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A discrete contextual stochastic model for complex and variant patterns like handwritten Chinese characters for character recognition is studied and a formulation for discriminative training of CS model parameters is introduced and its practical usage investigated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715709"
                        ],
                        "name": "S. Katagiri",
                        "slug": "S.-Katagiri",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Katagiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katagiri"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32924048,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e142635b24b57cfeb20e0e69bf7836dabe44aa7f",
            "isKey": false,
            "numCitedBy": 709,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A formulation is proposed for minimum-error classification, in which the misclassification probability is to be minimized based on a given set of training samples. A fundamental technique for designing a classifier that approaches the objective of minimum classification error in a more direct manner than traditional methods is given. The method is contrasted with several traditional classifier designs in typical experiments to demonstrate the superiority of the new learning formulation. The method can applied to other classifier structures as well. Experimental results pertaining to a speech recognition task are provided to show the effectiveness of the technique. >"
            },
            "slug": "Discriminative-learning-for-minimum-error-[pattern-Juang-Katagiri",
            "title": {
                "fragments": [],
                "text": "Discriminative learning for minimum error classification [pattern recognition]"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A fundamental technique for designing a classifier that approaches the objective of minimum classification error in a more direct manner than traditional methods is given and is contrasted with several traditional classifier designs in typical experiments to demonstrate the superiority of the new learning formulation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716393"
                        ],
                        "name": "P. Woodland",
                        "slug": "P.-Woodland",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Woodland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Woodland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46299348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a6ae3d667a5c2601c1852a0753c8b1c749fec1e",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes, and evaluates on a large scale, the lattice based framework for discriminative training of large vocabulary speech recognition systems based on Gaussian mixture hidden Markov models (HMMs). This paper concentrates on the maximum mutual information estimation (MMIE) criterion which has been used to train HMM systems for conversational telephone speech transcription using up to 265 hours of training data. These experiments represent the largest-scale application of discriminative training techniques for speech recognition of which the authors are aware. Details are given of the MMIE lattice-based implementation used with the extended Baum-Welch algorithm, which makes training of such large systems computationally feasible. Techniques for improving generalization using acoustic scaling and weakened language models are discussed. The overall technique has allowed the estimation of triphone and quinphone HMM parameters which has led to significant reductions in word error rate for the transcription of conversational telephone speech relative to our best systems trained using maximum likelihood estimation (MLE). This is in contrast to some previous studies, which have concluded that there is little benefit in using discriminative training for the most difficult large vocabulary speech recognition tasks. The lattice MMIE-based discriminative training scheme is also shown to out-perform the frame discrimination technique. Various properties of the lattice-based MMIE training scheme are investigated including comparisons of different lattice processing strategies (full search and exact-match) and the effect of lattice size on performance. Furthermore a scheme based on the linear interpolation of the MMIE and MLE objective functions is shown to reduce the danger of over-training. It is shown that HMMs trained with MMIE benefit as much as MLE-trained HMMs from applying model adaptation using maximum likelihood linear regression (MLLR). This has allowed the straightforward integration of MMIE-trained HMMs into complex multi-pass systems for transcription of conversational telephone speech and has contributed to our MMIE-trained systems giving the lowest word error rates in both the 2000 and 2001 NIST Hub5 evaluations."
            },
            "slug": "Large-scale-discriminative-training-of-hidden-for-Woodland-Povey",
            "title": {
                "fragments": [],
                "text": "Large scale discriminative training of hidden Markov models for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that HMMs trained with MMIE benefit as much as MLE-trained HMMs from applying model adaptation using maximum likelihood linear regression (MLLR), which has allowed the straightforward integration of MMIe- trained HMMs into complex multi-pass systems for transcription of conversational telephone speech."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782065"
                        ],
                        "name": "V. Valtchev",
                        "slug": "V.-Valtchev",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Valtchev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Valtchev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144687429"
                        ],
                        "name": "J. Odell",
                        "slug": "J.-Odell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Odell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Odell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716393"
                        ],
                        "name": "P. Woodland",
                        "slug": "P.-Woodland",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Woodland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Woodland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "It was later extended and applied to MMI-based discriminative training of continuous-density HMMs (CDHMMs) [2], [20], [41], [59], [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16475975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e931146bf240bd83bb720c3dfdc5ad81951e34bb",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "MMIE-training-of-large-vocabulary-recognition-Valtchev-Odell",
            "title": {
                "fragments": [],
                "text": "MMIE training of large vocabulary recognition systems"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144490010"
                        ],
                        "name": "R. Schl\u00fcter",
                        "slug": "R.-Schl\u00fcter",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Schl\u00fcter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schl\u00fcter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15552313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "647edf2d639da0649f056db89c8ac0e20ff61f8c",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Acknowledgements At this point, I would like to express my thanks to all the people who supported and accompanied me during the progress of this work. In particular, I would like to thank: Prof. Dr.-Ing. Hermann Ney for introducing me into the very interesting area of speech recognition research, for the possibility to realize this work at the Lehrstuhl f\u00fcr Infor-matik VI lead by him, for his continuous interest and numerous fruitful and clarifying discussions; Prof. Renato De Mori, who leads the Laboratoire d'Informatique at the Universit\u00e9 d'Avignon et des Pays Vaucluse, for kindly taking over the task of the co-referee for this work; for helping me to get this work started; Wolfgang Macherey and Boris M\u00fcller for performing much of the implementations and experiments presented in this work; Frank Wessel for accompaniment on enlighting paths through word graphs; excellent support with the computing equipment; All the people at the Lehrstuhl f\u00fcr Informatik VI for many constructive discussions and the good atmosphere; and all the other people for the time we spent together; And finally my parents Hedwig and Leo for enabling and supporting me with my work and my interests. Abstract In this work, a framework for efficient discriminative training and modeling is developed and implemented for both small and large vocabulary continuous speech recognition. Special attention will be directed to the comparison and formalization of varying discrim-inative training criteria and corresponding optimization methods, discriminative acoustic model evaluation and feature extraction. A formally unifying approach for a class of discriminative training criteria including Maximum Mutual Information (MMI) and Minimum Classification Error (MCE) criterion is presented, including the optimization methods gradient descent (GD) and extended Baum-Welch (EB) algorithm. Using discriminative criteria, novel approaches to splitting of mixture Gaussian densities and to linear feature transformation are derived. Furthermore , efficient algorithms for the application of discriminative training to speech recognition with both small and large vocabulary are developed. Finally, a novel evaluation method for the stochastic models used in speech recognition is derived using methods related to discriminative training. Experiments have been carried out on the TI digit string corpus for American English continuous digit strings, the SieTill corpus for telephone line recorded German continuous digit strings, the Verbmobil corpus for German spontaneous speech and the Wall Street Journal corpus for American English read speech."
            },
            "slug": "Investigations-on-discriminative-training-criteria-Schl\u00fcter",
            "title": {
                "fragments": [],
                "text": "Investigations on discriminative training criteria"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A framework for efficient discriminative training and modeling is developed and implemented for both small and large vocabulary continuous speech recognition and a novel evaluation method for the stochastic models used in speech recognition is derived using methods related to discrim inative training."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782065"
                        ],
                        "name": "V. Valtchev",
                        "slug": "V.-Valtchev",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Valtchev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Valtchev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144687429"
                        ],
                        "name": "J. Odell",
                        "slug": "J.-Odell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Odell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Odell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716393"
                        ],
                        "name": "P. Woodland",
                        "slug": "P.-Woodland",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Woodland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Woodland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31428006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5f2fbcec556a5598464d1ae5347cea228952eff",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a framework for optimising the parameters of a continuous density HMM-based large vocabulary recognition system using a maximum mutual information estimation (MMIE) criterion. To limit the computational complexity arising from the need to find confusable speech segments in the large search space of alternative utterance hypotheses, word lattices generated from the training data are used. Experiments are presented on the Wall Street journal database using up to 66 hours of training data. These show that lattices combined with an improved estimation algorithm makes MMIE training practicable even for very complex recognition systems and large training sets. Furthermore, experimental results show that MMIE training can yield useful increases in recognition accuracy."
            },
            "slug": "Lattice-based-discriminative-training-for-large-Valtchev-Odell",
            "title": {
                "fragments": [],
                "text": "Lattice-based discriminative training for large vocabulary speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A framework for optimising the parameters of a continuous density HMM-based large vocabulary recognition system using a maximum mutual information estimation (MMIE) criterion is described and experimental results show that MMIE training can yield useful increases in recognition accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2452389"
                        ],
                        "name": "E. McDermott",
                        "slug": "E.-McDermott",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "McDermott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798550"
                        ],
                        "name": "Timothy J. Hazen",
                        "slug": "Timothy-J.-Hazen",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Hazen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy J. Hazen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "Consequently, the objective function of MCE is usually optimized using the generalized probabilistic descent (GPD) [9], [28], [29] algorithm or other gradient-based methods [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6500422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e5b19be128ba816fd6b486aba1f04ceb37b411e",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Though many studies have shown the effectiveness of the minimum classification error (MCE) approach to discriminative training of HMM for speech recognition, few if any have reported MCE results for large (> 100 hours) training sets in the context of real-world, continuous speech recognition. Here we report large gains in performance for the MIT JUPITER weather information task as a result of MCE-based batch optimization of acoustic models. Investigation of word error rate versus computation time showed that small MCE models significantly outperform the maximum likelihood (ML) baseline at all points of equal computation time, resulting in up to 20% word error rate reduction for in-vocabulary utterances. The overall MCE loss function was minimized using Quickprop, a simple but effective second-order optimization method suited to parallelization over large training sets."
            },
            "slug": "Minimum-classification-error-training-of-landmark-McDermott-Hazen",
            "title": {
                "fragments": [],
                "text": "Minimum classification error training of landmark models for real-time continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Investigation of word error rate versus computation time showed that small MCE models significantly outperform the maximum likelihood (ML) baseline at all points of equal computation time, resulting in up to 20% word error rates reduction for in-vocabulary utterances."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145395455"
                        ],
                        "name": "H. Kuo",
                        "slug": "H.-Kuo",
                        "structuredName": {
                            "firstName": "Hong-Kwang",
                            "lastName": "Kuo",
                            "middleNames": [
                                "Jeff"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kuo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40031121"
                        ],
                        "name": "Yuqing Gao",
                        "slug": "Yuqing-Gao",
                        "structuredName": {
                            "firstName": "Yuqing",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuqing Gao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1396674,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4de9182986f7188e0f8260ad89e88b119734884",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional statistical models for speech recognition have mostly been based on a Bayesian framework using generative models such as hidden Markov models (HMMs). This paper focuses on a new framework for speech recognition using maximum entropy direct modeling, where the probability of a state or word sequence given an observation sequence is computed directly from the model. In contrast to HMMs, features can be asynchronous and overlapping. This model therefore allows for the potential combination of many different types of features, which need not be statistically independent of each other. In this paper, a specific kind of direct model, the maximum entropy Markov model (MEMM), is studied. Even with conventional acoustic features, the approach already shows promising results for phone level decoding. The MEMM significantly outperforms traditional HMMs in word error rate when used as stand-alone acoustic models. Preliminary results combining the MEMM scores with HMM and language model scores show modest improvements over the best HMM speech recognizer."
            },
            "slug": "Maximum-entropy-direct-models-for-speech-Kuo-Gao",
            "title": {
                "fragments": [],
                "text": "Maximum entropy direct models for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The maximum entropy Markov model (MEMM) is studied, and preliminary results combining the MEMM scores with HMM and language model scores show modest improvements over the best HMM speech recognizer."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10888973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a7958b418bceb48a315384568091ab1898b1640",
            "isKey": false,
            "numCitedBy": 2272,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "slug": "Discriminative-Training-Methods-for-Hidden-Markov-Collins",
            "title": {
                "fragments": [],
                "text": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results on part-of-speech tagging and base noun phrase chunking are given, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713978"
                        ],
                        "name": "D. Nahamoo",
                        "slug": "D.-Nahamoo",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Nahamoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nahamoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774515"
                        ],
                        "name": "M. Picheny",
                        "slug": "M.-Picheny",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Picheny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Picheny"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33275295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "039900eaeeddd13752aa8d6c61759f0b0e54f0de",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Training methods for designing better decoders are compared. The training problem is considered as a statistical parameter estimation problem. In particular, the conditional maximum likelihood estimate (CMLE), which estimates the parameter values that maximize the conditional probability of words given acoustics during training, is compared to the maximum-likelihood estimate, which is obtained by maximizing the joint probability of the words and acoustics. For minimizing the decoding error rate of the (optimal) maximum a posteriori probability (MAP) decoder, it is shown that the CMLE (or maximum mutual information estimate, MMIE) may be preferable when the model is incorrect. In this sense, the CMLE/MMIE appears more robust than the MLE. >"
            },
            "slug": "On-a-model-robust-training-method-for-speech-N\u00e1das-Nahamoo",
            "title": {
                "fragments": [],
                "text": "On a model-robust training method for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "For minimizing the decoding error rate of the (optimal) maximum a posteriori probability (MAP) decoder, it is shown that the CMLE (or maximum mutual information estimate, MMIE) may be preferable when the model is incorrect."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60769407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1aa31d5deb45f477a6de45b3b75b62c7f4a213e7",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This thesis examines the acoustic-modeling problem in automatic speech recognition from an information-theoretic point of view. This problem is to design a speech-recognition system which can extract from the speech waveform as much information as possible about the corresponding word sequence. The information extraction process is broken down into two steps: a signal processing step which converts a speech waveform into a sequence of information bearing acoustic feature vectors, and a step which models such a sequence. This thesis is primarily concerned with the use of hidden Markov models to model sequences of feature vectors which lie in a continuous space such as R sub N. It explores the trade-off between packing a lot of information into such sequences and being able to model them accurately. The difficulty of developing accurate models of continuous parameter sequences is addressed by investigating a method of parameter estimation which is specifically designed to cope with inaccurate modeling assumptions."
            },
            "slug": "The-acoustic-modeling-problem-in-automatic-speech-Brown",
            "title": {
                "fragments": [],
                "text": "The acoustic-modeling problem in automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This thesis is primarily concerned with the use of hidden Markov models to model sequences of feature vectors which lie in a continuous space such as R sub N and explores the trade-off between packing a lot of information into such sequences and being able to model them accurately."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13410,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 284549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "722a3c365a134a9f9b9ae1511f018d9b1ecff3de",
            "isKey": false,
            "numCitedBy": 1016,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Most connectionist or \"neural network\" learning systems use some form of the back-propagation algorithm. However, back-propagation learning is too slow for many applications, and it scales up poorly as tasks become larger and more complex. The factors governing learning speed are poorly understood. I have begun a systematic, empirical study of learning speed in backprop-like algorithms, measured against a variety of benchmark problems. The goal is twofold: to develop faster learning algorithms and to contribute to the development of a methodology that will be of value in future studies of this kind. This paper is a progress report describing the results obtained during the first six months of this study. To date I have looked only at a limited set of benchmark problems, but the results on these are encouraging: I have developed a new learning algorithm that is faster than standard backprop by an order of magnitude or more and that appears to scale up very well as the problem size increases. This research was sponsored in part by the National Science Foundation under Contract Number EET-8716324 and by the Defense Advanced Research Projects Agency (DOD), ARPA Order No. 4976 under Contract F33615-87C-1499 and monitored by the Avionics Laboratory, Air Force Wright Aeronautical Laboratories, Aeronautical Systems Division (AFSC), Wright-Patterson AFB, OH 45433-6543. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of these agencies or of the U.S. Government."
            },
            "slug": "An-empirical-study-of-learning-speed-in-networks-Fahlman",
            "title": {
                "fragments": [],
                "text": "An empirical study of learning speed in back-propagation networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new learning algorithm is developed that is faster than standard backprop by an order of magnitude or more and that appears to scale up very well as the problem size increases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145659093"
                        ],
                        "name": "S. Kapadia",
                        "slug": "S.-Kapadia",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Kapadia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kapadia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782065"
                        ],
                        "name": "V. Valtchev",
                        "slug": "V.-Valtchev",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Valtchev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Valtchev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57374885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd6387ca1949d61356adee35708dcdbee1e4fd05",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Experiences with a phoneme recognition system for the TIMIT database which uses multiple mixture continuous-density monophone HMMs (hidden Markov models) trained using MMI (maximum mutual information) is reported. A comprehensive set of results are presented comparing the ML (maximum likelihood) and MMI training criteria for both diagonal and full covariance models. These results using simple monophone HMMs show that clear performance gains are achieved by MMI training. These results are comparable with the best reported by others, including those which use context-dependent models. In addition, a number of performance and implementation issues which are crucial to successful MMI training are discussed.<<ETX>>"
            },
            "slug": "MMI-training-for-continuous-phoneme-recognition-on-Kapadia-Valtchev",
            "title": {
                "fragments": [],
                "text": "MMI training for continuous phoneme recognition on the TIMIT database"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "These results using simple monophone HMMs show that clear performance gains are achieved by MMI training, and are comparable with the best reported by others, including those which use context-dependent models."
            },
            "venue": {
                "fragments": [],
                "text": "1993 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145320076"
                        ],
                        "name": "W. Chou",
                        "slug": "W.-Chou",
                        "structuredName": {
                            "firstName": "Wu",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Chou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "This flexibility in classifier design for sequential pattern recognition has been a fertile field of research, and many approaches have been developed [22], [29], [47]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6763115,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a37210d3c21a66f2da3e7564690090c225b78919",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a concatenated \"super\" string model based minimum classification error (MCE) model adaptation approach is described. We show that the error rate minimization in the proposed approach can be formulated into maximizing a special ratio of two positive functions. The proposed string model is used to derive the growth transform based error rate minimization for MCE linear regression (MCELR). It provides an effective solution to apply MCE approach to acoustic model adaptation with sparse data. The proposed MCELR approach is studied and compared with the maximum likelihood linear regression (MLLR) based model adaptation. Experiments on large vocabulary speech recognition tasks are performed. Experimental results indicate that the proposed MCELR model adaptation can lead to significant speech recognition performance improvement and its performance advantage over the MLLR based approach is observed even when the amount of adaptation data is sparse."
            },
            "slug": "Minimum-classification-error-linear-regression-for-He-Chou",
            "title": {
                "fragments": [],
                "text": "Minimum classification error linear regression for acoustic model adaptation of continuous density HMMs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experimental results indicate that the proposed MCELR model adaptation can lead to significant speech recognition performance improvement and its performance advantage over the MLLR based approach is observed even when the amount of adaptation data is sparse."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103072546"
                        ],
                        "name": "J. Eagon",
                        "slug": "J.-Eagon",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Eagon",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Eagon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14153120,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "69fc8c03d21e22e30d6642824c37158b314f36c3",
            "isKey": false,
            "numCitedBy": 1122,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Summary. The object of this note is to prove the theorem below and sketch two applications, one to statistical estimation for (proba-bilistic) functions of Markov processes [l] and one to Blakley's model for ecology [4]. 2. Result. THEOREM. Let P(x)=P({xij}) be a polynomial with nonnegative coefficients homogeneous of degree d in its variables {##}. Let x= {##} be any point of the domain D: ## \u00a7:(), ]pLi ## = 1, i = l, \u2022 \u2022 \u2022 , p, j=l, \u2022 \u2022 \u2022 , q%. For x= {xij} \u00a3\u00a3> let 3(#) = 3{##} denote the point of D whose i, j coordinate is (dP\\ \\ f \u00ab dP 3(*)<i = (Xij 7\u2014) / 2* *<i \u2014 \\ dXij\\(X)// ,-i dXij (\u00bb> Then P(3(x))>P(x) unless 3(x)=x. Notation, fi will denote a doubly indexed array of nonnegative integers: fx= {M#}> i = l> \u2022 \u2022 \u2022 > <lu i=l, \u2022 \u2022 \u2022 , A #* then denotes Ilf-iH\u00ee-i^* Similarly, c M is an abbreviation for C[ MiJ }. The polynomial P({xij}) is then written P(x) = ]CM V^-In our notation : (1) 3(&)*i = (Z) \u00abWnys*) / JLH CpiiijX\u00bb."
            },
            "slug": "An-inequality-with-applications-to-statistical-for-Baum-Eagon",
            "title": {
                "fragments": [],
                "text": "An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16345121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "788a5bd62d36c4acbac01a2b76708690b6d4a0a4",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We outline a structured speech model, as a special and perhaps extreme form of probabilistic generative modeling. The model is equipped with long-contextual-span capabilities that are missing in the HMM approach. Compact (and physically meaningful) parameterization of the model is made possible by the continuity constraint in the hidden vocal tract resonance (VTR) domain. The target-directed VTR dynamics jointly characterize coarticulation and incomplete articulation (reduction). Preliminary evaluation results are presented on the standard TIMIT phonetic recognition task, showing the best result in this task reported in the literature without using many heterogeneous classifier combinations. The pros and cons of our structured generative modeling approach, in comparison with the structured discriminative classification approach, are discussed."
            },
            "slug": "A-Generative-Modeling-Framework-for-Structured-Deng-Yu",
            "title": {
                "fragments": [],
                "text": "A Generative Modeling Framework for Structured Hidden Speech Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A structured speech model is outlined, equipped with long-contextual-span capabilities that are missing in the HMM approach, and the pros and cons of the structured generative modeling approach in comparison with the structured discriminative classification approach are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801698"
                        ],
                        "name": "A. Gunawardana",
                        "slug": "A.-Gunawardana",
                        "structuredName": {
                            "firstName": "Asela",
                            "lastName": "Gunawardana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gunawardana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716907"
                        ],
                        "name": "W. Byrne",
                        "slug": "W.-Byrne",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Byrne",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Byrne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [21] SEPTEMBER 2008 In (25), X 1 . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 216
                            }
                        ],
                        "text": "\" ) Given the misclassification measure, the loss function can be defined for each training token r, and it is usually defined through a sigmoid function as originally proposed in [28], [29] : IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 where \u03b1 > 0 is the slope of the sigmoid function, often determined empirically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 204
                            }
                        ],
                        "text": "Although a sufficiently large D i guarantees monotone convergence of the GT-based iterative estimation formulas, i.e., (52), (57) for the discrete HMM and (70), (71) for the CDHMM, IEEE SIGNAL PROCESSING MAGAZINE [28] SEPTEMBER 2008 \u03bc i = R r =1 T r t =1 \u03b3 (i, r, t)x t + D i \u03bc i R r =1 T r t=1 \u03b3 (i, r, t) + D i , ( 70) i = R r=1 T r t=1 \u03b3 (i, r, t)(x t \u2212 \u03bc i )(x t \u2212 \u03bc i ) T + D i i + D i (\u03bc i \u2212 \u03bc i )(\u03bc i \u2212 \u03bc i ) T R r=1 T r t=1 \u03b3 (i, r, t) + D i (71) IEEE SIGNAL PROCESSING MAGAZINE [29] SEPTEMBER 2008 the value of D i from the monotone convergence proof is a very loose upper bound and it can be too large for a reasonable convergence speed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 157
                            }
                        ],
                        "text": ") is given by Therefore, parameter optimization of MMI-based discriminative learning is to maximize the following objective function: IEEE SIGNAL PROCESSING MAGAZINE [17] SEPTEMBER 2008 where P(s r ) is the language model probability of pattern sequence s r ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 58
                            }
                        ],
                        "text": "From (50), (41), and (36), we have IEEE SIGNAL PROCESSING MAGAZINE [26] SEPTEMBER 2008 b i (k) = R r=1 T r t=1 s.t. x r,t =k s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t) + b i (k) R r =1 T r t =1 d(r, t, i) R r=1 T r t=1 s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t) + R r=1 T r t=1 d(r, t, i) (49) a i, j = R r =1 T r t =1 s p(s |X, , )(C(s) \u2212 O(( ))\u03be i, j,r,s r (t ) + a i, j R r =1 T r t =1 d(r, t \u2212 1, i ) R r =1 T r t =1 s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t ) + R r=1 T r t =1 d(r, t \u2212 1, i ) (54) IEEE SIGNAL PROCESSING MAGAZINE [27] SEPTEMBER 2008 The theoretical basis for setting D i to ensure that (52) and (57) are growth transformations is the requirement described in (32) that d(s) of (58) be sufficiently large so that In practice, D i and\u02dcD i given by (59) and (60) have often been found to be over conservative and unnecessarily large, causing slower convergence than those obtained through some empirical methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [25] SEPTEMBER 2008 term-I = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 q p(q |X, s, , ) R r =1 T r t =1 log p(x r,t |q r,t , ,) = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T r t =1 I i =1 q,q r,t =i p(q |X, s, , ) \u00d7 log p(x r,t |q r,t = i, ,) = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T r t =1 I i =1 \u03b3 i,r,s r (t ) log p(x r,t |q r,t = i, )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "It was later extended and applied to MMI-based discriminative training of continuous-density HMMs (CDHMMs) [2], [20], [41], [59], [61]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [20] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [20] SEPTEMBER 2008 O MCE (() = R r =1 s r p(X r , s r |)\u03b4(s r , S r ) s r p(X r , s r"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14404772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b3acf384bc8fd68a72cbb3c88392d6a89f34cec",
            "isKey": true,
            "numCitedBy": 72,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simplified derivation of the extended Baum-Welch procedure, which shows that it can be used for Maximum Mutual Information (MMI) of a large class of continuous emission density hidden Markov models (HMMs). We use the extended Baum-Welch procedure for discriminative estimation of MLLR-type speaker adaptation transformations. The resulting adaptation procedure, termed Conditional Maximum Likelihood Linear Regression (CMLLR), is used successfully for supervised and unsupervised adaptation tasks on the Switchboard corpus, yielding an improvement over MLLR. The interaction of unsupervised CMLLR with segmental minimum Bayes risk lattice voting procedures is also explored, showing that the two procedures are complimentary."
            },
            "slug": "Discriminative-speaker-adaptation-with-conditional-Gunawardana-Byrne",
            "title": {
                "fragments": [],
                "text": "Discriminative speaker adaptation with conditional maximum likelihood linear regression"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A simplified derivation of the extended Baum-Welch procedure is presented, which shows that it can be used for Maximum Mutual Information (MMI) of a large class of continuous emission density hidden Markov models (HMMs)."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143634393"
                        ],
                        "name": "Rong Yan",
                        "slug": "Rong-Yan",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151812271"
                        ],
                        "name": "Jian Zhang",
                        "slug": "Jian-Zhang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7661726"
                        ],
                        "name": "Alexander Hauptmann",
                        "slug": "Alexander-Hauptmann",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hauptmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Hauptmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7191989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd65d03a385eeda4f76c15e5b02bc9edf32822bb",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In video object classification, insufficient labeled data may at times be easily augmented with pairwise constraints on sample points, i.e, whether they are in the same class or not. In this paper, we proposed a discriminative learning approach, which incorporates pairwise constraints into a conventional margin-based learning framework. The proposed approach offers several advantages over existing approaches dealing with pairwise constraints. First, as opposed to learning distance metrics, the new approach derives its classification power by directly modeling the decision boundary. Second, most previous work handles labeled data by converting them to pairwise constraints and thus leads too much more computation. The proposed approach can handle pairwise constraints together with labeled data so that the computation is greatly reduced. The proposed approach is evaluated on a people classification task with two surveillance video datasets."
            },
            "slug": "A-discriminative-learning-framework-with-pairwise-Yan-Zhang",
            "title": {
                "fragments": [],
                "text": "A discriminative learning framework with pairwise constraints for video object classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A discriminative learning approach, which incorporates pairwise constraints into a conventional margin-based learning framework, which offers several advantages over existing approaches dealing with Pairwise constraints."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038713"
                        ],
                        "name": "Chinching Yen",
                        "slug": "Chinching-Yen",
                        "structuredName": {
                            "firstName": "Chinching",
                            "lastName": "Yen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chinching Yen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071412"
                        ],
                        "name": "S. Kuo",
                        "slug": "S.-Kuo",
                        "structuredName": {
                            "firstName": "Shyh-shiaw",
                            "lastName": "Kuo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kuo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9391905"
                        ],
                        "name": "Chin-Hui Lee",
                        "slug": "Chin-Hui-Lee",
                        "structuredName": {
                            "firstName": "Chin-Hui",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Hui Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2410347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccff7fcf935c9c64400070339312559ed0dae9d1",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, discriminative training is studied to improve the performance of our pseudo two-dimensional (2-D) hidden Markov model (PHMM) based text recognition system. The aim of this discriminative training is to adjust model parameters to directly minimize the classification error rate. Experimental results have shown great reduction in recognition error rate even for PHMMs already well-trained using conventional maximum likelihood (ML) approaches."
            },
            "slug": "Minimum-error-rate-training-for-PHMM-based-text-Yen-Kuo",
            "title": {
                "fragments": [],
                "text": "Minimum error rate training for PHMM-based text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "In this work, discriminative training is studied to improve the performance of the pseudo two-dimensional (2-D) hidden Markov model (PHMM) based text recognition system by adjusting model parameters to directly minimize the classification error rate."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801692"
                        ],
                        "name": "Y. Normandin",
                        "slug": "Y.-Normandin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Normandin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Normandin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60832448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47cdade65e2cd0c8d09ab5d9ad37a31eca17614e",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes ways in which the concept of maximum mutual information estimation (MMIE) can be used to improve the performance of HMM-based speech recognition systems. First, the basic MMIE concept is introduced with some intuition on how it works. Then we show how the concept can be extended to improve the power of the basic models. Since estimating HMM parameters with MMIE training can be computationally expensive, this problem is studied at length and some solutions proposed and demonstrated. Experiments are presented to demonstrate the usefulness of the MMIE technique."
            },
            "slug": "Maximum-Mutual-Information-Estimation-of-Hidden-Normandin",
            "title": {
                "fragments": [],
                "text": "Maximum Mutual Information Estimation of Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter describes ways in which the concept of maximum mutual information estimation (MMIE) can be used to improve the performance of HMM-based speech recognition systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5474833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f12451245667a85d0ee225a80880fc93c71cc8b",
            "isKey": false,
            "numCitedBy": 3304,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure."
            },
            "slug": "Minimum-Error-Rate-Training-in-Statistical-Machine-Och",
            "title": {
                "fragments": [],
                "text": "Minimum Error Rate Training in Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12460646"
                        ],
                        "name": "Shuying Sun",
                        "slug": "Shuying-Sun",
                        "structuredName": {
                            "firstName": "Shuying",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuying Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145043258"
                        ],
                        "name": "C. Greenwood",
                        "slug": "C.-Greenwood",
                        "structuredName": {
                            "firstName": "Celia",
                            "lastName": "Greenwood",
                            "middleNames": [
                                "M.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Greenwood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41482227,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "fcc16389d13765526b60deaf5677e73bed1838b3",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge of haplotypes is useful for understanding block structure in the genome and disease risk associations. Direct measurement of haplotypes in the absence of family data is presently impractical, and hence, several methods have been developed for reconstructing haplotypes from population data. We have developed a new population\u2010based method using a Bayesian Hidden Markov model for the source of the ancestral haplotype segments. In our Bayesian model, a higher order Markov model is used as the prior for ancestral haplotypes, to account for linkage disequilibrium. Our model includes parameters for the genotyping error rate, the mutation rate, and the recombination rate at each position. Computation is done by Markov Chain Monte Carlo using the forward\u2010backward algorithm to efficiently sum over all possible state sequences of the Hidden Markov model. We have used the model to reconstruct the haplotypes of 129 children at a region on chromosome 5 in the data set of Daly et al. [ 2001 ] (for which true haplotypes are obtained based on parental genotypes) and of 30 children at selected regions in the CEU and YRI data of the HAPMAP project. The results are quite close to the family\u2010based reconstructions and comparable with the state\u2010of\u2010the\u2010art PHASE program. Our haplotype reconstruction method does not require division of the markers into small blocks of loci. The recombination rates inferred from our model can help to predict haplotype block boundaries, and estimate recombination hotspots. Genet. Epidemiol. 2007. \u00a9 2007 Wiley\u2010Liss, Inc."
            },
            "slug": "Haplotype-inference-using-a-Bayesian-Hidden-Markov-Sun-Greenwood",
            "title": {
                "fragments": [],
                "text": "Haplotype inference using a Bayesian Hidden Markov model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new population\u2010based method using a Bayesian Hidden Markov model for the source of the ancestral haplotype segments, and the recombination rates inferred from the model can help to predict haplotype block boundaries, and estimate recombination hotspots."
            },
            "venue": {
                "fragments": [],
                "text": "Genetic epidemiology"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700994"
                        ],
                        "name": "R. Battiti",
                        "slug": "R.-Battiti",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Battiti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Battiti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27960650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf6f07e699587c8d52faf829a289f8cbc7f11a5",
            "isKey": false,
            "numCitedBy": 1216,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "On-line first-order backpropagation is sufficiently fast and effective for many large-scale classification problems but for very high precision mappings, batch processing may be the method of choice. This paper reviews first- and second-order optimization methods for learning in feedforward neural networks. The viewpoint is that of optimization: many methods can be cast in the language of optimization techniques, allowing the transfer to neural nets of detailed results about computational complexity and safety procedures to ensure convergence and to avoid numerical problems. The review is not intended to deliver detailed prescriptions for the most appropriate methods in specific applications, but to illustrate the main characteristics of the different methods and their mutual relations."
            },
            "slug": "First-and-Second-Order-Methods-for-Learning:-and-Battiti",
            "title": {
                "fragments": [],
                "text": "First- and Second-Order Methods for Learning: Between Steepest Descent and Newton's Method"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "First- and second-order optimization methods for learning in feedforward neural networks are reviewed to illustrate the main characteristics of the different methods and their mutual relations."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768120"
                        ],
                        "name": "T. Jebara",
                        "slug": "T.-Jebara",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Jebara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jebara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14354944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7aa0221a6608465f69b399cebf690d24090a0935",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Jensen's inequality is a powerful mathematical tool and one of the workhorses in statistical learning. Its applications therein include the EM algorithm, Bayesian estimation and Bayesian inference. Jensen computes simple lower bounds on otherwise intractable quantities such as products of sums and latent log-likelihoods. This simplification then permits operations like integration and maximization. Quite often (i.e. in discriminative learning) upper bounds are needed as well. We derive and prove an efficient analytic inequality that provides such variational upper bounds. This inequality holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statistical models. We also discuss applications of the upper bounds including maximum conditional likelihood, large margin discriminative models and conditional Bayesian inference. Convergence, efficiency and prediction results are shown."
            },
            "slug": "On-Reversing-Jensen's-Inequality-Jebara-Pentland",
            "title": {
                "fragments": [],
                "text": "On Reversing Jensen's Inequality"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work derives and proves an efficient analytic inequality that holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statistical models, including maximum conditional likelihood, large margin discriminative models and conditional Bayesian inference."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684010"
                        ],
                        "name": "P. Gopalakrishnan",
                        "slug": "P.-Gopalakrishnan",
                        "structuredName": {
                            "firstName": "Ponani",
                            "lastName": "Gopalakrishnan",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gopalakrishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749064"
                        ],
                        "name": "D. Kanevsky",
                        "slug": "D.-Kanevsky",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Kanevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kanevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713978"
                        ],
                        "name": "D. Nahamoo",
                        "slug": "D.-Nahamoo",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Nahamoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nahamoo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "Such a proof was developed in the recent work of [2] for GT-based MMI training for CDHMMs, and it holds for our common rational-function discriminative training criterion as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "For CDHMMs, the observation space is not quantized."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [21] SEPTEMBER 2008 In (25), X 1 . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "The above-mentioned difficulty for CDHMMs can be overcome and the same derivation can still be used, if it can be shown that there exists a sufficiently large but still bounded constant D so that V((; ) of (61), with the integrand defined by (64) , is still a valid auxiliary function of F((; ); i.e., an increase of the value of V((; ) can guarantee an increase of the value of F((; )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 216
                            }
                        ],
                        "text": "\" ) Given the misclassification measure, the loss function can be defined for each training token r, and it is usually defined through a sigmoid function as originally proposed in [28], [29] : IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 where \u03b1 > 0 is the slope of the sigmoid function, often determined empirically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 204
                            }
                        ],
                        "text": "Although a sufficiently large D i guarantees monotone convergence of the GT-based iterative estimation formulas, i.e., (52), (57) for the discrete HMM and (70), (71) for the CDHMM, IEEE SIGNAL PROCESSING MAGAZINE [28] SEPTEMBER 2008 \u03bc i = R r =1 T r t =1 \u03b3 (i, r, t)x t + D i \u03bc i R r =1 T r t=1 \u03b3 (i, r, t) + D i , ( 70) i = R r=1 T r t=1 \u03b3 (i, r, t)(x t \u2212 \u03bc i )(x t \u2212 \u03bc i ) T + D i i + D i (\u03bc i \u2212 \u03bc i )(\u03bc i \u2212 \u03bc i ) T R r=1 T r t=1 \u03b3 (i, r, t) + D i (71) IEEE SIGNAL PROCESSING MAGAZINE [29] SEPTEMBER 2008 the value of D i from the monotone convergence proof is a very loose upper bound and it can be too large for a reasonable convergence speed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "\u2026each training token r, and it is usually defined through a sigmoid function as originally proposed in [28], [29] : IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 where \u03b1 > 0 is the slope of the sigmoid function, often determined\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 157
                            }
                        ],
                        "text": ") is given by Therefore, parameter optimization of MMI-based discriminative learning is to maximize the following objective function: IEEE SIGNAL PROCESSING MAGAZINE [17] SEPTEMBER 2008 where P(s r ) is the language model probability of pattern sequence s r ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 58
                            }
                        ],
                        "text": "From (50), (41), and (36), we have IEEE SIGNAL PROCESSING MAGAZINE [26] SEPTEMBER 2008 b i (k) = R r=1 T r t=1 s.t. x r,t =k s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t) + b i (k) R r =1 T r t =1 d(r, t, i) R r=1 T r t=1 s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t) + R r=1 T r t=1 d(r, t, i) (49) a i, j = R r =1 T r t =1 s p(s |X, , )(C(s) \u2212 O(( ))\u03be i, j,r,s r (t ) + a i, j R r =1 T r t =1 d(r, t \u2212 1, i ) R r =1 T r t =1 s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t ) + R r=1 T r t =1 d(r, t \u2212 1, i ) (54) IEEE SIGNAL PROCESSING MAGAZINE [27] SEPTEMBER 2008 The theoretical basis for setting D i to ensure that (52) and (57) are growth transformations is the requirement described in (32) that d(s) of (58) be sufficiently large so that In practice, D i and\u02dcD i given by (59) and (60) have often been found to be over conservative and unnecessarily large, causing slower convergence than those obtained through some empirical methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "The formulation (25) applies to discriminative learning for CDHMMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [25] SEPTEMBER 2008 term-I = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 q p(q |X, s, , ) R r =1 T r t =1 log p(x r,t |q r,t , ,) = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T r t =1 I i =1 q,q r,t =i p(q |X, s, , ) \u00d7 log p(x r,t |q r,t = i, ,) = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T r t =1 I i =1 \u03b3 i,r,s r (t ) log p(x r,t |q r,t = i, )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "This method is based on the reverse Jensen inequality, upon which an elegant solution for HMMs with exponential-family densities is constructed [26]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "It was later extended and applied to MMI-based discriminative training of continuous-density HMMs (CDHMMs) [2], [20], [41], [59], [61]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [20] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [20] SEPTEMBER 2008 O MCE (() = R r =1 s r p(X r , s r |)\u03b4(s r , S r ) s r p(X r , s r"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 198
                            }
                        ],
                        "text": "The third area is the algorithmic properties of the MCE and MPE/MWE-based learning methods under the parameter estimation framework of growth transformation for sequential pattern recognition using HMMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "The EBW algorithm became popular for its successful application in MMI-based discriminative training of discrete HMMs [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14827986,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b3f258f923da64f99c11610ed13fedd3827ea5f6",
            "isKey": true,
            "numCitedBy": 247,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The well-known Baum-Eagon inequality (1967) provides an effective iterative scheme for finding a local maximum for homogeneous polynomials with positive coefficients over a domain of probability values. However, in many applications the goal is to maximize a general rational function. In view of this, the Baum-Eagon inequality is extended to rational functions. Some of the applications of this inequality to statistical estimation problems are briefly described. >"
            },
            "slug": "An-inequality-for-rational-functions-with-to-some-Gopalakrishnan-Kanevsky",
            "title": {
                "fragments": [],
                "text": "An inequality for rational functions with applications to some statistical estimation problems"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The well-known Baum-Eagon inequality provides an effective iterative scheme for finding a local maximum for homogeneous polynomials with positive coefficients over a domain of probability values."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755472"
                        ],
                        "name": "J. Droppo",
                        "slug": "J.-Droppo",
                        "structuredName": {
                            "firstName": "Jasha",
                            "lastName": "Droppo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Droppo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17808855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b83d4c9b7122c3e801884fca81537a7c74b1b45",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a general discriminative training method for both the front end feature extractor and back end acoustic model of an automatic speech recognition system. The front end and back end parameters are jointly trained using the Rprop algorithm against a maximum mutual information (MMI) objective function. Results are presented on the Aurora 2 noisy English digit recognition task. It is shown that discriminative training of the front end or back end alone can improve accuracy, but joint training is considerably better"
            },
            "slug": "Joint-Discriminative-Front-End-and-Back-End-for-Droppo-Acero",
            "title": {
                "fragments": [],
                "text": "Joint Discriminative Front End and Back End Training for Improved Speech Recognition Accuracy"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This paper presents a general discriminative training method for both the front end feature extractor and back end acoustic model of an automatic speech recognition system that can improve accuracy, but joint training is considerably better."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3137672"
                        ],
                        "name": "Martin A. Riedmiller",
                        "slug": "Martin-A.-Riedmiller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Riedmiller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Riedmiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145326760"
                        ],
                        "name": "H. Braun",
                        "slug": "H.-Braun",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "Braun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Braun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16848428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "916ceefae4b11dadc3ee754ce590381c568c90de",
            "isKey": false,
            "numCitedBy": 4443,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A learning algorithm for multilayer feedforward networks, RPROP (resilient propagation), is proposed. To overcome the inherent disadvantages of pure gradient-descent, RPROP performs a local adaptation of the weight-updates according to the behavior of the error function. Contrary to other adaptive techniques, the effect of the RPROP adaptation process is not blurred by the unforeseeable influence of the size of the derivative, but only dependent on the temporal behavior of its sign. This leads to an efficient and transparent adaptation process. The capabilities of RPROP are shown in comparison to other adaptive techniques.<<ETX>>"
            },
            "slug": "A-direct-adaptive-method-for-faster-backpropagation-Riedmiller-Braun",
            "title": {
                "fragments": [],
                "text": "A direct adaptive method for faster backpropagation learning: the RPROP algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A learning algorithm for multilayer feedforward networks, RPROP (resilient propagation), is proposed that performs a local adaptation of the weight-updates according to the behavior of the error function to overcome the inherent disadvantages of pure gradient-descent."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150023694"
                        ],
                        "name": "A. N\u00e1das",
                        "slug": "A.-N\u00e1das",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "N\u00e1das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. N\u00e1das"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120638127,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "79eb272eaf061cf4e65b8e61c9f02c027b3b6933",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The choice of method for training a speech recognizer is posed as an optimization problem. The currently used method of maximum likelihood, while heuristic, is shown to be superior under certain assumptions to another heuristic: the method of conditional maximum likelihood."
            },
            "slug": "A-decision-theorectic-formulation-of-a-training-in-N\u00e1das",
            "title": {
                "fragments": [],
                "text": "A decision theorectic formulation of a training problem in speech recognition and a comparison of training by unconditional versus conditional maximum likelihood"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The currently used method of maximum likelihood, while heuristic, is shown to be superior under certain assumptions to another heuristic: the method of conditional maximum likelihood."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146104372"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2154894307"
                        ],
                        "name": "Yangsheng Xu",
                        "slug": "Yangsheng-Xu",
                        "structuredName": {
                            "firstName": "Yangsheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangsheng Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152857529"
                        ],
                        "name": "C. S. Chen",
                        "slug": "C.-S.-Chen",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Chen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7188770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "674a0634e8c5d9748fd15b40f23d42f5c4b704d1",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we discuss the problem of how human skill can be represented as a parametric model using a hidden Markov model (HMM), and how an HMM-based skill model can be used to learn human skill. HMM is feasible to characterize a doubly stochastic process--measurable action and immeasurable mental states--that is involved in the skill learning. We formulated the learning problem as a multidimensional HMM and developed a testbed for a variety of skill learning applications. Based on \"the most likely performance\" criterion, the best action sequence can be selected from all previously measured action data by modeling the skill as an HMM. The proposed method has been implemented in the teleoperation control of space station robot system, and some important implementation issues have been discussed. The method allows a robot to learn human skill in certain tasks and to improve motion performance."
            },
            "slug": "Hidden-Markov-model-approach-to-skill-learning-and-Yang-Xu",
            "title": {
                "fragments": [],
                "text": "Hidden Markov model approach to skill learning and its application to telerobotics"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The problem of how human skill can be represented as a parametric model using a hidden Markov model (HMM) and how an HMM-based skill model can be used to learn human skill are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Robotics Autom."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187514"
                        ],
                        "name": "R. Durbin",
                        "slug": "R.-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Durbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708077"
                        ],
                        "name": "S. Eddy",
                        "slug": "S.-Eddy",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Eddy",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eddy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197258"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2852254,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "571f5bbecd3a083a2bb6844f59a3f8cea237252e",
            "isKey": false,
            "numCitedBy": 4477,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Probablistic models are becoming increasingly important in analyzing the huge amount of data being produced by large-scale DNA-sequencing efforts such as the Human Genome Project. For example, hidden Markov models are used for analyzing biological sequences, linguistic-grammar-based probabilistic models for identifying RNA secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it is accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time presents the state of the art in this new and important field."
            },
            "slug": "Biological-Sequence-Analysis:-Probabilistic-Models-Durbin-Eddy",
            "title": {
                "fragments": [],
                "text": "Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702103"
                        ],
                        "name": "E. Birney",
                        "slug": "E.-Birney",
                        "structuredName": {
                            "firstName": "Ewan",
                            "lastName": "Birney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Birney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11958830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d02cc73bf0f0e9e63642543d6a1c1df53e75688",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The vast increase of data in biology has meant that many aspects of computational science have been drawn into the field. Two areas of crucial importance are large-scale data management and machine learning. The field between computational science and biology is varyingly described as \"computational biology\" or \"bioinformatics.\" This paper reviews machine learning techniques based on the use of hidden Markov models (HMMs) for investigating biomolecular sequences. The approach is illustrated with brief descriptions of gene-prediction HMMs and protein family HMMs."
            },
            "slug": "Hidden-Markov-models-in-biological-sequence-Birney",
            "title": {
                "fragments": [],
                "text": "Hidden Markov models in biological sequence analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper reviews machine learning techniques based on the use of hidden Markov models (HMMs) for investigating biomolecular sequences and describes gene-prediction HMMs and protein family HMMs."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27578127,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e1053197256c6c3c0631377ec23a3f7dc1cb4781",
            "isKey": false,
            "numCitedBy": 7616,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1. Introduction. Problems to be considered Characteristics of 'real-world' problems Finite-precision arithmetic and measurement of error Exercises 2. Nonlinear Problems in One Variable. What is not possible Newton's method for solving one equation in one unknown Convergence of sequences of real numbers Convergence of Newton's method Globally convergent methods for solving one equation in one uknown Methods when derivatives are unavailable Minimization of a function of one variable Exercises 3. Numerical Linear Algebra Background. Vector and matrix norms and orthogonality Solving systems of linear equations-matrix factorizations Errors in solving linear systems Updating matrix factorizations Eigenvalues and positive definiteness Linear least squares Exercises 4. Multivariable Calculus Background Derivatives and multivariable models Multivariable finite-difference derivatives Necessary and sufficient conditions for unconstrained minimization Exercises 5. Newton's Method for Nonlinear Equations and Unconstrained Minimization. Newton's method for systems of nonlinear equations Local convergence of Newton's method The Kantorovich and contractive mapping theorems Finite-difference derivative methods for systems of nonlinear equations Newton's method for unconstrained minimization Finite difference derivative methods for unconstrained minimization Exercises 6. Globally Convergent Modifications of Newton's Method. The quasi-Newton framework Descent directions Line searches The model-trust region approach Global methods for systems of nonlinear equations Exercises 7. Stopping, Scaling, and Testing. Scaling Stopping criteria Testing Exercises 8. Secant Methods for Systems of Nonlinear Equations. Broyden's method Local convergence analysis of Broyden's method Implementation of quasi-Newton algorithms using Broyden's update Other secant updates for nonlinear equations Exercises 9. Secant Methods for Unconstrained Minimization. The symmetric secant update of Powell Symmetric positive definite secant updates Local convergence of positive definite secant methods Implementation of quasi-Newton algorithms using the positive definite secant update Another convergence result for the positive definite secant method Other secant updates for unconstrained minimization Exercises 10. Nonlinear Least Squares. The nonlinear least-squares problem Gauss-Newton-type methods Full Newton-type methods Other considerations in solving nonlinear least-squares problems Exercises 11. Methods for Problems with Special Structure. The sparse finite-difference Newton method Sparse secant methods Deriving least-change secant updates Analyzing least-change secant methods Exercises Appendix A. A Modular System of Algorithms for Unconstrained Minimization and Nonlinear Equations (by Robert Schnabel) Appendix B. Test Problems (by Robert Schnabel) References Author Index Subject Index."
            },
            "slug": "Numerical-methods-for-unconstrained-optimization-Dennis-Schnabel",
            "title": {
                "fragments": [],
                "text": "Numerical methods for unconstrained optimization and nonlinear equations"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Newton's Method for Nonlinear Equations and Unconstrained Minimization and methods for solving nonlinear least-squares problems with Special Structure."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in computational mathematics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "This posterior probability can be computed using an efficient forward-backward algorithm [50]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 291
                            }
                        ],
                        "text": "\u2026comes from the fact that sentence tokens in the training set are independent of each other. \u03b3 i,r,s r (t ) is the occupation probability of state i at time t , given the label sequence s r and observation sequence X r , which can be obtained through an efficient forward-backward algorithm [50]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7788300,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "df50c6e1903b1e2d657f78c28ab041756baca86a",
            "isKey": false,
            "numCitedBy": 8924,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Fundamentals of Speech Recognition. 2. The Speech Signal: Production, Perception, and Acoustic-Phonetic Characterization. 3. Signal Processing and Analysis Methods for Speech Recognition. 4. Pattern Comparison Techniques. 5. Speech Recognition System Design and Implementation Issues. 6. Theory and Implementation of Hidden Markov Models. 7. Speech Recognition Based on Connected Word Models. 8. Large Vocabulary Continuous Speech Recognition. 9. Task-Oriented Applications of Automatic Speech Recognition."
            },
            "slug": "Fundamentals-of-speech-recognition-Rabiner-Juang",
            "title": {
                "fragments": [],
                "text": "Fundamentals of speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book presents a meta-modelling framework for speech recognition that automates the very labor-intensive and therefore time-heavy and therefore expensive and expensive process of manually modeling speech."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall signal processing series"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47932744"
                        ],
                        "name": "Xuedong Huang",
                        "slug": "Xuedong-Huang",
                        "structuredName": {
                            "firstName": "Xuedong",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuedong Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145058181"
                        ],
                        "name": "H. Hon",
                        "slug": "H.-Hon",
                        "structuredName": {
                            "firstName": "Hsiao-Wuen",
                            "lastName": "Hon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [21] SEPTEMBER 2008 In (25), X 1 . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 216
                            }
                        ],
                        "text": "\" ) Given the misclassification measure, the loss function can be defined for each training token r, and it is usually defined through a sigmoid function as originally proposed in [28], [29] : IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [18] SEPTEMBER 2008 where \u03b1 > 0 is the slope of the sigmoid function, often determined empirically."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 204
                            }
                        ],
                        "text": "Although a sufficiently large D i guarantees monotone convergence of the GT-based iterative estimation formulas, i.e., (52), (57) for the discrete HMM and (70), (71) for the CDHMM, IEEE SIGNAL PROCESSING MAGAZINE [28] SEPTEMBER 2008 \u03bc i = R r =1 T r t =1 \u03b3 (i, r, t)x t + D i \u03bc i R r =1 T r t=1 \u03b3 (i, r, t) + D i , ( 70) i = R r=1 T r t=1 \u03b3 (i, r, t)(x t \u2212 \u03bc i )(x t \u2212 \u03bc i ) T + D i i + D i (\u03bc i \u2212 \u03bc i )(\u03bc i \u2212 \u03bc i ) T R r=1 T r t=1 \u03b3 (i, r, t) + D i (71) IEEE SIGNAL PROCESSING MAGAZINE [29] SEPTEMBER 2008 the value of D i from the monotone convergence proof is a very loose upper bound and it can be too large for a reasonable convergence speed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [25] SEPTEMBER 2008 term-I = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 q p(q |X, s, , ) R r =1 T r t =1 log p(x r,t |q r,t , ,) = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T r t =1 I i =1 q,q r,t =i p(q |X, s, , ) \u00d7 log p(x r,t |q r,t = i, ,) = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 157
                            }
                        ],
                        "text": ") is given by Therefore, parameter optimization of MMI-based discriminative learning is to maximize the following objective function: IEEE SIGNAL PROCESSING MAGAZINE [17] SEPTEMBER 2008 where P(s r ) is the language model probability of pattern sequence s r ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 58
                            }
                        ],
                        "text": "From (50), (41), and (36), we have IEEE SIGNAL PROCESSING MAGAZINE [26] SEPTEMBER 2008 b i (k) = R r=1 T r t=1 s.t. x r,t =k s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t) + b i (k) R r =1 T r t =1 d(r, t, i) R r=1 T r t=1 s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t) + R r=1 T r t=1 d(r, t, i) (49) a i, j = R r =1 T r t =1 s p(s |X, , )(C(s) \u2212 O(( ))\u03be i, j,r,s r (t ) + a i, j R r =1 T r t =1 d(r, t \u2212 1, i ) R r =1 T r t =1 s p(s |X, , )(C(s) \u2212 O(( ))\u03b3 i,r,s r (t ) + R r=1 T r t =1 d(r, t \u2212 1, i ) (54) IEEE SIGNAL PROCESSING MAGAZINE [27] SEPTEMBER 2008 The theoretical basis for setting D i to ensure that (52) and (57) are growth transformations is the requirement described in (32) that d(s) of (58) be sufficiently large so that In practice, D i and\u02dcD i given by (59) and (60) have often been found to be over conservative and unnecessarily large, causing slower convergence than those obtained through some empirical methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [25] SEPTEMBER 2008 term-I = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 q p(q |X, s, , ) R r =1 T r t =1 log p(x r,t |q r,t , ,) = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T r t =1 I i =1 q,q r,t =i p(q |X, s, , ) \u00d7 log p(x r,t |q r,t = i, ,) = s p(s |X, , )(C(s) \u2212 O(( )) \u00d7 R r =1 T r t =1 I i =1 \u03b3 i,r,s r (t ) log p(x r,t |q r,t = i, )."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "IEEE SIGNAL PROCESSING MAGAZINE [20] SEPTEMBER 2008 IEEE SIGNAL PROCESSING MAGAZINE [20] SEPTEMBER 2008 O MCE (() = R r =1 s r p(X r , s r |)\u03b4(s r , S r ) s r p(X r , s r"
                    },
                    "intents": []
                }
            ],
            "corpusId": 59832957,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "74fe8a40571758823eb1858ccc9411e2b43fe7ea",
            "isKey": true,
            "numCitedBy": 959,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "A filling assembly for vacuum filling impervious open mouth paper bags with finely divided particulate material. A filling head is provided with at least two independent vertically extending chambers through which a selective vacuum or relief may be applied. Particulate material is fed through a central opening in the filling head whereby interstitial air is withdrawn from between the particles of particulate material as the material falls downwardly into an impervious open mouth bag. Clamping jaws serve to seal the bag against the filler head during the filling operation and include clamping flanges which extend outwardly from the jaws and tightly close the outer edges of the open mouth bag. A shroud extends from the clamping jaws to provide a sealed enclosure about the bag during the vacuum filling operation. A vacuum source is selectively applied to the shroud to open the impervious bag and keep the bag open during the vacuum filling operation."
            },
            "slug": "Spoken-Language-Processing-Acero-Huang",
            "title": {
                "fragments": [],
                "text": "Spoken Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A filling assembly for vacuum filling impervious open mouth paper bags with finely divided particulate material is provided with at least two independent vertically extending chambers through which a selective vacuum or relief may be applied."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44649985,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4153961c9b8807d73a50fead9962b3bd91ea6887",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A table for use in teaching children, particularly those with learning disabilities. The table includes a translucent top upon which a paper worksheet can be placed. An optical projector is mounted below the table top to direct an image onto the translucent table top. The image is seen through the paper worksheet and may be followed or traced by the child to develop psychomotor brain patterns."
            },
            "slug": "Linear-models-for-structure-prediction-Pereira",
            "title": {
                "fragments": [],
                "text": "Linear models for structure prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A table for use in teaching children, particularly those with learning disabilities, that includes a translucent top upon which a paper worksheet can be placed."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057698634"
                        ],
                        "name": "W. Chou",
                        "slug": "W.-Chou",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Chou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 208894924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8e5eb2e0505f1b806d7baf4834d64f5578343ce",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "11."
            },
            "slug": "Minimum-Classification-Error-(MCE)-Approach-in-Chou",
            "title": {
                "fragments": [],
                "text": "Minimum Classification Error (MCE) Approach in Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "Consequently, the objective function of MCE is usually optimized using the generalized probabilistic descent (GPD) [9], [28], [29] algorithm or other gradient-based methods [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "Algorithmically, it is difficult for GPD to parallelize the parameter learning process, which is critical for large scale tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "Despite the popularity and many successful applications, the gradient descent based sequential learning using GPD has two main drawbacks."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Minimum classification error approach in pattern recognition, \" in Pattern Recognition in Speech and Language Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Boca Raton, FL: CRC"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70205884"
                        ],
                        "name": "Volume Assp",
                        "slug": "Volume-Assp",
                        "structuredName": {
                            "firstName": "Volume",
                            "lastName": "Assp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volume Assp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123245131,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "5614c9fd73fb025f156ae8811e6afd78ffd63ccc",
            "isKey": false,
            "numCitedBy": 511,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ACOUSTICS.-SPEECH.-AND-SIGNAL-PROCESSING-Assp",
            "title": {
                "fragments": [],
                "text": "ACOUSTICS. SPEECH. AND SIGNAL PROCESSING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50818581"
                        ],
                        "name": "G. Sell",
                        "slug": "G.-Sell",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Sell",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54933170,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d01ceefe8df12cc5569af0257d38cac989443e6e",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Growth-transformations-for-functions-on-manifolds.-Baum-Sell",
            "title": {
                "fragments": [],
                "text": "Growth transformations for functions on manifolds."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107314775"
                        ],
                        "name": "C. C. Taylor",
                        "slug": "C.-C.-Taylor",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Taylor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. C. Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1625830,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "ca23e7a71ace53d6a5b2a553ff37c63365d22b8a",
            "isKey": false,
            "numCitedBy": 5721,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Recognition-Ripley-Taylor",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization method for discriminative train"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . IEEE Int . Conf . Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative methods for HMM-based speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Discriminative methods for HMM-based speech recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A minimum classification error framework for generalized linear classifier in machine learning and its application to text categorization / retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . Int . Conf . Machine Learning Applications , Dec ."
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative training for automatic speech recognition using the minimum classification error framework"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing System (NIPS) Workshop"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "[61]; 2) minimum classification error (MCE) [1], [9], [23], [28], [29], [35], [36], [38], [49], [52], [54]; and 3) minimum phone error (MPE) and closely related minimum word error (MWE) [13], [45]\u2013[48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis and comparison of two feature extraction/compensation algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Lett"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis and comparison of two feature extraction / compensation algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Lett ."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization method for discriminative training"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Interspeech"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A minimum classification error framework for generalized linear classifier in machine learning and its application to text categorization/retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int. Conf. Machine Learning Applications"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An inequality with applications to statistical prediction for functions of Markov processes and to a model of ecology"
            },
            "venue": {
                "fragments": [],
                "text": "Bull. Amer. Math. Soc"
            },
            "year": 1967
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 14
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 75,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Discriminative-learning-in-sequential-pattern-He-Deng/d63cdc1d1f023c63f8aa3b64cd5e853670680c3e?sort=total-citations"
}