{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8313435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e19a94d547ee023837c14c361139185e2353fc0",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the model\u2019s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar."
            },
            "slug": "Max-Margin-Parsing-Taskar-Klein",
            "title": {
                "fragments": [],
                "text": "Max-Margin Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines is presented, which allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 79
                            }
                        ],
                        "text": "In some cases, though, we can find a concise certificate of optimality that guarantees that y(i) = arg maxy[w fi(y)+ i(y)] without expressing loss-augmented inference as a concise convex program."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 221
                            }
                        ],
                        "text": "Our framework extends the max-margin formulation for Markov networks (Taskar et al., 2003; Taskar et al., 2004a) and context free grammars (Taskar et al., 2004b), and is similar to other formulations (Altun et al., 2003; Tsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 564746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93aa298b40bb3ec23c25239089284fdf61ded917",
            "isKey": false,
            "numCitedBy": 1455,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "slug": "Support-vector-machine-learning-for-interdependent-Tsochantaridis-Hofmann",
            "title": {
                "fragments": [],
                "text": "Support vector machine learning for interdependent and structured output spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs, and demonstrates the versatility and effectiveness of the method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": false,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482074"
                        ],
                        "name": "Vassil Chatalbashev",
                        "slug": "Vassil-Chatalbashev",
                        "structuredName": {
                            "firstName": "Vassil",
                            "lastName": "Chatalbashev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vassil Chatalbashev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our framework extends the max-margin formulation for Markov networks (Taskar et al., 2003;  Taskar et al., 2004a ) and context free grammars (Taskar et al., 2004b), and is similar to other formulations (Altun et al., 2003; Tsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 91
                            }
                        ],
                        "text": "Our framework extends the max-margin formulation for Markov networks (Taskar et al., 2003; Taskar et al., 2004a) and context free grammars (Taskar et al., 2004b), and is similar to other formulations (Altun et al., 2003; Tsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This formulation generalizes the idea used by  Taskar et al. (2004a)  to provide a polynomial time estimation procedure for a certain family of Markov networks; it can also be used to derive the max-margin formulations of Taskar et al. (2003); Taskar et al. (2004b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 46
                            }
                        ],
                        "text": "This formulation generalizes the idea used by Taskar et al. (2004a) to provide a polynomial time estimation procedure for a certain family of Markov networks; it can also be used to derive the max-margin formulations of Taskar et al. (2003); Taskar et al. (2004b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11312524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "702c2fde33ccb4328be06405c11e208a4b3ee347",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov networks are extensively used to model complex sequential, spatial, and relational interactions in fields as diverse as image processing, natural language analysis, and bioinformatics. However, inference and learning in general Markov networks is intractable. In this paper, we focus on learning a large subclass of such models (called associative Markov networks) that are tractable or closely approximable. This subclass contains networks of discrete variables with K labels each and clique potentials that favor the same labels for all variables in the clique. Such networks capture the \"guilt by association\" pattern of reasoning present in many domains, in which connected (\"associated\") variables tend to have the same label. Our approach exploits a linear programming relaxation for the task of finding the best joint assignment in such networks, which provides an approximate quadratic program (QP) for the problem of learning a margin-maximizing Markov network. We show that for associative Markov network over binary-valued variables, this approximate QP is guaranteed to return an optimal parameterization for Markov networks of arbitrary topology. For the nonbinary case, optimality is not guaranteed, but the relaxation produces good solutions in practice. Experimental results with hypertext and newswire classification show significant advantages over standard approaches."
            },
            "slug": "Learning-associative-Markov-networks-Taskar-Chatalbashev",
            "title": {
                "fragments": [],
                "text": "Learning associative Markov networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper exploits a linear programming relaxation for the task of finding the best joint assignment in associative Markov networks, which provides an approximate quadratic program (QP) for the problem of learning a margin-maximizing Markov network."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028880"
                        ],
                        "name": "G. Pollastri",
                        "slug": "G.-Pollastri",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Pollastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pollastri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1593083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b7a0048801f9d43dc48a8f04367be813146b05a",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a general methodology for the design of large-scale recursive neural network architectures (DAG-RNNs) which comprises three fundamental steps: (1) representation of a given domain using suitable directed acyclic graphs (DAGs) to connect visible and hidden node variables; (2) parameterization of the relationship between each variable and its parent variables by feedforward neural networks; and (3) application of weight-sharing within appropriate subsets of DAG connections to capture stationarity and control model complexity. Here we use these principles to derive several specific classes of DAG-RNN architectures based on lattices, trees, and other structured graphs. These architectures can process a wide range of data structures with variable sizes and dimensions. While the overall resulting models remain probabilistic, the internal deterministic dynamics allows efficient propagation of information, as well as training by gradient descent, in order to tackle large-scale problems. These methods are used here to derive state-of-the-art predictors for protein structural features such as secondary structure (1D) and both fine- and coarse-grained contact maps (2D). Extensions, relationships to graphical models, and implications for the design of neural architectures are briefly discussed. The protein prediction servers are available over the Web at: www.igb.uci.edu/tools.htm ."
            },
            "slug": "The-Principled-Design-of-Large-Scale-Recursive-and-Baldi-Pollastri",
            "title": {
                "fragments": [],
                "text": "The Principled Design of Large-Scale Recursive Neural Network Architectures--DAG-RNNs and the Protein Structure Prediction Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "These methods are used to derive state-of-the-art predictors for protein structural features such as secondary structure and both fine- and coarse-grained contact maps and implications for the design of neural architectures are briefly discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2274348"
                        ],
                        "name": "Khashayar Rohanimanesh",
                        "slug": "Khashayar-Rohanimanesh",
                        "structuredName": {
                            "firstName": "Khashayar",
                            "lastName": "Rohanimanesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Khashayar Rohanimanesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6038991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0329663498462521483612649c0dffc85d9d9419",
            "isKey": false,
            "numCitedBy": 901,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "slug": "Dynamic-conditional-random-fields:-factorized-for-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "On a natural-language chunking task, it is shown that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9743839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e6779bb55f7fbed5684ded55df51747ea678a84",
            "isKey": false,
            "numCitedBy": 669,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems."
            },
            "slug": "Partially-labeled-classification-with-Markov-random-Szummer-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Partially labeled classification with Markov random walks"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work combines a limited number of labeled examples with a Markov random walk representation over the unlabeled examples and develops and compares several estimation criteria/algorithms suited to this representation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145004630"
                        ],
                        "name": "M. Anthony",
                        "slug": "M.-Anthony",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Anthony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 112
                            }
                        ],
                        "text": "1 uses the covering number bounds of Zhang [2002] (in the Data-Dependent Structural Risk Minimization framework [Shawe-Taylor et al., 1998]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6789514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5",
            "isKey": false,
            "numCitedBy": 619,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper introduces some generalizations of Vapnik's (1982) method of structural risk minimization (SRM). As well as making explicit some of the details on SRM, it provides a result that allows one to trade off errors on the training sample against improved generalization performance. It then considers the more general case when the hierarchy of classes is chosen in response to the data. A result is presented on the generalization performance of classifiers with a \"large margin\". This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines). The paper concludes with a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets. Four examples are given of such functions, including the Vapnik-Chervonenkis (1971) dimension measured on the sample."
            },
            "slug": "Structural-Risk-Minimization-Over-Data-Dependent-Shawe-Taylor-Bartlett",
            "title": {
                "fragments": [],
                "text": "Structural Risk Minimization Over Data-Dependent Hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A result is presented that allows one to trade off errors on the training sample against improved generalization performance, and a more general result in terms of \"luckiness\" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911526"
                        ],
                        "name": "A. Corduneanu",
                        "slug": "A.-Corduneanu",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Corduneanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Corduneanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1761253,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7de4569c7353030fec21bbb38c06323dd69f777c",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate a principle for classification with the knowledge of the marginal distribution over the data points (unlabeled data). The principle is cast in terms of Tikhonov style regularization where the regularization penalty articulates the way in which the marginal density should constrain otherwise unrestricted conditional distributions. Specifically, the regularization penalty penalizes any information introduced between the examples and labels beyond what is provided by the available labeled examples. The work extends Szummer and Jaakkola's information regularization (NIPS 2002) to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation. We show in addition how the information regularizer can be used as a measure of complexity of the classification task with unlabeled data and prove a relevant sample-complexity bound. We illustrate the regularization principle in practice by restricting the class of conditional distributions to be logistic regression models and constructing the regularization penalty from a finite set of unlabeled examples."
            },
            "slug": "On-Information-Regularization-Corduneanu-Jaakkola",
            "title": {
                "fragments": [],
                "text": "On Information Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The work extends Szummer and Jaakkola's information regularization to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation, and shows in addition how the information regularizer can be used as a measure of complexity of the classification task with unlabeled data and prove a relevant sample-complexity bound."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 94
                            }
                        ],
                        "text": "Following the lines of the recent work on maximum margin estimation for probabilistic models (Collins, 2002; Altun et al., 2003; Taskar et al., 2003), we present a discriminative estimation framework for structured models based on the large margin principle underlying support vector machines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Collins, M. (2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 76
                            }
                        ],
                        "text": "Our generalization bound significantly tightens previous results of Collins [Collins, 2001] and suggests possibilities for analyzing per-label generalization properties of graphical models."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 87
                            }
                        ],
                        "text": "Such a result was, until now, an open problem for margin-based sequence classification [Collins, 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10576017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a6cda5c73b3da91ce4260b2b70ca5c226b39edf",
            "isKey": true,
            "numCitedBy": 132,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental problem in statistical parsing is the choice of criteria and algo-algorithms used to estimate the parameters in a model. The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum-likelihood estimation. The assumptions under which maximum-likelihood estimation is justified are arguably quite strong. This chapter discusses the statistical theory underlying various parameter-estimation methods, and gives algorithms which depend on alternatives to (smoothed) maximum-likelihood estimation. We first give an overview of results from statistical learning theory. We then show how important concepts from the classification literature - specifically, generalization results based on margins on training data - can be derived for parsing models. Finally, we describe parameter estimation algorithms which are motivated by these generalization bounds."
            },
            "slug": "Parameter-Estimation-for-Statistical-Parsing-Theory-Collins",
            "title": {
                "fragments": [],
                "text": "Parameter Estimation for Statistical Parsing Models: Theory and Practice of"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This chapter discusses the statistical theory underlying various parameter-estimation methods, and gives algorithms which depend on alternatives to maximum-likelihood estimation, and describes parameter estimation algorithms which are motivated by these generalization bounds."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 135
                            }
                        ],
                        "text": "Recently, a number of algorithms have been proposed for automatically learning distance metrics as a preprocessing step for clustering [Xing et al., 2002; Bar-Hillelet al., 2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2643381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1a2d203733208deda7427c8e20318334193d9d7",
            "isKey": false,
            "numCitedBy": 3026,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many \"plausible\" ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider \"similar.\" For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in \u211dn, learns a distance metric over \u211dn that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance."
            },
            "slug": "Distance-Metric-Learning-with-Application-to-with-Xing-Ng",
            "title": {
                "fragments": [],
                "text": "Distance Metric Learning with Application to Clustering with Side-Information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in \ufffd\u201dn, learns a distance metric over \u211dn that respects these relationships."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10151608,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "cfc6d0c8260594ebc5dd20ee558d29b1014ed41a",
            "isKey": false,
            "numCitedBy": 2190,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy."
            },
            "slug": "On-the-Algorithmic-Implementation-of-Multiclass-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper describes the algorithmic implementation of multiclass kernel-based vector machines using a generalized notion of the margin to multiclass problems, and describes an efficient fixed-point algorithm for solving the reduced optimization problems and proves its convergence."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135485"
                        ],
                        "name": "Tom. Mitchell",
                        "slug": "Tom.-Mitchell",
                        "structuredName": {
                            "firstName": "Tom.",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207228399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278841ab0cb24c1abcb75e363aeed1fa741c8cc4",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu"
            },
            "slug": "Combining-labeled-and-unlabeled-data-with-Blum-Mitchell",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data with co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A PAC-style analysis is provided for a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views, to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "We assume for simplicity that the features are rich enough to satisfy the constraints, which is analogous to the separable case formulation in SVMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 80
                            }
                        ],
                        "text": "The proof uses a covering number argument analogous to previous results in SVMs [Zhang, 2002]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "Proof: Similar to the proof of Zhang\u2019s Theorem 2 and Corollary 1 [Zhang, 2002] where in Step 3 (derandomization) we substitute the vector-valued FM and the metric\u03c1M ."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "In particular, as in SVMs, we can use the kernel trick in the dual to efficiently learn in high-dimensional feature spaces."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10983659,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "23afab3f249477d086819e890ac4aa417998568c",
            "isKey": true,
            "numCitedBy": 246,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, sample complexity bounds have been derived for problems involving linear functions such as neural networks and support vector machines. In many of these theoretical studies, the concept of covering numbers played an important role. It is thus useful to study covering numbers for linear function classes. In this paper, we investigate two closely related methods to derive upper bounds on these covering numbers. The first method, already employed in some earlier studies, relies on the so-called Maurey's lemma; the second method uses techniques from the mistake bound framework in online learning. We compare results from these two methods, as well as their consequences in some learning formulations."
            },
            "slug": "Covering-Number-Bounds-of-Certain-Regularized-Zhang",
            "title": {
                "fragments": [],
                "text": "Covering Number Bounds of Certain Regularized Linear Function Classes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper investigates two closely related methods to derive upper bounds on covering numbers for linear function classes by relying on the so-called Maurey's lemma and techniques from the mistake bound framework in online learning."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13936575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6",
            "isKey": false,
            "numCitedBy": 1544,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models."
            },
            "slug": "Shallow-Parsing-with-Conditional-Random-Fields-Sha-Pereira",
            "title": {
                "fragments": [],
                "text": "Shallow Parsing with Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work shows how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682522"
                        ],
                        "name": "Se\u00e1n Slattery",
                        "slug": "Se\u00e1n-Slattery",
                        "structuredName": {
                            "firstName": "Se\u00e1n",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se\u00e1n Slattery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14179936,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95dacab16a2f93cc60c4e9c202196972dc951795",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine learning typically involves discovering regularities in a training set, then applying these learned regularities to classify objects in a test set. In this paper we present an approach to discovering additional regularities in the test set, and show that in relational domains such test set regularities can be used to improve classification accuracy beyond that achieved using the training set alone. For example, we have previously shown how FOIL, a relational learner, can learn to classify Web pages by discovering training set regularities in the words occurring on target pages, and on other pages related by hyperlinks. Here we show how the classification accuracy of FOIL on this task can be improved by discovering additional regularities on the test set pages that must be classified. Our approach can be seen as an extension to Kleinberg\u2019s Hubs and Authorities algorithm that analyzes hyperlink relations among Web pages. We present evidence that this new algorithm leads to better test set precision and recall on three binary Web classification tasks where the test set Web pages are taken from different Web sites than the training set."
            },
            "slug": "Discovering-Test-Set-Regularities-in-Relational-Slattery-Mitchell",
            "title": {
                "fragments": [],
                "text": "Discovering Test Set Regularities in Relational Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper presents an approach to discovering additional regularities in the test set, and shows that in relational domains such test set regularities can be used to improve classification accuracy beyond that achieved using the training set alone."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145349582"
                        ],
                        "name": "Sanjiv Kumar",
                        "slug": "Sanjiv-Kumar",
                        "structuredName": {
                            "firstName": "Sanjiv",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjiv Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1282113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015293bf7c4cf7ce50a01ce1ceb11f584d123d25",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classification of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classification problems using the graph min-cut algorithms. The performance of the model was verified on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments."
            },
            "slug": "Discriminative-Fields-for-Modeling-Spatial-in-Kumar-Hebert",
            "title": {
                "fragments": [],
                "text": "Discriminative Fields for Modeling Spatial Dependencies in Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed DRF model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 97
                            }
                        ],
                        "text": "A number of recent papers have considered discriminative approaches for natural language parsing [Johnsonet al., 1999; Collins, 2000; Johnson, 2001; Geman & Johnson, 2002; Miyao & Tsujii, 2002; Clark & Curran, 2004; Kaplan et al., 2004; Collins, 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 42
                            }
                        ],
                        "text": "For example, the maximum entropy approach [Johnson, 2001] defines a conditional log-linear model:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 32
                            }
                        ],
                        "text": "For example, several approaches [Johnson, 2001; Geman & Johnson, 2002; Miyao & Tsujii, 2002; Clark & Curran, 2004; Kaplanet al., 2004] are based on conditional log-linear (maximum entropy) models, where variants of the inside-outside algorithm can be used to efficiently calculate gradients of the log-likelihood function, despite the exponential number of trees represented by the parse forest."
                    },
                    "intents": []
                }
            ],
            "corpusId": 739426,
            "fieldsOfStudy": [
                "Economics",
                "Computer Science"
            ],
            "id": "435245be302b3dc8ed244b1e6b2dba0b92baacf8",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper compares two different ways of estimating statistical language models. Many statistical NLP tagging and parsing models are estimated by maximizing the (joint) likelihood of the fully-observed training data. However, since these applications only require the conditional probability distributions, these distributions can in principle be learnt by maximizing the conditional likelihood of the training data. Perhaps somewhat surprisingly, models estimated by maximizing the joint were superior to models estimated by maximizing the conditional, even though some of the latter models intuitively had access to \"more information\"."
            },
            "slug": "Joint-and-Conditional-Estimation-of-Tagging-and-Johnson",
            "title": {
                "fragments": [],
                "text": "Joint and Conditional Estimation of Tagging and Parsing Models"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper compares two different ways of estimating statistical language models by maximizing the joint likelihood of the fully-observed training data and finds that some of the latter models intuitively had access to \"more information\" than the former."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060101052"
                        ],
                        "name": "Terry Koo",
                        "slug": "Terry-Koo",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Koo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terry Koo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 97
                            }
                        ],
                        "text": "A number of recent papers have considered discriminative approaches for natural language parsing [Johnsonet al., 1999; Collins, 2000; Johnson, 2001; Geman & Johnson, 2002; Miyao & Tsujii, 2002; Clark & Curran, 2004; Kaplan et al., 2004; Collins, 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 21
                            }
                        ],
                        "text": "In reranking methods [Johnson et al., 1999; Collins, 2000; Shen et al., 2003], an initial parser is used to generate a number of candidate parses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 405878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "844db702be4bc149b06b822b47247e15f5894cc3",
            "isKey": false,
            "numCitedBy": 776,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 F-measure, a 13 relative decrease in F-measure error over the baseline model's score of 88.2. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation."
            },
            "slug": "Discriminative-Reranking-for-Natural-Language-Collins-Koo",
            "title": {
                "fragments": [],
                "text": "Discriminative Reranking for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The boosting approach to ranking problems described in Freund et al. (1998) is applied to parsing the Wall Street Journal treebank, and it is argued that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67337974"
                        ],
                        "name": "Miyao Yusuke",
                        "slug": "Miyao-Yusuke",
                        "structuredName": {
                            "firstName": "Miyao",
                            "lastName": "Yusuke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miyao Yusuke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737901"
                        ],
                        "name": "Junichi Tsujii",
                        "slug": "Junichi-Tsujii",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Tsujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junichi Tsujii"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15084210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51836a978517a4fdd6b68f69d3821c0d1a339e09",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm is proposed for maximum entropy modeling. It enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. A probabilistic event is represented by a feature forest, which is a packed representation of features with ambiguities. The parameters are efficiently estimated by traversing each node in a feature forest by dynamic programming. Experiments showed the algorithm worked efficiently even when ambiguities in a feature forest cause an exponential explosion of unpacked structures."
            },
            "slug": "Maximum-entropy-estimation-for-feature-forests-Yusuke-Tsujii",
            "title": {
                "fragments": [],
                "text": "Maximum entropy estimation for feature forests"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An algorithm is proposed for maximum entropy modeling that enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255005"
                        ],
                        "name": "Alessandro Vullo",
                        "slug": "Alessandro-Vullo",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Vullo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Vullo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 124
                            }
                        ],
                        "text": "We used two datasets containing sequences with experimentally verified bonding patterns: SP39\n(used in Baldi et al. (2004); Vullo and Frasconi (2004); Fariselli and Casadio (2001)) and DIPRO2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "In order to avoid biases during testing, we adopt the same dataset splitting procedure as the one used in previous work (Fariselli & Casadio, 2001; Vullo & Frasconi, 2004; Baldi et al., 2004).2\nModels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 137
                            }
                        ],
                        "text": "We apply the model to the SP39 dataset by using 4-fold cross-validation, which replicates the experimental setup of Baldi et al. (2004); Vullo and Frasconi (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 327386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b407dfcd313e4d93011fad22ccf2c4d70e66ae5",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nWe focus on the prediction of disulfide bridges in proteins starting from their amino acid sequence and from the knowledge of the disulfide bonding state of each cysteine. The location of disulfide bridges is a structural feature that conveys important information about the protein main chain conformation and can therefore help towards the solution of the folding problem. Existing approaches based on weighted graph matching algorithms do not take advantage of evolutionary information. Recursive neural networks (RNN), on the other hand, can handle in a natural way complex data structures such as graphs whose vertices are labeled by real vectors, allowing us to incorporate multiple alignment profiles in the graphical representation of disulfide connectivity patterns.\n\n\nRESULTS\nThe core of the method is the use of machine learning tools to rank alternative disulfide connectivity patterns. We develop an ad-hoc RNN architecture for scoring labeled undirected graphs that represent connectivity patterns. In order to compare our algorithm with previous methods, we report experimental results on the SWISS-PROT 39 dataset. We find that using multiple alignment profiles allows us to obtain significant prediction accuracy improvements, clearly demonstrating the important role played by evolutionary information.\n\n\nAVAILABILITY\nThe Web interface of the predictor is available at http://neural.dsi.unifi.it/cysteines"
            },
            "slug": "Disulfide-connectivity-prediction-using-recursive-Vullo-Frasconi",
            "title": {
                "fragments": [],
                "text": "Disulfide connectivity prediction using recursive neural networks and evolutionary information"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Recursive neural networks (RNN) can handle in a natural way complex data structures such as graphs whose vertices are labeled by real vectors, allowing us to incorporate multiple alignment profiles in the graphical representation of disulfide connectivity patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746222"
                        ],
                        "name": "\u00c9. Tardos",
                        "slug": "\u00c9.-Tardos",
                        "structuredName": {
                            "firstName": "\u00c9va",
                            "lastName": "Tardos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c9. Tardos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16241328,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a3b3aad58ecc6aed599c7567d4fe07ad3480a866",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "In a traditional classification problem, we wish to assign one of k labels (or classes) to each of n objects, in a way that is consistent with some observed data that we have about the problem. An active line of research in this area is concerned with classification when one has information about pairwise relationships among the objects to be classified; this issue is one of the principal motivations for the framework of Markov random fields, and it arises in areas such as image processing, biometry: and document analysis. In its most basic form, this style of analysis seeks a classification that optimizes a combinatorial function consisting of assignment costs-based on the individual choice of label we make for each object-and separation costs-based on the pair of choices we make for two \"related\" objects. We formulate a general classification problem of this type, the metric labeling problem; we show that it contains as special cases a number of standard classification frameworks, including several arising from the theory of Markov random fields. From the perspective of combinatorial optimization, our problem can be viewed as a substantial generalization of the multiway cut problem, and equivalent to a type of uncapacitated quadratic assignment problem. We provide the first non-trivial polynomial-time approximation algorithms for a general family of classification problems of this type. Our main result is an O(log k log log k)-approximation algorithm for the metric labeling problem, with respect to an arbitrary metric on a set of k labels, and an arbitrary weighted graph of relationships on a set of objects. For the special case in which the labels are endowed with the uniform metric-all distances are the same-our methods provide a 2-approximation."
            },
            "slug": "Approximation-algorithms-for-classification-with-Kleinberg-Tardos",
            "title": {
                "fragments": [],
                "text": "Approximation algorithms for classification problems with pairwise relationships: metric labeling and Markov random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work provides the first non-trivial polynomial-time approximation algorithms for a general family of classification problems of this type, the metric labeling problem, and shows that it contains as special cases a number of standard classification frameworks, including several arising from the theory of Markov random fields."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746034"
                        ],
                        "name": "L. Getoor",
                        "slug": "L.-Getoor",
                        "structuredName": {
                            "firstName": "Lise",
                            "lastName": "Getoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Getoor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991343"
                        ],
                        "name": "E. Segal",
                        "slug": "E.-Segal",
                        "structuredName": {
                            "firstName": "Eran",
                            "lastName": "Segal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Segal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12897830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "991dc062c3fcfbe98e27e769843548be63fa8299",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Most text classification methods treat each document as an independent instance. However, in many text domains, documents are linked and the topics of linked documents are correlated. For example, web pages of related topics are often connected by hyperlinks and scientific papers from related fields are commonly linked by citations. We propose a unified probabilistic model for both the textual content and the link structure of a document collection. Our model is based on the recently introduced framework of Probabilistic Relational Models (PRMs), which allows us to capture correlations between linked documents. We show how to learn these models from data and use them efficiently for classification. Since exact methods for classification in these large models are intractable, we utilize belief propagation, an approximate inference algorithm. Belief propagation automatically induces a very natural behavior, where our knowledge about one document helps us classify related ones, which in turn help us classify others. We present preliminary empirical results on a dataset of university web pages."
            },
            "slug": "Probabilistic-Models-of-Text-and-Link-Structure-for-Getoor-Segal",
            "title": {
                "fragments": [],
                "text": "Probabilistic Models of Text and Link Structure for Hypertext Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a unified probabilistic model for both the textual content and the link structure of a document collection based on the recently introduced framework of Probabilistic Relational Models (PRMs), which allows us to capture correlations between linked documents."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6130401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98eed3f082351c4821d1edb315846207a8fefbe9",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG(+/-). They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG(+/-) algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG(+/-) and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data."
            },
            "slug": "Exponentiated-Gradient-Versus-Gradient-Descent-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions, which is quite tight already on simple artificial data."
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 296750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90929a6aa901ba958eb4960aeeb594c752e08369",
            "isKey": false,
            "numCitedBy": 2230,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation\u2014which is borne out in repeated experiments\u2014that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster."
            },
            "slug": "On-Discriminative-vs.-Generative-Classifiers:-A-of-Ng-Jordan",
            "title": {
                "fragments": [],
                "text": "On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is shown, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145726861"
                        ],
                        "name": "Ronald E. Parr",
                        "slug": "Ronald-E.-Parr",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Parr",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald E. Parr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2262405"
                        ],
                        "name": "Shobha Venkataraman",
                        "slug": "Shobha-Venkataraman",
                        "structuredName": {
                            "firstName": "Shobha",
                            "lastName": "Venkataraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shobha Venkataraman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1598016,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2430b4748c4ffe8782ae4763d327ce48f3655639",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of planning under uncertainty in large Markov Decision Processes (MDPs). Factored MDPs represent a complex state space using state variables and the transition model using a dynamic Bayesian network. This representation often allows an exponential reduction in the representation size of structured MDPs, but the complexity of exact solution algorithms for such MDPs can grow exponentially in the representation size. In this paper, we present two approximate solution algorithms that exploit structure in factored MDPs. Both use an approximate value function represented as a linear combination of basis functions, where each basis function involves only a small subset of the domain variables. A key contribution of this paper is that it shows how the basic operations of both algorithms can be performed efficiently in closed form, by exploiting both additive and context-specific structure in a factored MDP. A central element of our algorithms is a novel linear program decomposition technique, analogous to variable elimination in Bayesian networks, which reduces an exponentially large LP to a provably equivalent, polynomial-sized one. One algorithm uses approximate linear programming, and the second approximate dynamic programming. Our dynamic programming algorithm is novel in that it uses an approximation based on max-norm, a technique that more directly minimizes the terms that appear in error bounds for approximate MDP algorithms. We provide experimental results on problems with over 10^40 states, demonstrating a promising indication of the scalability of our approach, and compare our algorithm to an existing state-of-the-art approach, showing, in some problems, exponential gains in computation time."
            },
            "slug": "Efficient-Solution-Algorithms-for-Factored-MDPs-Guestrin-Koller",
            "title": {
                "fragments": [],
                "text": "Efficient Solution Algorithms for Factored MDPs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents two approximate solution algorithms that exploit structure in factored MDPs by using an approximate value function represented as a linear combination of basis functions, where each basis function involves only a small subset of the domain variables."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207155342,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f65020fc3b1692d7989e099d6b6e698be5a50a93",
            "isKey": false,
            "numCitedBy": 2546,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using \"inverse reinforcement learning\" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function."
            },
            "slug": "Apprenticeship-learning-via-inverse-reinforcement-Abbeel-Ng",
            "title": {
                "fragments": [],
                "text": "Apprenticeship learning via inverse reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work thinks of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and gives an algorithm for learning the task demonstrated by the expert, based on using \"inverse reinforcement learning\" to try to recover the unknown reward function."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198578"
                        ],
                        "name": "J. Yedidia",
                        "slug": "J.-Yedidia",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yedidia",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yedidia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 25
                            }
                        ],
                        "text": "However, recent analysis [Yedidia et al., 2000] provides some theoretical justification."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 15300022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2799fd1254689eec52f86daf3668a5aac3ea943",
            "isKey": false,
            "numCitedBy": 1127,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief propagation (BP) was only supposed to work for treelike networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. \n \nWe show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. \n \nMore importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more accurate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we derive generalized belief propagation (GBP) versions of these Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable increase in complexity. We illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP."
            },
            "slug": "Generalized-Belief-Propagation-Yedidia-Freeman",
            "title": {
                "fragments": [],
                "text": "Generalized Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics, and generalized belief propagation (GBP) versions of these Kikuchi approximations are derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "Following the lines of the recent work on maximum margin estimation for probabilistic models (Collins, 2002; Altun et al., 2003; Taskar et al., 2003), we present a discriminative estimation framework for structured models based on the large margin principle underlying support vector machines."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10888973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a7958b418bceb48a315384568091ab1898b1640",
            "isKey": true,
            "numCitedBy": 2272,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "slug": "Discriminative-Training-Methods-for-Hidden-Markov-Collins",
            "title": {
                "fragments": [],
                "text": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results on part-of-speech tagging and base noun phrase chunking are given, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145658132"
                        ],
                        "name": "David Pinto",
                        "slug": "David-Pinto",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876441"
                        ],
                        "name": "Xing Wei",
                        "slug": "Xing-Wei",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1092004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6991606a1a9d5c285af385ee9159fd46cc14048e",
            "isKey": false,
            "numCitedBy": 440,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to find tables and extract information from them is a necessary component of data mining, question answering, and other information retrieval tasks. Documents often contain tables in order to communicate densely packed, multi-dimensional information. Tables do this by employing layout patterns to efficiently indicate fields and records in two-dimensional form.Their rich combination of formatting and content present difficulties for traditional language modeling techniques, however. This paper presents the use of conditional random fields (CRFs) for table extraction, and compares them with hidden Markov models (HMMs). Unlike HMMs, CRFs support the use of many rich and overlapping layout and language features, and as a result, they perform significantly better. We show experimental results on plain-text government statistical reports in which tables are located with 92% F1, and their constituent lines are classified into 12 table-related categories with 94% accuracy. We also discuss future work on undirected graphical models for segmenting columns, finding cells, and classifying them as data cells or label cells."
            },
            "slug": "Table-extraction-using-conditional-random-fields-Pinto-McCallum",
            "title": {
                "fragments": [],
                "text": "Table extraction using conditional random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Unlike HMMs, CRFs support the use of many rich and overlapping layout and language features, and as a result, they perform significantly better, and are compared with hidden Markov models (HMMs)."
            },
            "venue": {
                "fragments": [],
                "text": "DG.O"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745732"
                        ],
                        "name": "M. Charikar",
                        "slug": "M.-Charikar",
                        "structuredName": {
                            "firstName": "Moses",
                            "lastName": "Charikar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Charikar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721077"
                        ],
                        "name": "V. Guruswami",
                        "slug": "V.-Guruswami",
                        "structuredName": {
                            "firstName": "Venkatesan",
                            "lastName": "Guruswami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Guruswami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40009100"
                        ],
                        "name": "A. Wirth",
                        "slug": "A.-Wirth",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Wirth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wirth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 65
                            }
                        ],
                        "text": "2 Semidefinite programming relaxation An alternative formulation [Charikar et al., 2003] is the SDP relaxation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15619292,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "c24b20e02b698cd4189232d14e9d2733b925a890",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of clustering a collection of elements based on pairwise judgments of similarity and dissimilarity. N. Bansal et al. (2002) cast the problem thus: given a graph G whose edges are labeled \"+\" (similar) or \"-\" (dissimilar), partition the vertices into clusters so that the number of pairs correctly (resp. incorrectly) classified with respect to the input labeling is maximized (resp. minimized). Complete graphs, where the classifier labels every edge, and general graphs, where some edges are not labeled, are both worth studying. We answer several questions left open by N. Bansal et al. (2002) and provide a sound overview of clustering with qualitative information. We give a factor 4 approximation for minimization on complete graphs, and a factor O(log n) approximation for general graphs. For the maximization version, a PTAS for complete graphs is shown by N. Bansal et al. (2002); we give a factor 0.7664 approximation for general graphs, noting that a PTAS is unlikely by proving APX-hardness. We also prove the APX-hardness of minimization on complete graphs."
            },
            "slug": "Clustering-with-qualitative-information-Charikar-Guruswami",
            "title": {
                "fragments": [],
                "text": "Clustering with qualitative information"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work considers the problem of clustering a collection of elements based on pairwise judgments of similarity and dissimilarity, and gives a factor 4 approximation for minimization on complete graphs, and a factor O(log n) approximation for general graphs."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727384"
                        ],
                        "name": "E. Demaine",
                        "slug": "E.-Demaine",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Demaine",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Demaine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754163"
                        ],
                        "name": "Nicole Immorlica",
                        "slug": "Nicole-Immorlica",
                        "structuredName": {
                            "firstName": "Nicole",
                            "lastName": "Immorlica",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicole Immorlica"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 688120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fc31960e4c461888538000fa3e6fa8981783cfc",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the following general correlation-clustering problem [1]: given a graph with real edge weights (both positive and negative), partition the vertices into clusters to minimize the total absolute weight of cut positive edges and uncut negative edges. Thus, large positive weights (representing strong correlations between endpoints) encourage those endpoints to belong to a common cluster; large negative weights encourage the endpoints to belong to different clusters; and weights with small absolute value represent little information. In contrast to most clustering problems, correlation clustering specifies neither the desired number of clusters nor a distance threshold for clustering; both of these parameters are effectively chosen to be the best possible by the problem definition."
            },
            "slug": "Correlation-Clustering-with-Partial-Information-Demaine-Immorlica",
            "title": {
                "fragments": [],
                "text": "Correlation Clustering with Partial Information"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This work considers the following general correlation-clustering problem: given a graph with real edge weights, partition the vertices into clusters to minimize the total absolute weight of cut positive edges and uncut negative edges."
            },
            "venue": {
                "fragments": [],
                "text": "RANDOM-APPROX"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144050371"
                        ],
                        "name": "Jennifer Neville",
                        "slug": "Jennifer-Neville",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Neville",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennifer Neville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144774129"
                        ],
                        "name": "David D. Jensen",
                        "slug": "David-D.-Jensen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Jensen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David D. Jensen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13884034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc168ffb0eaa4074e91ca481e9788e554e7ae616",
            "isKey": false,
            "numCitedBy": 402,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Relational data offer a unique opportunity for improving the c lassification accuracy o f statistical m odels. If two objects are related, inferring something about one object can aid inferences about the other. We present an iterative classification p rocedure that exploits this characteristic of relational data. This approach uses simple Bayesian classifiers in an iterative fashion, dynamically upd ating the attributes of some objects as inferences are made about related ob jects. Inferences made with h igh confidence in initial iterations are fed back into the data and are used to inform subsequent i nferences about related ob jects. We evaluate the performance of this approach on a binary classification task. Experiments indicate that it erative classification significantly increases accuracy when compared to a single-pass approach."
            },
            "slug": "Iterative-Classification-in-Relational-Data-Neville-Jensen",
            "title": {
                "fragments": [],
                "text": "Iterative Classification in Relational Data"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "An iterative classification approach that uses simple Bayesian classifiers in an iterative fashion, dynamically upd ating the attributes of some objects as inferences are made about related ob jects."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2014414"
                        ],
                        "name": "L. Vandenberghe",
                        "slug": "L.-Vandenberghe",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Vandenberghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vandenberghe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 74
                            }
                        ],
                        "text": "(4), and this formulation is concise, we can use Lagrangian duality (see (Boyd & Vandenberghe, 2004) for an excellent review) to define a joint, concise convex problem for estimating the parameters w."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37925315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f607f03272e4d62708f5b2441355f9e005cb452",
            "isKey": false,
            "numCitedBy": 38725,
            "numCiting": 276,
            "paperAbstract": {
                "fragments": [],
                "text": "Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics."
            },
            "slug": "Convex-Optimization-Boyd-Vandenberghe",
            "title": {
                "fragments": [],
                "text": "Convex Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A comprehensive introduction to the subject of convex optimization shows in detail how such problems can be solved numerically with great efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144557047"
                        ],
                        "name": "M. Craven",
                        "slug": "M.-Craven",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Craven",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2922396"
                        ],
                        "name": "Dan DiPasquo",
                        "slug": "Dan-DiPasquo",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "DiPasquo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan DiPasquo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682522"
                        ],
                        "name": "Se\u00e1n Slattery",
                        "slug": "Se\u00e1n-Slattery",
                        "structuredName": {
                            "firstName": "Se\u00e1n",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se\u00e1n Slattery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2312137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8446830f3c05b97c4d12a0751c022d1ae6a5115b",
            "isKey": false,
            "numCitedBy": 800,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system."
            },
            "slug": "Learning-to-Extract-Symbolic-Knowledge-from-the-Web-Craven-DiPasquo",
            "title": {
                "fragments": [],
                "text": "Learning to Extract Symbolic Knowledge from the World Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web, and several machine learning algorithms for this task are described."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107124"
                        ],
                        "name": "Ben Wellner",
                        "slug": "Ben-Wellner",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Wellner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Wellner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10070991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d0d54f2ec2c97270704f7865de15cec51728b9d",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Coreference analysis, also known as record linkage or identity uncertainty, is a difficult and important problem in natural language processing, databases, citation matching and many other tasks. This paper introduces several discriminative, conditionalprobability models for coreference analysis, all examples of undirected graphical models. Unlike many historical approaches to coreference, the models presented here are relational\u2014they do not assume that pairwise coreference decisions should be made independently from each other. Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies\u2014 paralleling the advantages of conditional random fields over hidden Markov models. We present experiments on proper noun coreference in two text data sets, showing results in which we reduce error by nearly 28% or more over traditional thresholded record-linkage, and by up to 33% over an alternative coreference technique previously used in natural language processing."
            },
            "slug": "Toward-Conditional-Models-of-Identity-Uncertainty-McCallum-Wellner",
            "title": {
                "fragments": [],
                "text": "Toward Conditional Models of Identity Uncertainty with Application to Proper Noun Coreference"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper introduces several discriminative, conditionalprobability models for coreference analysis, all examples of undirected graphical models that can incorporate a great variety of features of the input without having to be concerned about their dependencies."
            },
            "venue": {
                "fragments": [],
                "text": "IIWeb"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3168839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce2bcdfcb2118f5d42360265f27fe7000e416f20",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm that induces a class of models with thin junction trees\u2014models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph. By ensuring that the junction tree is thin, inference in our models remains tractable throughout the learning process. This allows both an efficient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efficiently with the final model. We illustrate the approach with applications in handwritten digit recognition and DNA splice site detection."
            },
            "slug": "Thin-Junction-Trees-Bach-Jordan",
            "title": {
                "fragments": [],
                "text": "Thin Junction Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An algorithm is presented that induces a class of models with thin junction trees\u2014models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph that allows both an efficient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efficiently with the final model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 69
                            }
                        ],
                        "text": "To situate these numbers with respect to other models, the parser in [Collins, 1999],which 1In this length 1 case, these are the same feature."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 7901127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fc44ff7f37ec5585310666c183c65e0a0bb2446",
            "isKey": false,
            "numCitedBy": 2062,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."
            },
            "slug": "Head-Driven-Statistical-Models-for-Natural-Language-Collins",
            "title": {
                "fragments": [],
                "text": "Head-Driven Statistical Models for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Three statistical models for natural language parsing are described, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11495042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a600850ac0120cb09a0b7de7da80bb6a7a76de06",
            "isKey": false,
            "numCitedBy": 3370,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."
            },
            "slug": "Accurate-Unlexicalized-Parsing-Klein-Manning",
            "title": {
                "fragments": [],
                "text": "Accurate Unlexicalized Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is demonstrated that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39609402"
                        ],
                        "name": "Jianlin Cheng",
                        "slug": "Jianlin-Cheng",
                        "structuredName": {
                            "firstName": "Jianlin",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianlin Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255005"
                        ],
                        "name": "Alessandro Vullo",
                        "slug": "Alessandro-Vullo",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Vullo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Vullo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 103
                            }
                        ],
                        "text": "We used two datasets containing sequences with experimentally verified bonding patterns: SP39\n(used in Baldi et al. (2004); Vullo and Frasconi (2004); Fariselli and Casadio (2001)) and DIPRO2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 172
                            }
                        ],
                        "text": "In order to avoid biases during testing, we adopt the same dataset splitting procedure as the one used in previous work (Fariselli & Casadio, 2001; Vullo & Frasconi, 2004; Baldi et al., 2004).2\nModels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 68
                            }
                        ],
                        "text": "The DSSP program\n1The DIPRO2 dataset was made publicly available by Baldi et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 229
                            }
                        ],
                        "text": "Such bonds are a very important feature of protein structure, as they enhance conformational stability by reducing the number of configurational states and decreasing the entropic cost of folding a protein into its native state (Baldi et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 50
                            }
                        ],
                        "text": "We also compare our model to the DAG-RNN model of Baldi et al. (2004), the current top-performing system, which uses recursive neural networks also with the same set of input features."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 116
                            }
                        ],
                        "text": "We apply the model to the SP39 dataset by using 4-fold cross-validation, which replicates the experimental setup of Baldi et al. (2004); Vullo and Frasconi (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 163
                            }
                        ],
                        "text": "\u2026alignment of 2D shapes using weighted bipartite point matching (Belongie et al., 2002), disulfide connectivity prediction using weighted non-bipartite matchings (Baldi et al., 2004), clustering using spanning trees and graph cuts (Duda et al., 2000), and other combinatorial and graph structures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 159
                            }
                        ],
                        "text": "Recently, there has been increasing interest in applying computational techniques to the task of predicting disulfide connectivity (Fariselli & Casadio, 2001; Baldi et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5488893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c84883546b5ae47a27abae19f89f47d1650d4b8",
            "isKey": true,
            "numCitedBy": 54,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The formation of disulphide bridges among cysteines is an important feature of protein structures. Here we develop new methods for the prediction of disulphide bond connectivity. We first build a large curated data set of proteins containing disulphide bridges and then use 2-Dimensional Recursive Neural Networks to predict bonding probabilities between cysteine pairs. These probabilities in turn lead to a weighted graph matching problem that can be addressed efficiently. We show how the method consistently achieves better results than previous approaches on the same validation data. In addition, the method can easily cope with chains with arbitrary numbers of bonded cysteines. Therefore, it overcomes one of the major limitations of previous approaches restricting predictions to chains containing no more than 10 oxidized cysteines. The method can be applied both to situations where the bonded state of each cysteine is known or unknown, in which case bonded state can be predicted with 85% precision and 90% recall. The method also yields an estimate for the total number of disulphide bridges in each chain."
            },
            "slug": "Large-Scale-Prediction-of-Disulphide-Bond-Baldi-Cheng",
            "title": {
                "fragments": [],
                "text": "Large-Scale Prediction of Disulphide Bond Connectivity"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A large curated data set of proteins containing disulphide bridges is built and 2-Dimensional Recursive Neural Networks are used to predict bonding probabilities between cysteine pairs, which lead to a weighted graph matching problem that can be addressed efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "We provide a brief outline of one variant of BP, referring to [Murphy et al., 1999] for more details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16462148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19908640236767427ebf0524dc3a4bb09d65145e",
            "isKey": false,
            "numCitedBy": 1774,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, researchers have demonstrated that \"loopy belief propagation\" -- the use of Pearl's polytree algorithm in a Bayesian network with loops -- can perform well in the context of error-correcting codes. The most dramatic instance of this is the near Shannon-limit performance of \"Turbo Codes\" -- codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured Bayesian network. \n \nIn this paper we ask: is there something special about the error-correcting code context, or does loopy propagation work as an approximate inference scheme in a more general setting? We compare the marginals computed using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR. We find that the loopy beliefs often converge and when they do, they give a good approximation to the correct marginals. However, on the QMR network, the loopy beliefs oscillated and had no obvious relationship to the correct posteriors. We present some initial investigations into the cause of these oscillations, and show that some simple methods of preventing them lead to the wrong results."
            },
            "slug": "Loopy-Belief-Propagation-for-Approximate-Inference:-Murphy-Weiss",
            "title": {
                "fragments": [],
                "text": "Loopy Belief Propagation for Approximate Inference: An Empirical Study"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper compares the marginals computed using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR, and finds that the loopy beliefs often converge and when they do, they give a good approximation to the correct marginals."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755231"
                        ],
                        "name": "C. Heuberger",
                        "slug": "C.-Heuberger",
                        "structuredName": {
                            "firstName": "Clemens",
                            "lastName": "Heuberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Heuberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 207
                            }
                        ],
                        "text": "Beyond the closely related large-margin methods for probabilistic models, our max-margin formulation has ties to a body of work called inverse combinatorial and convex optimization (for a recent survey, see Heuberger (2004))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18105696,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bebf024557b353d26465322e1030c0dd9816cbbe",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a (combinatorial) optimization problem and a feasible solution to it, the corresponding inverse optimization problem is to find a minimal adjustment of the cost function such that the given solution becomes optimum.Several such problems have been studied in the last twelve years. After formalizing the notion of an inverse problem and its variants, we present various methods for solving them. Then we discuss the problems considered in the literature and the results that have been obtained. Finally, we formulate some open problems."
            },
            "slug": "Inverse-Combinatorial-Optimization:-A-Survey-on-and-Heuberger",
            "title": {
                "fragments": [],
                "text": "Inverse Combinatorial Optimization: A Survey on Problems, Methods, and Results"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work formalizes the notion of an inverse problem and its variants, and presents various methods for solving them."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comb. Optim."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682522"
                        ],
                        "name": "Se\u00e1n Slattery",
                        "slug": "Se\u00e1n-Slattery",
                        "structuredName": {
                            "firstName": "Se\u00e1n",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se\u00e1n Slattery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791498"
                        ],
                        "name": "R. Ghani",
                        "slug": "R.-Ghani",
                        "structuredName": {
                            "firstName": "Rayid",
                            "lastName": "Ghani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ghani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 174
                            }
                        ],
                        "text": "We also experimented with incorporating meta-data: words appearing in the title of the page, in anchors of links to the page and in the last header before a link to the page [Yang et al., 2002]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 295949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "056dc21b3283e6a69783921367babea2d220684c",
            "isKey": false,
            "numCitedBy": 353,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Hypertext poses new research challenges for text classification. Hyperlinks, HTML tags, category labels distributed over linked documents, and meta data extracted from related Web sites all provide rich information for classifying hypertext documents. How to appropriately represent that information and automatically learn statistical patterns for solving hypertext classification problems is an open question. This paper seeks a principled approach to providing the answers. Specifically, we define five hypertext regularities which may (or may not) hold in a particular application domain, and whose presence (or absence) may significantly influence the optimal design of a classifier. Using three hypertext datasets and three well-known learning algorithms (Naive Bayes, Nearest Neighbor, and First Order Inductive Learner), we examine these regularities in different domains, and compare alternative ways to exploit them. Our results show that the identification of hypertext regularities in the data and the selection of appropriate representations for hypertext in particular domains are crucial, but seldom obvious, in real-world problems. We find that adding the words in the linked neighborhood to the page having those links (both inlinks and outlinks) were helpful for all our classifiers on one data set, but more harmful than helpful for two out of the three classifiers on the remaining datasets. We also observed that extracting meta data from related Web sites was extremely useful for improving classification accuracy in some of those domains. Finally, the relative performance of the classifiers being tested provided insights into their strengths and limitations for solving classification problems involving diverse and often noisy Web pages."
            },
            "slug": "A-Study-of-Approaches-to-Hypertext-Categorization-Yang-Slattery",
            "title": {
                "fragments": [],
                "text": "A Study of Approaches to Hypertext Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper examines five hypertext regularities which may (or may not) hold in a particular application domain, and whose presence (or absence) may significantly influence the optimal design of a classifier."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Intelligent Information Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732982"
                        ],
                        "name": "R. Ahuja",
                        "slug": "R.-Ahuja",
                        "structuredName": {
                            "firstName": "Ravindra",
                            "lastName": "Ahuja",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ahuja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795576"
                        ],
                        "name": "J. Orlin",
                        "slug": "J.-Orlin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Orlin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Orlin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124549716,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8287acd2617ec940de52886dcb78af00af761f60",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study inverse optimization problems defined as follows: Let S denote the set of feasible solutions of an optimization problem P, let c be a specified cost vector, and x0 be a given feasible solution. The solution x\u00b0 may or may not be an optimal solution of P with respect to the cost vector c. The inverse optimization problem is to perturb the cost vector c to d so that x0 is an optimal solution of P with respect to d and lid clip is minimum, where lid clip is some selected Lp norm. In this paper, we consider the inverse linear programming problem under the L 1 norm (where we minimize Ejj ldj -cj, with J denoting the index set of variables xj) and under the Lo norm (where we minimize max{ldj cjl: j E J}). We show that the dual of the inverse linear programming problem with the L 1 norm reduces to a modification of the original problem obtained by eliminating the non-binding constraints (with respect to x) and imposing the following additional lower and upper bound constraints: Ixj xj < 1 for all j J. We next study the inverse linear programming problem with the Loo norm and show that its dual reduces to a modification of the original problem obtained by eliminating the non-binding constraints (with respect to x) and imposing the following single additional constraint: jEj xj x9\u00b0 < 1. Finally, we show that (under reasonable regularity conditions) if the problem P is polynomially solvable then the inverse versions of P under L1 and L, norms are also polynomially solvable. This result uses ideas from the ellipsoid algorithm and, therefore, does not lead to combinatorial algorithms for solving inverse optimization problems. 1 Sloan School of Management, MIT, Cambridge, MA 02139, USA; On leave from Indian Institute of Technology, Kanpur 208 016, INDIA. 2 Sloan School of Management, MIT, Cambridge, MA 02139, USA."
            },
            "slug": "Inverse-Optimization,-Part-I:-Linear-Programming-Ahuja-Orlin",
            "title": {
                "fragments": [],
                "text": "Inverse Optimization, Part I: Linear Programming and General Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper considers the inverse linear programming problem under the L 1 norm (where the authors minimize Ejj ldj -cj, with J denoting the index set of variables xj) and under the Lo norm ( where they minimize max{ldj cjl: j E J}), and shows that (under reasonable regularity conditions) the inverse versions of P under L1 and L, norms are also polynomially solvable."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 64
                            }
                        ],
                        "text": "There has been much research in this setting for classification [Blum & Mitchell, 1998; Nigam et al., 2000; Chapelleet al., 2002; Szummer & Jaakkola, 2001; Zhu et al., 2003; Corduneanu & Jaakkola, 2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 686980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2de29049d62de925cf709024b92774cd82b0a5a",
            "isKey": false,
            "numCitedBy": 3072,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%."
            },
            "slug": "Text-Classification-from-Labeled-and-Unlabeled-EM-Nigam-McCallum",
            "title": {
                "fragments": [],
                "text": "Text Classification from Labeled and Unlabeled Documents using EM"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents, and presents two extensions to the algorithm that improve classification accuracy under these conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5178437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32fad9f3b7ab236f326d6927c2965a01118e8e93",
            "isKey": false,
            "numCitedBy": 488,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Spectral clustering refers to a class of techniques which rely on the eigen-structure of a similarity matrix to partition points into disjoint clusters with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing this cost function with respect to the partition leads to a new spectral clustering algorithm. Minimizing with respect to the similarity matrix leads to an algorithm for learning the similarity matrix. We develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors."
            },
            "slug": "Learning-Spectral-Clustering-Bach-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning Spectral Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new cost function for spectral clustering is derived based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745655"
                        ],
                        "name": "C. Bouman",
                        "slug": "C.-Bouman",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bouman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bouman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153530088"
                        ],
                        "name": "M. Shapiro",
                        "slug": "M.-Shapiro",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Shapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shapiro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5571990,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1dc2443a955a6d3ef06c10d1071998c267e3a229",
            "isKey": false,
            "numCitedBy": 724,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Many approaches to Bayesian image segmentation have used maximum a posteriori (MAP) estimation in conjunction with Markov random fields (MRF). Although this approach performs well, it has a number of disadvantages. In particular, exact MAP estimates cannot be computed, approximate MAP estimates are computationally expensive to compute, and unsupervised parameter estimation of the MRF is difficult. The authors propose a new approach to Bayesian image segmentation that directly addresses these problems. The new method replaces the MRF model with a novel multiscale random field (MSRF) and replaces the MAP estimator with a sequential MAP (SMAP) estimator derived from a novel estimation criteria. Together, the proposed estimator and model result in a segmentation algorithm that is not iterative and can be computed in time proportional to MN where M is the number of classes and N is the number of pixels. The also develop a computationally efficient method for unsupervised estimation of model parameters. Simulations on synthetic images indicate that the new algorithm performs better and requires much less computation than MAP estimation using simulated annealing. The algorithm is also found to improve classification accuracy when applied to the segmentation of multispectral remotely sensed images with ground truth data."
            },
            "slug": "A-multiscale-random-field-model-for-Bayesian-image-Bouman-Shapiro",
            "title": {
                "fragments": [],
                "text": "A multiscale random field model for Bayesian image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Simulations on synthetic images indicate that the new algorithm performs better and requires much less computation than MAP estimation using simulated annealing, and is found to improve classification accuracy when applied to the segmentation of multispectral remotely sensed images with ground truth data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 183
                            }
                        ],
                        "text": "However, we compare to standard text classifiers such as Naive Bayes, Logistic Regression, and Support Vector Machines, which have been demonstrated to be successful on this data set [Joachims, 1999]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14591650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8",
            "isKey": false,
            "numCitedBy": 3047,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more."
            },
            "slug": "Transductive-Inference-for-Text-Classification-Joachims",
            "title": {
                "fragments": [],
                "text": "Transductive Inference for Text Classification using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "An analysis of why Transductive Support Vector Machines are well suited for text classi cation is presented, and an algorithm for training TSVMs, handling 10,000 examples and more is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 109
                            }
                        ],
                        "text": "Following the lines of the recent work on maximum margin estimation for probabilistic models (Collins, 2002; Altun et al., 2003; Taskar et al., 2003), we present a discriminative estimation framework for structured models based on the large margin principle underlying support vector machines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 59
                            }
                        ],
                        "text": "In some cases, though, we can find a concise certificate of optimality that guarantees that y(i) = arg maxy[w fi(y)+ i(y)] without expressing loss-augmented inference as a concise convex program."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 201
                            }
                        ],
                        "text": "Our framework extends the max-margin formulation for Markov networks (Taskar et al., 2003; Taskar et al., 2004a) and context free grammars (Taskar et al., 2004b), and is similar to other formulations (Altun et al., 2003; Tsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 9699301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398",
            "isKey": true,
            "numCitedBy": 556,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which we call Hidden Markov Support Vector Machine. The proposed architecture handles dependencies between neighboring labels using Viterbi decoding. In contrast to standard HMM training, the learning procedure is discriminative and is based on a maximum/soft margin criterion. Compared to previous methods like Conditional Random Fields, Maximum Entropy Markov Models and label sequence boosting, HM-SVMs have a number of advantages. Most notably, it is possible to learn non-linear discriminant functions via kernel functions. At the same time, HM-SVMs share the key advantages with other discriminative methods, in particular the capability to deal with overlapping features. We report experimental evaluations on two tasks, named entity recognition and part-of-speech tagging, that demonstrate the competitiveness of the proposed approach."
            },
            "slug": "Hidden-Markov-Support-Vector-Machines-Altun-Tsochantaridis",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which it is called HM-SVMs and handles dependencies between neighboring labels using Viterbi decoding."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152465203"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13079001,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2685fc6cc208c67088afa9e8c2743511852fc19c",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unification-based grammar (UBG). Existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one, or in order to calculate the statistics needed to estimate a grammar from a training corpus. This paper describes a graph-based dynamic programming algorithm for calculating these statistics from the packed UBG parse representations of Maxwell and Kaplan (1995) which does not require enumerating all parses. Like many graphical algorithms, the dynamic programming algorithm's complexity is worst-case exponential, but is often polynomial. The key observation is that by using Maxwell and Kaplan packed representations, the required statistics can be rewritten as either the max or the sum of a product of functions. This is exactly the kind of problem which can be solved by dynamic programming over graphical models."
            },
            "slug": "Dynamic-programming-for-parsing-and-estimation-of-Geman-Johnson",
            "title": {
                "fragments": [],
                "text": "Dynamic programming for parsing and estimation of stochastic unification-based grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A graph-based dynamic programming algorithm for calculating statistics from the packed UBG parse representations of Maxwell and Kaplan (1995) which does not require enumerating all parses."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5885617,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2479a5cf6cefefb83166c612564787414e47131f",
            "isKey": false,
            "numCitedBy": 812,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce and analyze a new algorithm for linear classification which combines Rosenblatt's perceptron algorithm with Helmbold and Warmuth's leave-one-out method. Like Vapnik's maximal-margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik's algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin classifiers on the same problem, while saving significantly on computation time and programming effort."
            },
            "slug": "Large-Margin-Classification-Using-the-Perceptron-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Large Margin Classification Using the Perceptron Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new algorithm for linear classification which combines Rosenblatt's perceptron algorithm with Helmbold and Warmuth's leave-one-out method is introduced, which is much simpler to implement, and much more efficient in terms of computation time."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115816494"
                        ],
                        "name": "Ji Zhu",
                        "slug": "Ji-Zhu",
                        "structuredName": {
                            "firstName": "Ji",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15413835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "746e1c10ca4af0f736682dda3c967d371eeb086c",
            "isKey": false,
            "numCitedBy": 561,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The support vector machine (SVM) is known for its good performance in two-class classification, but its extension to multiclass classification is still an ongoing research issue. In this article, we propose a new approach for classification, called the import vector machine (IVM), which is built on kernel logistic regression (KLR). We show that the IVM not only performs as well as the SVM in two-class classification, but also can naturally be generalized to the multiclass case. Furthermore, the IVM provides an estimate of the underlying probability. Similar to the support points of the SVM, the IVM model uses only a fraction of the training data to index kernel basis functions, typically a much smaller fraction than the SVM. This gives the IVM a potential computational advantage over the SVM."
            },
            "slug": "Kernel-Logistic-Regression-and-the-Import-Vector-Zhu-Hastie",
            "title": {
                "fragments": [],
                "text": "Kernel Logistic Regression and the Import Vector Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "It is shown that the IVM not only performs as well as the SVM in two-class classification, but also can naturally be generalized to the multiclass case, and provides an estimate of the underlying probability."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785465"
                        ],
                        "name": "P. Fariselli",
                        "slug": "P.-Fariselli",
                        "structuredName": {
                            "firstName": "Piero",
                            "lastName": "Fariselli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fariselli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753858"
                        ],
                        "name": "R. Casadio",
                        "slug": "R.-Casadio",
                        "structuredName": {
                            "firstName": "Rita",
                            "lastName": "Casadio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Casadio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 151
                            }
                        ],
                        "text": "We used two datasets containing sequences with experimentally verified bonding patterns: SP39\n(used in Baldi et al. (2004); Vullo and Frasconi (2004); Fariselli and Casadio (2001)) and DIPRO2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 121
                            }
                        ],
                        "text": "In order to avoid biases during testing, we adopt the same dataset splitting procedure as the one used in previous work (Fariselli & Casadio, 2001; Vullo & Frasconi, 2004; Baldi et al., 2004).2\nModels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 132
                            }
                        ],
                        "text": "Recently, there has been increasing interest in applying computational techniques to the task of predicting disulfide connectivity (Fariselli & Casadio, 2001; Baldi et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 23
                            }
                        ],
                        "text": "Following the lines of Fariselli and Casadio (2001), we predict the connectivity pattern by finding the maximum weighted matching in a graph in which each vertex represents a cysteine residue, and each edge represents the \u201cattraction strength\u201d between the cysteines it connects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 934093,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "539c18b03f44cd2fbe9ca11e277f27c7c7519bd9",
            "isKey": true,
            "numCitedBy": 115,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nA major problem in protein structure prediction is the correct location of disulfide bridges in cysteine-rich proteins. In protein-folding prediction, the location of disulfide bridges can strongly reduce the search in the conformational space. Therefore the correct prediction of the disulfide connectivity starting from the protein residue sequence may also help in predicting its 3D structure.\n\n\nRESULTS\nIn this paper we equate the problem of predicting the disulfide connectivity in proteins to a problem of finding the graph matching with the maximum weight. The graph vertices are the residues of cysteine-forming disulfide bridges, and the weight edges are contact potentials. In order to solve this problem we develop and test different residue contact potentials. The best performing one, based on the Edmonds-Gabow algorithm and Monte-Carlo simulated annealing reaches an accuracy significantly higher than that obtained with a general mean force contact potential. Significantly, in the case of proteins with four disulfide bonds in the structure, the accuracy is 17 times higher than that of a random predictor. The method presented here can be used to locate putative disulfide bridges in protein-folding.\n\n\nAVAILABILITY\nThe program is available upon request from the authors.\n\n\nCONTACT\nCasadio@alma.unibo.it; Piero@biocomp.unibo.it."
            },
            "slug": "Prediction-of-disulfide-connectivity-in-proteins-Fariselli-Casadio",
            "title": {
                "fragments": [],
                "text": "Prediction of disulfide connectivity in proteins"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The problem of predicting the disulfide connectivity in proteins is compared to a problem of finding the graph matching with the maximum weight, and the method presented here can be used to locate putativedisulfide bridges in protein-folding."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109071318"
                        ],
                        "name": "Zhenhong Liu",
                        "slug": "Zhenhong-Liu",
                        "structuredName": {
                            "firstName": "Zhenhong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenhong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103203738"
                        ],
                        "name": "Jianzhon Zhang",
                        "slug": "Jianzhon-Zhang",
                        "structuredName": {
                            "firstName": "Jianzhon",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianzhon Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2874100,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d5424715ffdb43ba3057fc1b80da326ac17f083f",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "As far as we know, for most polynomially solvable network optimization problems, their inverse problems under l1 or l\u221e norm have been studied, except the inverse maximum-weight matching problem in non-bipartite networks. In this paper we discuss the inverse problem of maximum-weight perfect matching in a non-bipartite network under l1 and l\u221e norms. It has been proved that the inverse maximum-weight perfect matching under l\u221e norm can be formulated as a maximum-mean alternating cycle problem of an undirected network, and can be solved in polynomial time by a binary search algorithm and in strongly polynomial time by an ascending algorithm, and under l1 norm it can be solved by the ellipsoid method. Therefore, inverse problems of maximum-weight perfect matching under l1 and l\u221e norms are solvable in polynomial time."
            },
            "slug": "On-Inverse-Problems-of-Optimum-Perfect-Matching-Liu-Zhang",
            "title": {
                "fragments": [],
                "text": "On Inverse Problems of Optimum Perfect Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It has been proved that the inverse maximum-weight perfect matching under l\u221e norm can be formulated as a maximum-mean alternating cycle problem of an undirected network, and can be solved in polynomial time by a binary search algorithm and in strongly polynometric time by an ascending algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comb. Optim."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34999823"
                        ],
                        "name": "T. Frie\u00df",
                        "slug": "T.-Frie\u00df",
                        "structuredName": {
                            "firstName": "Thilo-Thomas",
                            "lastName": "Frie\u00df",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Frie\u00df"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145990261"
                        ],
                        "name": "C. Campbell",
                        "slug": "C.-Campbell",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Campbell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Campbell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 19
                            }
                        ],
                        "text": "The kernel-adatron [Friess et al., 1998] and voted-perceptron algorithms [Freund & Schapire, 1998] for large-margin classifiers have a similar online optimization scheme."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13162938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e761bc3b6028308dcd48f9ba0964533c2e6fe43",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines work by mapping training data for classiication tasks into a high dimensional feature space. In the feature space they then nd a maximal margin hyperplane which separates the data. This hyperplane is usually found using a quadratic programming routine which is computation-ally intensive, and is non trivial to implement. In this paper we propose an adaptation of the Adatron algorithm for clas-siication with kernels in high dimensional spaces. The algorithm is simple and can nd a solution very rapidly with an exponentially fast rate of convergence (in the number of iterations) towards the optimal solution. Experimental results with real and artiicial datasets are provided."
            },
            "slug": "The-Kernel-Adatron-Algorithm:-A-Fast-and-Simple-for-Frie\u00df-Cristianini",
            "title": {
                "fragments": [],
                "text": "The Kernel-Adatron Algorithm: A Fast and Simple Learning Procedure for Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper proposes an adaptation of the Adatron algorithm for clas-siication with kernels in high dimensional spaces that can find a solution very rapidly with an exponentially fast rate of convergence towards the optimal solution."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145850390"
                        ],
                        "name": "D. Burton",
                        "slug": "D.-Burton",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Burton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10504869,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9984e6b86450c5dad3a38aa5411607094552cec6",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The inverse shortest paths problem in a graph is considered, that is, the problem of recovering the arc costs given some information about the shortest paths in the graph. The problem is first motivated by some practical examples arising from applications. An algorithm based on the Goldfarb-Idnani method for convex quadratic programming is then proposed and analyzed for one of the instances of the problem. Preliminary numerical results are reported."
            },
            "slug": "On-an-instance-of-the-inverse-shortest-paths-Burton-Toint",
            "title": {
                "fragments": [],
                "text": "On an instance of the inverse shortest paths problem"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An algorithm based on the Goldfarb-Idnani method for convex quadratic programming is proposed and analyzed for one of the instances of the inverse shortest paths problem in a graph."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209133"
                        ],
                        "name": "C. Leslie",
                        "slug": "C.-Leslie",
                        "structuredName": {
                            "firstName": "Christina",
                            "lastName": "Leslie",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leslie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709847"
                        ],
                        "name": "E. Eskin",
                        "slug": "E.-Eskin",
                        "structuredName": {
                            "firstName": "Eleazar",
                            "lastName": "Eskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Eskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 207
                            }
                        ],
                        "text": "Our framework can be applied (just as in context free grammar estimation) to efficiently learn a more complex edit function that depends on the contextual string features, perhaps using novel string kernels [Haussler, 1999; Leslie et al., 2002; Lodhiet al., 2000]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9725578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c9071391823bd9b4458669c262344fd5daed676",
            "isKey": false,
            "numCitedBy": 1093,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new sequence-similarity kernel, the spectrum kernel, for use with support vector machines (SVMs) in a discriminative approach to the protein classification problem. Our kernel is conceptually simple and efficient to compute and, in experiments on the SCOP database, performs well in comparison with state-of-the-art methods for homology detection. Moreover, our method produces an SVM classifier that allows linear time classification of test sequences. Our experiments provide evidence that string-based kernels, in conjunction with SVMs, could offer a viable and computationally efficient alternative to other methods of protein classification and homology detection."
            },
            "slug": "The-Spectrum-Kernel:-A-String-Kernel-for-SVM-Leslie-Eskin",
            "title": {
                "fragments": [],
                "text": "The Spectrum Kernel: A String Kernel for SVM Protein Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A new sequence-similarity kernel, the spectrum kernel, is introduced for use with support vector machines (SVMs) in a discriminative approach to the protein classification problem and performs well in comparison with state-of-the-art methods for homology detection."
            },
            "venue": {
                "fragments": [],
                "text": "Pacific Symposium on Biocomputing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39839719"
                        ],
                        "name": "Libin Shen",
                        "slug": "Libin-Shen",
                        "structuredName": {
                            "firstName": "Libin",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Libin Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3028658"
                        ],
                        "name": "Anoop Sarkar",
                        "slug": "Anoop-Sarkar",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop Sarkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714374"
                        ],
                        "name": "A. Joshi",
                        "slug": "A.-Joshi",
                        "structuredName": {
                            "firstName": "Aravind",
                            "lastName": "Joshi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Joshi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 21
                            }
                        ],
                        "text": "In reranking methods [Johnson et al., 1999; Collins, 2000; Shen et al., 2003], an initial parser is used to generate a number of candidate parses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1857060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a46152d8ad27ae47086334c33c8376185b40340d",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the use of Lexicalized Tree Adjoining Grammar (LTAG) as a source of features that are useful for reranking the output of a statistical parser. In this paper, we extend the notion of a tree kernel over arbitrary sub-trees of the parse to the derivation trees and derived trees provided by the LTAG formalism, and in addition, we extend the original definition of the tree kernel, making it more lexicalized and more compact. We use LTAG based features for the parse reranking task and obtain labeled recall and precision of 89.7%/90.0% on WSJ section 23 of Penn Treebank for sentences of length \u2264 100 words. Our results show that the use of LTAG based tree kernel gives rise to a 17% relative difference in f-score improvement over the use of a linear kernel without LTAG based features."
            },
            "slug": "Using-LTAG-Based-Features-in-Parse-Reranking-Shen-Sarkar",
            "title": {
                "fragments": [],
                "text": "Using LTAG Based Features in Parse Reranking"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper extends the notion of a tree kernel over arbitrary sub-trees of the parse to the derivation trees and derived trees provided by the LTAG formalism, and in addition, it extends the original definition of the tree kernel, making it more lexicalized and more compact."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702610"
                        ],
                        "name": "Andrea Passerini",
                        "slug": "Andrea-Passerini",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Passerini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Passerini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255005"
                        ],
                        "name": "Alessandro Vullo",
                        "slug": "Alessandro-Vullo",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Vullo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Vullo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 0
                            }
                        ],
                        "text": "[Fariselli et al., 1999; Fiser & Simon, 2000; Martelli et al., 2002; Frasconi et al., 2002; Ceroniet al., 2003] Currently the top performing systems have accuracies around 85%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 952085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "133f9d639c0f4676ef3f30a094b305bf6ef894f4",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Cysteines may form covalent bonds, known as disulfide bridges, that have an important role in stabilizing the native conformation of proteins. Several methods have been proposed for predicting the bonding state of cysteines, either using local context or using global protein descriptors. In this paper we introduce an SVM based predictor that operates in two stages. The first stage is a multi-class classifier that operates at the protein level. The second stage is a binary classifier that refines the prediction by exploiting local context enriched with evolutionary information in the form of multiple alignment profiles. The prediction accuracy of the system is 83.6% measured by 5-fold cross validation, on a set of 716 proteins from the September 2001 PDB Select dataset."
            },
            "slug": "A-two-stage-SVM-architecture-for-predicting-the-of-Frasconi-Passerini",
            "title": {
                "fragments": [],
                "text": "A two-stage SVM architecture for predicting the disulfide bonding state of cysteines"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper introduces an SVM based predictor that operates in two stages, the first stage is a multi-class classifier that operates at the protein level and the second is a binary classifiers that refines the prediction by exploiting local context enriched with evolutionary information in the form of multiple alignment profiles."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 56
                            }
                        ],
                        "text": "The set of symbols we use is based on the Penn Treebank [Marcus et al., 1993]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 41
                            }
                        ],
                        "text": "6: Example parse tree from Penn Treebank [Marcus et al., 1993]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987437"
                        ],
                        "name": "E. Matusov",
                        "slug": "E.-Matusov",
                        "structuredName": {
                            "firstName": "Evgeny",
                            "lastName": "Matusov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Matusov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983801"
                        ],
                        "name": "R. Zens",
                        "slug": "R.-Zens",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 98
                            }
                        ],
                        "text": "In machine translation, matchings are used to map words of the two languages in aligned sentences [Matusov et al., 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9563285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "209a62a4deb87456d195c67c1ae712205a7e4c3d",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the word alignment problem for statistical machine translation. We aim at creating a symmetric word alignment allowing for reliable one-to-many and many-to-one word relationships. We perform the iterative alignment training in the source-to-target and the target-to-source direction with the well-known IBM and HMM alignment models. Using these models, we robustly estimate the local costs of aligning a source word and a target word in each sentence pair. Then, we use efficient graph algorithms to determine the symmetric alignment with minimal total costs (i. e. maximal alignment probability). We evaluate the automatic alignments created in this way on the German--English Verbmobil task and the French--English Canadian Hansards task. We show statistically significant improvements of the alignment quality compared to the best results reported so far. On the Verbmobil task, we achieve an improvement of more than 1% absolute over the baseline error rate of 4.7%."
            },
            "slug": "Symmetric-Word-Alignments-for-Statistical-Machine-Matusov-Zens",
            "title": {
                "fragments": [],
                "text": "Symmetric Word Alignments for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper addresses the word alignment problem for statistical machine translation by creating a symmetric word alignment allowing for reliable one- to-many and many-to-one word relationships and shows statistically significant improvements of the alignment quality compared to the best results reported so far."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10687508"
                        ],
                        "name": "A. Ceroni",
                        "slug": "A.-Ceroni",
                        "structuredName": {
                            "firstName": "Alessio",
                            "lastName": "Ceroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ceroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702610"
                        ],
                        "name": "Andrea Passerini",
                        "slug": "Andrea-Passerini",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Passerini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Passerini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255005"
                        ],
                        "name": "Alessandro Vullo",
                        "slug": "Alessandro-Vullo",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Vullo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Vullo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 740220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a38ce6d4abcd7a9e94c3e05400677247d139584",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Cysteines may form covalent bonds, known as disulfide bridges, that have an important role in stabilizing the native conformation of proteins. Several methods have been proposed for predicting the bonding state of cysteines, either using local context or using global protein descriptors. In this paper we introduce an SVM based predictor that operates in two stages. The first stage is a multi-class classifier that operates at the protein level, using either standard Gaussian or spectrum kernels. The second stage is a binary classifier that refines the prediction by exploiting local context enriched with evolutionary information in the form of multiple alignment profiles. At both stages, we enriched profile encoding with information about cysteine conservation. The prediction accuracy of the system is 85% measured by 5-fold cross validation, on a set of 716 proteins from the September 2001 PDB Select dataset."
            },
            "slug": "Predicting-the-Disulfide-Bonding-State-of-Cysteines-Ceroni-Frasconi",
            "title": {
                "fragments": [],
                "text": "Predicting the Disulfide Bonding State of Cysteines with Combinations of Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An SVM based predictor that operates in two stages that refines the prediction by exploiting local context enriched with evolutionary information in the form of multiple alignment profiles, and enriched profile encoding with information about cysteine conservation is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "J. VLSI Signal Process."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6802974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9eea85e590f6e522e3681b8e45012684c60b0fd",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar (CCG). A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including non-standard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers."
            },
            "slug": "Parsing-the-WSJ-Using-CCG-and-Log-Linear-Models-Clark-Curran",
            "title": {
                "fragments": [],
                "text": "Parsing the WSJ Using CCG and Log-Linear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation and a new efficient parsing algorithm for CCG which maximises expected recall of dependencies is developed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35674406"
                        ],
                        "name": "Shigeo Abe DrEng",
                        "slug": "Shigeo-Abe-DrEng",
                        "structuredName": {
                            "firstName": "Shigeo",
                            "lastName": "DrEng",
                            "middleNames": [
                                "Abe"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shigeo Abe DrEng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 232
                            }
                        ],
                        "text": "\u2026alignment of 2D shapes using weighted bipartite point matching (Belongie et al., 2002), disulfide connectivity prediction using weighted non-bipartite matchings (Baldi et al., 2004), clustering using spanning trees and graph cuts (Duda et al., 2000), and other combinatorial and graph structures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9384346,
            "fieldsOfStudy": [
                "Mathematics",
                "Environmental Science"
            ],
            "id": "65a69968bb8c41aad0113cec4c2d981bddf50bc8",
            "isKey": false,
            "numCitedBy": 13095,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Classification \u2022 Supervised \u2013 parallelpiped \u2013 minimum distance \u2013 maximum likelihood (Bayes Rule) > non-parametric > parametric \u2013 support vector machines \u2013 neural networks \u2013 context classification \u2022 Unsupervised (clustering) \u2013 K-Means \u2013 ISODATA \u2022 Pattern recognition in remote sensing has been based on the intuitive notion that pixels belonging to the same class should have similar gray values in a given band. \u2013 Given two spectral bands, pixels from the same class plotted in a two-dimensional histogram should appear as a localized cluster. \u2013 If n images, each in a different spectral band, are available, pixels from the same class should form a localized cluster in n-space."
            },
            "slug": "Pattern-Classification-DrEng",
            "title": {
                "fragments": [],
                "text": "Pattern Classification"
            },
            "venue": {
                "fragments": [],
                "text": "Springer London"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 131
                            }
                        ],
                        "text": "Associative interactions arise naturally in the context of image processing, where nearby pixels are likely to have the same label [Besag, 1986; Boykov et al., 1999b]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 125
                            }
                        ],
                        "text": "In image processing, neighboring pixels exhibit spatial label coherence in denoising, segmentation and stereo correspondence [Besag, 1986; Boykov et al., 1999a]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 255
                            }
                        ],
                        "text": "The chief computational bottleneck in applying Markov networks for other large-scale prediction problems is inference, which is NP-hard in general networks suitable in a broad range of practical Markov network structures, including grid-topology networks [Besag, 1986]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15128952,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "47865b56fee61d9c9ff477f7c79f090cc6663d3a",
            "isKey": true,
            "numCitedBy": 4635,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "may 7th, 1986, Professor A. F. M. Smith in the Chair] SUMMARY A continuous two-dimensional region is partitioned into a fine rectangular array of sites or \"pixels\", each pixel having a particular \"colour\" belonging to a prescribed finite set. The true colouring of the region is unknown but, associated with each pixel, there is a possibly multivariate record which conveys imperfect information about its colour according to a known statistical model. The aim is to reconstruct the true scene, with the additional knowledge that pixels close together tend to have the same or similar colours. In this paper, it is assumed that the local characteristics of the true scene can be represented by a nondegenerate Markov random field. Such information can be combined with the records by Bayes' theorem and the true scene can be estimated according to standard criteria. However, the computational burden is enormous and the reconstruction may reflect undesirable largescale properties of the random field. Thus, a simple, iterative method of reconstruction is proposed, which does not depend on these large-scale characteristics. The method is illustrated by computer simulations in which the original scene is not directly related to the assumed random field. Some complications, including parameter estimation, are discussed. Potential applications are mentioned briefly."
            },
            "slug": "On-the-Statistical-Analysis-of-Dirty-Pictures-Besag",
            "title": {
                "fragments": [],
                "text": "On the Statistical Analysis of Dirty Pictures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082299938"
                        ],
                        "name": "D. Tal",
                        "slug": "D.-Tal",
                        "structuredName": {
                            "firstName": "Doron",
                            "lastName": "Tal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 64
                            }
                        ],
                        "text": "We selected images from the Berkeley Image Segmentation Dataset [Martin et al., 2001] for which two users had significantly different segmentations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a1ed876196ec9733acb1daa6d65e35ff0414291",
            "isKey": false,
            "numCitedBy": 6039,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties."
            },
            "slug": "A-database-of-human-segmented-natural-images-and-to-Martin-Fowlkes",
            "title": {
                "fragments": [],
                "text": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes is presented and an error measure is defined which quantifies the consistency between segmentations of differing granularities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145884132"
                        ],
                        "name": "J. Edmonds",
                        "slug": "J.-Edmonds",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Edmonds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Edmonds"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "Theorem 5.1 (Edmonds, 1965) A perfect matching M is a maximum weight perfect matching if and only if there are no augmenting alternating cycles."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15379135,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "abf5d359f5df2c9f25c9f06f1e6689ebd1408751",
            "isKey": true,
            "numCitedBy": 1628,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm is described for optimally pamng a finit e set of objects. That is, given a real numerical weight for each unordered pair of objects in a se t Y, to selec t a family of mutually di sjoint pairs th e sum of whose wei ghts is maximum . The well-known optimum assignment proble m [5)2 is the sp ecial case where Y partitions into two se ts A and B suc h that pairs contained in A and pairs contain ed in Bare not positively weighted and therefo re are superfluous to the problem. For this \"bipartite\" case the algorithm becomes a variant of the Hungarian method [3]. The problem is treated in terms of a graph G whose nodes (vertices) are the objects Y and whose edges are pairs of objects, including at leas t all of th e positively weighted pairs. A matching in G is a subse t of its edges such that no two mee t the same node in in G. The proble m is to find a maximum-weight-sum matching in C. Th e special case where all th e positive weights are one is treated in detail in [2] and [6]. The description here of the more general algorithm uses the terminology set up in [2]. Paper [2] (especially sec . 5) helps also to motivate thi s paper, though it is not r eally a prerequisate till section 7 here . The incr ease in difficulty of the maximum weightsum matc hing algorithm relative to the s ize of the graph is not expone ntial, and only moderately algebraic. The algorithm does not involve any \"blind-alley programming\" -which, essentially, amounts to testing a great many combinations . The emphasis in this paper is on relating the matching problem to the theory of continuous linear"
            },
            "slug": "Maximum-matching-and-a-polyhedron-with-0,1-vertices-Edmonds",
            "title": {
                "fragments": [],
                "text": "Maximum matching and a polyhedron with 0,1-vertices"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The emphasis in this paper is on relating the matching problem to the theory of continuous linear programming, and the algorithm described does not involve any \"blind-alley programming\" -which, essentially, amounts to testing a great many combinations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689196"
                        ],
                        "name": "A. Schrijver",
                        "slug": "A.-Schrijver",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schrijver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schrijver"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 218
                            }
                        ],
                        "text": "Our framework extends the max-margin formulation for Markov networks (Taskar et al., 2003; Taskar et al., 2004a) and context free grammars (Taskar et al., 2004b), and is similar to other formulations (Altun et al., 2003; Tsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 270
                            }
                        ],
                        "text": "A spanning tree is optimal with respect to a set of edge weights if and only if for every edge (j, k) in the tree connecting Vj [jk] and Vk[jk], the weight of (j, k) is larger than (or equal to) the weight of any other edge (j\u2032, k\u2032) in the graph with j\u2032 \u2208 Vj [jk], k\u2032 \u2208 Vk[jk] (Schrijver, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 233
                            }
                        ],
                        "text": "For example, a maximum weight spanning tree and perfect nonbipartite matching problems can be expressed as linear programs with exponentially many constraints, but no polynomial formulation as a convex optimization problem is known (Schrijver, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 120
                            }
                        ],
                        "text": "This LP is guaranteed to have integral (0/1) solutions (as long as P and R are integers) for any scoring function s(y) (Schrijver, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 97
                            }
                        ],
                        "text": "The case of non-perfect matchings can be handled by a reduction to perfect matchings as follows (Schrijver, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 209100259,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "f9e591692d6aab0e1cf0c1ea4948597195657edf",
            "isKey": true,
            "numCitedBy": 3419,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Reading a book is also kind of better solution when you have no enough money or time to get your own adventure. This is one of the reasons we show the combinatorial optimization polyhedra and efficiency as your friend in spending the time. For more representative collections, this book not only offers it's strategically book resource. It can be a good friend, really good friend with much knowledge."
            },
            "slug": "Combinatorial-optimization.-Polyhedra-and-Schrijver",
            "title": {
                "fragments": [],
                "text": "Combinatorial optimization. Polyhedra and efficiency."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This book shows the combinatorial optimization polyhedra and efficiency as your friend in spending the time in reading a book."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6252802"
                        ],
                        "name": "W. Kabsch",
                        "slug": "W.-Kabsch",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Kabsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kabsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144766425"
                        ],
                        "name": "C. Sander",
                        "slug": "C.-Sander",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Sander",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sander"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 102
                            }
                        ],
                        "text": "The sequences are annotated with secondary structure and solvent accessibility information from DSSP (Kabsch & Sander, 1983)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "Because DSSP utilizes true 3D structure information in assigning secondary structure, the model cannot be used in for unknown proteins."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The DSSP program\n1The DIPRO2 dataset was made publicly available by Baldi et al. (2004)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29185760,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "225ba9111ed4b4e0f5bbe12c69e7235e353ff3ff",
            "isKey": false,
            "numCitedBy": 13106,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "For a successful analysis of the relation between amino acid sequence and protein structure, an unambiguous and physically meaningful definition of secondary structure is essential. We have developed a set of simple and physically motivated criteria for secondary structure, programmed as a pattern\u2010recognition process of hydrogen\u2010bonded and geometrical features extracted from x\u2010ray coordinates. Cooperative secondary structure is recognized as repeats of the elementary hydrogen\u2010bonding patterns \u201cturn\u201d and \u201cbridge.\u201d Repeating turns are \u201chelices,\u201d repeating bridges are \u201cladders,\u201d connected ladders are \u201csheets.\u201d Geometric structure is defined in terms of the concepts torsion and curvature of differential geometry. Local chain \u201cchirality\u201d is the torsional handedness of four consecutive C\u03b1 positions and is positive for right\u2010handed helices and negative for ideal twisted \u03b2\u2010sheets. Curved pieces are defined as \u201cbends.\u201d Solvent \u201cexposure\u201d is given as the number of water molecules in possible contact with a residue. The end result is a compilation of the primary structure, including SS bonds, secondary structure, and solvent exposure of 62 different globular proteins. The presentation is in linear form: strip graphs for an overall view and strip tables for the details of each of 10.925 residues. The dictionary is also available in computer\u2010readable form for protein structure prediction work."
            },
            "slug": "Dictionary-of-protein-secondary-structure:-Pattern-Kabsch-Sander",
            "title": {
                "fragments": [],
                "text": "Dictionary of protein secondary structure: Pattern recognition of hydrogen\u2010bonded and geometrical features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A set of simple and physically motivated criteria for secondary structure, programmed as a pattern\u2010recognition process of hydrogen\u2010bonded and geometrical features extracted from x\u2010ray coordinates is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Biopolymers"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145002066"
                        ],
                        "name": "D. Younger",
                        "slug": "D.-Younger",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Younger",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Younger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 125
                            }
                        ],
                        "text": "We can use a Viterbi-style dynamic programming algorithm called CKY to compute the highest score parse tree in O(|P|n3) time [Younger, 1967; Manning & Sch \u00fctze, 1999]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40504606,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2",
            "isKey": false,
            "numCitedBy": 1035,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recognition-and-Parsing-of-Context-Free-Languages-Younger",
            "title": {
                "fragments": [],
                "text": "Recognition and Parsing of Context-Free Languages in Time n^3"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679221"
                        ],
                        "name": "H. Berman",
                        "slug": "H.-Berman",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "Berman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Berman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34819339"
                        ],
                        "name": "J. Westbrook",
                        "slug": "J.-Westbrook",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Westbrook",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Westbrook"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2272834"
                        ],
                        "name": "Zukang Feng",
                        "slug": "Zukang-Feng",
                        "structuredName": {
                            "firstName": "Zukang",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zukang Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1848948"
                        ],
                        "name": "G. Gilliland",
                        "slug": "G.-Gilliland",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Gilliland",
                            "middleNames": [
                                "L"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gilliland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145506090"
                        ],
                        "name": "T. Bhat",
                        "slug": "T.-Bhat",
                        "structuredName": {
                            "firstName": "Talapady",
                            "lastName": "Bhat",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bhat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3309680"
                        ],
                        "name": "H. Weissig",
                        "slug": "H.-Weissig",
                        "structuredName": {
                            "firstName": "Helge",
                            "lastName": "Weissig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Weissig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145522"
                        ],
                        "name": "I. Shindyalov",
                        "slug": "I.-Shindyalov",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Shindyalov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Shindyalov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2048644"
                        ],
                        "name": "P. Bourne",
                        "slug": "P.-Bourne",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Bourne",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bourne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 53
                            }
                        ],
                        "text": "It consists of 1018 non-redundant proteins from PDB (Berman et al., 2000) as of May 2004 which contain intra-chain disulfide bonds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8721150,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "548fe25043edbe4539c68550383875466ed0d777",
            "isKey": false,
            "numCitedBy": 31055,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "The Protein Data Bank [PDB; Berman, Westbrook et al. (2000), Nucleic Acids Res. 28, 235-242; http://www.pdb.org/] is the single worldwide archive of primary structural data of biological macromolecules. Many secondary sources of information are derived from PDB data. It is the starting point for studies in structural bioinformatics. This article describes the goals of the PDB, the systems in place for data deposition and access, how to obtain further information and plans for the future development of the resource. The reader should come away with an understanding of the scope of the PDB and what is provided by the resource."
            },
            "slug": "The-Protein-Data-Bank-Berman-Westbrook",
            "title": {
                "fragments": [],
                "text": "The Protein Data Bank"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The goals of the PDB are described, the systems in place for data deposition and access, how to obtain further information and plans for the future development of the resource are described."
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic Acids Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755687"
                        ],
                        "name": "A. Bairoch",
                        "slug": "A.-Bairoch",
                        "structuredName": {
                            "firstName": "Amos",
                            "lastName": "Bairoch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bairoch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722884"
                        ],
                        "name": "R. Apweiler",
                        "slug": "R.-Apweiler",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Apweiler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Apweiler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32649061,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "b75c9516971fc73866ab6750215b6b1cf6bf6119",
            "isKey": false,
            "numCitedBy": 1835,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "SWISS-PROT is a curated protein sequence database which strives to provide a high level of annotations (such as the description of the function of a protein, structure of its domains, post-translational modifications, variants, etc.), a minimal level of redundancy and high level of integration with other databases. Recent developments of the database include: an increase in the number and scope of model organisms; cross-references to two additional databases; a variety of new documentation files and the creation of TrEMBL, a computer annotated supplement to SWISS-PROT. This supplement consists of entries in SWISS-PROT-like format derived from the translation of all coding sequences (CDS) in the EMBL nucleotide sequence database, except the CDS already included in SWISS-PROT."
            },
            "slug": "The-SWISS-PROT-protein-sequence-data-bank-and-its-Bairoch-Apweiler",
            "title": {
                "fragments": [],
                "text": "The SWISS-PROT protein sequence data bank and its supplement TrEMBL"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This supplement consists of entries in SWiss-PROT-like format derived from the translation of all coding sequences in the EMBL nucleotide sequence database, except the CDS already included in SWISS- PROT."
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic Acids Res."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48266310"
                        ],
                        "name": "M. Matsumura",
                        "slug": "M.-Matsumura",
                        "structuredName": {
                            "firstName": "Masazumi",
                            "lastName": "Matsumura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Matsumura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8094935"
                        ],
                        "name": "G. Signor",
                        "slug": "G.-Signor",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Signor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Signor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2307666"
                        ],
                        "name": "B. Matthews",
                        "slug": "B.-Matthews",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Matthews",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Matthews"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 230
                            }
                        ],
                        "text": "Such bonds are a very important feature of protein structure since they enhance conformational stability by reducing the number of configurational states and decreasing the entropic cost of folding a protein into its native state [Matsumura et al., 1989]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4254713,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "68e99b5b800139f8934439a20c07d491c572a68d",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "DISULPHIDE bonds can significantly stabilize the native structures of proteins1\u20133. The effect is presumed to be due mainly to a decrease in the configurational chain entropy of the unfolded polypeptide4\u20137. In phage T4 lysozyme, a disulphide-free enzyme, engineered disulphide mutants that crosslink residues 3\u201397, 9\u2013164 and 21\u2013142 are significantly more stable than the wild-type protein8\u201311. To investigate the effect of multiple-disulphide bonds on protein stability, mutants were constructed in which two or three stabilizing disulphide bridges were combined in the same protein. Reversible thermal denaturation shows that the increase in melting temperature resulting from the individual disulphide bonds is approximately additive. The triple-disulphide variant unfolds at a temperature 23.4 \u00b0C higher than wild-type lysozyme. The results demonstrate that a combination of disulphide bonds, each of which contributes to stability, can achieve substantial overall improvement in the stability of a protein."
            },
            "slug": "Substantial-increase-of-protein-stability-by-bonds-Matsumura-Signor",
            "title": {
                "fragments": [],
                "text": "Substantial increase of protein stability by multiple disulphide bonds"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that a combination of disulphide bonds, each of which contributes to stability, can achieve substantial overall improvement in the stability of a protein."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 179
                            }
                        ],
                        "text": "For example, to learn a probabilistic model P (y | x) over bipartite matchings using maximum likelihood requires computing the normalizing partition function, which is#P-complete [Valiant, 1979; Garey & Johnson, 1979]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 179
                            }
                        ],
                        "text": "For example, to learn a probabilistic modelP (y | x) over bipartite matchings using maximum likelihood requires computing the normalizing partition function, which is #P-complete [Valiant, 1979; Garey & Johnson, 1979]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 69
                            }
                        ],
                        "text": "However, even simply counting the number of matchings is #P-complete [Valiant, 1979; Garey & Johnson, 1979]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1637832,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9c188b3291a7e83d667764be3377a99e15b4d988",
            "isKey": true,
            "numCitedBy": 2561,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Complexity-of-Computing-the-Permanent-Valiant",
            "title": {
                "fragments": [],
                "text": "The Complexity of Computing the Permanent"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32129876"
                        ],
                        "name": "R. B. Potts",
                        "slug": "R.-B.-Potts",
                        "structuredName": {
                            "firstName": "Renfrey",
                            "lastName": "Potts",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. B. Potts"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 71
                            }
                        ],
                        "text": "In this setting, a common approach is to use a generalized Potts model [Potts, 1952], which penalizes assignments that do not have the same label across the edge: \u03c6ij(k, l) = \u03bbij, \u2200k 6= l and \u03c6ij(k, k) = 1, where\u03bbij \u2264 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 47
                            }
                        ],
                        "text": "This class of networks extends the Potts model [Potts, 1952] often used in computer vision and allows exact MAP inference in the case of binary variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122689941,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4aeea86e589383e2ec2d4214e919ebda9277c452",
            "isKey": false,
            "numCitedBy": 1685,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "In considering the statistics of the \u2018no-field\u2019 square Ising lattice in which each unit is capable of two configurations and only nearest neighbours interact, Kramers and Wannier (3) were able to deduce an inversion transformation under which the partition function of the lattice is invariant when the temperature is transformed from a low to a high (\u2018inverted\u2019) value. The important property of this inversion transformation is that its fixed point gives the transition point of the lattice."
            },
            "slug": "Some-generalized-order-disorder-transformations-Potts",
            "title": {
                "fragments": [],
                "text": "Some generalized order - disorder transformations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34196655,
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "id": "b05b67aca720d0bc39bc9afad02a19f522c7a1bc",
            "isKey": false,
            "numCitedBy": 2415,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Objective\u2014To evaluate the pharmacokinetics of a novel commercial formulation of ivermectin after administration to goats.\r\n\r\nAnimals\u20146 healthy adult goats.\r\n\r\nProcedure\u2014Ivermectin (200 \u03bcg/kg) was initially administered IV to each goat, and plasma samples were obtained for 36 days. After a washout period of 3 weeks, each goat received a novel commercial formulation of ivermectin (200 \u03bcg/kg) by SC injection. Plasma samples were then obtained for 42 days. Drug concentrations were quantified by use of high-performance liquid chromatography with fluorescence detection.\r\n\r\nResults\u2014Pharmacokinetics of ivermectin after IV administration were best described by a 2-compartment open model; values for main compartmental variables included volume of distribution at a steady state (9.94 L/kg), clearance (1.54 L/kg/d), and area under the plasma concentration-time curve (AUC; 143 [ng\u2022d]/mL). Values for the noncompartmental variables included mean residence time (7.37 days), AUC (153 [ng\u2022d]/mL), and clearance (1.43 L/kg/d). After SC administration, noncompartmental pharmacokinetic analysis was conducted. Values of the variables calculated by use of this method included maximum plasma concentration (Cmax; 21.8 ng/mL), time to reach Cmax (3 days), and bioavailability (F; 91.8%).\r\n\r\nConclusions and Clinical Relevance\u2014The commercial formulation used in this study is a good option to consider when administering ivermectin to goats because of the high absorption, which is characterized by high values of F. In addition, the values of Cmax and time to reach Cmax are higher than those reported by other investigators who used other routes of administration."
            },
            "slug": "Pharmacokinetics-of-a-novel-formulation-of-after-to-Ng-Russell",
            "title": {
                "fragments": [],
                "text": "Pharmacokinetics of a novel formulation of ivermectin after administration to goats"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Pharmacokinetics of ivermectin after IV administration were best described by a 2-compartment open model; values for main compartmental variables included volume of distribution at a steady state, area under the plasma concentration-time curve, and area underThe AUC curve."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 270
                            }
                        ],
                        "text": "A spanning tree is optimal with respect to a set of edge weights if and only if for every edge (j, k) in the tree connecting Vj [jk] and Vk[jk], the weight of (j, k) is larger than (or equal to) the weight of any other edge (j\u2032, k\u2032) in the graph with j\u2032 \u2208 Vj [jk], k\u2032 \u2208 Vk[jk] (Schrijver, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 88
                            }
                        ],
                        "text": "It is an open problem to derive a polynomial sized LP formulation for perfect matchings [Schrijver, 2003]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 233
                            }
                        ],
                        "text": "For example, a maximum weight spanning tree and perfect nonbipartite matching problems can be expressed as linear programs with exponentially many constraints, but no polynomial formulation as a convex optimization problem is known (Schrijver, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 96
                            }
                        ],
                        "text": "The case of non-perfect matchings can be handled by a reduction to perfect matchings as follows [Schrijver, 2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 198
                            }
                        ],
                        "text": "For example, maximum weight perfect (non-bipartite) matching and spanning tree problems can be expressed as linear programs withexponentiallymany constraints, but no polynomial formulation is known [Bertsimas & Tsitsiklis, 1997; Schrijver, 2003]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 199
                            }
                        ],
                        "text": "For example, maximum weight perfect (non-bipartite) matching and spanning tree problems can be expressed as linear programs with exponentiallymany constraints, but no polynomial formulation is known [Bertsimas & Tsitsiklis, 1997; Schrijver, 2003]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 120
                            }
                        ],
                        "text": "This LP is guaranteed to have integral (0/1) solutions (as long as P and R are integers) for any scoring function s(y) (Schrijver, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 97
                            }
                        ],
                        "text": "The case of non-perfect matchings can be handled by a reduction to perfect matchings as follows (Schrijver, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2003).Combinatorial optimization: Polyhedra and efficiency"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 129
                            }
                        ],
                        "text": "Following the lines of the recent work on maximum margin estimation for probabilistic models (Collins, 2002; Altun et al., 2003; Taskar et al., 2003), we present a discriminative estimation framework for structured models based on the large margin principle underlying support vector machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 70
                            }
                        ],
                        "text": "Our framework extends the max-margin formulation for Markov networks (Taskar et al., 2003; Taskar et al., 2004a) and context free grammars (Taskar et al., 2004b), and is similar to other formulations (Altun et al., 2003; Tsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 220
                            }
                        ],
                        "text": "This formulation generalizes the idea used by Taskar et al. (2004a) to provide a polynomial time estimation procedure for a certain family of Markov networks; it can also be used to derive the max-margin formulations of Taskar et al. (2003); Taskar et al. (2004b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Max margin Markov networks. Proc. NIPS"
            },
            "venue": {
                "fragments": [],
                "text": "Max margin Markov networks. Proc. NIPS"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 109
                            }
                        ],
                        "text": "Following the lines of the recent work on maximum margin estimation for probabilistic models (Collins, 2002; Altun et al., 2003; Taskar et al., 2003), we present a discriminative estimation framework for structured models based on the large margin principle underlying support vector machines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 201
                            }
                        ],
                        "text": "Our framework extends the max-margin formulation for Markov networks (Taskar et al., 2003; Taskar et al., 2004a) and context free grammars (Taskar et al., 2004b), and is similar to other formulations (Altun et al., 2003; Tsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden markov support vector"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "We assume for simplicity that the features are rich enough to satisfy the constraints, which is analogous to the separable case formulation in SVMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 117
                            }
                        ],
                        "text": "Instead, we use a coordinate dual ascent method analogous to the sequential minimal optimization (SMO) used for SVMs [Platt, 1999]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 167
                            }
                        ],
                        "text": "To the best of our knowledge, there are no upper bounds on the speed of convergence of SMO, but experimental evidence has shown it a very effective algorithm for SVMs [Platt, 1999]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "In particular, as in SVMs, we can use the kernel trick in the dual to efficiently learn in high-dimensional feature spaces."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using sparseness and analytic QP to speed training of support vector"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 151
                            }
                        ],
                        "text": "We used two datasets containing sequences with experimentally verified bonding patterns: SP39\n(used in Baldi et al. (2004); Vullo and Frasconi (2004); Fariselli and Casadio (2001)) and DIPRO2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 121
                            }
                        ],
                        "text": "In order to avoid biases during testing, we adopt the same dataset splitting procedure as the one used in previous work (Fariselli & Casadio, 2001; Vullo & Frasconi, 2004; Baldi et al., 2004).2\nModels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 132
                            }
                        ],
                        "text": "Recently, there has been increasing interest in applying computational techniques to the task of predicting disulfide connectivity (Fariselli & Casadio, 2001; Baldi et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 23
                            }
                        ],
                        "text": "Following the lines of Fariselli and Casadio (2001), we predict the connectivity pattern by finding the maximum weighted matching in a graph in which each vertex represents a cysteine residue, and each edge represents the \u201cattraction strength\u201d between the cysteines it connects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction of disulfide connectivity"
            },
            "venue": {
                "fragments": [],
                "text": "in proteins. Bioinformatics,"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49026551"
                        ],
                        "name": "D. Greig",
                        "slug": "D.-Greig",
                        "structuredName": {
                            "firstName": "Darryl",
                            "lastName": "Greig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Greig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146294528"
                        ],
                        "name": "B. Porteous",
                        "slug": "B.-Porteous",
                        "structuredName": {
                            "firstName": "Baroness",
                            "lastName": "Porteous",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Porteous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887111"
                        ],
                        "name": "A. Seheult",
                        "slug": "A.-Seheult",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Seheult",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Seheult"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115691220,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a717b20e99b76cb228b47694140ed3dce082b530",
            "isKey": false,
            "numCitedBy": 1257,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Exact-Maximum-A-Posteriori-Estimation-for-Binary-Greig-Porteous",
            "title": {
                "fragments": [],
                "text": "Exact Maximum A Posteriori Estimation for Binary Images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38718108"
                        ],
                        "name": "S. B. Needleman",
                        "slug": "S.-B.-Needleman",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Needleman",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. B. Needleman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081525461"
                        ],
                        "name": "C. D. Wunsch",
                        "slug": "C.-D.-Wunsch",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wunsch",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Wunsch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 90088359,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f3c9d483dfc4b78ccf0530b9854a7932aa5469cb",
            "isKey": false,
            "numCitedBy": 321,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-General-Method-Applicable-to-the-Search-for-in-of-Needleman-Wunsch",
            "title": {
                "fragments": [],
                "text": "A General Method Applicable to the Search for Similarities in the Amino Acid Sequence of Two Proteins"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 232
                            }
                        ],
                        "text": "\u2026alignment of 2D shapes using weighted bipartite point matching (Belongie et al., 2002), disulfide connectivity prediction using weighted non-bipartite matchings (Baldi et al., 2004), clustering using spanning trees and graph cuts (Duda et al., 2000), and other combinatorial and graph structures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 361680,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e75fceff79fefa063d00ebc56a20c7df5485cf2b",
            "isKey": false,
            "numCitedBy": 4146,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-classification,-2nd-Edition-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification, 2nd Edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 196008710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78053512af13466c569e5946acfc3953bbfc9d36",
            "isKey": false,
            "numCitedBy": 18023,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Classification-Hart-Duda",
            "title": {
                "fragments": [],
                "text": "Pattern Classification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643905196"
                        ],
                        "name": "M. KleinbergJon",
                        "slug": "M.-KleinbergJon",
                        "structuredName": {
                            "firstName": "M",
                            "lastName": "KleinbergJon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. KleinbergJon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 150
                            }
                        ],
                        "text": "The structure of the relational graph has been used extensively to infer importance in scientific publications [Egghe & Rousseau, 1990] and hypertext [Kleinberg, 1999]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 80
                            }
                        ],
                        "text": "We can view a section of a webpage as a fine-grained version of Kleinberg\u2019s hub [Kleinberg, 1999] (a page that contains a lot of links to pages of particular category)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 216000619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "058861dd838ddf4a6a36860f54e82e11f8945b32",
            "isKey": false,
            "numCitedBy": 7576,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set ..."
            },
            "slug": "Authoritative-sources-in-a-hyperlinked-environment-KleinbergJon",
            "title": {
                "fragments": [],
                "text": "Authoritative sources in a hyperlinked environment"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided the authors have effective means for understanding it."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 91
                            }
                        ],
                        "text": "Our framework extends the max-margin formulation for Markov networks (Taskar et al., 2003; Taskar et al., 2004a) and context free grammars (Taskar et al., 2004b), and is similar to other formulations (Altun et al., 2003; Tsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 46
                            }
                        ],
                        "text": "This formulation generalizes the idea used by Taskar et al. (2004a) to provide a polynomial time estimation procedure for a certain family of Markov networks; it can also be used to derive the max-margin formulations of Taskar et al. (2003); Taskar et al. (2004b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning associative Markov networks. Proc. ICML"
            },
            "venue": {
                "fragments": [],
                "text": "Learning associative Markov networks. Proc. ICML"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimators for stochastic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 64
                            }
                        ],
                        "text": "There has been much research in this setting for classification [Blum & Mitchell, 1998; Nigam et al., 2000; Chapelleet al., 2002; Szummer & Jaakkola, 2001; Zhu et al., 2003; Corduneanu & Jaakkola, 2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning using gaussian"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 0
                            }
                        ],
                        "text": "[Fariselli et al., 1999; Fiser & Simon, 2000; Martelli et al., 2002; Frasconi et al., 2002; Ceroniet al., 2003] Currently the top performing systems have accuracies around 85%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Role of evolutionary information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cluster kernels for semi-supervised"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A neural network-based method for predicting the disulfide connectivity in proteins"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 41
                            }
                        ],
                        "text": "This problem can be solved in O(L3) time [Gabow, 1973; Lawler, 1976]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 229
                            }
                        ],
                        "text": "\u2022 Combinatorial structures Many important computational tasks are formulated as combinatorial optimization problems such as the maximum weight bipartite and perfect matching, spanning tree, graph-cut, edge-cover, and many others [Lawler, 1976; Papadimitriou & Steiglitz, 1982; Cormenet al., 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 215
                            }
                        ],
                        "text": "Many important computational tasks are formulated as combinatorial optimization problems such as the maximum weight bipartite and perfect matching, spanning tree, graph-cut, edge-cover, bin-packing, and many others [Lawler, 1976; Papadimitriou & Steiglitz, 1982; Cormenet al., 2001]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1976).Combinatorial optimization: Networks and matroids"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative probabilistic models for rela"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic classification and clustering"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Global protein function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An experimental comparison of min-cut/max-flow algorithms for energy minimization in computer vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning distance functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 41
                            }
                        ],
                        "text": "This problem can be solved in O(L3) time [Gabow, 1973; Lawler, 1976]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1973).Implementation of algorithms for maximum matching on nonbipartite"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 91
                            }
                        ],
                        "text": "Our framework extends the max-margin formulation for Markov networks (Taskar et al., 2003; Taskar et al., 2004a) and context free grammars (Taskar et al., 2004b), and is similar to other formulations (Altun et al., 2003; Tsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 46
                            }
                        ],
                        "text": "This formulation generalizes the idea used by Taskar et al. (2004a) to provide a polynomial time estimation procedure for a certain family of Markov networks; it can also be used to derive the max-margin formulations of Taskar et al. (2003); Taskar et al. (2004b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Max margin parsing. Proc. EMNLP"
            },
            "venue": {
                "fragments": [],
                "text": "Max margin parsing. Proc. EMNLP"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1982).Combinatorial optimization: Algorithms and complexity"
            },
            "venue": {
                "fragments": [],
                "text": "Englewood Cliffs, NJ: Prentice-Hall. Pearl, J. (1988).Probabilistic reasoning in intelligent systems"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 124
                            }
                        ],
                        "text": "As in the case of logistic regression, maximum conditional likelihood estimation for Markov networks can also be kernelized [Altun et al., 2004; Lafferty et al., 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 127
                            }
                        ],
                        "text": "Maximum likelihood models with kernels are generally non-sparse and require pruning or greedy support vector selection methods [Wahba et al., 1993; Zhu & Hastie, 2001; Lafferty et al., 2004; Altun et al., 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kernel conditional random fields: Representation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 26
                            }
                        ],
                        "text": "The proof is presented in Taskar (2004), Chap."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning structured prediction models: A large margin approach"
            },
            "venue": {
                "fragments": [],
                "text": "Doctoral dissertation,"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning probabilistic models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 138
                            }
                        ],
                        "text": "Multiple alignments were computed by running PSIBLAST using default settings to align the sequence with all sequences in the NR database (Altschul et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 83291172,
            "fieldsOfStudy": [],
            "id": "964404598b566ba2aea0c098199901d3092496ff",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gapped BLAST and PSI-BLAST: A new"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Map estimation via agreement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A network flow method for solving inverse combinatorial"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 153
                            }
                        ],
                        "text": "Also, in training settings with missing data and hidden variables, probabilistic interpretation permits the use of well-understood algorithms such as EM [Dempster et al., 1977]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 106
                            }
                        ],
                        "text": "This setting has been studied mainly in the probabilistic, generative models often using the EM algorithm [Dempster et al., 1977; Cowellet al., 1999]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 158
                            }
                        ],
                        "text": "It scores candidate connectivity patterns according to their similarity with respect to the correct pattern, and uses a recursive neural network architecture [Frasconi et al., 1998] to score candidate patterns."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A general framework for adaptive structures"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 0
                            }
                        ],
                        "text": "[Fariselli et al., 1999; Fiser & Simon, 2000; Martelli et al., 2002; Frasconi et al., 2002; Ceroniet al., 2003] Currently the top performing systems have accuracies around 85%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction of the disulfidebonding state of cysteines in proteins"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 136
                            }
                        ],
                        "text": "Later work showed how to efficiently learn model parameters and structure (equivalent of clique selection in Markov networks) from data [Friedman et al., 1999]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 5
                            }
                        ],
                        "text": "PRMs [Koller & Pfeffer, 1998; Friedman et al., 1999; Getooret al., 2002] are a relational extension of Bayesian networks [Pearl, 1988]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning probabilistic relational"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 124
                            }
                        ],
                        "text": "As in the case of logistic regression, maximum conditional likelihood estimation for Markov networks can also be kernelized [Altun et al., 2004; Lafferty et al., 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 127
                            }
                        ],
                        "text": "Maximum likelihood models with kernels are generally non-sparse and require pruning or greedy support vector selection methods [Wahba et al., 1993; Zhu & Hastie, 2001; Lafferty et al., 2004; Altun et al., 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponential families for conditional random"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis and classification of disulphide connectivity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 132
                            }
                        ],
                        "text": "The objective is to minimize the training log-loss with an additional regularization term, usually the squared-norm of the weightsw [Lafferty et al., 2001]: 1 2 ||w||2 \u2212 C \u2211"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 36
                            }
                        ],
                        "text": "conditional Markov networks or CRFs [Lafferty et al., 2001])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conditional random fields: Probabilistic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Correlation clustering - minimizing disagreements on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text classification using"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1996).Probabilistic theory of pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "Recently, the Exponentiated Gradient [Kivinen & Warmuth, 1997] algorithm has been adopted to solve our structured QP for max-margin estimation [Bartlett et al., 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponentiated gradient"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 105
                            }
                        ],
                        "text": "In hypertext or bibliographic classification, labels of linked and co-cited documents tend to be similar [Chakrabarti et al., 1998; Taskaret al., 2002]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Enhanced hypertext categorization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 96
                            }
                        ],
                        "text": "For many classes, for example decision trees and multi-layer neural networks, it is intractable [Bishop, 1995; Quinlan, 2001], and we must resort to approximate, greedy optimization methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1995).Neural networks for pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 131
                            }
                        ],
                        "text": "Major research has been devoted to gene-finding, alignment of sequences, protein structure prediction, molecular pathway discovery [Gusfield, 1997; Durbin et al., 1998]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1997).Algorithms on strings, trees, and sequences : Computer science and computational biology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What energy functions can be minimized using graph cuts?PAMI"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast approximate energy minimization via"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 28,
            "methodology": 41,
            "result": 13
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 124,
        "totalPages": 13
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-structured-prediction-models:-a-large-Taskar-Chatalbashev/8d56b2a75aa5624660b60787e1f38ee2c70d493a?sort=total-citations"
}