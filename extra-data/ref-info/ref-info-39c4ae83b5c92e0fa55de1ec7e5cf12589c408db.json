{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804700"
                        ],
                        "name": "Krishna Subramanian",
                        "slug": "Krishna-Subramanian",
                        "structuredName": {
                            "firstName": "Krishna",
                            "lastName": "Subramanian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krishna Subramanian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145603129"
                        ],
                        "name": "P. Natarajan",
                        "slug": "P.-Natarajan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Natarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Natarajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128428"
                        ],
                        "name": "M. Decerbo",
                        "slug": "M.-Decerbo",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Decerbo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Decerbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752462"
                        ],
                        "name": "D. Casta\u00f1\u00f3n",
                        "slug": "D.-Casta\u00f1\u00f3n",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Casta\u00f1\u00f3n",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Casta\u00f1\u00f3n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18378610,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "477831f8f3b0ac37042fdf6ea4b3e7842739e797",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new approach for analysis of images for text-localization and extraction. Our approach puts very few constraints on the font, size and color of text and is capable of handling both scene text and artificial text well. In this paper, we exploit two well-known features of text: approximately constant stroke width and local contrast, and develop a fast, simple, and effective algorithm to detect character strokes. We also show how these can be used for accurate extraction and motivate some advantages of using this approach for text localization over other color-space segmentation based approaches. We analyze the performance of our stroke detection algorithm on images collected for the robust-reading competitions at ICDAR 2003."
            },
            "slug": "Character-Stroke-Detection-for-Text-Localization-Subramanian-Natarajan",
            "title": {
                "fragments": [],
                "text": "Character-Stroke Detection for Text-Localization and Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper develops a fast, simple, and effective algorithm to detect character strokes and analyzes the performance of the stroke detection algorithm on images collected for the robust-reading competitions at ICDAR 2003."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064132"
                        ],
                        "name": "V. Dinh",
                        "slug": "V.-Dinh",
                        "structuredName": {
                            "firstName": "Viet",
                            "lastName": "Dinh",
                            "middleNames": [
                                "Cuong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Dinh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2844363"
                        ],
                        "name": "S. Chun",
                        "slug": "S.-Chun",
                        "structuredName": {
                            "firstName": "Seong",
                            "lastName": "Chun",
                            "middleNames": [
                                "Soo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330406"
                        ],
                        "name": "Seungwook Cha",
                        "slug": "Seungwook-Cha",
                        "structuredName": {
                            "firstName": "Seungwook",
                            "lastName": "Cha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seungwook Cha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2013547"
                        ],
                        "name": "Hanjin Ryu",
                        "slug": "Hanjin-Ryu",
                        "structuredName": {
                            "firstName": "Hanjin",
                            "lastName": "Ryu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanjin Ryu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3153528"
                        ],
                        "name": "S. Sull",
                        "slug": "S.-Sull",
                        "structuredName": {
                            "firstName": "Sanghoon",
                            "lastName": "Sull",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sull"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42134356,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "395a3e9674fcc5686dcdd943ed94175272f7fb00",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Text appearing in video provides semantic knowledge and significant information for video indexing and retrieval system. This paper proposes an effective method for text detection in video based on the similarity in stroke width of text (which is defined as the distance between two edges of a stroke). From the observation that text regions can be characterized by a dominant fixed stroke width, edge detection with local adaptive thresholds is first devised to keep text- while reducing background-regions. Second, morphological dilation operator with adaptive structuring element size determined by stroke width value is exploited to roughly localize text regions. Finally, to reduce false alarm and refine text location, a new multi-frame refinement method is applied. Experimental results show that the proposed method is not only robust to different levels of background complexity, but also effective to different fonts (size, color) and languages of text."
            },
            "slug": "An-Efficient-Method-for-Text-Detection-in-Video-on-Dinh-Chun",
            "title": {
                "fragments": [],
                "text": "An Efficient Method for Text Detection in Video Based on Stroke Width Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Experimental results show that the proposed method for text detection in video based on the similarity in stroke width of text is not only robust to different levels of background complexity, but also effective to different fonts (size, color) and languages of text."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 75
                            }
                        ],
                        "text": "The problem of text detection was considered in a number of recent studies [1, 2, 3, 4, 5, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 24
                            }
                        ],
                        "text": "Several prev algorithms [3, 18, 19] rely on class regions and therefore are not segmentation mask required for subse method carries enough information segmentation and so a good mask is r detected text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 75
                            }
                        ],
                        "text": "The problem of text detection was considered in a number of recent studies [1, 2, 3, 4, 5, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "The feature was dominant enough to be used by itself, without the need for actual character recognition step as used in some previous works [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "language-specific ltering stage [3] or candidate window lows us to come up algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61234963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37ba7b9a823e8a400046bd149b7756adf5d698da",
            "isKey": true,
            "numCitedBy": 512,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives an algorithm for detecting and reading text in natural images. The algorithm is intended for use by blind and visually impaired subjects walking through city scenes. We first obtain a dataset of city images taken by blind and normally sighted subjects. From this dataset, we manually label and extract the text regions. Next we perform statistical analysis of the text regions to determine which image features are reliable indicators of text and have low entropy (i.e. feature response is similar for all text images). We obtain weak classifiers by using joint probabilities for feature responses on and off text. These weak classifiers are used as input to an AdaBoost machine learning algorithm to train a strong classifier. In practice, we trained a cascade with 4 strong classifiers containing 79 features. An adaptive binarization and extension algorithm is applied to those regions selected by the cascade classifier. Commercial OCR software is used to read the text or reject it as a non-text region. The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "slug": "Detecting-and-reading-text-in-natural-scenes-Chen-Yuille",
            "title": {
                "fragments": [],
                "text": "Detecting and reading text in natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The overall algorithm has a success rate of over 90% (evaluated by complete detection and reading of the text) on the test set and the unread text is typically small and distant from the viewer."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694936"
                        ],
                        "name": "Qixiang Ye",
                        "slug": "Qixiang-Ye",
                        "structuredName": {
                            "firstName": "Qixiang",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qixiang Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689702"
                        ],
                        "name": "Qingming Huang",
                        "slug": "Qingming-Huang",
                        "structuredName": {
                            "firstName": "Qingming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153706048"
                        ],
                        "name": "W. Gao",
                        "slug": "W.-Gao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725937"
                        ],
                        "name": "Debin Zhao",
                        "slug": "Debin-Zhao",
                        "structuredName": {
                            "firstName": "Debin",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Debin Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17956059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcb8cd892adbfded8373716a53787f55da89180a",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-and-robust-text-detection-in-images-and-video-Ye-Huang",
            "title": {
                "fragments": [],
                "text": "Fast and robust text detection in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95055206"
                        ],
                        "name": "Cheolkon Jung",
                        "slug": "Cheolkon-Jung",
                        "structuredName": {
                            "firstName": "Cheolkon",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheolkon Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48873890"
                        ],
                        "name": "Qifeng Liu",
                        "slug": "Qifeng-Liu",
                        "structuredName": {
                            "firstName": "Qifeng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qifeng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722566"
                        ],
                        "name": "Joongkyu Kim",
                        "slug": "Joongkyu-Kim",
                        "structuredName": {
                            "firstName": "Joongkyu",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joongkyu Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, the work [ 25 ] uses the id consistency for detecting text overlays The limitations of the method incl integration over scales and orientation again, the inherent attenuation to horizo"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3221142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "288a094920181484498cdfaa8534c554628362fc",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-stroke-filter-and-its-application-to-text-Jung-Liu",
            "title": {
                "fragments": [],
                "text": "A stroke filter and its application to text localization"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 75
                            }
                        ],
                        "text": "The problem of text detection was considered in a number of recent studies [1, 2, 3, 4, 5, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8387d4998f810cd2b60bd81545cb993087bc8788",
            "isKey": false,
            "numCitedBy": 467,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Many images, especially those used for page design on Web pages, as well as videos contain visible text. If these text occurrences could be detected, segmented, and recognized automatically, they would be a valuable source of high-level semantics for indexing and retrieval. We propose a novel method for localizing and segmenting text in complex images and videos. Text lines are identified by using a complex-valued multilayer feed-forward network trained to detect text at a fixed scale and position. The network's output at all scales and positions is integrated into a single text-saliency map, serving as a starting point for candidate text lines. In the case of video, these candidate text lines are refined by exploiting the temporal redundancy of text in video. Localized text lines are then scaled to a fixed height of 100 pixels and segmented into a binary image with black characters on white background. For videos, temporal redundancy is exploited to improve segmentation performance. Input images and videos can be of any size due to a true multiresolution approach. Moreover, the system is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video, so that one text bitmap is created for all instances of that text line. Therefore, our text segmentation results can also be used for object-based video encoding such as that enabled by MPEG-4."
            },
            "slug": "Localizing-and-segmenting-text-in-images-and-videos-Lienhart-Wernicke",
            "title": {
                "fragments": [],
                "text": "Localizing and segmenting text in images and videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel method for localizing and segmenting text in complex images and videos that is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits Syst. Video Technol."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522004"
                        ],
                        "name": "J. Gllavata",
                        "slug": "J.-Gllavata",
                        "structuredName": {
                            "firstName": "Julinda",
                            "lastName": "Gllavata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gllavata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738703"
                        ],
                        "name": "R. Ewerth",
                        "slug": "R.-Ewerth",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Ewerth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ewerth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685922"
                        ],
                        "name": "B. Freisleben",
                        "slug": "B.-Freisleben",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Freisleben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Freisleben"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Several prev algorithms [3,  18 , 19] rely on class regions and therefore are not segmentation mask required for subse method carries enough information segmentation and so a good mask is r detected text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31975917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "593e78f18ba5f5577b34ed81663db3e5d7d569cd",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Text localization and recognition in images is important for searching information in digital photo archives, video databases and Web sites. However, since text is often printed against a complex background, it is often difficult to detect. In this paper, a robust text localization approach is presented, which can automatically detect horizontally aligned text with different sizes, fonts, colors and languages. First, a wavelet transform is applied to the image and the distribution of high-frequency wavelet coefficients is considered to statistically characterize text and non-text areas. Then, the k-means algorithm is used to classify text areas in the image. The detected text areas undergo a projection analysis in order to refine their localization. Finally, a binary segmented text image is generated, to be used as input to an OCR engine. The detection performance of our approach is demonstrated by presenting experimental results for a set of video frames taken from the MPEG-7 video test set."
            },
            "slug": "Text-detection-in-images-based-on-unsupervised-of-Gllavata-Ewerth",
            "title": {
                "fragments": [],
                "text": "Text detection in images based on unsupervised classification of high-frequency wavelet coefficients"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A robust text localization approach is presented, which can automatically detect horizontally aligned text with different sizes, fonts, colors and languages and is demonstrated by presenting experimental results for a set of video frames taken from the MPEG-7 video test set."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108073159"
                        ],
                        "name": "Yangxing Liu",
                        "slug": "Yangxing-Liu",
                        "structuredName": {
                            "firstName": "Yangxing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangxing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144149214"
                        ],
                        "name": "S. Goto",
                        "slug": "S.-Goto",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Goto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758094"
                        ],
                        "name": "T. Ikenaga",
                        "slug": "T.-Ikenaga",
                        "structuredName": {
                            "firstName": "Takeshi",
                            "lastName": "Ikenaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ikenaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15640263,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99258fe39006a823ab0ba77e5d1711c5e6d13d0e",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection in color images has become an active research area in the past few decades. In this paper, we present a novel approach to accurately detect text in color images possibly with a complex background. The proposed algorithm is based on the combination of connected component and texture feature analysis of unknown text region contours. First, we utilize an elaborate color image edge detection algorithm to extract all possible text edge pixels. Connected component analysis is performed on these edge pixels to detect the external contour and possible internal contours of potential text regions. The gradient and geometrical characteristics of each region contour are carefully examined to construct candidate text regions and classify part non-text regions. Then each candidate text region is verified with texture features derived from wavelet domain. Finally, the Expectation maximization algorithm is introduced to binarize each text region to prepare data for recognition. In contrast to previous approach, our algorithm combines both the efficiency of connected component based method and robustness of texture based analysis. Experimental results show that our proposed algorithm is robust in text detection with respect to different character size, orientation, color and language and can provide reliable text binarization result."
            },
            "slug": "A-Contour-Based-Robust-Algorithm-for-Text-Detection-Liu-Goto",
            "title": {
                "fragments": [],
                "text": "A Contour-Based Robust Algorithm for Text Detection in Color Images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the proposed algorithm is robust in text detection with respect to different character size, orientation, color and language and can provide reliable text binarization result."
            },
            "venue": {
                "fragments": [],
                "text": "IEICE Trans. Inf. Syst."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": false,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856656"
                        ],
                        "name": "Hae-Kwang Kim",
                        "slug": "Hae-Kwang-Kim",
                        "structuredName": {
                            "firstName": "Hae-Kwang",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hae-Kwang Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 75
                            }
                        ],
                        "text": "The problem of text detection was considered in a number of recent studies [1, 2, 3, 4, 5, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8081258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f455a0f0e6d402648da33930fb39ef5587aab0e3",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An efficient automatic text detection and location method for video documents is proposed and its application for the content-based retrieval of video is presented and discussed. Target frames are selected at fixed time intervals from shots detected by a scene-change detection method. For each selected frame, segmentation by color clustering is performed around color peaks using a color histogram. For each color plane, text-lines are detected using heuristics, and the temporal and spatial position and the text-image of each text-line are stored in a database. Experimental results for text detection in video images and the performance of the method are reported for various video documents. A user interface for text-image based browsing is designed for direct content-based access to video documents, and other applications are discussed."
            },
            "slug": "Efficient-Automatic-Text-Location-Method-and-and-of-Kim",
            "title": {
                "fragments": [],
                "text": "Efficient Automatic Text Location Method and Content-Based Indexing and Structuring of Video Database"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An efficient automatic text detection and location method for video documents is proposed and its application for the content-based retrieval of video is presented and discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729041"
                        ],
                        "name": "J. Canny",
                        "slug": "J.-Canny",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Canny",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Canny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13284142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec",
            "isKey": false,
            "numCitedBy": 27658,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge."
            },
            "slug": "A-Computational-Approach-to-Edge-Detection-Canny",
            "title": {
                "fragments": [],
                "text": "A Computational Approach to Edge Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "There is a natural uncertainty principle between detection and localization performance, which are the two main goals, and with this principle a single operator shape is derived which is optimal at any scale."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6379469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce39eb5cc1049a1060a499d6b6e94c8b2ec11da1",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the robust reading competitions forICDAR 2003. With the rapid growth in research over thelast few years on recognizing text in natural scenes, thereis an urgent need to establish some common benchmarkdatasets, and gain a clear understanding of the current stateof the art. We use the term robust reading to refer to text imagesthat are beyond the capabilities of current commercialOCR packages. We chose to break down the robust readingproblem into three sub-problems, and run competitionsfor each stage, and also a competition for the best overallsystem. The sub-problems we chose were text locating,character recognition and word recognition.By breaking down the problem in this way, we hope togain a better understanding of the state of the art in eachof the sub-problems. Furthermore, our methodology involvesstoring detailed results of applying each algorithm toeach image in the data sets, allowing researchers to study indepth the strengths and weaknesses of each algorithm. Thetext locating contest was the only one to have any entries.We report the results of this contest, and show cases wherethe leading algorithms succeed and fail."
            },
            "slug": "ICDAR-2003-robust-reading-competitions-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The robust reading problem was broken down into three sub-problems, and competitions for each stage, and also a competition for the best overall system, which was the only one to have any entries."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144752865"
                        ],
                        "name": "Jian Liang",
                        "slug": "Jian-Liang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 75
                            }
                        ],
                        "text": "The problem of text detection was considered in a number of recent studies [1, 2, 3, 4, 5, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 40
                            }
                        ],
                        "text": "For comprehensive survey detection, see [1, 2]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5053740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82b6f95e805a92887f8efccf5a0dc8d5783676f5",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 131,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.The increasing availability of high-performance, low-priced, portable digital imaging devices has created a tremendous opportunity for supplementing traditional scanning for document image acquisition. Digital cameras attached to cellular phones, PDAs, or wearable computers, and standalone image or video devices are highly mobile and easy to use; they can capture images of thick books, historical manuscripts too fragile to touch, and text in scenes, making them much more versatile than desktop scanners. Should robust solutions to the analysis of documents captured with such devices become available, there will clearly be a demand in many domains. Traditional scanner-based document analysis techniques provide us with a good reference and starting point, but they cannot be used directly on camera-captured images. Camera-captured images can suffer from low resolution, blur, and perspective distortion, as well as complex layout and interaction of the content and background. In this paper we present a survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras. We begin by describing typical imaging devices and the imaging process. We discuss document analysis from a single camera-captured image as well as multiple frames and highlight some sample applications under development and feasible ideas for future development."
            },
            "slug": "Camera-based-analysis-of-text-and-documents:-a-Liang-Doermann",
            "title": {
                "fragments": [],
                "text": "Camera-based analysis of text and documents: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras, and some sample applications under development and feasible ideas for future development is presented."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809629"
                        ],
                        "name": "N. Otsu",
                        "slug": "N.-Otsu",
                        "structuredName": {
                            "firstName": "Nobuyuki",
                            "lastName": "Otsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Otsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "tric tests were learned on set [8] by optimizing on the training set we onents representing letters ovided by annotation) by ing Otsu algorithm [20], nnected components."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15326934,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "1d4816c612e38dac86f2149af667a5581686cdef",
            "isKey": false,
            "numCitedBy": 32882,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A nonparametric and unsupervised method ofautomatic threshold selection for picture segmentation is presented. An optimal threshold is selected by the discriminant criterion, namely, so as to maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zerothand the first-order cumulative moments of the gray-level histogram. It is straightforward to extend the method to multithreshold problems. Several experimental results are also presented to support the validity of the method."
            },
            "slug": "A-threshold-selection-method-from-gray-level-Otsu",
            "title": {
                "fragments": [],
                "text": "A threshold selection method from gray level histograms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1842569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf50fe5622253f401e892ed943a18033e18b7b9",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the results of the ICDAR 2005 competition for locating text in camera captured scenes. For this we used the same data as the ICDAR 2003 competition, which has been kept private until now. This allows a direct comparison with the 2003 entries. The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f-score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition. The paper also discusses the Web-based deployment and evaluation of text locating systems, and one of the leading entries has now been deployed in this way. This mode of usage could lead to more complete and more immediate knowledge of the strengths and weaknesses of each newly developed system."
            },
            "slug": "ICDAR-2005-text-locating-competition-results-Lucas",
            "title": {
                "fragments": [],
                "text": "ICDAR 2005 text locating competition results"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main result is that the leading 2005 entry has improved significantly on the leading 2003 entry, with an increase in average f- score from 0.5 to 0.62, where the f-score is the same adapted information retrieval measure used for the 2003 competition."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144947684"
                        ],
                        "name": "A. Baumgartner",
                        "slug": "A.-Baumgartner",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Baumgartner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36494513"
                        ],
                        "name": "C. Steger",
                        "slug": "C.-Steger",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Steger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Steger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34752408"
                        ],
                        "name": "H. Mayer",
                        "slug": "H.-Mayer",
                        "structuredName": {
                            "firstName": "Helmut",
                            "lastName": "Mayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mayer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757667"
                        ],
                        "name": "W. Eckstein",
                        "slug": "W.-Eckstein",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Eckstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Eckstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49604396"
                        ],
                        "name": "H. Ebner",
                        "slug": "H.-Ebner",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "Ebner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ebner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7351768,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "2103c862ad8010937cac562a0d4b51ed49d5faa3",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach for the automatic extraction of roads from digital aerial imagery is proposed. It makes use of several versions of the same aerial image with different resolutions. Roads are modeled as a network of intersections and links between these intersections, and are found by a grouping process. The context of roads is hierarchically structured into a global and a local level. The automatic segmentation of the aerial image into different global contexts, i.e., rural, forest, and urban area, is used to focus the extraction to the most promising regions. For the actual extraction of the roads, edges are extracted in the original high resolution image (0.2 to 0.5 m) and lines are extracted in an image of reduced resolution. Using both resolution levels and explicit knowledge about roads, hypotheses for road segments are generated. They are grouped iteratively into larger segments. In addition to the grouping algorithms, knowledge about the local context, e.g., shadows cast by a tree onto a road segment, is used to bridge gaps. To construct the road network, finally intersections are extracted. Examples and results of an evaluation based on manually plotted reference data are given, indicating the potential of the approach."
            },
            "slug": "AUTOMATIC-ROAD-EXTRACTION-BASED-ON-MULTI-SCALE,-AND-Baumgartner-Steger",
            "title": {
                "fragments": [],
                "text": "AUTOMATIC ROAD EXTRACTION BASED ON MULTI-SCALE, GROUPING, AND CONTEXT"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39608278"
                        ],
                        "name": "Lindi J. Quackenbush",
                        "slug": "Lindi-J.-Quackenbush",
                        "structuredName": {
                            "firstName": "Lindi",
                            "lastName": "Quackenbush",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lindi J. Quackenbush"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "detection competitions, as well as the participating algorithms, see [9] and [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Most technique rely on the assumptions listed above, an directly applicable for text detection techniques, see [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9984535,
            "fieldsOfStudy": [
                "Environmental Science",
                "Mathematics"
            ],
            "id": "84f47e553be07d2697653c3cecfd5860d9ae967f",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "The automated extraction of linear features from remotely sensed imagery has been the subject of extensive research over several decades. Recent studies show promise for extraction of feature information for applications such as updating geographic information systems (GIS). Research has been stimulated by the increase in available imagery in recent years following the launch of several airborne and satellite sensors. However, while the expansion in the range and availability of image data provides new possibilities for deriving image related products, it also places new demands on image processing. Efficiently dealing with the vast amount of available data necessitates an increase in automation, while still taking advantage of the skills of a human operator. This paper provides an overview of the types of imagery being used for linear feature extraction. The paper also describes methods used for feature extraction and considers quantitative and qualitative accuracy assessment of these procedures."
            },
            "slug": "A-Review-of-Techniques-for-Extracting-Linear-from-Quackenbush",
            "title": {
                "fragments": [],
                "text": "A Review of Techniques for Extracting Linear Features from Imagery"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145839132"
                        ],
                        "name": "Peter Doucette",
                        "slug": "Peter-Doucette",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Doucette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Doucette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809268"
                        ],
                        "name": "P. Agouris",
                        "slug": "P.-Agouris",
                        "structuredName": {
                            "firstName": "Peggy",
                            "lastName": "Agouris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Agouris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791122"
                        ],
                        "name": "A. Stefanidis",
                        "slug": "A.-Stefanidis",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Stefanidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stefanidis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5795077,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "04eaab3d73d544420b89684604ad2c4e27e56075",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This work presents a novel methodology for fully automated road centerline extraction that exploits spectral content from high resolution multispectral images. Preliminary detection of candidate road centerline components is performed with Anti-parallel-edge Centerline Extraction (ACE). This is followed by constructing a road vector topology with a fuzzy grouping model that links nodes from a self-organized mapping of the ACE components. Following topology construction, a Self-Supervised Road Classification (SSRC) feedback loop is implemented to automate the process of training sample selection and refinement for a road class, as well as deriving practical spectral definitions for non-road classes. SSRC demonstrates a potential to provide dramatic improvement in road extraction results by exploiting spectral content. Road centerline extraction results are presented for three 1 m colorinfrared suburban scenes which show significant improvement following SSRC."
            },
            "slug": "Automated-Road-Extraction-from-High-Resolution-Doucette-Agouris",
            "title": {
                "fragments": [],
                "text": "Automated Road Extraction from High Resolution Multispectral Imagery"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156795"
                        ],
                        "name": "C. Kirbas",
                        "slug": "C.-Kirbas",
                        "structuredName": {
                            "firstName": "Cemil",
                            "lastName": "Kirbas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kirbas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740663"
                        ],
                        "name": "Francis K. H. Quek",
                        "slug": "Francis-K.-H.-Quek",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Quek",
                            "middleNames": [
                                "K.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francis K. H. Quek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In ultiscale pyramid of strategy; moreover, e detected using this mentation, see [ 13 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 810806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48404b15b16038ab30349a6b10d9d1773353b2a4",
            "isKey": false,
            "numCitedBy": 1039,
            "numCiting": 221,
            "paperAbstract": {
                "fragments": [],
                "text": "Vessel segmentation algorithms are the critical components of circulatory blood vessel analysis systems. We present a survey of vessel extraction techniques and algorithms. We put the various vessel extraction approaches and techniques in perspective by means of a classification of the existing research. While we have mainly targeted the extraction of blood vessels, neurosvascular structure in particular, we have also reviewed some of the segmentation methods for the tubular objects that show similar characteristics to vessels. We have divided vessel segmentation algorithms and techniques into six main categories: (1) pattern recognition techniques, (2) model-based approaches, (3) tracking-based approaches, (4) artificial intelligence-based approaches, (5) neural network-based approaches, and (6) tube-like object detection approaches. Some of these categories are further divided into subcategories. We have also created tables to compare the papers in each category against such criteria as dimensionality, input type, preprocessing, user interaction, and result type."
            },
            "slug": "A-review-of-vessel-extraction-techniques-and-Kirbas-Quek",
            "title": {
                "fragments": [],
                "text": "A review of vessel extraction techniques and algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has mainly targeted the extraction of blood vessels, neurosvascular structure in particular, but has also reviewed some of the segmentation methods for the tubular objects that show similar characteristics to vessels."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120738541"
                        ],
                        "name": "Y. Sun",
                        "slug": "Y.-Sun",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Sun",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Studies that use vessel widt feature for tracking vessels starting from seed include [14,  15 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21194284,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "9bb9714a56aa9737e2fe2a26c46a22f8737e7da2",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A tracking algorithm for identification of vessel contours in digital coronary arteriograms was developed and validated. Given an initial start-of-search point, the tracking process was fully automated by utilizing the spatial continuity of the vessel's centerline, orientation, diameter, and density. The incremental sections along a major vessel were sequentially identified, based on the assumptions of geometric similarity and continuation between adjacent incremental sections. The algorithm consisted of an extrapolation-update process which was guided by a matched filter. The filter parameters were adapted to the measured lumen width. The tracking process was robust and extremely efficient as indicated by test results on synthetic images, digital subtraction angiograms, and cineangiograms. The algorithm provided accurate measurement of lumen width and percent stenosis that was relatively invariant to the vessel's orientation, dynamic range, background variation, and degree of blurring."
            },
            "slug": "Automated-identification-of-vessel-contours-in-by-Sun",
            "title": {
                "fragments": [],
                "text": "Automated identification of vessel contours in coronary arteriograms by an adaptive tracking algorithm."
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A tracking algorithm for identification of vessel contours in digital coronary arteriograms was developed and validated and provided accurate measurement of lumen width and percent stenosis that was relatively invariant to the vessel's orientation, dynamic range, background variation, and degree of blurring."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE transactions on medical imaging"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2448449"
                        ],
                        "name": "Y. J. Tejwani",
                        "slug": "Y.-J.-Tejwani",
                        "structuredName": {
                            "firstName": "Yogendra",
                            "lastName": "Tejwani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. J. Tejwani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "rouped together if his we modify the gorithm [17] by binary mask to a s of the pixels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 713769,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "3e5ad6a48fe0b8e97825e9ec1e831d18eeb5605a",
            "isKey": false,
            "numCitedBy": 2456,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A scheme is developed for classifying the types of motion perceived by a humanlike robot. It is assumed that the robot receives visual images of the scene using a perspective system model. Equations, theorems, concepts, clues, etc., relating the objects, their positions, and their motion to their images on the focal plane are presented.<<ETX>>"
            },
            "slug": "Robot-vision-Tejwani",
            "title": {
                "fragments": [],
                "text": "Robot vision"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A scheme is developed for classifying the types of motion perceived by a humanlike robot and equations, theorems, concepts, clues, etc., relating the objects, their positions, and their motion to their images on the focal plane are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Symposium on Circuits and Systems,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71537926"
                        ],
                        "name": "Seokil Park",
                        "slug": "Seokil-Park",
                        "structuredName": {
                            "firstName": "Seokil",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seokil Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108780525"
                        ],
                        "name": "Jong-Sil Lee",
                        "slug": "Jong-Sil-Lee",
                        "structuredName": {
                            "firstName": "Jong-Sil",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jong-Sil Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054349230"
                        ],
                        "name": "Ja-Myung Koo",
                        "slug": "Ja-Myung-Koo",
                        "structuredName": {
                            "firstName": "Ja-Myung",
                            "lastName": "Koo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ja-Myung Koo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "96819438"
                        ],
                        "name": "O. Kwon",
                        "slug": "O.-Kwon",
                        "structuredName": {
                            "firstName": "O.",
                            "lastName": "Kwon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kwon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41215324"
                        ],
                        "name": "S. Hong",
                        "slug": "S.-Hong",
                        "structuredName": {
                            "firstName": "Seung",
                            "lastName": "Hong",
                            "middleNames": [
                                "Hong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Studies that use vessel widt feature for tracking vessels starting from seed include [ 14 , 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62603059,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "dd1c059492d0c9fd8fae9c72f23050e344c49647",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new tracking algorithm for the main artery contours in a digital angiogram. The proposed work extracts features and profiles the narrow blood vessel, mainly the blood vessel in the digital subtraction angiography image. A consecutive value is performed on the boundary detection by calculating maximum-likelihood (ML) estimation on adjacent pixels. The proposed algorithm adaptively detects the position of the centerline as a direction vector with the entire vessel's direction field. ML estimation is most effective at profiling for a vessel's contour having anomalies and noise. This proposed algorithm is intended to support radiologists in diagnosis, radiation therapy planning, and surgical planning."
            },
            "slug": "Adaptive-tracking-algorithm-based-on-direction-ML-Park-Lee",
            "title": {
                "fragments": [],
                "text": "Adaptive tracking algorithm based on direction field using ML estimation in angiogram"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new tracking algorithm for the main artery contours in a digital angiogram that adaptively detects the position of the centerline as a direction vector with the entire vessel's direction field is presented."
            },
            "venue": {
                "fragments": [],
                "text": "TENCON '97 Brisbane - Australia. Proceedings of IEEE TENCON '97. IEEE Region 10 Annual Conference. Speech and Image Technologies for Computing and Telecommunications (Cat. No.97CH36162)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31660014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aec72293f575034117758730afe450b972d5634a",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Special-issue-on-camera-based-text-and-document-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Special issue on camera-based text and document recognition"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Detecting-text-in-natural-scenes-with-stroke-width-Epshtein-Ofek/39c4ae83b5c92e0fa55de1ec7e5cf12589c408db?sort=total-citations"
}