{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733049"
                        ],
                        "name": "Eneko Agirre",
                        "slug": "Eneko-Agirre",
                        "structuredName": {
                            "firstName": "Eneko",
                            "lastName": "Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eneko Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785173"
                        ],
                        "name": "German Rigau",
                        "slug": "German-Rigau",
                        "structuredName": {
                            "firstName": "German",
                            "lastName": "Rigau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "German Rigau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 246
                            }
                        ],
                        "text": "These vary from simple edge-counting (Rada, Mili, and Bicknell 1989) to attempts to factor in peculiarities of the network structure by considering link direction (Hirst and St-Onge 1998), relative depth (Leacock and Chodorow 1998), and density (Agirre and Rigau 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6012701,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3751d4d5e9332f6e98824a6fd814e2ce8f497daf",
            "isKey": false,
            "numCitedBy": 476,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper present a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiments have been automatically evaluted against SemCor, the sense-tagged version of the Brown Corpus."
            },
            "slug": "Word-Sense-Disambiguation-using-Conceptual-Density-Agirre-Rigau",
            "title": {
                "fragments": [],
                "text": "Word Sense Disambiguation using Conceptual Density"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose, for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121307140"
                        ],
                        "name": "Malti Patel",
                        "slug": "Malti-Patel",
                        "structuredName": {
                            "firstName": "Malti",
                            "lastName": "Patel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Malti Patel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948254"
                        ],
                        "name": "J. Bullinaria",
                        "slug": "J.-Bullinaria",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bullinaria",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bullinaria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144053576"
                        ],
                        "name": "J. Levy",
                        "slug": "J.-Levy",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Levy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 113
                            }
                        ],
                        "text": "The BNC has been used extensively in building vector space models for many tasks relevant for cognitive science (Patel, Bullinaria, and Levy 1998; McDonald 2000; McDonald and Brew 2004) and NLP (McCarthy et al. 2004; Weeds 2003; Widdows 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 214
                            }
                        ],
                        "text": "In order to allow a fair comparison, we trained the word-based model on the same corpus as the dependency-based model (the complete BNC) and selected parameters that have been considered \u201coptimal\u201d in the literature (Patel, Bullinaria, and Levy 1998; Lowe and McDonald 2000; McDonald 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 21
                            }
                        ],
                        "text": "A number of studies (Patel, Bullinaria, and Levy 1998; Levy and Bullinaria 2001; McDonald 2000) have explored the parameter space for word-based models in detail, using evaluation benchmarks such as human similarity judgments or synonymy choice tests."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14991868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "384720361a8cae1f2bbb87e05215cc58fef74ac1",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many connectionist language processing models have now reached a level of detail at which more realistic representations of semantics are required. In this paper we discuss the extraction of semantic representations from the word co-occurrence statistics of large text corpora and present a preliminary investigation into the validation and optimisation of such representations. We find that there is significantly more variation across the extraction procedures and evaluation criteria than is commonly assumed."
            },
            "slug": "Extracting-Semantic-Representations-from-Large-Text-Patel-Bullinaria",
            "title": {
                "fragments": [],
                "text": "Extracting Semantic Representations from Large Text Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "There is significantly more variation across the extraction procedures and evaluation criteria than is commonly assumed in the extraction of semantic representations from the word co-occurrence statistics of large text corpora."
            },
            "venue": {
                "fragments": [],
                "text": "NCPW"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40591835"
                        ],
                        "name": "K. Lund",
                        "slug": "K.-Lund",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50611682"
                        ],
                        "name": "C. Burgess",
                        "slug": "C.-Burgess",
                        "structuredName": {
                            "firstName": "Curt",
                            "lastName": "Burgess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burgess"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 111
                            }
                        ],
                        "text": "A large number of modeling studies in psycholinguistics have focused on simulating semantic priming phenomena (Lund and Burgess 1996; Lowe and McDonald 2000; McDonald 2000; McDonald and Brew 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 25
                            }
                        ],
                        "text": "B can be a set of words (Lund and Burgess 1996) or lemmas (McDonald 2000), words with their parts of speech (Widdows 2003) or words with a syntactic relation such as subject or object (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 4
                            }
                        ],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 77
                            }
                        ],
                        "text": "Contexts are defined as a small number of words surrounding the target word (Lund and Burgess 1996; Lowe and McDonald 2000) or as entire paragraphs\u2014even documents (Salton, Wang, and Yang 1975; Landauer and Dumais 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 73
                            }
                        ],
                        "text": "Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 61
                            }
                        ],
                        "text": "In contrast, the Hyperspace Analogue to Language model (HAL; Lund and Burgess 1996) creates a word-based semantic space: each target word t is represented by a k element vector, whose dimensions correspond to context words c1...k."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61090106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4",
            "isKey": true,
            "numCitedBy": 1721,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL)."
            },
            "slug": "Producing-high-dimensional-semantic-spaces-from-Lund-Burgess",
            "title": {
                "fragments": [],
                "text": "Producing high-dimensional semantic spaces from lexical co-occurrence"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word, which provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110878863"
                        ],
                        "name": "S. Banerjee",
                        "slug": "S.-Banerjee",
                        "structuredName": {
                            "firstName": "Satanjeev",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 190
                            }
                        ],
                        "text": "\u2026(1965) is routinely used in NLP and cognitive science for development purposes\u2014for example, for evaluating automatic measures of semantic similarity (Resnik 1995; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003) or for exploring the parameter space of vector space models (McDonald 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56684730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bddf98047d69af505a0e33643565ecec280fd1c9",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new measure of semantic relatedness between concepts that is based on the number of shared words (overlaps) in their definitions (glosses). This measure is unique in that it extends the glosses of the concepts under consideration to include the glosses of other concepts to which they are related according to a given concept hierarchy. We show that this new measure reasonably correlates to human judgments. We introduce a new method of word sense disambiguation based on extended gloss overlaps, and demonstrate that it fares well on the SENSEVAL-2 lexical sample data."
            },
            "slug": "Extended-Gloss-Overlaps-as-a-Measure-of-Semantic-Banerjee-Pedersen",
            "title": {
                "fragments": [],
                "text": "Extended Gloss Overlaps as a Measure of Semantic Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A new measure of semantic relatedness between concepts that is based on the number of shared words (overlaps) in their definitions (glosses) and reasonably correlates to human judgments is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2243033"
                        ],
                        "name": "Oliver Culo",
                        "slug": "Oliver-Culo",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Culo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oliver Culo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7965906"
                        ],
                        "name": "Sabine Schulte im Walde",
                        "slug": "Sabine-Schulte-im-Walde",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Schulte im Walde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sabine Schulte im Walde"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14795149,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "ce04b5b319a9cbcd1e4c5fae26243dd0e26b9328",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we address the task of comparing and combining different semantic verb classifications within one language. We present a methodology for the manual analysis of individual resources on the level of semantic features. The resulting representations can be aligned across resources, and allow a contrastive analysis of these resources. In a case study on the Manner of Motion domain across four German verb classifications, we find that some features are used in all resources, while others reflect individual emphases on specific meaning aspects. We also provide evidence that feature representations can ultimately provide the basis for linking verb classes themselves across resources, which allows us to combine their coverage and descriptive detail."
            },
            "slug": "Comparing-and-combining-semantic-verb-Culo-Erk",
            "title": {
                "fragments": [],
                "text": "Comparing and combining semantic verb classifications"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This article presents a methodology for the manual analysis of individual resources on the level of semantic features, and provides evidence that feature representations can ultimately provide the basis for linking verb classes themselves across resources, which allows their coverage and descriptive detail."
            },
            "venue": {
                "fragments": [],
                "text": "Lang. Resour. Evaluation"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 161
                            }
                        ],
                        "text": "In traditional vector-based models, the context\n1 For the sake of simplicity, we use R without a subscript to denote the set of dependency relations provided by MINIPAR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 45
                            }
                        ],
                        "text": "Even in cases where many relations are used (Lin 1998a; Lin and Pantel 2001), only direct relations are taken into account, ignoring potentially important co-occurrence patterns between, for instance, the subject and the object of a verb, or between a verb and its non-local argument (e.g., in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 30
                            }
                        ],
                        "text": "We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 145
                            }
                        ],
                        "text": "\u2026is the set of basis elements co-occurring with t, Jaccard\u2019s coefficient is defined as:\nsimJacc(t1, t2) = Attr(t1) \u2229 Attr(t2) Attr(t1) \u222a Attr(t2)\n(2)\nLin (1998a) constructs a semantic space similar to Grefenstette (1994) except that the matrix cells represent the number of times a target word\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 240
                            }
                        ],
                        "text": "Using (8), these paths are mapped to the following basis elements:\n\u3008lorry, carry\u3009 carry\n\u3008lorry, a\u3009 a\n\u3008lorry, carry, apples\u3009 apples\n\u3008lorry, carry, might\u3009 might\n\u3008lorry, carry, apples, sweet\u3009 sweet\nA different mapping is used in Grefenstette (1994) and Lin (1998a), who consider only paths of length 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 140
                            }
                        ],
                        "text": "\u2026vector space models have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 250
                            }
                        ],
                        "text": "\u2026typically reflect argument structure (e.g., subject, object, indirect object) or modification (e.g., adjective\u2013noun, noun\u2013noun) and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999; Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 201
                            }
                        ],
                        "text": "Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 185
                            }
                        ],
                        "text": "B can be a set of words (Lund and Burgess 1996) or lemmas (McDonald 2000), words with their parts of speech (Widdows 2003) or words with a syntactic relation such as subject or object (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 87
                            }
                        ],
                        "text": "In order to construct dependency spaces, the BNC was parsed with MINIPAR, version 0.5 (Lin 1998b, 2001), a wide-coverage dependency parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 233
                            }
                        ],
                        "text": "We describe traditional word-based co-occurrence models as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994) and Lin (1998a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 172
                            }
                        ],
                        "text": "Furthermore, the potential of syntax-based models has not been fully realized since most previous approaches limit themselves to a specific model class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 149
                            }
                        ],
                        "text": "When evaluated on the SUSANNE corpus (Sampson 1995), the parser achieved a precision of 89% and a recall of 79% in identifying labeled dependencies (Lin 1998b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MINIPAR employs a manually constructed grammar and a lexicon derived from WordNet with the addition of proper names (130,000 entries in total)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 63
                            }
                        ],
                        "text": "[Det,det,N] and [N,subj,V] are examples for labels provided by MINIPAR (see Figure 4, right-hand side)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "We base our discussion\nand experiments on the broad-coverage dependency parser MINIPAR, version 0.5 (Lin 1998a, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 218
                            }
                        ],
                        "text": "\u2026(Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 143
                            }
                        ],
                        "text": "\u2026the cosine being the most popular:\nsimcos( x, y ) =\nn\u2211 i=1\nxiyi\u221a n\u2211\ni=1 x2i\n\u221a n\u2211\ni=1 y2i\n(1)\nSyntax-based semantic space models (Grefenstette 1994; Lin 1998a) go beyond mere co-occurrence by capturing syntactic relationships between words such as subject\u2013verb or modifier\u2013noun,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MINIPAR uses a distributed chart parsing algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 6
                            }
                        ],
                        "text": "Definition 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 96
                            }
                        ],
                        "text": "For example, the construction of the space is either based on all relations (Grefenstette 1994; Lin 1998a) or a fixed subset (Lee 1999), but there is no qualitative distinction between different relations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 58
                            }
                        ],
                        "text": "Recall that we obtain dependency relations using MINIPAR (Lin 1998b), whereas McCarthy et al. employ Briscoe and Carroll\u2019s (2002) parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 51
                            }
                        ],
                        "text": "**p < 0.01 (2-tailed)\nthesauri (Grefenstette 1994; Lin 1998a; Curran and Moens 2002) or used in applications that utilize semantic similarity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 68
                            }
                        ],
                        "text": "The system can create dependency spaces from the output of MINIPAR (Lin 1998b, 2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 143
                            }
                        ],
                        "text": "The basis elements (r, w) are treated as a single unit and are often called attributes (Grefenstette 1994; Curran and Moens 2002) or features (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 97
                            }
                        ],
                        "text": "An ideal syntactic formalism should abstract over surface word order, mirror semantic relationships as closely as possible, and incorporate word-based information in addition to syntactic analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 44
                            }
                        ],
                        "text": "Examples include word sense discrimination (Lin 1998a; Sch\u00fctze 1998), automatic thesaurus construction (Grefenstette 1994; Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general similarity-based approaches to NLP."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "\u2026does not exceed the window size k:\ncont(t) = {\u03c0 \u2208 \u03a0t | abs(pos(start(\u03c0)) \u2212 pos(end(\u03c0))) \u2264 k} (5)\nThe dependency-based models proposed by Grefenstette (1994) and Lin (1998a) consider minimal syntactic contexts in the form of individual dependency relations, namely, dependency paths of length\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 38
                            }
                        ],
                        "text": "1998b. Dependency-based evaluation of MINIPAR."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15698938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd1901f34cc3673072264104885d70555b1a4cdc",
            "isKey": true,
            "numCitedBy": 1928,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is."
            },
            "slug": "Automatic-Retrieval-and-Clustering-of-Similar-Words-Lin",
            "title": {
                "fragments": [],
                "text": "Automatic Retrieval and Clustering of Similar Words"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A word similarity measure based on the distributional pattern of words allows the automatically constructed thesaurus to be significantly closer to WordNet than Roget Thesaurus is."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 124
                            }
                        ],
                        "text": "The central assumption here is that the context surrounding a given word provides important information about its meaning (Harris 1968)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Sahlgren (2006) uses Random Indexing, a method comparable to LSA, to represent the meaning of words and reports a 75.0% accuracy on the same TOEFL items."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 63
                            }
                        ],
                        "text": "11 We omit LSA (Landauer and Dumais 1997) and Random indexing (Sahlgren 2006) from our comparison, because these models were not evaluated on unseen data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 147
                            }
                        ],
                        "text": "These are routinely used as a testbed for assessing how well vector-based models capture lexical knowledge (Landauer and Dumais 1997; Turney 2001; Sahlgren 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11917163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1521ddb27860cc8834f8a82e62665bf983c8ad2c",
            "isKey": true,
            "numCitedBy": 600,
            "numCiting": 166,
            "paperAbstract": {
                "fragments": [],
                "text": "The word-space model is a computational model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in ter ..."
            },
            "slug": "The-Word-Space-Model-:-Using-distributional-to-and-Sahlgren",
            "title": {
                "fragments": [],
                "text": "The Word-Space Model : Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "The word-space model is a computational model of word meaning that utilizes the distributional patterns of words collected over large text data to represent semantic similarity between words in terabytes of data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 238
                            }
                        ],
                        "text": "Although replicating our study with Briscoe and Carroll\u2019s parser (2002) is outside of the scope of this article, we should note that the two parsers yield comparable performances and employ a similar inventory of dependency relations (see Curran (2004) for more discussion)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 284
                            }
                        ],
                        "text": "\u2026typically reflect argument structure (e.g., subject, object, indirect object) or modification (e.g., adjective\u2013noun, noun\u2013noun) and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999; Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 227290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e",
            "isKey": false,
            "numCitedBy": 296,
            "numCiting": 235,
            "paperAbstract": {
                "fragments": [],
                "text": "Lexical-semantic resources, including thesauri and WORDNET, have been successfully incorporated into a wide range of applications in Natural Language Processing. However they are very difficult and expensive to create and maintain, and their usefulness has been severely hampered by their limited coverage, bias and inconsistency. Automated and semi-automated methods for developing such resources are therefore crucial for further resource development and improved application performance. Systems that extract thesauri often identify similar words using the distributional hypothesis that similar words appear in similar contexts. This approach involves using corpora to examine the contexts each word appears in and then calculating the similarity between context distributions. Different definitions of context can be used, and I begin by examining how different types of extracted context influence similarity. To be of most benefit these systems must be capable of finding synonyms for rare words. Reliable context counts for rare events can only be extracted from vast collections of text. In this dissertation I describe how to extract contexts from a corpus of over 2 billion words. I describe techniques for processing text on this scale and examine the trade-off between context accuracy, information content and quantity of text analysed. Distributional similarity is at best an approximation to semantic similarity. I develop improved approximations motivated by the intuition that some events in the context distribution are more indicative of meaning than others. For instance, the object-of-verb context wear is far more indicative of a clothing noun than get. However, existing distributional techniques do not effectively utilise this information. The new context-weighted similarity metric I propose in this dissertation significantly outperforms every distributional similarity metric described in the literature. Nearest-neighbour similarity algorithms scale poorly with vocabulary and context vector size. To overcome this problem I introduce a new context-weighted approximation algorithm with bounded complexity in context vector size that significantly reduces the system runtime with only a minor performance penalty. I also describe a parallelized version of the system that runs on a Beowulf cluster for the 2 billion word experiments. To evaluate the context-weighted similarity measure I compare ranked similarity lists against gold-standard resources using precision and recall-based measures from Information Retrieval,"
            },
            "slug": "From-distributional-to-semantic-similarity-Curran",
            "title": {
                "fragments": [],
                "text": "From distributional to semantic similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This dissertation describes how to extract contexts from a corpus of over 2 billion words and introduces a new context-weighted approximation algorithm with bounded complexity in context vector size that significantly reduces the system runtime with only a minor performance penalty."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085030"
                        ],
                        "name": "M. Moens",
                        "slug": "M.-Moens",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Moens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 210
                            }
                        ],
                        "text": "\u2026typically reflect argument structure (e.g., subject, object, indirect object) or modification (e.g., adjective\u2013noun, noun\u2013noun) and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999; Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 154
                            }
                        ],
                        "text": "Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 204
                            }
                        ],
                        "text": "Furthermore, the potential of syntax-based models has not been fully realized since most previous approaches limit themselves to a specific model class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 71
                            }
                        ],
                        "text": "A replication of Grefenstette\u2019s study with a more sophisticated parser (Curran and Moens 2002) revealed that additional syntactic information yields further improvements."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 38
                            }
                        ],
                        "text": "For example, we may choose to consider only paths of length 1, or paths with length \u2265 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "**p < 0.01 (2-tailed)\nthesauri (Grefenstette 1994; Lin 1998a; Curran and Moens 2002) or used in applications that utilize semantic similarity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 107
                            }
                        ],
                        "text": "The basis elements (r, w) are treated as a single unit and are often called attributes (Grefenstette 1994; Curran and Moens 2002) or features (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 123
                            }
                        ],
                        "text": "Examples include word sense discrimination (Lin 1998a; Sch\u00fctze 1998), automatic thesaurus construction (Grefenstette 1994; Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general similarity-based approaches to NLP."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10782902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8066c5522ebfa8f0f08589dcbe5f315bfec90c1",
            "isKey": true,
            "numCitedBy": 87,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Context is used in many NLP systems as an indicator of a term's syntactic and semantic function. The accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term. However, the quantity variable is no longer fixed by limited corpus resources. Given fixed training time and computational resources, it makes sense for systems to invest time in extracting high quality contextual information from a fixed corpus. However, with an effectively limitless quantity of text available, extraction rate and representation size need to be considered. We use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity, time and size on a corpus of 300 million words."
            },
            "slug": "Scaling-Context-Space-Curran-Moens",
            "title": {
                "fragments": [],
                "text": "Scaling Context Space"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work uses thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity, time and size on a corpus of 300 million words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 159
                            }
                        ],
                        "text": "For example, we could experiment with more coarse-grained functions based on parts-of-speech or more fine-grained ones such as the relation-word pairs used by McCarthy et al. (2004). We would also like to observe the impact of singular value decomposition (SVD) on our semantic spaces along the lines of Kanejiya et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52800448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "084c55d6432265785e3ff86a2e900a49d501c00a",
            "isKey": false,
            "numCitedBy": 7802,
            "numCiting": 294,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications."
            },
            "slug": "Foundations-of-statistical-natural-language-Manning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Foundations of statistical natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear and provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708425"
                        ],
                        "name": "Jay J. Jiang",
                        "slug": "Jay-J.-Jiang",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Jiang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jay J. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075147"
                        ],
                        "name": "D. Conrath",
                        "slug": "D.-Conrath",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Conrath",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Conrath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 112
                            }
                        ],
                        "text": "A number of hybrid approaches have also been proposed that combine WordNet with corpus statistics (Resnik 1995; Jiang and Conrath 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1359050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b64e068a8face2540fc436af40dbcd2b0912bbf",
            "isKey": false,
            "numCitedBy": 3339,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task."
            },
            "slug": "Semantic-Similarity-Based-on-Corpus-Statistics-and-Jiang-Conrath",
            "title": {
                "fragments": [],
                "text": "Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts that combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data."
            },
            "venue": {
                "fragments": [],
                "text": "ROCLING/IJCLCLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 161
                            }
                        ],
                        "text": "In traditional vector-based models, the context\n1 For the sake of simplicity, we use R without a subscript to denote the set of dependency relations provided by MINIPAR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 45
                            }
                        ],
                        "text": "Even in cases where many relations are used (Lin 1998a; Lin and Pantel 2001), only direct relations are taken into account, ignoring potentially important co-occurrence patterns between, for instance, the subject and the object of a verb, or between a verb and its non-local argument (e.g., in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 145
                            }
                        ],
                        "text": "\u2026is the set of basis elements co-occurring with t, Jaccard\u2019s coefficient is defined as:\nsimJacc(t1, t2) = Attr(t1) \u2229 Attr(t2) Attr(t1) \u222a Attr(t2)\n(2)\nLin (1998a) constructs a semantic space similar to Grefenstette (1994) except that the matrix cells represent the number of times a target word\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 240
                            }
                        ],
                        "text": "Using (8), these paths are mapped to the following basis elements:\n\u3008lorry, carry\u3009 carry\n\u3008lorry, a\u3009 a\n\u3008lorry, carry, apples\u3009 apples\n\u3008lorry, carry, might\u3009 might\n\u3008lorry, carry, apples, sweet\u3009 sweet\nA different mapping is used in Grefenstette (1994) and Lin (1998a), who consider only paths of length 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 140
                            }
                        ],
                        "text": "\u2026vector space models have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 250
                            }
                        ],
                        "text": "\u2026typically reflect argument structure (e.g., subject, object, indirect object) or modification (e.g., adjective\u2013noun, noun\u2013noun) and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999; Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 201
                            }
                        ],
                        "text": "Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 185
                            }
                        ],
                        "text": "B can be a set of words (Lund and Burgess 1996) or lemmas (McDonald 2000), words with their parts of speech (Widdows 2003) or words with a syntactic relation such as subject or object (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 87
                            }
                        ],
                        "text": "In order to construct dependency spaces, the BNC was parsed with MINIPAR, version 0.5 (Lin 1998b, 2001), a wide-coverage dependency parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 233
                            }
                        ],
                        "text": "We describe traditional word-based co-occurrence models as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994) and Lin (1998a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 172
                            }
                        ],
                        "text": "Furthermore, the potential of syntax-based models has not been fully realized since most previous approaches limit themselves to a specific model class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 149
                            }
                        ],
                        "text": "When evaluated on the SUSANNE corpus (Sampson 1995), the parser achieved a precision of 89% and a recall of 79% in identifying labeled dependencies (Lin 1998b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MINIPAR employs a manually constructed grammar and a lexicon derived from WordNet with the addition of proper names (130,000 entries in total)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 63
                            }
                        ],
                        "text": "[Det,det,N] and [N,subj,V] are examples for labels provided by MINIPAR (see Figure 4, right-hand side)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "We base our discussion\nand experiments on the broad-coverage dependency parser MINIPAR, version 0.5 (Lin 1998a, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 218
                            }
                        ],
                        "text": "\u2026(Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 143
                            }
                        ],
                        "text": "\u2026the cosine being the most popular:\nsimcos( x, y ) =\nn\u2211 i=1\nxiyi\u221a n\u2211\ni=1 x2i\n\u221a n\u2211\ni=1 y2i\n(1)\nSyntax-based semantic space models (Grefenstette 1994; Lin 1998a) go beyond mere co-occurrence by capturing syntactic relationships between words such as subject\u2013verb or modifier\u2013noun,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MINIPAR uses a distributed chart parsing algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 96
                            }
                        ],
                        "text": "For example, the construction of the space is either based on all relations (Grefenstette 1994; Lin 1998a) or a fixed subset (Lee 1999), but there is no qualitative distinction between different relations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 58
                            }
                        ],
                        "text": "Recall that we obtain dependency relations using MINIPAR (Lin 1998b), whereas McCarthy et al. employ Briscoe and Carroll\u2019s (2002) parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 51
                            }
                        ],
                        "text": "**p < 0.01 (2-tailed)\nthesauri (Grefenstette 1994; Lin 1998a; Curran and Moens 2002) or used in applications that utilize semantic similarity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 68
                            }
                        ],
                        "text": "The system can create dependency spaces from the output of MINIPAR (Lin 1998b, 2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 143
                            }
                        ],
                        "text": "The basis elements (r, w) are treated as a single unit and are often called attributes (Grefenstette 1994; Curran and Moens 2002) or features (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 44
                            }
                        ],
                        "text": "Examples include word sense discrimination (Lin 1998a; Sch\u00fctze 1998), automatic thesaurus construction (Grefenstette 1994; Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general similarity-based approaches to NLP."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "\u2026does not exceed the window size k:\ncont(t) = {\u03c0 \u2208 \u03a0t | abs(pos(start(\u03c0)) \u2212 pos(end(\u03c0))) \u2264 k} (5)\nThe dependency-based models proposed by Grefenstette (1994) and Lin (1998a) consider minimal syntactic contexts in the form of individual dependency relations, namely, dependency paths of length\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 38
                            }
                        ],
                        "text": "1998b. Dependency-based evaluation of MINIPAR."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59702881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99328d4b34d1ac02252258a9437b8b2c1acdb92c",
            "isKey": true,
            "numCitedBy": 773,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we first present a dependency-based method for parser evaluation. We then use the method to evaluate a broad-coverage parser, called MINIPAR, with the SUSANNE corpus. The method allows us to evaluate not only the overall performance of the parser, but also its performance with respect to different grammatical relationships and phenomena. The evaluation results show that MINIPAR is able to cover about 79% of the dependency relationships in the SUSANNE corpus with about 89% precision."
            },
            "slug": "Dependency-Based-Evaluation-of-Minipar-Lin",
            "title": {
                "fragments": [],
                "text": "Dependency-Based Evaluation of Minipar"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A dependency-based method for parser evaluation is presented and a broad-coverage parser, called MINIPAR, is evaluated with the SUSANNE corpus."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7965906"
                        ],
                        "name": "Sabine Schulte im Walde",
                        "slug": "Sabine-Schulte-im-Walde",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Schulte im Walde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sabine Schulte im Walde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2292034"
                        ],
                        "name": "Alissa Melinger",
                        "slug": "Alissa-Melinger",
                        "structuredName": {
                            "firstName": "Alissa",
                            "lastName": "Melinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alissa Melinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46617131"
                        ],
                        "name": "Michael Roth",
                        "slug": "Michael-Roth",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40289509"
                        ],
                        "name": "A. Weber",
                        "slug": "A.-Weber",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60968097,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cb00a7aa80770307c7ff93a622caf72f368d81c1",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a study to distinguish and quantify the various types of semantic associations provided by humans, to investigate their properties, and to discuss the impact that our analyses may have on NLP tasks. Specifically, we concentrate on two issues related to word properties and word relations: (1) We address the task of modelling word meaning by empirical features in data-intensive lexical semantics. Relying on large-scale corpus-based resources, we identify the contextual categories and functions that are activated by the associates and therefore contribute to the salient meaning components of individual words and across words. As a result, we discuss conceptual roles and present evidence for the usefulness of co-occurrence information in distributional descriptions. (2) We assume that semantic associates provide a means to investigate the range of semantic relations between words and contexts, and we provide insight into which types of semantic relations are treated as important or salient by the speakers of the language."
            },
            "slug": "An-Empirical-Characterisation-of-Response-Types-in-Walde-Melinger",
            "title": {
                "fragments": [],
                "text": "An Empirical Characterisation of Response Types in German Association Norms"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This article presents a study to distinguish and quantify the various types of semantic associations provided by humans, to investigate their properties, and to discuss the impact that their analyses may have on NLP tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400286782"
                        ],
                        "name": "P. Wiemer-Hastings",
                        "slug": "P.-Wiemer-Hastings",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Wiemer-Hastings",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wiemer-Hastings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331564"
                        ],
                        "name": "Iraide Zipitria",
                        "slug": "Iraide-Zipitria",
                        "structuredName": {
                            "firstName": "Iraide",
                            "lastName": "Zipitria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iraide Zipitria"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 0
                            }
                        ],
                        "text": "Wiemer-Hastings and Zipitria (2001) construct a semantic space similar to LSA, but enhanced with part-of-speech tags with the aim of modeling human raters in an intelligent tutoring context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "where lso(s1, s2) is the lowest super-ordinate (most specific common subsumer) of synsets (that is, senses) s1 and s2. We used the WordNet Similarity Package (Pedersen, Patwardhan, and Michelizzi 2004), which provides an implementation of Jiang and Conrath\u2019s (1997) measure (version 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 764162,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4c46ac28ac0a23af428437afcb443730c5c3b58",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Rules for Syntax, Vectors for Semantics Peter Wiemer-Hastings (Peter.Wiemer-Hastings@ed.ac.uk) Iraide Zipitria (iraidez@cogsci.ed.ac.uk) University of Edinburgh Division of Informatics 2 Buccleuch Place Edinburgh EH8 9LW Scotland Abstract Latent Semantic Analysis (LSA) has been shown to perform many linguistic tasks as well as humans do, and has been put forward as a model of human linguistic competence. But LSA pays no attention to word order, much less sentence structure. Researchers in Natural Language Processing have made significant progress in quickly and accurately deriv- ing the syntactic structure of texts. But there is little agree- ment on how best to represent meaning, and the representa- tions are brittle and difficult to build. This paper evaluates a model of language understanding that combines information from rule-based syntactic processing with a vector-based se- mantic representation which is learned from a corpus. The model is evaluated as a cognitive model, and as a potential technique for natural language understanding. Motivations Latent Semantic Analysis (LSA) was originally developed for the task of information retrieval, selecting a text which matches a query from a large database (Deerwester, Du- mais, Furnas, Landauer, & Harshman, 1990) 1 . More re- cently, LSA has been evaluated by psychologists as a model for human lexical acquisition (Landauer & Dumais, 1997). It has been applied to other textual tasks and found to gen- erally perform at levels matching human performance. All this despite the fact that LSA pays no attention to word or- der, let alone syntax. This led Landauer to claim that syntax apparently has no contribution to the meaning of a sentence, and may only serve as a working memory crutch for sen- tence processing, or in a stylistic role (Landauer, Laham, Rehder, & Schreiner, 1997). The tasks that LSA has been shown to perform well on can be separated into two groups: those that deal with sin- gle words and those that deal with longer texts. For exam- ple, on the synonym selection part of the TOEFL (Test of English as a Foreign Language), LSA was as accurate at choosing the correct synonym (out of 4 choices) as were successful foreign applicants to US universities (Landauer et al., 1997). For longer texts, Rehder et al (1998) showed that for evaluating author knowledge, LSA does steadily worse for texts shorter than 200 words. More specifically, 1 We do not describe the functioning of the LSA mechanism here. For a complete description, see (Deerwester et al., 1990; Landauer & Dumais, 1997) for 200-word essay segments, LSA accounted for 60% of the variance in human scores. For 60-word essay segments, LSA scores accounted for only 10% of the variance. In work on judging the quality of single-sentence student answers in an intelligent tutoring context, we have shown in previous work that although LSA nears the performance of intermediate-knowledge human raters, it lags far behind expert performance (Wiemer-Hastings, Wiemer-Hastings, & Graesser, 1999b). Furthermore, when we compared LSA to a keyword-based approach, LSA performed only marginally better (Wiemer-Hastings, Wiemer-Hastings, & Graesser, 1999a). This accords with unpublished results on short answer sentences from Walter Kintsch, personal com- munication, January 1999. In the field of Natural Language Processing, the eras of excessive optimism and ensuing disappointment have been followed by study increases in the systems\u2019 ability to pro- cess the syntactic structure of texts with rule-based mecha- nisms. The biggest recent developments have been due to the augmentation of the rules with corpus-derived proba- bilities for when they should be applied (Charniak, 1997; Collins, 1996, 1998, for example). Unfortunately, progress in the area of computing the se- mantic content of texts has not been so successful. Two ba- sic variants of semantic theories have been developed. One is based on some form of logic. The other is represented by connections within semantic networks. In fact, the latter can be simply converted into a logic-based representation. Such theories are brittle in two ways. First, they require every concept and every connection between concepts to be defined by a human knowledge engineer. Multi-purpose representations are not feasible because of the many techni- cal senses of words in every different domain. Second, such representations can not naturally make the graded judge- ments that humans do. Humans can compare any two things (even apples and oranges!), but aside from count- ing feature overlap, logic-based representations have diffi- culty with relationships other than subsumption and \u201chas- as-part\u201d. Due to these various motivations, we are pursuing a two- pronged research project. First, we want to evaluate the combination of a syntactic processing mechanism with an LSA-based semantic representation as a cognitive model of human sentence similarity judgements. Second, we are"
            },
            "slug": "Rules-for-Syntax,-Vectors-for-Semantics-Wiemer-Hastings-Zipitria",
            "title": {
                "fragments": [],
                "text": "Rules for Syntax, Vectors for Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A model of language understanding that combines information from rule-based syntactic processing with a vector-based se- mantic representation which is learned from a corpus is evaluated as a cognitive model, and as a potential technique for natural language understanding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47473549"
                        ],
                        "name": "Colin Bannard",
                        "slug": "Colin-Bannard",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Bannard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Bannard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145465286"
                        ],
                        "name": "Timothy Baldwin",
                        "slug": "Timothy-Baldwin",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Baldwin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Baldwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876168"
                        ],
                        "name": "A. Lascarides",
                        "slug": "A.-Lascarides",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Lascarides",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lascarides"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 175
                            }
                        ],
                        "text": "Central to the construction process is the notion of paths, namely sequences of dependency edges extracted from the dependency parse of a sentence (we define paths formally in Section 3.2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 219
                            }
                        ],
                        "text": "\u2026have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and Carroll 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2356182,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "7feb6ba5666a5b106c5c141c4356587164d15614",
            "isKey": true,
            "numCitedBy": 138,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a distributional approach to the semantics of verb-particle constructions (e.g. put up, make off). We report first on a framework for implementing and evaluating such models. We then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb-particle constructions."
            },
            "slug": "A-Statistical-Approach-to-the-Semantics-of-Bannard-Baldwin",
            "title": {
                "fragments": [],
                "text": "A Statistical Approach to the Semantics of Verb-Particles"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A distributional approach to the semantics of verb-particle constructions (e.g. put up, make off) is described and some techniques for using statistical models acquired from corpus data to infer the meaning of verbs are reported on."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30663130"
                        ],
                        "name": "E. Dura",
                        "slug": "E.-Dura",
                        "structuredName": {
                            "firstName": "Elzbieta",
                            "lastName": "Dura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Dura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 94
                            }
                        ],
                        "text": "The path value function v assigns weights to paths, thus allowing linguistic knowledge to influence the construction of the space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5598004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1251807ffb08f881a6cb67c2ec6eb322206bfaf",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "It seems the time is ripe for the two to meet: NLP has grown out of prototypes and IR is having hard time trying to improve precision. Two examples of possible approaches are considered below. Lexware is a lexicon-based system for text analysis of Swedish applied in an information retrieval task. NLIR is an information retrieval system using intensive natural language processing to provide index terms on a higher level of abstraction than stems."
            },
            "slug": "Natural-Language-in-Information-Retrieval-Dura",
            "title": {
                "fragments": [],
                "text": "Natural Language in Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The time is ripe for the two to meet: NLP has grown out of prototypes and IR is having hard time trying to improve precision, so two examples of possible approaches are considered."
            },
            "venue": {
                "fragments": [],
                "text": "CICLing"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "The use of tokens may be appropriate for other applications such as word sense discrimination (Sch\u00fctze 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 55
                            }
                        ],
                        "text": "Examples include word sense discrimination (Lin 1998a; Sch\u00fctze 1998), automatic thesaurus construction (Grefenstette 1994; Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general similarity-based approaches to NLP."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 145
                            }
                        ],
                        "text": "\u2026a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 198
                            }
                        ],
                        "text": "Dependency-Based Construction of Semantic Space Models\nSebastian Pad\u00f3\u2217 Saarland University\nMirella Lapata\u2217\u2217 University of Edinburgh\nTraditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8754851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c",
            "isKey": true,
            "numCitedBy": 1401,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words."
            },
            "slug": "Automatic-Word-Sense-Discrimination-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Automatic Word Sense Discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering that demonstrates good performance of context- group discrimination for a sample of natural and artificial ambiguous words."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2299876"
                        ],
                        "name": "R. Koeling",
                        "slug": "R.-Koeling",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Koeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koeling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586618"
                        ],
                        "name": "Diana McCarthy",
                        "slug": "Diana-McCarthy",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "McCarthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana McCarthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 142
                            }
                        ],
                        "text": "They argue that the latter measure is more efficient for large scale WSD and use it exclusively in all subsequent work (McCarthy et al. 2004; Koeling, McCarthy, and Carroll 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16737045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a859c95269fdfe81b1a4ebf1f8255bfdbabad25a",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Distributions of the senses of words are often highly skewed. This fact is exploited by word sense disambiguation (WSD) systems which back off to the predominant sense of a word when contextual clues are not strong enough. The domain of a document has a strong influence on the sense distribution of words, but it is not feasible to produce large manually annotated corpora for every domain of interest. In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words. We apply an existing method for acquiring predominant sense information automatically from raw text, and for our sample demonstrate that (1) acquiring such information automatically from a mixed-domain corpus is more accurate than deriving it from SemCor, and (2) acquiring it automatically from text in the same domain as the target domain performs best by a large margin. We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain, and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus."
            },
            "slug": "Domain-Specific-Sense-Distributions-and-Predominant-Koeling-McCarthy",
            "title": {
                "fragments": [],
                "text": "Domain-Specific Sense Distributions and Predominant Sense Acquisition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes the construction of three sense annotated corpora in different domains for a sample of English words and demonstrates that acquiring predominant sense information automatically from a mixed-domain corpus is more accurate than deriving it from SemCor."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095398"
                        ],
                        "name": "Alexander Budanitsky",
                        "slug": "Alexander-Budanitsky",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Budanitsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Budanitsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40770371"
                        ],
                        "name": "K. Alcock",
                        "slug": "K.-Alcock",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Alcock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Alcock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072593976"
                        ],
                        "name": "Jiang\u2014 Conrath",
                        "slug": "Jiang\u2014-Conrath",
                        "structuredName": {
                            "firstName": "Jiang\u2014",
                            "lastName": "Conrath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang\u2014 Conrath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14764558,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6db4e86e6377cd703aaaf3a3b471b62e033757ae",
            "isKey": false,
            "numCitedBy": 916,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Five different proposed measures of similarity or semantic distance in WordNet were experimentally compared by examining their performance in a real-word spelling correction system. It was found that Jiang and Conrath\u2019s measure gave the best results overall. That of Hirst and St-Onge seriously over-related, that of Resnik seriously under-related, and those of Lin and of Leacock and Chodorow fell in between."
            },
            "slug": "Semantic-distance-in-WordNet:-An-experimental,-of-Budanitsky-Hirst",
            "title": {
                "fragments": [],
                "text": "Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Five different proposed measures of similarity or semantic distance in WordNet were experimentally compared by examining their performance in a real-word spelling correction system and found that Jiang and Conrath\u2019s measure gave the best results overall."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586618"
                        ],
                        "name": "Diana McCarthy",
                        "slug": "Diana-McCarthy",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "McCarthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana McCarthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39877126"
                        ],
                        "name": "B. Keller",
                        "slug": "B.-Keller",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 258
                            }
                        ],
                        "text": "\u2026have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and Carroll 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 24
                            }
                        ],
                        "text": "Consider again the graph in Figure 4."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13907505,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "b4b8e1077c7aa34456cfd811a881d1a8e677c321",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser. We examine various measures using the nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set. We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus."
            },
            "slug": "Detecting-a-Continuum-of-Compositionality-in-Verbs-McCarthy-Keller",
            "title": {
                "fragments": [],
                "text": "Detecting a Continuum of Compositionality in Phrasal Verbs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399394776"
                        ],
                        "name": "D. St-Onge",
                        "slug": "D.-St-Onge",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "St-Onge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. St-Onge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14394781,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "be3f6b91b0eae23e37b3fb877b6cc7fc7dfcef5a",
            "isKey": false,
            "numCitedBy": 791,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language utterances are, in general, highlyambiguous, and a unique interpretationcan usuallybe determined only by taking into account the constraining in\ufb02uence of the context in which theutterance occurred. Much of the research in natural language understanding in the last twenty yearscan be thought of as attempts to characterize and represent context and then derive interpretationsthat\ufb01t best with that context. Typically, this research was heavy with AI, taking context to be nothing lessthan a complete conceptual understanding of the preceding utterances. This was reasonable, as suchan understanding of a text was often the main task anyway. However, there are many text-processingtasksthatrequireonlya partialunderstandingofthetext, andhencea \u2018lighter\u2019representationofcontextis suf\ufb01cient. In this paper, we examine the idea oflexical chains as such a representation. We showhow they can be constructed by means of WordNet, and how they can be applied in one particularlinguistic task: the detection and correction of malapropisms.A malapropism is the confounding of an intended word with another word of similar sound orsimilar spelling that has a quite different and malapropos meaning, e.g., an ingenuous [for ingenious]machine forpeelingoranges. In thisexample, there isaone-letterdifference betweenthe malapropismand the correct word. Ignorance, or a simple typing mistake, might cause such errors. However, sinceingenuous is a correctly spelled word, traditional spelling checkers cannot detect this kind of mistake.In section 4, we will propose an algorithm for detecting and correcting malapropisms that is based onthe construction of lexical chains."
            },
            "slug": "Lexical-chains-as-representations-of-context-for-of-Hirst-St-Onge",
            "title": {
                "fragments": [],
                "text": "Lexical chains as representations of context for the detection and correction of malapropisms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "How lexical chains can be constructed by means of WordNet, and how they can be applied in one particularlinguistic task: the detection and correction of malapropisms is shown."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 63
                            }
                        ],
                        "text": "Any other parser with broadly similar dependency output (e.g., Briscoe and Carroll 2002) could serve our purposes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 52
                            }
                        ],
                        "text": "As an example, the set of paths anchored at lorry in Figure 4 is:\n{\u3008lorry, carry\u3009, \u3008lorry, a\u3009, (two paths of length 1)\n\u3008lorry, carry, apples\u3009, \u3008lorry, carry, might\u3009, (two paths of length 2)\n\u3008lorry, carry, apples, sweet\u3009} (one path of length 3)\nDefinition 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 146
                            }
                        ],
                        "text": "Furthermore, they used a slightly smaller corpus (only the written part of the BNC, amounting to 90% of the total corpus) and a different parser (Briscoe and Carroll 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5823614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8fe99b6dc76dc342fe9fb47740fee40381fa13d",
            "isKey": true,
            "numCitedBy": 326,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a robust accurate domain-independent approach to statistical parsing incorporated into the new release of the ANLT toolkit, and publicly available as a research tool. The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts; it has also been used as a component in an open-domain question answering project. The performance of the system is competitive with that of statistical parsers using highly lexicalised parse selection models. However, we plan to extend the system to improve parse coverage, depth and accuracy."
            },
            "slug": "Robust-Accurate-Statistical-Annotation-of-General-Briscoe-Carroll",
            "title": {
                "fragments": [],
                "text": "Robust Accurate Statistical Annotation of General Text"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A robust accurate domain-independent approach to statistical parsing incorporated into the new release of the ANLT toolkit, and publicly available as a research tool."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984521"
                        ],
                        "name": "Siddharth Patwardhan",
                        "slug": "Siddharth-Patwardhan",
                        "structuredName": {
                            "firstName": "Siddharth",
                            "lastName": "Patwardhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siddharth Patwardhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3075310"
                        ],
                        "name": "Jason Michelizzi",
                        "slug": "Jason-Michelizzi",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Michelizzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Michelizzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 40
                            }
                        ],
                        "text": "We used the WordNet Similarity Package (Pedersen, Patwardhan, and Michelizzi 2004), which provides an implementation of Jiang and Conrath\u2019s (1997) measure (version 0.06).13 We re-estimated the IC counts from the BNC because those provided with the package are derived from the manually annotated\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 12
                            }
                        ],
                        "text": "We used the WordNet Similarity Package (Pedersen, Patwardhan, and Michelizzi 2004), which provides an implementation of Jiang and Conrath\u2019s (1997) measure (version 0.06).13 We re-estimated the IC counts from the BNC because those provided with the package are derived from the manually annotated SemCor and would positively bias our results."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1499545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "495f3405da229b903797472c64d09d83659fdb34",
            "isKey": true,
            "numCitedBy": 1783,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related."
            },
            "slug": "WordNet::Similarity-Measuring-the-Relatedness-of-Pedersen-Patwardhan",
            "title": {
                "fragments": [],
                "text": "WordNet::Similarity - Measuring the Relatedness of Concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets)."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 229
                            }
                        ],
                        "text": "The BNC has been used extensively in building vector space models for many tasks relevant for cognitive science (Patel, Bullinaria, and Levy 1998; McDonald 2000; McDonald and Brew 2004) and NLP (McCarthy et al. 2004; Weeds 2003; Widdows 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 109
                            }
                        ],
                        "text": "B can be a set of words (Lund and Burgess 1996) or lemmas (McDonald 2000), words with their parts of speech (Widdows 2003) or words with a syntactic relation such as subject or object (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6987562,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c2195bdf698caba4c3b9753cb0ce5acddff5b73",
            "isKey": true,
            "numCitedBy": 155,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an unsupervised algorithm for placing unknown words into a taxonomy and evaluates its accuracy on a large and varied sample of words. The algorithm works by first using a large corpus to find semantic neighbors of the unknown word, which we accomplish by combining latent semantic analysis with part-of-speech information. We then place the unknown word in the part of the taxonomy where these neighbors are most concentrated, using a class-labelling algorithm developed especially for this task. This method is used to reconstruct parts of the existing Word-Net database, obtaining results for common nouns, proper nouns and verbs. We evaluate the contribution made by part-of-speech tagging and show that automatic filtering using the class-labelling algorithm gives a fourfold improvement in accuracy."
            },
            "slug": "Unsupervised-methods-for-developing-taxonomies-by-Widdows",
            "title": {
                "fragments": [],
                "text": "Unsupervised methods for developing taxonomies by combining syntactic and statistical information"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An unsupervised algorithm for placing unknown words into a taxonomy and its accuracy on a large and varied sample of words is evaluated and it is shown that automatic filtering using the class-labelling algorithm gives a fourfold improvement in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586618"
                        ],
                        "name": "Diana McCarthy",
                        "slug": "Diana-McCarthy",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "McCarthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana McCarthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2299876"
                        ],
                        "name": "R. Koeling",
                        "slug": "R.-Koeling",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Koeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koeling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2500077"
                        ],
                        "name": "Julie Weeds",
                        "slug": "Julie-Weeds",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Weeds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julie Weeds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 161
                            }
                        ],
                        "text": "Our following experiment applies the dependency space introduced in this article to word sense disambiguation (WSD), a task which has received much attention in NLP and is ultimately important for document understanding."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "We automatically infer predominant senses in untagged text by incorporating our syntax-based semantic spaces into the modeling paradigm proposed by McCarthy et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "In order to demonstrate the scope of our framework, we evaluate our models on tasks popular in both cognitive science and NLP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 299
                            }
                        ],
                        "text": "Our contributions are threefold: a novel framework for semantic spaces that incorporates syntactic information in the form of dependency relations and generalizes previous syntax-based vector-based models; an application of this framework to a wide range of tasks relevant to cognitive modeling and NLP; and an empirical comparison of our dependency-based models against state-of-the-art word-based models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "McCarthy et al. (2004) undertook a thorough comparison and obtained best results with 50 neighbors using Lesk\u2019s (1986) and Jiang and Conrath\u2019s (1997) measures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "Given the importance of WSD for basic NLP tasks and multilingual applications, a variety of approaches have been proposed for disambiguating word senses."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 49
                            }
                        ],
                        "text": "Furthermore, both the dependency-based model and McCarthy et al. (2004) significantly outperform the word-based model."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Despite their widespread use in NLP, syntax-based semantic spaces have attracted little attention in cognitive science and computational psycholinguistics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 195
                            }
                        ],
                        "text": "The BNC has been used extensively in building vector space models for many tasks relevant for cognitive science (Patel, Bullinaria, and Levy 1998; McDonald 2000; McDonald and Brew 2004) and NLP (McCarthy et al. 2004; Weeds 2003; Widdows 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 170
                            }
                        ],
                        "text": "Failure to do so would indicate that our model is deficient because it cannot capture basic semantic relatedness, a notion underlying many tasks in cognitive science and NLP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 181
                            }
                        ],
                        "text": "In this experiment, we examine whether the dependency-based models discussed in this article can be used for the sense ranking task, thereby assessing their potential for practical NLP tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "The potential applications are many and varied both for cognitive science and NLP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "We evaluated our framework on tasks relevant for NLP and cognitive science and compared it against state-of-the-art models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "Syntax-based vector space models have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and Carroll 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 149
                            }
                        ],
                        "text": "\u2026a sense at random for each word type from its sense inventory and assuming that this is the first sense:\nRandomsr = 1|Wps| \u2211\nw \u2208Wps\n1 |S(w)| (22)\nLike McCarthy et al. (2004), we also assessed the word sense disambiguation potential (Accwsd) of the automatically acquired first senses for each word\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 158
                            }
                        ],
                        "text": "For example, we could experiment with more coarse-grained functions based on parts-of-speech or more fine-grained ones such as the relation\u2013word pairs used by McCarthy et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "We evaluate our framework on tasks relevant for cognitive science and NLP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 8
                            }
                        ],
                        "text": "Because McCarthy et al. (2004) use all available basis elements, their semantic space grows linearly with vocabulary (i.e., corpus) size."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 120
                            }
                        ],
                        "text": "They argue that the latter measure is more efficient for large scale WSD and use it exclusively in all subsequent work (McCarthy et al. 2004; Koeling, McCarthy, and Carroll 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "A number of NLP tasks could also benefit from the framework presented in this article."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "\u2026representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 119
                            }
                        ],
                        "text": "The total number of nouns after applying the frequency cutoffs was 2,75012 and the average sense ambiguity was 4.55\n12 McCarthy et al. (2004) use 2,595 nouns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "Our final experiment concentrates on unsupervised word sense disambiguation (WSD), thereby exploring the potential of the proposed framework for NLP applications requiring large scale semantic processing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 57
                            }
                        ],
                        "text": "Note that we have a slightly different set of nouns from McCarthy et al. (2004); this is due to the use of a different\nparser and a larger corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "McCarthy et al. (2004) use their ranking model to automatically infer the first senses of all nouns attested in SemCor, a subset of the Brown corpus containing 23,346 lemmas annotated with senses according to WordNet 1.6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "Vector space models of word co-occurrence have proved a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 236
                            }
                        ],
                        "text": "Examples include word sense discrimination (Lin 1998a; Sch\u00fctze 1998), automatic thesaurus construction (Grefenstette 1994; Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general similarity-based approaches to NLP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "Experiments 2 and 3 examined the usefulness of our framework for NLP: we used our model to detect synonymy relations and to automatically acquire prevalent senses for polysemous words."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "The benchmark data set collected by Rubenstein and Goodenough (1965) is routinely used in NLP and cognitive science for development purposes\u2014for example, for evaluating automatic measures of semantic similarity (Resnik 1995; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003) or for exploring the parameter space of vector space models (McDonald 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 68
                            }
                        ],
                        "text": "An interesting observation is that our dependency model outperforms McCarthy et al. (2004) by a large margin (8.3"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 31
                            }
                        ],
                        "text": "We obtained results similar to McCarthy et al. (2004) on the sense ranking task and demonstrated that our model performs significantly better on WSD."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "The ability to identify the intended reading of a polysemous word (the word sense) in context is crucial for accomplishing many NLP tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "McCarthy et al. (2004) show that the annotation bottleneck can be avoided by inferring the first sense heuristic automatically from raw text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1044865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1aac9a51700b4f548ed4d406d3987c8008876521",
            "isKey": true,
            "numCitedBy": 371,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of hand-tagged data. Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL-2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domain-specific corpora."
            },
            "slug": "Finding-Predominant-Word-Senses-in-Untagged-Text-McCarthy-Koeling",
            "title": {
                "fragments": [],
                "text": "Finding Predominant Word Senses in Untagged Text"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically, and demonstrates that this method discovers appropriate predominant senses for words from two domain-specific corpora."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153592298"
                        ],
                        "name": "S. McDonald",
                        "slug": "S.-McDonald",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "McDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McDonald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895292"
                        ],
                        "name": "Chris Brew",
                        "slug": "Chris-Brew",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Brew",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Brew"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 173
                            }
                        ],
                        "text": "A large number of modeling studies in psycholinguistics have focused on simulating semantic priming phenomena (Lund and Burgess 1996; Lowe and McDonald 2000; McDonald 2000; McDonald and Brew 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 162
                            }
                        ],
                        "text": "The BNC has been used extensively in building vector space models for many tasks relevant for cognitive science (Patel, Bullinaria, and Levy 1998; McDonald 2000; McDonald and Brew 2004) and NLP (McCarthy et al. 2004; Weeds 2003; Widdows 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 207
                            }
                        ],
                        "text": "We start by simulating semantic priming, a phenomenon that has received much attention in computational psycholinguistics and is typically modeled using word-based semantic spaces (Landauer and Dumais 1997; McDonald and Brew 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 77
                            }
                        ],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 77
                            }
                        ],
                        "text": "The priming effects reported in Hodgson (1991) have recently been modeled by McDonald and Brew (2004), using an incremental vector-based model of contextual\n4 Increasing the dimensions of the space to 1,000 and 2,000 degraded performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 49
                            }
                        ],
                        "text": "Although we used a less sophisticated model than McDonald and Brew (2004), without an update procedure and an explicit computation of expectations, we obtained priming effects across all relations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 100
                            }
                        ],
                        "text": "Possible future experiments include mediated priming (Lowe and McDonald 2000) and multiple priming (McDonald and Brew 2004), intelligent tutoring (Kanejiya, Kumar, and Prasad 2003), and coherence rating (Foltz, Kintsch, and Landauer 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 112
                            }
                        ],
                        "text": "Because the original materials do not provide Unrelated primes, we emulated the unrelated pairs as described in McDonald and Brew (2004), by using the average distance of a target to all other primes of the same relation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 146
                            }
                        ],
                        "text": "Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 58
                            }
                        ],
                        "text": "For comparison, we also report the Prime Effect size that McDonald and Brew (2004) obtained in their simulation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18143695,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2f55b6959997e8464c19140644c6e3d6781a55b4",
            "isKey": true,
            "numCitedBy": 36,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most robust findings of experimental psycholinguistics is that the context in which a word is presented influences the effort involved in processing that word. We present a computational model of contextual facilitation based on word co-occurrence vectors, and empirically validate the model through simulation of three representative types of context manipulation: single word priming, multiple-priming and contextual constraint. The aim of our study is to find out whether special-purpose mechanisms are necessary in order to capture the pattern of the experimental results."
            },
            "slug": "A-Distributional-Model-of-Semantic-Context-Effects-McDonald-Brew",
            "title": {
                "fragments": [],
                "text": "A Distributional Model of Semantic Context Effects in Lexical Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The aim of the study is to find out whether special-purpose mechanisms are necessary in order to capture the pattern of the experimental results."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1948450"
                        ],
                        "name": "L. Talmy",
                        "slug": "L.-Talmy",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Talmy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Talmy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 219
                            }
                        ],
                        "text": "\u2026hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 337,
                                "start": 221
                            }
                        ],
                        "text": "Furthermore, much research in lexical semantics hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993; Pinker 1989; Green 1974; Gropen et al. 1989; Fillmore 1965)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18150029,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "96f8e185a95537aa3fc99c74ffb7755f5fad5884",
            "isKey": false,
            "numCitedBy": 1069,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter addresses the systematic relations in language between meaning and surface expression. Our approach to this has severa1 aspects. First, we assume we can isolate elements scparately within the domain of meaning and within the domain of surface expression. These are semantic elements like \u2018Motion\u2019, \u2018Path\u2019, \u2018Figure\u2019, \u2018Ground\u2019, \u2018Manner\u2019, anrl \u2018Cause\u2019, and surface elements like \u2018verb\u2019, \u2018adposition\u2019, \u2018subordinate clausc\u2019, and what we will cliaracterize as \u2018satellite\u2019. Second, we examine which semantic elements are expressed by which surface elements. This relatinnship is largely not orle-to-one. A coml)ination of semnntic elements can be expressed by a single surface elemcnt, or \u00fc single semantic element hy a combinatiun of surface elements. Or again, semantic elemeuts of different types can be exptessed by the same type of surface elenient, as well as the same type by severa1 dif-t\u2018erent ones. We finrl here a range of typological palterns anti universal principies. We do n\u00bbt look at every case of semantic-to-surface association, but only at enes that constitute a pervasive pattern, either within a language 01\u2019 across lunguages. Our particular concern is to understand how sucll patterns compare across languages. \u2018I\u2019llilt is, Eor a particular semantic domain, we ask if languages exhibit a wicle variety of patterns, \u00e4 comp\u00fcratively sniall n~~uiller of patlerns (a typology), or a single pattern (a universal). We will be intcrested primarily in the last two cases, as well as in the case where a pattern appears in no languages (universal exclusioii). Our approach can be S\\rll~rl~i~~iZetl a s iii tliis procedural otltliw:"
            },
            "slug": "Lexicalisation-patterns:-semantic-structure-in-Talmy",
            "title": {
                "fragments": [],
                "text": "Lexicalisation patterns: semantic structure in lexical forms"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This chapter addresses the systematic relations in language between meaning and surface expression with a range of typological palterns anti universal principies."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39731031"
                        ],
                        "name": "D. Higgins",
                        "slug": "D.-Higgins",
                        "structuredName": {
                            "firstName": "Derrick",
                            "lastName": "Higgins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Higgins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 93
                            }
                        ],
                        "text": "Rather than assuming that similar words tend to occur in similar contexts, Turney (2001) and Higgins (2004) propose models that capitalize on the collocational nature of semantically related words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "We would also like to compare the vector-based models against Turney\u2019s (2001) and Higgins\u2019 (2004) collocational models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Higgins (2004) proposes a modification to Equation (15): He dispenses with the NEAR operator by concentrating on word pairs that are strictly adjacent:\nSimilarityLC\u2212IR(w1, w2) = min(hits(w1, w2), hits(w2, w1))\nhits(w1)hits(w2) (16)\nNote that Equation (16) takes the minimum number of hits for the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12098732,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "865a653573604cb1527f724631367a272b663988",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "A great deal of work has been done of late on the statistical modeling of word similarity relations (cf.Sch\u00fctze (1992), Lund and Burgess (1996) Landauer and Dumais (1997), Lin (1998), Turney (2001)). While this has largely been viewed as an engineering task (with the notable exception of much writing on Latent Semantic Analysis (LSA)), the relative success of different approaches to constructing word similarity measures is highly relevant to issues in theoretical semantics and language acquisition. With this background in mind, this paper has two main aims. First, we will present yet another statistical approach to the calculation of word-similarity scores (LC-IR), which significantly outperforms other methods on standard benchmarks including the 80-question set of TOEFL\u00ae synonym test items first employed by Landauer and Dumais (1997).1 Second, we hope to demonstrate that"
            },
            "slug": "Which-Statistics-Reflect-Semantics-Rethinking-and-Higgins",
            "title": {
                "fragments": [],
                "text": "Which Statistics Reflect Semantics? Rethinking Synonymy and Word Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper will present yet another statistical approach to the calculation of word-similarity scores (LC-IR), which significantly outperforms other methods on standard benchmarks including the 80-question set of TOEFL\u00ae synonym test items first employed by Landauer and Dumais (1997)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402141228"
                        ],
                        "name": "M. Zhitomirsky-Geffet",
                        "slug": "M.-Zhitomirsky-Geffet",
                        "structuredName": {
                            "firstName": "Maayan",
                            "lastName": "Zhitomirsky-Geffet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zhitomirsky-Geffet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7422894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "847a6a30249ce2af4f242207bc1b7751824965e3",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a novel bootstrapping approach for improving the quality of feature vector weighting in distributional word similarity. The method was motivated by attempts to utilize distributional similarity for identifying the concrete semantic relationship of lexical entailment. Our analysis revealed that a major reason for the rather loose semantic similarity obtained by distributional similarity methods is insufficient quality of the word feature vectors, caused by deficient feature weighting. This observation led to the definition of a bootstrapping scheme which yields improved feature weights, and hence higher quality feature vectors. The underlying idea of our approach is that features which are common to similar words are also most characteristic for their meanings, and thus should be promoted. This idea is realized via a bootstrapping step applied to an initial standard approximation of the similarity space. The superior performance of the bootstrapping method was assessed in two different experiments, one based on direct human gold-standard annotation and the other based on an automatically created disambiguation dataset. These results are further supported by applying a novel quantitative measurement of the quality of feature weighting functions. Improved feature weighting also allows massive feature reduction, which indicates that the most characteristic features for a word are indeed concentrated at the top ranks of its vector. Finally, experiments with three prominent similarity measures and two feature weighting functions showed that the bootstrapping scheme is robust and is independent of the original functions over which it is applied."
            },
            "slug": "Articles:-Bootstrapping-Distributional-Feature-Zhitomirsky-Geffet-Dagan",
            "title": {
                "fragments": [],
                "text": "Articles: Bootstrapping Distributional Feature Vector Quality"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A novel bootstrapping approach for improving the quality of feature vector weighting in distributional word similarity, motivated by attempts to utilize distributional similarity for identifying the concrete semantic relationship of lexical entailment."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153592298"
                        ],
                        "name": "S. McDonald",
                        "slug": "S.-McDonald",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "McDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McDonald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 143
                            }
                        ],
                        "text": "A large number of modeling studies in psycholinguistics have focused on simulating semantic priming phenomena (Lund and Burgess 1996; Lowe and McDonald 2000; McDonald 2000; McDonald and Brew 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 59
                            }
                        ],
                        "text": "B can be a set of words (Lund and Burgess 1996) or lemmas (McDonald 2000), words with their parts of speech (Widdows 2003) or words with a syntactic relation such as subject or object (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 147
                            }
                        ],
                        "text": "The BNC has been used extensively in building vector space models for many tasks relevant for cognitive science (Patel, Bullinaria, and Levy 1998; McDonald 2000; McDonald and Brew 2004) and NLP (McCarthy et al. 2004; Weeds 2003; Widdows 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 96
                            }
                        ],
                        "text": "We describe traditional word-based co-occurrence models as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994) and Lin (1998a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 257
                            }
                        ],
                        "text": "In order to allow a fair comparison, we trained the word-based model on the same corpus as the dependency-based model (the complete BNC) and selected parameters that have been considered \u201coptimal\u201d in the literature (Patel, Bullinaria, and Levy 1998; Lowe and McDonald 2000; McDonald 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "Contexts are defined as a small number of words surrounding the target word (Lund and Burgess 1996; Lowe and McDonald 2000) or as entire paragraphs\u2014even documents (Salton, Wang, and Yang 1975; Landauer and Dumais 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 103
                            }
                        ],
                        "text": "A popular choice is the k most frequent words (minus the stop words) in a corpus, typically 100\u2013 2,000 (McDonald 2000; Levy and Bullinaria 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 81
                            }
                        ],
                        "text": "A number of studies (Patel, Bullinaria, and Levy 1998; Levy and Bullinaria 2001; McDonald 2000) have explored the parameter space for word-based models in detail, using evaluation benchmarks such as human similarity judgments or synonymy choice tests."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 136
                            }
                        ],
                        "text": "To avoid overfitting, exploration of the parameter space is typically performed on a development data set different from the test data (McDonald 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 149
                            }
                        ],
                        "text": "We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 63
                            }
                        ],
                        "text": "Possible future experiments include mediated priming (Lowe and McDonald 2000) and multiple priming (McDonald and Brew 2004), intelligent tutoring (Kanejiya, Kumar, and Prasad 2003), and coherence rating (Foltz, Kintsch, and Landauer 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 39
                            }
                        ],
                        "text": "In our experiments, we follow Lowe and McDonald (2000) in using the well-known log-likelihood ratio G2 (Dunning 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 39
                            }
                        ],
                        "text": "Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 279
                            }
                        ],
                        "text": "\u2026(1965) is routinely used in NLP and cognitive science for development purposes\u2014for example, for evaluating automatic measures of semantic similarity (Resnik 1995; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003) or for exploring the parameter space of vector space models (McDonald 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30529950,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6d8018bd8b288baca0c55522877efd1b49258747",
            "isKey": true,
            "numCitedBy": 74,
            "numCiting": 172,
            "paperAbstract": {
                "fragments": [],
                "text": "A central concern of psycholinguistic research is explaining the relative ease or difficulty involved in processing words. In this thesis, we explore the connection between lexical processing effort and measurable properties of the linguistic environment. Distributional information (information about a word\u2019s contexts of use) is easily extracted from large language corpora in the form of co-occurrence statistics. We claim that such simple distributional statistics can form the basis of a parsimonious model of lexical processing effort. Adopting the purposive style of explanation advocated by the recent rational analysis approach to understanding cognition, we propose that the primary function of the human language processor is to recover meaning from an utterance. We assume that for this task to be efficient, a useful processing strategy is to use prior knowledge in order to build expectations about the meaning of upcoming words. Processing effort can then be seen as reflecting the difference between \u2018expected\u2019 meaning and \u2018actual\u2019 meaning. Applying the tools of information theory to lexical representations constructed from simple distributional statistics, we show how this quantity can be estimated as the amount of information conveyed by a word about its contexts of use. The hypothesis that properties of the linguistic environment are relevant to lexical processing effort is evaluated against a wide range of empirical data, including both new experimental studies and computational reanalyses of published behavioural data. Phenomena accounted for using the current approach include: both singleword and multiple-word lexical priming, isolated word recognition, the effect of contextual constraint on eye movements during reading, sentence and \u2018feature\u2019 priming, and picture naming performance by Alzheimer\u2019s patients. Besides explaining a broad range of empirical findings, our model provides an integrated account of both context-dependent and context-independent processing behaviour, offers an objective alternative to the influential spreading activation model of contextual facilitation, and invites reinterpretation of a number of controversial issues in the literature, such as the word frequency effect and the need for distinct mechanisms to explain semantic and associative priming. We conclude by emphasising the important role of distributional information in explanations of lexical processing effort, and suggest that environmental factors in general should given a more prominent place in theories of human language processing."
            },
            "slug": "Environmental-Determinants-of-Lexical-Processing-McDonald",
            "title": {
                "fragments": [],
                "text": "Environmental Determinants of Lexical Processing Effort"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This model provides an integrated account of both context-dependent and context-independent processing behaviour, offers an objective alternative to the influential spreading activation model of contextual facilitation, and invites reinterpretation of a number of controversial issues in the literature."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118415117"
                        ],
                        "name": "Michael P. Jones",
                        "slug": "Michael-P.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael P. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10796472"
                        ],
                        "name": "James H. Martin",
                        "slug": "James-H.-Martin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H. Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 49
                            }
                        ],
                        "text": "Examples include contextual spelling correction (Jones and Martin 1997), summarization (Barzilay 2003; Erkan and Radev 2004) and question answering (Lin and Pantel 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 143
                            }
                        ],
                        "text": "\u2026(Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 78
                            }
                        ],
                        "text": "In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8592573,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "232e66748382ded9d217de554574fbf70df0f6b6",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Contextual spelling errors are defined as the use of an incorrect, though valid, word in a particular sentence or context. Traditional spelling checkers flag misspelled words, but they do not typically attempt to identify words that are used incorrectly in a sentence. We explore the use of Latent Semantic Analysis for correcting these incorrectly used words and the results are compared to earlier work based on a Bayesian classifier."
            },
            "slug": "Contextual-Spelling-Correction-Using-Latent-Jones-Martin",
            "title": {
                "fragments": [],
                "text": "Contextual Spelling Correction Using Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The use of Latent Semantic Analysis for correcting incorrectly used words is explored and the results are compared to earlier work based on a Bayesian classifier."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144514944"
                        ],
                        "name": "W. Lowe",
                        "slug": "W.-Lowe",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Lowe (2001) defines a semantic space model as a quadruple \u3008B, A, S, V\u3009."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 74
                            }
                        ],
                        "text": "We describe traditional word-based co-occurrence models as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994) and Lin (1998a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 12
                            }
                        ],
                        "text": "However, as Lowe (2001) notes, raw counts are likely to give misleading results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15089423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12afb4e89aefd688f8c73d52b44cebca49221f3e",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Towards a Theory of Semantic Space Will Lowe (wlowe02@tufts.edu) Center for Cognitive Studies Tufts University; MA 21015 USA Abstract This paper adds some theory to the growing literature of semantic space models. We motivate semantic space models from the perspective of distributional linguistics and show how an explicit mathematical formulation can provide a better understanding of existing models and suggest changes and improvements. In addition to pro- viding a theoretical framework for current models, we consider the implications of statistical aspects of language data that have not been addressed in the psychological modeling literature. Statistical approaches to language must deal principally with count data, and this data will typically have a highly skewed frequency distribution due to Zipf\u2019s law. We consider the consequences of these facts for the construction of semantic space models, and present methods for removing frequency biases from se- mantic space models. Introduction There is a growing literature on the empirical adequacy of semantic space models across a wide range of sub- ject domains (Burgess et al., 1998; Landauer et al., 1998; Foltz et al., 1998; McDonald and Lowe, 1998; Lowe and McDonald, 2000). However, semantic space mod- els are typically structured and parameterized differently by each researcher. Levy and Bullinaria (2000) have ex- plored the implications of parameter changes empirically by running multiple simulations, but there has up until now been no work that places semantic space models in an overarching theoretical framework; consequently there there are few statements of how semantic spaces ought to be structured in the light of their intended pur- pose. In this paper we attempt to develop a theoretical framework for semantic space models by synthesizing theoretical analyses from vector space information re- trieval and categorical data analysis with new basic re- search. The structure of the paper is as follows. The next sec- tion brie\u00a4y motivates semantic space models using ideas from distributional linguistics. We then review Zipf\u2019s law and its consequences the distributional character of linguistic data. The \u00a3nal section presents a formal de\u00a3- nition of semantic space models and considers what ef- fects different choices of component have on the result- ing models. Motivating Semantic Space Firth (1968) observed that \u201cyou shall know a word by the company it keeps\u201d. If we interpret company as lex- ical company, the words that occur near to it in text or speech, then two related claims are possible. The \u00a3rst is unexceptional: we come to know about the syntactic character of a word by examining the other words that may and may not occur around it in text. Syntactic theory then postulates latent variables e.g. parts of speech and branching structure, that control the distributional prop- erties of words and restrictions on their contexts of occur- rence. The second claim is that we come to know about the semantic character of a word by examining the other words that may and may not occur around it in text. The intuition for this distributional characterization of semantics is that whatever makes words similar or dis- similar in meaning, it must show up distributionally, in the lexical company of the word. Otherwise the suppos- edly semantic difference is not available to hearers and it is not easy to see how it may be learned. If words are similar to the extent that they occur in the similar contexts then we may de\u00a3ne a statistical re- placement test (Finch, 1993) which tests the meaning- fulness of the result of switching one word for another in a sentence. When a corpus of meaningful sentences is available the test may be reversed (Lowe, 2000a), and un- der a suitable representation of lexical context, we may hold each word constant and estimate its typical sur- rounding context. A semantic space model is a way of representing similarity of typical context in a Euclidean space with axes determined by local word co-occurrence counts. Counting the co-occurrence of a target word with a \u00a3xed set of D other words makes it possible to position the target in a space of dimension D. A target\u2019s position with respect to other words then expresses similarity of lexical context. Since the basic notion from distributional linguistics is \u2018intersubstitutability in context\u2019, a semantic space model is effective to the extent it realizes this idea accurately. Zipf\u2019s Law The frequency of a word is (approximately) proportional to the reciprocal of its rank in a frequency list (Zipf, 1949; Mandelbrot, 1954). This is Zipf\u2019s Law. Zipf\u2019s law ensures dramatically skewed distributions for almost"
            },
            "slug": "Towards-a-Theory-of-Semantic-Space-Lowe",
            "title": {
                "fragments": [],
                "text": "Towards a Theory of Semantic Space"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A theoretical framework for semantic space models is developed by synthesizing theoretical analyses from vector space information re- trieval and categorical data analysis with new basic re- search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97764777"
                        ],
                        "name": "D. Id",
                        "slug": "D.-Id",
                        "structuredName": {
                            "firstName": "Drown",
                            "lastName": "Id",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Id"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2102857068"
                        ],
                        "name": "Y. A. R. O W",
                        "slug": "Y.-A.-R.-O-W",
                        "structuredName": {
                            "firstName": "Y",
                            "lastName": "O W",
                            "middleNames": [
                                "A",
                                "R"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. A. R. O W"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095253744"
                        ],
                        "name": "R. A. D. U. F. L. O. R. Ia N",
                        "slug": "R.-A.-D.-U.-F.-L.-O.-R.-Ia-N",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Ia N",
                            "middleNames": [
                                "A",
                                "D",
                                "U",
                                "F",
                                "L",
                                "O",
                                "R"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. A. D. U. F. L. O. R. Ia N"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15605004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9b2040cc48c41cf3ccd85e4e95b3baefc1b0459",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a comprehensive empirical exploration and evaluation of a diverse range of data characteristics which influence word sense disambiguation performance. It focuses on a set of six core supervised algorithms, including three variants of Bayesian classifiers, a cosine model, non-hierarchical decision lists, and an extension of the transformation-based learning model. Performance is investigated in detail with respect to the following parameters: (a) target language (English, Spanish, Swedish and Basque); (b) part of speech; (c) sense granularity; (d) inclusion and exclusion of major feature classes; (e) variable context width (further broken down by part-of-speech of keyword); (f) number of training examples; (g) baseline probability of the most likely sense; (h) sense distributional entropy; (i) number of senses per keyword; (j) divergence between training and test data; (k) degree of (artificially introduced) noise in the training data; (l) the effectiveness of an algorithm\u2019s confidence rankings; and (m) a full keyword breakdown of the performance of each algorithm. The paper concludes with a brief analysis of similarities, differences, strengths and weaknesses of the algorithms and a hierarchical clustering of these algorithms based on agreement of sense classification behavior. Collectively, the paper constitutes the most comprehensive survey of evaluation measures and tests yet applied to sense disambiguation algorithms. And it does so over a diverse range of supervised algorithms, languages and parameter spaces in single unified experimental framework. 1 Prior comparative surveys of sense disambiguation performance Most previous comparative surveys of sense disambiguation performance have been limited to a single algorithm, a single language, a single word (or a few words) or all of the above. For example, Gale, Church and Yarowsky (1992), in one of the most comprehensive parameter-based studies to-date (including training size, context width and introduced noise), were limited to a single algorithm (a ratio-based Bayesian classifier) over binary sense distinctions on 12 English words. Yarowsky (1993) further detailed the contributions of variable feature types and part-of-speech sensitivity to context width, but was limited to one algorithm (decision list) and 30 binary homographs. In contrast, Leacock, Towell and Voorhees (1993) compared three distinct algorithms (a content vector model, a Bayesian classifier, and a singlelayer neural-net) on a more heavily polysemous word (line) but varied only training 294 D. Yarowsky and R. Florian data size. Mooney (1996) extended the line-based comparative survey to algorithms including N\u00e4\u0131ve Bayes, 3-nearest neighbors, perceptron, decision tree, decision list and PFOIL inductive logic programming variants. However, Mooney restricted comparisons to training size, training time and testing time. His conclusions that N\u00e4\u0131ve Bayes was consistently the top performer in this set of algorithms finds some support in the results of the current study below. In more recent comparative surveys, the inclusion of learning curves (based on variable training data size) have become the standard, but additional new parameters are rarely explored. One addition includes Ng\u2019s (1997) comparison of performance across different corpora (Brown Corpus and Wall Street Journal ). Other large-scale comparative studies include Pedersen (2001), who compared Bayesian and decision-tree algorithms in detail, and Stevenson and Wilks (2001), who investigated the relative efficacy of such knowledge sources as LDOCE subject codes and selectional preference over major parts of speech and sense granularities. A limiting factor in prior comparative analyses was the lack of a large-scale standard evaluation set. senseval1 (Kilgarriff and Palmer 2000) dramatically improved that status quo, and for the first time over 18 sense disambiguation systems were compared in a common evaluation framework. Kilgariff and Rosenzweig (2000) rigorously contrasted systems by keyword part-of-speech, presence of supervision, number of senses and sense entropy. However, because each system was developed and executed at independent sites, the evaluation process could not modify and contrast system internal parameters (such as variable context width or inclusion of feature classes) or even variable properties of data sets (such as training set size). Furthermore, systems not only used different algorithms but also different feature representations and feature extraction quality. Thus, it was not possible in this heterogeneous survey to isolate differences due to algorithm design, feature space utilized or the parameter settings chosen. While such a multi-site comparative exercise has been invaluable for exploring unprecedented diversity of methods in a common test set and standards, there remains a need for single-site comparative studies more closely able to control and systematically vary internal and external parameters across algorithms, maintaining other variables such as the utilized feature space as constant as possible. This paper presents such a survey. 2 Experimental framework and utilized algorithms All the algorithms contrasted in this study are based on a shared, uniform and rich contextual feature space, including word, part-of-speech and lemma in both variable-width bag-of-words wide context and in local n-gram collocations. They also include feature associations in salient syntactic relationships, such as verb-object, noun-modifier, etc. The feature space is shared with the classifier combination study in Florian, Cucerzan, Schafer and Yarowsky (this issue) and described more fully there. The Senseval2 WSD training data (Edmonds and Cotton 2001) are used with five-fold cross-validation for all evaluation. Details including training data size and number of senses of all 73 English polysemous keywords, can be found in Table 1 (Appendix). Evaluating sense disambiguation across diverse parameter spaces 295 Table 1. Keyword-itemized performance on Senseval2 English lexical sample task Num Num Model Samples Senses ML Entr FENBayes BayesRatio Cosine DL TBL begin.v 557 8 59.1% 0.2 79.4% 79.2% 80.3% 81.3% 83.1% call.v 132 23 25.7% 0.5 43.9% 38.6% 35.6% 39.4% 40.2% carry.v 132 27 23.5% 0.6 37.9% 43.2% 43.2% 39.4% 40.1% collaborate.v 57 2 91.2% 0.1 86.1% 94.7% 87.9% 91.2% 94.7% develop.v 133 15 30.1% 0.5 36.9% 38.4% 41.3% 40.6% 36.0% draw.v 82 32 8.5% 0.7 30.4% 31.6% 32.9% 35.2% 26.9% dress.v 119 14 39.3% 0.4 60.5% 59.6% 53.8% 45.4% 56.2% drift.v 63 9 20.5% 0.5 37.9% 34.6% 28.3% 41.2% 33.3% drive.v 84 15 32.1% 0.5 60.6% 60.7% 61.8% 54.7% 52.4% face.v 186 7 83.3% 0.2 80.1% 79.0% 75.3% 85.5% 81.7% ferret.v 2 1 100.0% 0.0 100.0% 100.0% 100.0% 100.0% 100.0% find.v 132 17 15.9% 0.6 41.6% 35.6% 35.6% 34.8% 28.7% keep.v 133 27 36.9% 0.5 35.3% 44.4% 36.9% 52.7% 60.9% leave.v 132 14 28.9% 0.5 44.8% 43.9% 41.0% 43.3% 39.5% live.v 129 10 49.6% 0.4 64.2% 62.7% 61.1% 61.9% 62.7% match.v 86 8 36.1% 0.4 36.0% 30.1% 33.7% 33.5% 45.4% play.v 129 25 10.8% 0.5 44.9% 40.3% 37.9% 45.7% 44.1% pull.v 122 33 22.2% 0.6 48.4% 42.7% 47.7% 44.4% 44.4% replace.v 86 4 51.2% 0.3 45.4% 45.3% 44.2% 47.7% 61.8% see.v 131 21 31.3% 0.5 37.4% 36.6% 27.4% 32.1% 32.8% serve.v 100 12 26.0% 0.5 59.0% 54.0% 53.0% 47.0% 50.0% strike.v 104 26 9.6% 0.6 43.1% 40.3% 34.5% 40.2% 43.1% train.v 125 9 23.2% 0.4 55.2% 48.8% 44.0% 56.8% 45.6% treat.v 88 6 29.5% 0.3 52.4% 53.4% 46.6% 43.2% 55.6% turn.v 131 43 9.9% 0.7 53.4% 52.7% 54.2% 55.7% 61.0% use.v 147 7 68.0% 0.2 66.6% 65.9% 51.0% 70.0% 70.0% wander.v 100 4 83.0% 0.1 78.0% 79.0% 63.0% 81.0% 82.0% wash.v 25 13 8.0% 0.8 52.0% 52.0% 56.0% 68.0% 40.0% work.v 119 21 27.6% 0.5 44.6% 46.3% 40.4% 40.5% 39.6% art.n 196 19 38.2% 0.4 59.7% 65.9% 63.8% 61.7% 67.3% authority.n 184 11 33.7% 0.3 69.1% 69.0% 64.1% 60.4% 66.4% bar.n 304 22 41.8% 0.4 71.4% 71.0% 69.4% 63.1% 65.1% bum.n 92 6 70.6% 0.3 69.5% 70.6% 62.0% 71.8% 73.9% chair.n 138 8 82.6% 0.2 91.3% 91.3% 88.4% 89.9% 88.4% channel.n 145 10 40.7% 0.4 60.0% 62.1% 61.4% 49.7% 48.3% child.n 129 9 60.4% 0.2 68.2% 66.6% 64.3% 72.1% 78.2% church.n 128 7 56.4% 0.2 75.9% 72.7% 65.6% 62.6% 68.9% circuit.n 170 16 27.1% 0.5 83.5% 83.5% 71.8% 63.5% 62.4% day.n 289 18 60.6% 0.3 69.9% 72.7% 67.5% 70.6% 72.3% detention.n 63 6 72.9% 0.3 96.9% 96.9% 90.4% 96.9% 96.9% dyke.n 58 4 83.0% 0.3 81.5% 79.8% 71.4% 71.1% 84.7% facility.n 114 6 53.5% 0.3 77.9% 70.1% 70.2% 72.0% 73.7% fatigue.n 85 8 71.8% 0.3 87.1% 85.9% 88.2% 84.7% 88.2% feeling.n 102 5 64.7% 0.2 68.7% 72.6% 69.7% 62.8% 69.7% grip.n 102 7 53.8% 0.3 69.8% 63.9% 59.9% 58.8% 58.7% hearth.n 64 5 63.8% 0.3 63.8% 60.8% 62.3% 63.8% 62.3% holiday.n 62 8 88.8% 0.2 96.9% 96.9% 96.9% 95.3% 95.3% lady.n 105 10 69.5% 0.3 81.9% 81.9% 79.0% 79.0% 78.1% material.n 140 17 37.1% 0.4 63.6% 64.3% 61.4% 49.3% 54.3% mouth.n 119 12 50.4% 0.3 59.7% 60.5% 58.9% 54.7% 63.2% nation.n 75 5 85.3% 0.2 81.3% 81.3% 74.7% 81.3% 82.7% nature.n 92 9 48.9% 0.4 60.9% 65.3% 64.2% 65.1% 58.7% 296 D. Yarowsky and R. Florian"
            },
            "slug": "Evaluating-sense-disambiguation-across-diverse-Id-Y.A.R.O",
            "title": {
                "fragments": [],
                "text": "Evaluating sense disambiguation across diverse parameter spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper presents a comprehensive empirical exploration and evaluation of a diverse range of data characteristics which influence word sense disambiguation performance on a set of six core supervised algorithms, including three variants of Bayesian classifiers, a cosine model, non-hierarchical decision lists, and an extension of the transformation-based learning model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153584694"
                        ],
                        "name": "R. Rapp",
                        "slug": "R.-Rapp",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Rapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rapp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45775190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0276f1b8e991fbff8d6dc28dd1fad6d69516447",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for the automatic extraction of words with similar meanings is presented which is based on the analysis of word distribution in large monolingual text corpora. It involves compiling matrices of word co-occurrences and reducing the dimensionality of the semantic space by conducting a singular value decomposition. This way problems of data sparseness are reduced and a generalization effect is achieved which considerably improves the results. The method is largely language independent and has been applied to corpora of English, French, German, and Russian, with the resulting thesauri being freely available. For the English thesaurus, an evaluation has been conducted by comparing it to experimental results as obtained from test persons who were asked to give judgements of word similarities. According to this evaluation, the machine generated results come close to native speaker\u2019s performance."
            },
            "slug": "The-automatic-generation-of-thesauri-of-related-for-Rapp",
            "title": {
                "fragments": [],
                "text": "The automatic generation of thesauri of related words for English, French, German, and Russian"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A method for the automatic extraction of words with similar meanings is presented which is based on the analysis of word distribution in large monolingual text corpora and has been applied to corpora of English, French, German, and Russian, with the resulting thesauri being freely available."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Speech Technol."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63043568,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afb180e7615fedf3479707aea9db9ee156f9406e",
            "isKey": false,
            "numCitedBy": 1659,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introducfion, Training and Testing Data, Experiment 1: The Local Context Classifier, Experiment 2: Measuring Word Similarity In Wordnet, Experiment 3: Combining Local Context and Wordnet Similarity Measures, Conclusions, References"
            },
            "slug": "Combining-Local-Context-and-Wordnet-Similarity-for-Fellbaum-Miller",
            "title": {
                "fragments": [],
                "text": "Combining Local Context and Wordnet Similarity for Word Sense Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This chapter contains sections titled: Introducfion, Training and Testing Data, Experiment 1: The Local Context Classifier, Experiment 2: Measuring Word Similarity In Wordnet, and Combining Local Context and Wordnet Similarity Measures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144915758"
                        ],
                        "name": "James Henderson",
                        "slug": "James-Henderson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143939590"
                        ],
                        "name": "Paola Merlo",
                        "slug": "Paola-Merlo",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Merlo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paola Merlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2845241"
                        ],
                        "name": "Ivan Petroff",
                        "slug": "Ivan-Petroff",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Petroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivan Petroff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145353908"
                        ],
                        "name": "G. Schneider",
                        "slug": "G.-Schneider",
                        "structuredName": {
                            "firstName": "Gerold",
                            "lastName": "Schneider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schneider"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 113
                            }
                        ],
                        "text": "The path value function v assigns weights to paths, thus allowing linguistic knowledge to influence the construction of the space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 32
                            }
                        ],
                        "text": "A similar result is reported in Henderson et al. (2002), who find that using the obliqueness hierarchy to isolate important index terms in an information retrieval task degrades performance."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 170
                            }
                        ],
                        "text": "On the other hand, attempts to generate more meaningful indexing terms for information retrieval (IR) using syntactic analysis (Salton and Smith 1989; Strzalkowski 1999; Henderson et al. 2002) have been largely unsuccessful."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12648380,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5a2ff2b249f4f23574b4193cb2288ef57a9de0e",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Self-Organizing Maps (SOMs) are a good method to cluster and visualize large collections of documents, but they are computationally expensive. In this paper, we investigate linguistically motivated reductions on the usual bag-of-words representation, to improve efficiency. We find that reducing the document representation to heads of verb and noun phrases reduces the heavy computational cost without degrading the quality of the map, especially in combination with term reduction techniques. More severe reductions which focus on subject and object nominal phrases are not advantageous."
            },
            "slug": "Using-Syntactic-Analysis-to-Increase-Efficiency-in-Henderson-Merlo",
            "title": {
                "fragments": [],
                "text": "Using Syntactic Analysis to Increase Efficiency in Visualizing Text Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper investigates linguistically motivated reductions on the usual bag-of-words representation of self-Organizing Maps, and finds that reducing the document representation to heads of verb and noun phrases reduces the heavy computational cost without degrading the quality of the map."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144053576"
                        ],
                        "name": "J. Levy",
                        "slug": "J.-Levy",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948254"
                        ],
                        "name": "J. Bullinaria",
                        "slug": "J.-Bullinaria",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bullinaria",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bullinaria"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 134
                            }
                        ],
                        "text": "We describe traditional word-based co-occurrence models as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994) and Lin (1998a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 55
                            }
                        ],
                        "text": "A number of studies (Patel, Bullinaria, and Levy 1998; Levy and Bullinaria 2001; McDonald 2000) have explored the parameter space for word-based models in detail, using evaluation benchmarks such as human similarity judgments or synonymy choice tests."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 118
                            }
                        ],
                        "text": "A popular choice is the k most frequent words (minus the stop words) in a corpus, typically 100\u2013 2,000 (McDonald 2000; Levy and Bullinaria 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15614566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "218ff7d56e30a47e238e4e1d8ae10002bd96473f",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Several recent papers have described how lexical properties of words can be captured by simple measurements of which other words tend to occur close to them. At a practical level, word co\u2013occurrence statistics are used to generate high dimensional vector space representations and appropriate distance metrics are defined on those spaces. The resulting co\u2013occurrence vectors have been used to account for phenomena ranging from semantic priming to vocabulary acquisition. We have developed a simple and highly efficient system for computing useful word co\u2013occurrence statistics, along with a number of criteria for optimizing and validating the resulting representations. Other workers have advocated various methods for reducing the number of dimensions in the co\u2013occurrence vectors. LundB LandauerD and Lowe&McDonald [8] have used a statistical reliability criterion. We have used a simpler framework that orders and truncates the dimensions according to their word frequency. Here we compare how the different methods perform for two evaluation criteria and briefly discuss the consequences of the different methodologies for work within cognitive or neural computation."
            },
            "slug": "Learning-Lexical-Properties-from-Word-Usage-Which-Levy-Bullinaria",
            "title": {
                "fragments": [],
                "text": "Learning Lexical Properties from Word Usage Patterns: Which Context Words Should be Used?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A simple and highly efficient system for computing useful word co\u2013occurrence statistics, along with a number of criteria for optimizing and validating the resulting representations, and the consequences of the different methodologies for work within cognitive or neural computation are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "NCPW"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "Automatic evaluation of students\u2019 answers using syntactically enhanced LSA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Sahlgren (2006) uses Random Indexing, a method comparable to LSA, to represent the meaning of words and reports a 75.0% accuracy on the same TOEFL items."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "It remains to be seen whether our models can capture the wide range of data that traditional and LSA-based models have accounted for."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 181
                            }
                        ],
                        "text": "We start by simulating semantic priming, a phenomenon that has received much attention in computational psycholinguistics and is typically modeled using word-based semantic spaces (Landauer and Dumais 1997; McDonald and Brew 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 53
                            }
                        ],
                        "text": "(a) place (b) crossroads (c) roundabout (d) building\nLandauer and Dumais (1997) were the first to propose the TOEFL items as a test for lexical semantic similarity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 25
                            }
                        ],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 192
                            }
                        ],
                        "text": "Contexts are defined as a small number of words surrounding the target word (Lund and Burgess 1996; Lowe and McDonald 2000) or as entire paragraphs\u2014even documents (Salton, Wang, and Yang 1975; Landauer and Dumais 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 31
                            }
                        ],
                        "text": "Latent Semantic Analysis (LSA; Landauer and Dumais 1997) is an example of a document-based vector space model that is commonly used in information retrieval and cognitive science."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 268
                            }
                        ],
                        "text": "\u2026(Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 16
                            }
                        ],
                        "text": "11 We omit LSA (Landauer and Dumais 1997) and Random indexing (Sahlgren 2006) from our comparison, because these models were not evaluated on unseen data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Mining the Web for synonyms: PMI-IR versus LSA on TOEFL."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "They use SVD to reduce the dimensionality of a semantic space that uses (word, part-of-speech) pairs as basis elements, obtaining better coverage compared with an LSA space constructed over word co-occurrences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "Their LSA model achieved an accuracy of 64.4% on 80 items, a performance comparable to the average score attained by non-native speakers taking the test."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 96
                            }
                        ],
                        "text": "Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "Their results, however, show that the tagged LSA space yields worse performance than a word-based model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "Wiemer-Hastings and Zipitria (2001) construct a semantic space similar to LSA, but enhanced with part-of-speech tags with the aim of modeling human raters in an intelligent tutoring context."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "They employ a documentbased semantic space, which they submit to SVD and subsequently compare against an LSA model that contains no syntactic information, again in the context of an intelligent\ntutoring system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 108
                            }
                        ],
                        "text": "These are routinely used as a testbed for assessing how well vector-based models capture lexical knowledge (Landauer and Dumais 1997; Turney 2001; Sahlgren 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Their results indicate that the syntactically enhanced model has better coverage than the LSA model (i.e., it is able to evaluate more student answers), although it displays a lower correlation with human raters than raw LSA."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1144461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68dd4b89ce1407372a29d05ca9e4e1a2e0513617",
            "isKey": true,
            "numCitedBy": 5788,
            "numCiting": 210,
            "paperAbstract": {
                "fragments": [],
                "text": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched."
            },
            "slug": "A-Solution-to-Plato's-Problem:-The-Latent-Semantic-Landauer-Dumais",
            "title": {
                "fragments": [],
                "text": "A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653729"
                        ],
                        "name": "Veronique Hoste",
                        "slug": "Veronique-Hoste",
                        "structuredName": {
                            "firstName": "Veronique",
                            "lastName": "Hoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veronique Hoste"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708288"
                        ],
                        "name": "Iris Hendrickx",
                        "slug": "Iris-Hendrickx",
                        "structuredName": {
                            "firstName": "Iris",
                            "lastName": "Hendrickx",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iris Hendrickx"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145977875"
                        ],
                        "name": "Antal van den Bosch",
                        "slug": "Antal-van-den-Bosch",
                        "structuredName": {
                            "firstName": "Antal",
                            "lastName": "van den Bosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antal van den Bosch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 274
                            }
                        ],
                        "text": "\u2026if the distribution of senses is skewed, as is often the case, the simple heuristic of choosing the most common or predominant sense in the training data (henceforth \u201cthe first sense heuristic\u201d) delivers results competitive with supervised approaches based on local context (Hoste et al. 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1403,
                                "start": 288
                            }
                        ],
                        "text": "Furthermore, if the distribution of senses is skewed, as is often the case, the simple heuristic of choosing the most common or predominant sense in the training data (henceforth \u201cthe first sense heuristic\u201d) delivers results competitive with supervised approaches based on local context (Hoste et al. 2002). Obtaining the first sense heuristic via annotation is obviously costly and time consuming. More importantly, one would expect that a word\u2019s first sense varies across domains and text genres (the word court in legal documents will most likely mean \u201ctribunal\u201d rather than \u201cyard\u201d). Therefore, manual annotation must be redone for most new languages, domains, and sense inventories. McCarthy et al. (2004) show that the annotation bottleneck can be avoided by inferring the first sense heuristic automatically from raw text. They argue that, even though the first sense heuristic is not a WSD method in itself, it can be usefully combined with context-based disambiguation methods in order to alleviate the data requirements for WSD. Their method builds on the observation that a word\u2019s distributionally similar neighbors often provide cues about its senses. In their model, sense ranking is equivalent to quantifying the degree of similarity between each neighbor and each sense description of a polysemous word. The sense most similar to the neighbors is the first sense. McCarthy et al.\u2019s (2004) approach crucially relies on the quality of the set of neighbors to acquire more or less accurate first senses."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 710,
                                "start": 288
                            }
                        ],
                        "text": "Furthermore, if the distribution of senses is skewed, as is often the case, the simple heuristic of choosing the most common or predominant sense in the training data (henceforth \u201cthe first sense heuristic\u201d) delivers results competitive with supervised approaches based on local context (Hoste et al. 2002). Obtaining the first sense heuristic via annotation is obviously costly and time consuming. More importantly, one would expect that a word\u2019s first sense varies across domains and text genres (the word court in legal documents will most likely mean \u201ctribunal\u201d rather than \u201cyard\u201d). Therefore, manual annotation must be redone for most new languages, domains, and sense inventories. McCarthy et al. (2004) show that the annotation bottleneck can be avoided by inferring the first sense heuristic automatically from raw text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 287
                            }
                        ],
                        "text": "Furthermore, if the distribution of senses is skewed, as is often the case, the simple heuristic of choosing the most common or predominant sense in the training data (henceforth \u201cthe first sense heuristic\u201d) delivers results competitive with supervised approaches based on local context (Hoste et al. 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23098626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62346756772d7e7721028b561af26d4076d3ef11",
            "isKey": true,
            "numCitedBy": 89,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Various Machine Learning (ML) approaches have been demonstrated to produce relatively successful Word Sense Disambiguation (WSD) systems. There are still unexplained differences among the performance measurements of different algorithms, hence it is warranted to deepen the investigation into which algorithm has the right \u2018bias\u2019 for this task. In this paper, we show that this is not easy to accomplish, due to intricate interactions between information sources, parameter settings, and properties of the training data. We investigate the impact of parameter optimization on generalization accuracy in a memory-based learning approach to English and Dutch WSD. A \u2018word-expert\u2019 architecture was adopted, yielding a set of classifiers, each specialized in one single wordform. The experts consist of multiple memory-based learning classifiers, each taking different information sources as input, combined in a voting scheme. We optimized the architectural and parametric settings for each individual word-expert by performing cross-validation experiments on the learning material. The results of these experiments show that the variation of both the algorithmic parameters and the information sources available to the classifiers leads to large fluctuations in accuracy. We demonstrate that optimization per word-expert leads to an overall significant improvement in the generalization accuracies of the produced WSD systems."
            },
            "slug": "Parameter-optimization-for-machine-learning-of-word-Hoste-Hendrickx",
            "title": {
                "fragments": [],
                "text": "Parameter optimization for machine-learning of word sense disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is demonstrated that optimization per word-expert leads to an overall significant improvement in the generalization accuracies of the produced WSD systems."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118907093"
                        ],
                        "name": "Maria Smith",
                        "slug": "Maria-Smith",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 128
                            }
                        ],
                        "text": "On the other hand, attempts to generate more meaningful indexing terms for information retrieval (IR) using syntactic analysis (Salton and Smith 1989; Strzalkowski 1999; Henderson et al. 2002) have been largely unsuccessful."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10029058,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d6007d33d1044983faa6e6f21e77f0d1ada04b8",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This study summarizes various linguistic approaches proposed for document analysis in information retrieval environments. Included are standard syntactic methods to generate complex content identifiers, and the use of semantic know-how obtained from machine-readable dictionaries and from specially constructed knowledge bases. A particular syntactic analysis methodology is also outlined and its usefulness for the automatic construction of book indexes is examined."
            },
            "slug": "On-the-application-of-syntactic-methodologies-in-Salton-Buckley",
            "title": {
                "fragments": [],
                "text": "On the application of syntactic methodologies in automatic text analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This study summarizes various linguistic approaches proposed for document analysis in information retrieval environments and includes standard syntactic methods to generate complex content identifiers, and the use of semantic know-how obtained from machine-readable dictionaries and from specially constructed knowledge bases."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '89"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2036336"
                        ],
                        "name": "P. Foltz",
                        "slug": "P.-Foltz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Foltz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Foltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 221
                            }
                        ],
                        "text": "Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 204
                            }
                        ],
                        "text": "Possible future experiments include mediated priming (Lowe and McDonald 2000) and multiple priming (McDonald and Brew 2004), intelligent tutoring (Kanejiya, Kumar, and Prasad 2003), and coherence rating (Foltz, Kintsch, and Landauer 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 62729021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4732c036d0b00d435bcd41ae904a9e936e4f683",
            "isKey": false,
            "numCitedBy": 793,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent Semantic Analysis (LSA) is used as a technique for measuring the coherence of texts. By comparing the vectors for 2 adjoining segments of text in a high\u2010dimensional semantic space, the method provides a characterization of the degree of semantic relatedness between the segments. We illustrate the approach for predicting coherence through reanalyzing sets of texts from 2 studies that manipulated the coherence of texts and assessed readers\u2019 comprehension. The results indicate that the method is able to predict the effect of text coherence on comprehension and is more effective than simple term\u2010term overlap measures. In this manner, LSA can be applied as an automated method that produces coherence predictions similar to propositional modeling. We describe additional studies investigating the application of LSA to analyzing discourse structure and examine the potential of LSA as a psychological model of coherence effects in text comprehension."
            },
            "slug": "The-Measurement-of-Textual-Coherence-with-Latent-Foltz-Kintsch",
            "title": {
                "fragments": [],
                "text": "The Measurement of Textual Coherence with Latent Semantic Analysis."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The approach for predicting coherence through reanalyzing sets of texts from 2 studies that manipulated the coherence of texts and assessed readers\u2019 comprehension indicates that the method is able to predict the effect of text coherence on comprehension and is more effective than simple term\u2010term overlap measures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 150
                            }
                        ],
                        "text": "\u2026(1965) is routinely used in NLP and cognitive science for development purposes\u2014for example, for evaluating automatic measures of semantic similarity (Resnik 1995; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003) or for exploring the parameter space of vector space models (McDonald 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 99
                            }
                        ],
                        "text": "A number of hybrid approaches have also been proposed that combine WordNet with corpus statistics (Resnik 1995; Jiang and Conrath 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1752785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "265be00bf112c6cb2fa3e8176bff8394a114dbde",
            "isKey": false,
            "numCitedBy": 3889,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66)."
            },
            "slug": "Using-Information-Content-to-Evaluate-Semantic-in-a-Resnik",
            "title": {
                "fragments": [],
                "text": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content, which performs encouragingly well and is significantly better than the traditional edge counting approach."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 165
                            }
                        ],
                        "text": "Central to the construction process is the notion of paths, namely sequences of dependency edges extracted from the dependency parse of a sentence (we define paths formally in Section 3.2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 209
                            }
                        ],
                        "text": "\u2026have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and Carroll 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16766506,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed039573e0491d5289d902513e015181338eca61",
            "isKey": true,
            "numCitedBy": 208,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Non-compositional expressions present a special challenge to NLP applications. We present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus. Our method is based on the hypothesis that when a phrase is non-composition, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word."
            },
            "slug": "Automatic-Identification-of-Non-compositional-Lin",
            "title": {
                "fragments": [],
                "text": "Automatic Identification of Non-compositional Phrases"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This work presents a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus based on the hypothesis that when a phrase is non-Compositional, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766712"
                        ],
                        "name": "R. Rada",
                        "slug": "R.-Rada",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Rada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116204"
                        ],
                        "name": "H. Mili",
                        "slug": "H.-Mili",
                        "structuredName": {
                            "firstName": "Hafedh",
                            "lastName": "Mili",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mili"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144494841"
                        ],
                        "name": "E. Bicknell",
                        "slug": "E.-Bicknell",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Bicknell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bicknell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7519237"
                        ],
                        "name": "M. Blettner",
                        "slug": "M.-Blettner",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Blettner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Blettner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 38
                            }
                        ],
                        "text": "These vary from simple edge-counting (Rada, Mili, and Bicknell 1989) to attempts to factor in peculiarities of the network structure by considering link direction (Hirst and St-Onge 1998), relative depth (Leacock and Chodorow 1998), and density (Agirre and Rigau 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18702948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e41fe310351f5d2f6ff2f930f9c062ba43cbe0f",
            "isKey": false,
            "numCitedBy": 2033,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Motivated by the properties of spreading activation and conceptual distance, the authors propose a metric, called distance, on the power set of nodes in a semantic net. Distance is the average minimum path length over all pairwise combinations of nodes between two subsets of nodes. Distance can be successfully used to assess the conceptual distance between sets of concepts when used on a semantic net of hierarchical relations. When other kinds of relationships, like 'cause', are used, distance must be amended but then can again be effective. The judgements of distance significantly correlate with the distance judgements that people make and help to determine whether one semantic net is better or worse than another. The authors focus on the mathematical characteristics of distance that presents novel cases and interpretations. Experiments in which distance is applied to pairs of concepts and to sets of concepts in a hierarchical knowledge base show the power of hierarchical relations in representing information about the conceptual distance between concepts. >"
            },
            "slug": "Development-and-application-of-a-metric-on-semantic-Rada-Mili",
            "title": {
                "fragments": [],
                "text": "Development and application of a metric on semantic nets"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experiments in which distance is applied to pairs of concepts and to sets of concepts in a hierarchical knowledge base show the power of hierarchical relations in representing information about the conceptual distance between concepts."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "\u2026[1999] for an overview), the cosine being the most popular:\nsimcos( x, y ) =\nn\u2211 i=1\nxiyi\u221a n\u2211\ni=1 x2i\n\u221a n\u2211\ni=1 y2i\n(1)\nSyntax-based semantic space models (Grefenstette 1994; Lin 1998a) go beyond mere co-occurrence by capturing syntactic relationships between words such as subject\u2013verb or\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Grefenstette (1994) uses a weighted version of Jaccard\u2019s coefficient, a measure of association commonly employed in information retrieval (Salton and McGill 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 11
                            }
                        ],
                        "text": "We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 216
                            }
                        ],
                        "text": "Using (8), these paths are mapped to the following basis elements:\n\u3008lorry, carry\u3009 carry\n\u3008lorry, a\u3009 a\n\u3008lorry, carry, apples\u3009 apples\n\u3008lorry, carry, might\u3009 might\n\u3008lorry, carry, apples, sweet\u3009 sweet\nA different mapping is used in Grefenstette (1994) and Lin (1998a), who consider only paths of length 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 62
                            }
                        ],
                        "text": "Figure 2 shows a syntax-based semantic space in the manner of Grefenstette (1994), using the basis elements (subj,lorry), (aux,might), (mod,sweet), and (obj,apples)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 181
                            }
                        ],
                        "text": "\u2026typically reflect argument structure (e.g., subject, object, indirect object) or modification (e.g., adjective\u2013noun, noun\u2013noun) and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999; Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 125
                            }
                        ],
                        "text": "Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026t, Jaccard\u2019s coefficient is defined as:\nsimJacc(t1, t2) = Attr(t1) \u2229 Attr(t2) Attr(t1) \u222a Attr(t2)\n(2)\nLin (1998a) constructs a semantic space similar to Grefenstette (1994) except that the matrix cells represent the number of times a target word t co-occurs with basis element (r, w), as shown in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 209
                            }
                        ],
                        "text": "We describe traditional word-based co-occurrence models as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994) and Lin (1998a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 133
                            }
                        ],
                        "text": "Syntax-based vector space models have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 153
                            }
                        ],
                        "text": "Furthermore, the potential of syntax-based models has not been fully realized since most previous approaches limit themselves to a specific model class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 199
                            }
                        ],
                        "text": "\u2026(Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "For example, the construction of the space is either based on all relations (Grefenstette 1994; Lin 1998a) or a fixed subset (Lee 1999), but there is no qualitative distinction between different relations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 32
                            }
                        ],
                        "text": "**p < 0.01 (2-tailed)\nthesauri (Grefenstette 1994; Lin 1998a; Curran and Moens 2002) or used in applications that utilize semantic similarity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 88
                            }
                        ],
                        "text": "The basis elements (r, w) are treated as a single unit and are often called attributes (Grefenstette 1994; Curran and Moens 2002) or features (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 78
                            }
                        ],
                        "text": "An ideal syntactic formalism should abstract over surface word order, mirror semantic relationships as closely as possible, and incorporate word-based information in addition to syntactic analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 104
                            }
                        ],
                        "text": "Examples include word sense discrimination (Lin 1998a; Sch\u00fctze 1998), automatic thesaurus construction (Grefenstette 1994; Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general similarity-based approaches to NLP."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 148
                            }
                        ],
                        "text": "\u2026and the end word end(\u03c0) does not exceed the window size k:\ncont(t) = {\u03c0 \u2208 \u03a0t | abs(pos(start(\u03c0)) \u2212 pos(end(\u03c0))) \u2264 k} (5)\nThe dependency-based models proposed by Grefenstette (1994) and Lin (1998a) consider minimal syntactic contexts in the form of individual dependency relations, namely,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59167516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4471e3117cdac2fae74d305d54b237bb3addd749",
            "isKey": true,
            "numCitedBy": 873,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. 1. Introduction. 2. Semantic Extraction. 3. Sextant. 4. Evaluation. 5. Applications. 6. Conclusion. 1: Preprocesors. 2. Webster Stopword List. 3: Similarity List. 4: Semantic Clustering. 5: Automatic Thesaurus Generation. 6. Corpora Treated. Index."
            },
            "slug": "Explorations-in-automatic-thesaurus-discovery-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Explorations in automatic thesaurus discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The aim of this monograph is to provide a catalog of words and phrases used in ThesaurusGeneration, as well as some examples of other writers' work, which have been used in similar contexts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 56
                            }
                        ],
                        "text": "Even in cases where many relations are used (Lin 1998a; Lin and Pantel 2001), only direct relations are taken into account, ignoring potentially important co-occurrence patterns between, for instance, the subject and the object of a verb, or between a verb and its non-local argument (e.g., in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 161
                            }
                        ],
                        "text": "\u2026have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and Carroll 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "Examples include contextual spelling correction (Jones and Martin 1997), summarization (Barzilay 2003; Erkan and Radev 2004) and question answering (Lin and Pantel 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 183
                            }
                        ],
                        "text": "Furthermore, the potential of syntax-based models has not been fully realized since most previous approaches limit themselves to a specific model class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12363172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4317b8a4490c84301907a61f5b8ebb26ab8828d",
            "isKey": true,
            "numCitedBy": 627,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the main challenges in question-answering is the potential mismatch between the expressions in questions and the expressions in texts. While humans appear to use inference rules such as \u2018X writes Y\u2019 implies \u2018X is the author of Y\u2019 in answering questions, such rules are generally unavailable to question-answering systems due to the inherent difficulty in constructing them. In this paper, we present an unsupervised algorithm for discovering inference rules from text. Our algorithm is based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus. Essentially, if two paths tend to link the same set of words, we hypothesize that their meanings are similar. We use examples to show that our system discovers many inference rules easily missed by humans."
            },
            "slug": "Discovery-of-inference-rules-for-question-answering-Lin-Pantel",
            "title": {
                "fragments": [],
                "text": "Discovery of inference rules for question-answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an unsupervised algorithm for discovering inference rules from text based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158159"
                        ],
                        "name": "G\u00fcnes Erkan",
                        "slug": "G\u00fcnes-Erkan",
                        "structuredName": {
                            "firstName": "G\u00fcnes",
                            "lastName": "Erkan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G\u00fcnes Erkan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215251"
                        ],
                        "name": "Dragomir R. Radev",
                        "slug": "Dragomir-R.-Radev",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Radev",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir R. Radev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 103
                            }
                        ],
                        "text": "Examples include contextual spelling correction (Jones and Martin 1997), summarization (Barzilay 2003; Erkan and Radev 2004) and question answering (Lin and Pantel 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 506350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44fca068eecce2203d111213e3691647914a3945",
            "isKey": false,
            "numCitedBy": 2472,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
            },
            "slug": "LexRank:-Graph-based-Lexical-Centrality-as-Salience-Erkan-Radev",
            "title": {
                "fragments": [],
                "text": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences is considered and the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47709773"
                        ],
                        "name": "H. Rubenstein",
                        "slug": "H.-Rubenstein",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Rubenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rubenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1898344"
                        ],
                        "name": "J. Goodenough",
                        "slug": "J.-Goodenough",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Goodenough",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodenough"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 36
                            }
                        ],
                        "text": "The benchmark data set collected by Rubenstein and Goodenough (1965) is routinely used in NLP and cognitive science for development purposes\u2014for example, for evaluating automatic measures of semantic similarity (Resnik 1995; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003) or for exploring\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 89
                            }
                        ],
                        "text": "Following previous work, we explored the parameter space of our dependency models on the Rubenstein and Goodenough (1965) data set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18309234,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7ef3ac14cdb484aaa2b039850093febd5cf73a21",
            "isKey": false,
            "numCitedBy": 1460,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "slug": "Contextual-correlates-of-synonymy-Rubenstein-Goodenough",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of synonymy"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The shapes of the functions indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2693903"
                        ],
                        "name": "S. Pinker",
                        "slug": "S.-Pinker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Pinker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pinker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 251
                            }
                        ],
                        "text": "\u2026hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 122333833,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "2fa50c1037c6e1796442de57f744a4b9b8a6a08b",
            "isKey": false,
            "numCitedBy": 1572,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 A learnability paradox: argument structure and the lexicon the logical problem of language acquisition Baker's paradox attempted solutions to Baker's paradox. Part 2 Constraints on lexical rules: morphological and phonological constraints semantic constraints how semantic and morphological constraints might resolve Baker's paradox evidence for criteria-governed productivity problems for the criteria-governed productivity theory. Part 3 Constraints and the nature of argument structure: overview - why lexical rules carry semantic constraints constraints of lexical rules as manifestations of more general phenomena a theory of argument structure on universality. Part 4 Possible and actual forms: the problem of negative exceptions transitive action verbs as evidence for narrow subclasses the nature of narrow conflation classes defining and motivating subclasses of verbs licensing the four alterations the relation between narrow-range and broad-range rules. Part 5 Representation: the need for a theory of lexicosemantic representation is a theory of lexical semantics feasible? evidence for a semantic subsystem underlying verb meanings a cross-linguistic inventory of components of verb meaning a theory of the representation of grammatically relevant semantic structures explicit representations of lexical rules an lexicosemantic structures summary. Part 6 Learning: linking rules lexical semantic structures broad conflation classes (thematic cores) and broad range lexical rules summary of learning mechanisms. Part 7 Development: developmental sequence for argument structure alterations the unlearning problem children's argument structure changing rules are always semantically conditioned do children's errors have the same cause as adults? acquisition of verb meaning and errors in argument structure some predictions about the acquisition of narrow-range rules summary of development. Part 8 Conclusions: a brief summary of the resolution of the paradox argument structure as a pointer between syntactic structure and propositions the autonomy of semantic representation implications for the semantic bootstrapping hyposthesis conservatism, listedness and the lexicon spatial schemas and abstract thought."
            },
            "slug": "Learnability-and-Cognition:-The-Acquisition-of-Pinker",
            "title": {
                "fragments": [],
                "text": "Learnability and Cognition: The Acquisition of Argument Structure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 75
                            }
                        ],
                        "text": "Rather than assuming that similar words tend to occur in similar contexts, Turney (2001) and Higgins (2004) propose models that capitalize on the collocational nature of semantically related words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 111
                            }
                        ],
                        "text": "The central assumption here is that the context surrounding a given word provides important information about its meaning (Harris 1968)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 134
                            }
                        ],
                        "text": "These are routinely used as a testbed for assessing how well vector-based models capture lexical knowledge (Landauer and Dumais 1997; Turney 2001; Sahlgren 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5509836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e517e1645708e7b050787bb4734002ea194a1958",
            "isKey": false,
            "numCitedBy": 1474,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a simple unsupervised learning algorithm for recognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test questions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74%. PMI-IR is contrasted with Latent Semantic Analysis (LSA), which achieves a score of 64% on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing)."
            },
            "slug": "Mining-the-Web-for-Synonyms:-PMI-IR-versus-LSA-on-Turney",
            "title": {
                "fragments": [],
                "text": "Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL"
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759302"
                        ],
                        "name": "Eleni Miltsakaki",
                        "slug": "Eleni-Miltsakaki",
                        "structuredName": {
                            "firstName": "Eleni",
                            "lastName": "Miltsakaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eleni Miltsakaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46181857"
                        ],
                        "name": "E. Prince",
                        "slug": "E.-Prince",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Prince",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Prince"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714374"
                        ],
                        "name": "A. Joshi",
                        "slug": "A.-Joshi",
                        "structuredName": {
                            "firstName": "Aravind",
                            "lastName": "Joshi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Joshi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 200
                            }
                        ],
                        "text": "There is ample evidence demonstrating that syntactic relations across and within sentences are crucial for sentence and discourse processing (West and Stanovich 1986; Neville et al. 1991; Fodor 1995; Miltsakaki 2003) and modulate cognitive behavior in sentence priming tasks (Morris 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 60483872,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "7779615eeb58dca5e770711154d976683b1d092c",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 118,
            "paperAbstract": {
                "fragments": [],
                "text": "The central claim of this thesis is that, unlike main clauses, adjunct subordinate clauses do not form independent processing units in the computation of entity-based topic continuity (attention structure) in discourse. This claim has two primary consequences. First, discourse entities in adjunct subordinate clauses are assigned lower salience than main clauses entities, especially subjects. Second, the process that selects antecedents for pronouns in main clauses is qualitatively different from the process of anaphoric interpretation in subordinate clauses. The former is affected by the mechanism responsible for directing attention in discourse. The latter depends heavily on verbs semantics and the effect of connectives. The claims of this thesis are empirically tested for English and Greek. Primary evidence for the low salience of entities in adjunct subordinate clauses comes from corpus studies, which show that entities in adjunct subordinate clauses (a)\u00a0make poor competitors in the selection of antecedents for subject pronouns in main clauses, and (b)\u00a0are unlikely to be referred to in the subsequent discourse with a pronoun (unless they are already old). Primary evidence for the two level anaphora resolution mechanism comes from psycholinguistic experiments designed to test if there is a consistent difference in the way we interpret pronouns in main and adjunct subordinate clauses. These findings form the basis for the specification of two new NLP models, a system for the automated evaluation of coherence in student essays and a two level anaphora resolution algorithm. They also make two significant contributions to the Centering Model, (a)\u00a0Centering's \u201cutterance\u201d is formally defined on the basis of empirical evidence, and (b)\u00a0Centering's Rough-Shift transition is for the first time validated as a reliable estimator of poor coherence, empirically tested on an operable essay scoring system."
            },
            "slug": "The-syntax-discourse-interface:-effects-of-the-on-Miltsakaki-Prince",
            "title": {
                "fragments": [],
                "text": "The syntax-discourse interface: effects of the main-subordinate distinction on attention structure"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "These findings form the basis for the specification of two new NLP models, a system for the automated evaluation of coherence in student essays and a two level anaphora resolution algorithm, and two significant contributions to the Centering Model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145091160"
                        ],
                        "name": "B. Levin",
                        "slug": "B.-Levin",
                        "structuredName": {
                            "firstName": "Beth",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Levin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 264
                            }
                        ],
                        "text": "\u2026hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 141
                            }
                        ],
                        "text": "Our guiding hypothesis is that syntactic structure in general and argument structure in particular is a close reflection of lexical meaning (Levin 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 62585813,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6cbc1eb25f4ab29a613418b3b0740e74141a0f17",
            "isKey": false,
            "numCitedBy": 3246,
            "numCiting": 246,
            "paperAbstract": {
                "fragments": [],
                "text": "In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, \"English Verb Classes and Alternations\" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University."
            },
            "slug": "English-Verb-Classes-and-Alternations:-A-Levin",
            "title": {
                "fragments": [],
                "text": "English Verb Classes and Alternations: A Preliminary Investigation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Beth Levin shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122880978"
                        ],
                        "name": "James M. Hodgson",
                        "slug": "James-M.-Hodgson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hodgson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Hodgson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 41
                            }
                        ],
                        "text": "From the 143 prime\u2013target pairs listed in Hodgson (1991) (one synonymy pair is missing in the original data set), seven pairs containing at least one low-frequency word (less than 100 occurrences in the BNC) were removed to avoid creating vectors with unreliable counts.6 We constructed a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 32
                            }
                        ],
                        "text": "The priming effects reported in Hodgson (1991) have recently been modeled by McDonald and Brew (2004), using an incremental vector-based model of contextual\n4 Increasing the dimensions of the space to 1,000 and 2,000 degraded performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 143378752,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f26ab52b2b191a6ada08645c95a1282378c02455",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A popular explanation of automatic semantic priming attributes those effects to the spreading of activation across a meaning-based network in lexical memory. A number of conflicting claims have been advanced regarding the kinds of information captured in that pre-lexical network. Lexical decision and naming experiments are reported which indicate (1) that automatic priming is supported equally well by a very broad range of relations, (2) that the quality of relation for individual prime-target pairs, as reflected in such measures as traditional associative strength, has no impact on the amount of priming produced, (3) that asymmetries between lexical decision and naming in the amount of priming they generate under stringent conditions of automaticity require the postulation of an extra-lexical source of automatic priming, and (4) the residual priming left for the putative lexical network to explain is at most a few milliseconds. Taken together, these findings suggest that explanations of automati..."
            },
            "slug": "Informational-constraints-on-pre-lexical-priming-Hodgson",
            "title": {
                "fragments": [],
                "text": "Informational constraints on pre-lexical priming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36855520"
                        ],
                        "name": "R. F. West",
                        "slug": "R.-F.-West",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "West",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F. West"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4248890"
                        ],
                        "name": "K. Stanovich",
                        "slug": "K.-Stanovich",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Stanovich",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stanovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 142
                            }
                        ],
                        "text": "There is ample evidence demonstrating that syntactic relations across and within sentences are crucial for sentence and discourse processing (West and Stanovich 1986; Neville et al. 1991; Fodor 1995; Miltsakaki 2003) and modulate cognitive behavior in sentence priming tasks (Morris 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 38513808,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "60bd96f3aa44485f1fe5dbe65d83d16e56334711",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "A series of experiments explored the effect of the syntactic structure of a sentence fragment on the processing of a subsequent target word. In both a naming and a lexical decision task, modal verb contexts followed by main verb targets and preposition contexts followed by noun targets produced faster response times than did the opposite pairings (i.e., modal/noun and preposition/ verb). This syntactic context effect occurred across several different variations in the method of context presentation. Also, unlike some previous findings on syntactic priming, the present effects did not disappear when a naming task was employed. The magnitude of the syntactic priming effect was similar in the naming and lexical decision tasks when the response times were slow, but was larger in the lexical decision task when the response times were faster. The implications of these results for recent discussions of the relationship between task structure and the locus of observed contextual effects are discussed."
            },
            "slug": "Robust-effects-of-syntactic-structure-on-visual-West-Stanovich",
            "title": {
                "fragments": [],
                "text": "Robust effects of syntactic structure on visual word processing"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The magnitude of the syntactic priming effect was similar in the naming and lexical decision tasks when the response times were slow, but was larger in the Lexical decision task when theresponse times were faster."
            },
            "venue": {
                "fragments": [],
                "text": "Memory & cognition"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158159"
                        ],
                        "name": "G\u00fcnes Erkan",
                        "slug": "G\u00fcnes-Erkan",
                        "structuredName": {
                            "firstName": "G\u00fcnes",
                            "lastName": "Erkan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G\u00fcnes Erkan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215251"
                        ],
                        "name": "Dragomir R. Radev",
                        "slug": "Dragomir-R.-Radev",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Radev",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir R. Radev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 103
                            }
                        ],
                        "text": "Examples include contextual spelling correction (Jones and Martin 1997), summarization (Barzilay 2003; Erkan and Radev 2004) and question answering (Lin and Pantel 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2962090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8937f6ecd5e19b85c2875bad3e10e320fd15623b",
            "isKey": false,
            "numCitedBy": 320,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."
            },
            "slug": "LexRank:-Graph-based-Centrality-as-Salience-in-Text-Erkan-Radev",
            "title": {
                "fragments": [],
                "text": "LexRank: Graph-based Centrality as Salience in Text Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences is considered and the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144553330"
                        ],
                        "name": "H. Neville",
                        "slug": "H.-Neville",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "Neville",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Neville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39151594"
                        ],
                        "name": "J. Nicol",
                        "slug": "J.-Nicol",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Nicol",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nicol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6060298"
                        ],
                        "name": "A. Barss",
                        "slug": "A.-Barss",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4750592"
                        ],
                        "name": "K. Forster",
                        "slug": "K.-Forster",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Forster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Forster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39997409"
                        ],
                        "name": "M. Garrett",
                        "slug": "M.-Garrett",
                        "structuredName": {
                            "firstName": "Merrill",
                            "lastName": "Garrett",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Garrett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 167
                            }
                        ],
                        "text": "There is ample evidence demonstrating that syntactic relations across and within sentences are crucial for sentence and discourse processing (West and Stanovich 1986; Neville et al. 1991; Fodor 1995; Miltsakaki 2003) and modulate cognitive behavior in sentence priming tasks (Morris 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5090452,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "21ace5a76dd71cefee73863d724eca1d0856bce9",
            "isKey": false,
            "numCitedBy": 839,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical considerations and diverse empirical data from clinical, psycholinguistic, and developmental studies suggest that language comprehension processes are decomposable into separate subsystems, including distinct systems for semantic and grammatical processing. Here we report that event-related potentials (ERPs) to syntactically well-formed but semantically anomalous sentences produced a pattern of brain activity that is distinct in timing and distribution from the patterns elicited by syntactically deviant sentences, and further, that different types of syntactic deviance produced distinct ERP patterns. Forty right-handed young adults read sentences presented at 2 words/sec while ERPs were recorded from over several positions between and within the hemispheres. Half of the sentences were semantically and grammatically acceptable and were controls for the remainder, which contained sentence medial words that violated (1) semantic expectations, (2) phrase structure rules, or (3) WH-movement constraints on Specificity and (4) Subjacency. As in prior research, the semantic anomalies produced a negative potential, N400, that was bilaterally distributed and was largest over posterior regions. The phrase structure violations enhanced the N125 response over anterior regions of the left hemisphere, and elicited a negative response (300-500 msec) over temporal and parietal regions of the left hemisphere. Violations of Specificity constraints produced a slow negative potential, evident by 125 msec, that was also largest over anterior regions of the left hemisphere. Violations of Subjacency constraints elicited a broadly and symmetrically distributed positivity that onset around 200 msec. The distinct timing and distribution of these effects provide biological support for theories that distinguish between these types of grammatical rules and constraints and more generally for the proposal that semantic and grammatical processes are distinct subsystems within the language faculty."
            },
            "slug": "Syntactically-Based-Sentence-Processing-Classes:-Neville-Nicol",
            "title": {
                "fragments": [],
                "text": "Syntactically Based Sentence Processing Classes: Evidence from Event-Related Brain Potentials"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The distinct timing and distribution of these effects provide biological support for theories that distinguish between these types of grammatical rules and constraints and more generally for the proposal that semantic and grammatical processes are distinct subsystems within the language faculty."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142223"
                        ],
                        "name": "M. Lesk",
                        "slug": "M.-Lesk",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lesk",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lesk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11892605,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "76e4e034c20bea86edcc6e71bbaddb47fafeecbc",
            "isKey": false,
            "numCitedBy": 2124,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The meaning of an English word can vary widely depending on which sense is intended. Does a fireman feed fires or put them out? It depends on whether or not he is on a steam locomotive. I am trying to decide automatically which sense of a word is intended (in written English) by using machine readable dictionaries, and looking for words in the sense definitions that overlap words in the definition of nearby words. The problem of deciding which sense of a word was intended by the writer is an important problem in information retrieval systems. At present most retrieval systems rely on manual indexing; if this is to be replaced with automatic text processing, it would be very desirable to recognize the correct sense of each word as often as possible. Previous work has generally either suggested (a) detailed frames describing the particular word senses,t*\u2019 or (b) global statistics about the word occurrences.3 The first has not yet been made available in any real application, and the second may give the wrong answer in specific local instances. This procedure uses available dictionaries, so that it will process any text; and uses solely the immediate context. To consider the example in the title, look at the definition of pine in the Oxford Advanced Learner\u2019s Dictionary of Current English: there are, of course, two major senses. \u201ckind of evergreen tree with needle-shaped leaves.. .\u201d and \u201cwaste away through sorrow or illness...\u201d And cone has three separate definitions: \u201csolid body which narrows to a\u2019 point . . . . *\u2019 \u201csomething of this shape w-hether solid or hollow...,\u201d and \u201cfruit of certain evergreen trees...\u201d Note that both evergreen and tree are common to two of the sense definitions: thus a program could guess that if the two words pine cone appear together, the likely senses are those of the tree and its fruit"
            },
            "slug": "Automatic-sense-disambiguation-using-machine-how-to-Lesk",
            "title": {
                "fragments": [],
                "text": "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This procedure uses available dictionaries, so that it will process any text; and uses solely the immediate context to decide which sense of a word is intended (in written English)."
            },
            "venue": {
                "fragments": [],
                "text": "SIGDOC '86"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2993688"
                        ],
                        "name": "Dharmendra Kanejiya",
                        "slug": "Dharmendra-Kanejiya",
                        "structuredName": {
                            "firstName": "Dharmendra",
                            "lastName": "Kanejiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dharmendra Kanejiya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119304006"
                        ],
                        "name": "Arun Kumar",
                        "slug": "Arun-Kumar",
                        "structuredName": {
                            "firstName": "Arun",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arun Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145518506"
                        ],
                        "name": "S. Prasad",
                        "slug": "S.-Prasad",
                        "structuredName": {
                            "firstName": "Surendra",
                            "lastName": "Prasad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Prasad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1668200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1d402d62f150c5ba85e575601e06ee97dd41ebb",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent semantic analysis (LSA) has been used in several intelligent tutoring systems(ITS's) for assessing students' learning by evaluating their answers to questions in the tutoring domain. It is based on word-document co-occurrence statistics in the training corpus and a dimensionality reduction technique. However, it doesn't consider the word-order or syntactic information, which can improve the knowledge representation and therefore lead to better performance of an ITS. We present here an approach called Syntactically Enhanced LSA (SELSA) which generalizes LSA by considering a word along with its syntactic neighborhood given by the part-of-speech tag of its preceding word, as a unit of knowledge representation. The experimental results on Auto-Tutor task to evaluate students' answers to basic computer science questions by SELSA and its comparison with LSA are presented in terms of several cognitive measures. SELSA is able to correctly evaluate a few more answers than LSA but is having less correlation with human evaluators than LSA has. It also provides better discrimination of syntactic-semantic knowledge representation than LSA."
            },
            "slug": "Automatic-Evaluation-of-Students\u2019-Answers-using-LSA-Kanejiya-Kumar",
            "title": {
                "fragments": [],
                "text": "Automatic Evaluation of Students\u2019 Answers using Syntactically Enhanced LSA"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Syntactically Enhanced LSA is presented here an approach which generalizes LSA by considering a word along with its syntactic neighborhood given by the part-of-speech tag of its preceding word, as a unit of knowledge representation, which provides better discrimination of syntactic-semantic knowledge representation than LSA."
            },
            "venue": {
                "fragments": [],
                "text": "HLT-NAACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33070393"
                        ],
                        "name": "R. Morris",
                        "slug": "R.-Morris",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Morris",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Morris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 276
                            }
                        ],
                        "text": "There is ample evidence demonstrating that syntactic relations across and within sentences are crucial for sentence and discourse processing (West and Stanovich 1986; Neville et al. 1991; Fodor 1995; Miltsakaki 2003) and modulate cognitive behavior in sentence priming tasks (Morris 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 23331798,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "c8a3f203278da35f444923a5e2c79612c266b471",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Readers' eye movements were recorded as they read an unambiguous noun in a sentence context. In Experiment 1, fixation durations on a target noun were shorter when it was embedded in context containing a subject noun and a verb that were weakly related to the target than when either content word was replaced with a more neutral word. These results were not affected by changes in the syntactic relations between the content words. In Experiment 2, the semantic relations between the message-level representation of the sentence and the target word were altered whereas the lexical content was held constant. Fixation time on the target word was shorter when the context was semantically related to the target word than when it was unrelated. Intralexical priming effects between the subject noun and the verb were also observed. The results suggest that both lexical and message-level representations can influence the access of an individual lexical item in a sentence context."
            },
            "slug": "Lexical-and-message-level-sentence-context-effects-Morris",
            "title": {
                "fragments": [],
                "text": "Lexical and message-level sentence context effects on fixation times in reading."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The results suggest that both lexical and message-level representations can influence the access of an individual lexical item in a sentence context."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. Learning, memory, and cognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153873834"
                        ],
                        "name": "A. Wong",
                        "slug": "A.-Wong",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40498308"
                        ],
                        "name": "Chung-Shu Yang",
                        "slug": "Chung-Shu-Yang",
                        "structuredName": {
                            "firstName": "Chung-Shu",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chung-Shu Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6473756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5f169880e30e1f76827d72f862555d00b01bed9",
            "isKey": false,
            "numCitedBy": 7618,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model."
            },
            "slug": "A-vector-space-model-for-automatic-indexing-Salton-Wong",
            "title": {
                "fragments": [],
                "text": "A vector space model for automatic indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents, demonstating the usefulness of the model."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841774"
                        ],
                        "name": "P. Tich\u00fd",
                        "slug": "P.-Tich\u00fd",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Tich\u00fd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tich\u00fd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 276
                            }
                        ],
                        "text": "\u2026hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 224840408,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "e9eba14c3d0912a274fe2b59b7a43e3fe28a84c0",
            "isKey": false,
            "numCitedBy": 360,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper deals with the semantics of mathematical notation. In arithmetic, for example, the syntactic shape of a formula represents a particular way of specifying, arriving at, or constructing an arithmetical object (that is, a number, a function, or a truth value). A general definition of this sense of \"construction\" is proposed and compared with related notions, in particular with Frege's concept of \"function\" and Carnap's concept of \"intensional isomorphism.\" It is argued that constructions constitute the proper subject matter of both logic and mathematics, and that a coherent semantic account of mathematical formulas cannot be given without assuming that they serve as names of constructions."
            },
            "slug": "Constructions-Tich\u00fd",
            "title": {
                "fragments": [],
                "text": "Constructions"
            },
            "venue": {
                "fragments": [],
                "text": "Philosophy of Science"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144514944"
                        ],
                        "name": "W. Lowe",
                        "slug": "W.-Lowe",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lowe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153592298"
                        ],
                        "name": "S. McDonald",
                        "slug": "S.-McDonald",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "McDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McDonald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 134
                            }
                        ],
                        "text": "A large number of modeling studies in psycholinguistics have focused on simulating semantic priming phenomena (Lund and Burgess 1996; Lowe and McDonald 2000; McDonald 2000; McDonald and Brew 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 87
                            }
                        ],
                        "text": "We describe traditional word-based co-occurrence models as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994) and Lin (1998a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 248
                            }
                        ],
                        "text": "In order to allow a fair comparison, we trained the word-based model on the same corpus as the dependency-based model (the complete BNC) and selected parameters that have been considered \u201coptimal\u201d in the literature (Patel, Bullinaria, and Levy 1998; Lowe and McDonald 2000; McDonald 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 53
                            }
                        ],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 100
                            }
                        ],
                        "text": "Contexts are defined as a small number of words surrounding the target word (Lund and Burgess 1996; Lowe and McDonald 2000) or as entire paragraphs\u2014even documents (Salton, Wang, and Yang 1975; Landauer and Dumais 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 54
                            }
                        ],
                        "text": "Possible future experiments include mediated priming (Lowe and McDonald 2000) and multiple priming (McDonald and Brew 2004), intelligent tutoring (Kanejiya, Kumar, and Prasad 2003), and coherence rating (Foltz, Kintsch, and Landauer 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 30
                            }
                        ],
                        "text": "In our experiments, we follow Lowe and McDonald (2000) in using the well-known log-likelihood ratio G2 (Dunning 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 122
                            }
                        ],
                        "text": "Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5930800,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3fcbc36a63721ae97a4933247f54a93c79c20704",
            "isKey": true,
            "numCitedBy": 84,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "McKoon and Ratcliff (1992) presented a theory of mediated priming where the priming effect is due to a direct but weak relatedness between prime and target. They also introduced a quantitative measure of word relatedness based on pointwise mutual information (Church and Hanks; 1990), and showed that stimuli chosen with the measure produced graded priming effects as predicted by their theory. Using stimuli from Balota and Lorch (1986), Livesay and Burgess (Livesay and Burgess; 1997, 1998) replicated the mediated priming effect in humans, but found that in HAL, a corpusderived semantic space (Lund, Burgess and Atchley; 1995), mediated primes were in fact further from their targets than unrelated words. They concluded from this that mediated priming is not due to direct but weak relatedness. In this paper we present an alternative semantic space model based on earlier work (McDonald and Lowe; 1998). We show how this space allows a) a detailed replication of Ratcliff and McKoon\u2019s experimental results using their stimuli and b) a replication of Livesay and Burgess\u2019s human experimental results showing mediated priming. We discuss the implications for McKoon and Ratcliff\u2019s theory of mediated priming."
            },
            "slug": "The-Direct-Route:-Mediated-Priming-in-Semantic-Lowe-McDonald",
            "title": {
                "fragments": [],
                "text": "The Direct Route: Mediated Priming in Semantic Space"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741283"
                        ],
                        "name": "R. Barzilay",
                        "slug": "R.-Barzilay",
                        "structuredName": {
                            "firstName": "Regina",
                            "lastName": "Barzilay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barzilay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59723731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce198cbb4dd0486597ee643b2467a16b4bc72d6a",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The number and variety of online news sources makes it difficult for people to track the news concerning even a single event. Redundancy causes such tracking to be extremely time-consuming: multiple news feeds on the same event tend to contain similar information. A summary of such news feeds can present important information in one short text, dramatically reducing reading time. The focus of this thesis is information fusion, a technique which, given multiple documents, identifies redundant information and synthesizes a coherent summary. This technique is embodied in MultiGen, a system that I have designed, implemented and evaluated over the course of my Ph.D. Unlike previous work in the area, MultiGen is a domain-independent system: it generates news summaries on a variety of topics in any domain. Another contribution to the state of the art is that the system generates the summary by reusing and altering phrases from the input articles, creating a more fluent and cohesive text. This is in contrast with other existing systems, which simply extract sentences from input articles and concatenate them together, leading to fluency problems. Currently MultiGen operates as part of Columbia's Newsblaster system. Everyday, Newsblaster downloads all news articles from a variety of sources, clusters articles by topic, and generates a cohesive, readable automatic summary of each document cluster. \nOne key challenge in multidocument summarization is eliminating redundant information in the produced summaries. Articles about the same event often contain descriptions of the same fact using different wording. To address this issue, we need a method to identify paraphrases\u2014fragments of text that convey similar meaning even if they are not identical in wording. Automatic identification of paraphrases was not addressed in previous research, although it is necessary for many applications, including question-answering, information extraction and natural language generation. This thesis presents unsupervised learning techniques to identify paraphrases given a corpus of multiple parallel texts. This type of corpus provides many instances of paraphrasing, because these texts preserve the meaning of the original source, but may use different words to convey the meaning. Both the data and the method are departures from past approaches to corpus based techniques. Our evaluation experiments show that the algorithm extracts paraphrases with high accuracy and significantly outperforms a state of the art algorithm developed for related tasks in machine translation."
            },
            "slug": "Information-fusion-for-multidocument-summarization:-McKeown-Barzilay",
            "title": {
                "fragments": [],
                "text": "Information fusion for multidocument summarization: paraphrasing and generation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This thesis presents unsupervised learning techniques to identify paraphrasing given a corpus of multiple parallel texts, and shows that the algorithm extracts paraphrases with high accuracy and significantly outperforms a state of the art algorithm developed for related tasks in machine translation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 161
                            }
                        ],
                        "text": "In traditional vector-based models, the context\n1 For the sake of simplicity, we use R without a subscript to denote the set of dependency relations provided by MINIPAR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 87
                            }
                        ],
                        "text": "In order to construct dependency spaces, the BNC was parsed with MINIPAR, version 0.5 (Lin 1998b, 2001), a wide-coverage dependency parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 117
                            }
                        ],
                        "text": "Central to the construction process is the notion of paths, namely sequences of dependency edges extracted from the dependency parse of a sentence (we define paths formally in Section 3.2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MINIPAR employs a manually constructed grammar and a lexicon derived from WordNet with the addition of proper names (130,000 entries in total)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 63
                            }
                        ],
                        "text": "[Det,det,N] and [N,subj,V] are examples for labels provided by MINIPAR (see Figure 4, right-hand side)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "We base our discussion\nand experiments on the broad-coverage dependency parser MINIPAR, version 0.5 (Lin 1998a, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "MINIPAR uses a distributed chart parsing algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 49
                            }
                        ],
                        "text": "Recall that we obtain dependency relations using MINIPAR (Lin 1998b), whereas McCarthy et al. employ Briscoe and Carroll\u2019s (2002) parser."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 68
                            }
                        ],
                        "text": "The system can create dependency spaces from the output of MINIPAR (Lin 1998b, 2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 38
                            }
                        ],
                        "text": "1998b. Dependency-based evaluation of MINIPAR."
                    },
                    "intents": []
                }
            ],
            "corpusId": 38974582,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "6ed3f00a00631f4e31cdb7626c823aa91005911d",
            "isKey": true,
            "numCitedBy": 41,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "LaTaT is a Language and Text Analysis Toolset. This paper gives a brief description of the components comprising LaTaT, including a Minimalist parser and language and concept learning programs."
            },
            "slug": "LaTaT:-Language-and-Text-Analysis-Tools-Lin",
            "title": {
                "fragments": [],
                "text": "LaTaT: Language and Text Analysis Tools"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paper gives a brief description of the components comprising LaTaT, including a Minimalist parser and language and concept learning programs, and a Language and Text Analysis Toolset."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2271549"
                        ],
                        "name": "M. Berry",
                        "slug": "M.-Berry",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berry",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Berry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401512392"
                        ],
                        "name": "G. O'Brien",
                        "slug": "G.-O'Brien",
                        "structuredName": {
                            "firstName": "Gavin",
                            "lastName": "O'Brien",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. O'Brien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7580761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06871028d4e71ceefe879853e9dc2183ea81bb32",
            "isKey": false,
            "numCitedBy": 1755,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Currently, most approaches to retrieving textual materials from scientific databases depend on a lexical match between words in users\u2019 requests and those in or assigned to documents in a database. ..."
            },
            "slug": "Using-Linear-Algebra-for-Intelligent-Information-Berry-Dumais",
            "title": {
                "fragments": [],
                "text": "Using Linear Algebra for Intelligent Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A lexical match between words in users\u2019 requests and those in or assigned to documents in a database helps retrieve textual materials from scientific databases."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 200
                            }
                        ],
                        "text": "\u2026typically reflect argument structure (e.g., subject, object, indirect object) or modification (e.g., adjective\u2013noun, noun\u2013noun) and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999; Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 144
                            }
                        ],
                        "text": "Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 126
                            }
                        ],
                        "text": "For example, the construction of the space is either based on all relations (Grefenstette 1994; Lin 1998a) or a fixed subset (Lee 1999), but there is no qualitative distinction between different relations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 127
                            }
                        ],
                        "text": "An ideal syntactic formalism should abstract over surface word order, mirror semantic relationships as closely as possible, and incorporate word-based information in addition to syntactic analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6305097,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6f3250ba47fdb413a0c113cc16d274517864f8ab",
            "isKey": true,
            "numCitedBy": 661,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences. Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions."
            },
            "slug": "Measures-of-Distributional-Similarity-Lee",
            "title": {
                "fragments": [],
                "text": "Measures of Distributional Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35400286"
                        ],
                        "name": "Z. Harris",
                        "slug": "Z.-Harris",
                        "structuredName": {
                            "firstName": "Zellig",
                            "lastName": "Harris",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 123
                            }
                        ],
                        "text": "The central assumption here is that the context surrounding a given word provides important information about its meaning (Harris 1968)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 32164517,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "99eadd5e29a85f30cafef7f2c915f384715e3b89",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We may not be able to make you love reading, but mathematical structures of language will lead you to love reading starting from now. Book is the window to open the new world. The world that you want is in the better stage and level. World will always guide you to even the prestige stage of the life. You know, this is some of how reading will give you the kindness. In this case, more books you read more knowledge you know, but it can mean also the bore is full."
            },
            "slug": "Mathematical-structures-of-language-Harris",
            "title": {
                "fragments": [],
                "text": "Mathematical structures of language"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors may not be able to make you love reading, but mathematical structures of language will lead you to love reading starting from now."
            },
            "venue": {
                "fragments": [],
                "text": "Interscience tracts in pure and applied mathematics"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 138
                            }
                        ],
                        "text": "Grefenstette (1994) uses a weighted version of Jaccard\u2019s coefficient, a measure of association commonly employed in information retrieval (Salton and McGill 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": false,
            "numCitedBy": 12605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3299116"
                        ],
                        "name": "F. B. Thompson",
                        "slug": "F.-B.-Thompson",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Thompson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. B. Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 38
                            }
                        ],
                        "text": "When evaluated on the SUSANNE corpus (Sampson 1995), the parser achieved a precision of 89% and a recall of 79% in identifying labeled dependencies (Lin 1998b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16173809,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "548ec2852d5d5240f18e91abd723db2e83e2f7d7",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "What about English as a programming language? Few would question that this is a desirable goal. On the other hand, I dare say every one of us has rather deep reservations both about its feasibility and about a number of problems that it entails. This paper presents a point of view which gives some clarity to the relationship between English and programming languages. This point of view has found substance in an experimental system called DEACON. The second paper in this session will describe the specific DEACON system and its capabilities."
            },
            "slug": "English-for-the-computer-Thompson",
            "title": {
                "fragments": [],
                "text": "English for the computer"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper presents a point of view which gives some clarity to the relationship between English and programming languages and has found substance in an experimental system called DEACON."
            },
            "venue": {
                "fragments": [],
                "text": "AFIPS '66 (Fall)"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67043343"
                        ],
                        "name": "Lucien Tesni\u00e8re",
                        "slug": "Lucien-Tesni\u00e8re",
                        "structuredName": {
                            "firstName": "Lucien",
                            "lastName": "Tesni\u00e8re",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucien Tesni\u00e8re"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 103
                            }
                        ],
                        "text": "More formally, dependency relations are asymmetric binary relationships between a head and a modifier (Tesni\u00e8re 1959)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 20
                            }
                        ],
                        "text": "For simplicity, we use nodes and their labels interchangeably, and the set of nodes corresponds to the words of the sentence: Vs = {w1, . . . , wn}."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 170605042,
            "fieldsOfStudy": [],
            "id": "28ae1b4f4e4bf55c27261259f25a15fe3f1c0916",
            "isKey": false,
            "numCitedBy": 1640,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Traite de linguistique paru de maniere posthume mais dont une ebauche avait ete publiee en 1953 sous le titre \"Esquisse d'une syntaxe structurale\""
            },
            "slug": "\u00c9l\u00e9ments-de-syntaxe-structurale-Tesni\u00e8re",
            "title": {
                "fragments": [],
                "text": "\u00c9l\u00e9ments de syntaxe structurale"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690411"
                        ],
                        "name": "U. Manber",
                        "slug": "U.-Manber",
                        "structuredName": {
                            "firstName": "Udi",
                            "lastName": "Manber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Manber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143921857"
                        ],
                        "name": "Sun Wu",
                        "slug": "Sun-Wu",
                        "structuredName": {
                            "firstName": "Sun",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sun Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 48
                            }
                        ],
                        "text": "Specifically, we indexed the BNC using Glimpse (Manber and Wu 1994), a fast and flexible indexing and query system.10 Glimpse supports approximate and exact matching, Boolean queries, wild cards, regular expressions, and many other options."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 739862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d9ca067d3f6df1cfc222dfa721ae156738942e3",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "GLIMPSE, which stands for GLobal IMPlicit SEarch, provides indexing and query schemes for file systems. The novelty of glimpse is that it uses a very small index - in most cases 2-4% of the size of the text - and still allows very flexible full-text retrieval including Boolean queries, approximate matching (i.e., allowing misspelling), and even searching for regular expressions. In a sense, glimpse extends agrep to entire file systems, while preserving most of its functionality and simplicity. Query times are typically slower than with inverted indexes, but they are still fast enough for many applications. For example, it took 5 seconds of CPU time to find all 19 occurrences of Usenix AND Winter in a file system containing 69MB of text spanning 4300 files. Glimpse is particularly designed for personal information, such as one's own file system. The main characteristic of personal information is that it is non-uniform and includes many types of documents. An information retrieval system for personal information should support many types of queries, flexible interaction, low overhead, and customization, All these are important features of glimpse."
            },
            "slug": "GLIMPSE:-A-Tool-to-Search-Through-Entire-File-Manber-Wu",
            "title": {
                "fragments": [],
                "text": "GLIMPSE: A Tool to Search Through Entire File Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Glimpse is particularly designed for personal information, such as one's own file system, that should support many types of queries, flexible interaction, low overhead, and customization, All these are important features of glimpse."
            },
            "venue": {
                "fragments": [],
                "text": "USENIX Winter"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115377844"
                        ],
                        "name": "T. Dunning",
                        "slug": "T.-Dunning",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Dunning",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dunning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 104
                            }
                        ],
                        "text": "In our experiments, we follow Lowe and McDonald (2000) in using the well-known log-likelihood ratio G2 (Dunning 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6465096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "025464b73f805e76689a7a20a48a9e9c0f4ff3ef",
            "isKey": false,
            "numCitedBy": 2829,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text."
            },
            "slug": "Accurate-Methods-for-the-Statistics-of-Surprise-and-Dunning",
            "title": {
                "fragments": [],
                "text": "Accurate Methods for the Statistics of Surprise and Coincidence"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The basis of a measure based on likelihood ratios that can be applied to the analysis of text is described, and in cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145175410"
                        ],
                        "name": "D. C. Howell",
                        "slug": "D.-C.-Howell",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Howell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. C. Howell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 94
                            }
                        ],
                        "text": "Eta-squared (\u03b72) is a statistic7 often used to measure the strength of an experimental effect (Howell 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60539820,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0991abe016abaa13ce431e83ac8d5863cdbce1e2",
            "isKey": false,
            "numCitedBy": 7306,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This seventh edition of Statistical Methods for Psychology, like the previous editions, surveys statistical techniques commonly used in the behavioral and social sciences, especially psychology and education. Although it is designed for advanced undergraduates and graduate students, it does not assume that students have had either a previous course in statistics or a course in mathematics beyond high-school algebra. Those students who have had an introductory course will find that the early material provides a welcome review. The book is suitable for either a one-term or a full-year course, and I have used it successfully for both. Since I have found that students, and faculty, frequently refer back to the book from which they originally learned statistics when they have a statistical problem, I have included material that will make the book a useful reference for future use. The instructor who wishes to omit this material will have no difficulty doing so. I have cut back on that material, however, to include only what is still likely to be useful. The idea of including every interesting idea had led to a book that was beginning to be daunting."
            },
            "slug": "Statistical-Methods-for-Psychology-Howell",
            "title": {
                "fragments": [],
                "text": "Statistical Methods for Psychology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50331001"
                        ],
                        "name": "J. Gropen",
                        "slug": "J.-Gropen",
                        "structuredName": {
                            "firstName": "Jess",
                            "lastName": "Gropen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gropen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 231
                            }
                        ],
                        "text": "\u2026hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118461862,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "0d1a2aad0ca0dfd02e1238cd5432c17837143fb9",
            "isKey": false,
            "numCitedBy": 460,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A contactless motion detector, comprising an output thyristor controlled by a switching network, has an oscillator coupled to the switching network via a pre-amplifier. The oscillator and the pre-amplifier are energized from a source of pulsating direct current via a supply circuit which is part of the switching network and includes a constant-current unit in parallel with the output thyristor. To prevent untimely switching of the thyristor when power is connected to the system, a delay unit retards the energization of the pre-amplifier until the oscillator has reached its operating condition. The delay unit may include one or more semiconductive devices, such as cascaded transistors or diodes, connected across a capacitor of a resistive/capacitive series circuit which bridges a smoothing capacitor inserted between a pair of bus bars."
            },
            "slug": "The-learnability-and-acquisition-of-the-dative-Gropen",
            "title": {
                "fragments": [],
                "text": "The learnability and acquisition of the dative alternation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "All our experiments were conducted on the British National Corpus (BNC), a 100- million word collection of samples of written and spoken English (Burnard 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 93
                            }
                        ],
                        "text": "Specifically, we evaluate the performance of our model on synonym questions from the Test of English as a Foreign Language (TOEFL)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "English for the Computer."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 84
                            }
                        ],
                        "text": "Arguably, this property holds great promise for languages less configurational than English."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 151
                            }
                        ],
                        "text": "We grouped all ambiguous noun tokens in SemCor into five frequency bands (frequencies were estimated from the BNC as it constitutes a larger sample of English than Semcor)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 64
                            }
                        ],
                        "text": "The test is designed to assess non-native speakers\u2019 knowledge of English."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 12
                            }
                        ],
                        "text": "The Test of English as a Foreign Language (TOEFL) is commonly used as a benchmark for comparing the merits of different similarity models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "English Verb Classes and Alternations: A Preliminary Investigation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 46
                            }
                        ],
                        "text": "The corpus represents a wide range of British English, including samples from newspapers, magazines, books (both academic and fiction), letters, essays, as well as spontaneous conversations, business or government meetings, radio shows, and phone-ins."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Users Guide for the British National Corpus. British National Corpus Consortium"
            },
            "venue": {
                "fragments": [],
                "text": "Users Guide for the British National Corpus. British National Corpus Consortium"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "\u2026[1999] for an overview), the cosine being the most popular:\nsimcos( x, y ) =\nn\u2211 i=1\nxiyi\u221a n\u2211\ni=1 x2i\n\u221a n\u2211\ni=1 y2i\n(1)\nSyntax-based semantic space models (Grefenstette 1994; Lin 1998a) go beyond mere co-occurrence by capturing syntactic relationships between words such as subject\u2013verb or\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Grefenstette (1994) uses a weighted version of Jaccard\u2019s coefficient, a measure of association commonly employed in information retrieval (Salton and McGill 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 10
                            }
                        ],
                        "text": "We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 216
                            }
                        ],
                        "text": "Using (8), these paths are mapped to the following basis elements:\n\u3008lorry, carry\u3009 carry\n\u3008lorry, a\u3009 a\n\u3008lorry, carry, apples\u3009 apples\n\u3008lorry, carry, might\u3009 might\n\u3008lorry, carry, apples, sweet\u3009 sweet\nA different mapping is used in Grefenstette (1994) and Lin (1998a), who consider only paths of length 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 62
                            }
                        ],
                        "text": "Figure 2 shows a syntax-based semantic space in the manner of Grefenstette (1994), using the basis elements (subj,lorry), (aux,might), (mod,sweet), and (obj,apples)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 181
                            }
                        ],
                        "text": "\u2026typically reflect argument structure (e.g., subject, object, indirect object) or modification (e.g., adjective\u2013noun, noun\u2013noun) and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999; Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 125
                            }
                        ],
                        "text": "Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "\u2026t, Jaccard\u2019s coefficient is defined as:\nsimJacc(t1, t2) = Attr(t1) \u2229 Attr(t2) Attr(t1) \u222a Attr(t2)\n(2)\nLin (1998a) constructs a semantic space similar to Grefenstette (1994) except that the matrix cells represent the number of times a target word t co-occurs with basis element (r, w), as shown in\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 209
                            }
                        ],
                        "text": "We describe traditional word-based co-occurrence models as exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levy and Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994) and Lin (1998a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 133
                            }
                        ],
                        "text": "Syntax-based vector space models have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 153
                            }
                        ],
                        "text": "Furthermore, the potential of syntax-based models has not been fully realized since most previous approaches limit themselves to a specific model class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 199
                            }
                        ],
                        "text": "\u2026(Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "For example, the construction of the space is either based on all relations (Grefenstette 1994; Lin 1998a) or a fixed subset (Lee 1999), but there is no qualitative distinction between different relations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 32
                            }
                        ],
                        "text": "**p < 0.01 (2-tailed)\nthesauri (Grefenstette 1994; Lin 1998a; Curran and Moens 2002) or used in applications that utilize semantic similarity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 88
                            }
                        ],
                        "text": "The basis elements (r, w) are treated as a single unit and are often called attributes (Grefenstette 1994; Curran and Moens 2002) or features (Lin 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 104
                            }
                        ],
                        "text": "Examples include word sense discrimination (Lin 1998a; Sch\u00fctze 1998), automatic thesaurus construction (Grefenstette 1994; Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general similarity-based approaches to NLP."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 148
                            }
                        ],
                        "text": "\u2026and the end word end(\u03c0) does not exceed the window size k:\ncont(t) = {\u03c0 \u2208 \u03a0t | abs(pos(start(\u03c0)) \u2212 pos(end(\u03c0))) \u2264 k} (5)\nThe dependency-based models proposed by Grefenstette (1994) and Lin (1998a) consider minimal syntactic contexts in the form of individual dependency relations, namely,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Explorations in Automatic Thesaurus Discovery. Kluwer Pad\u00f3 and Lapata Dependency-based Semantic Spaces Academic Publishers"
            },
            "venue": {
                "fragments": [],
                "text": "Explorations in Automatic Thesaurus Discovery. Kluwer Pad\u00f3 and Lapata Dependency-based Semantic Spaces Academic Publishers"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 161
                            }
                        ],
                        "text": "Our following experiment applies the dependency space introduced in this article to word sense disambiguation (WSD), a task which has received much attention in NLP and is ultimately important for document understanding."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "We automatically infer predominant senses in untagged text by incorporating our syntax-based semantic spaces into the modeling paradigm proposed by McCarthy et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "In order to demonstrate the scope of our framework, we evaluate our models on tasks popular in both cognitive science and NLP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 299
                            }
                        ],
                        "text": "Our contributions are threefold: a novel framework for semantic spaces that incorporates syntactic information in the form of dependency relations and generalizes previous syntax-based vector-based models; an application of this framework to a wide range of tasks relevant to cognitive modeling and NLP; and an empirical comparison of our dependency-based models against state-of-the-art word-based models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "McCarthy et al. (2004) undertook a thorough comparison and obtained best results with 50 neighbors using Lesk\u2019s (1986) and Jiang and Conrath\u2019s (1997) measures."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "Given the importance of WSD for basic NLP tasks and multilingual applications, a variety of approaches have been proposed for disambiguating word senses."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 49
                            }
                        ],
                        "text": "Furthermore, both the dependency-based model and McCarthy et al. (2004) significantly outperform the word-based model."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Despite their widespread use in NLP, syntax-based semantic spaces have attracted little attention in cognitive science and computational psycholinguistics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 195
                            }
                        ],
                        "text": "The BNC has been used extensively in building vector space models for many tasks relevant for cognitive science (Patel, Bullinaria, and Levy 1998; McDonald 2000; McDonald and Brew 2004) and NLP (McCarthy et al. 2004; Weeds 2003; Widdows 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 170
                            }
                        ],
                        "text": "Failure to do so would indicate that our model is deficient because it cannot capture basic semantic relatedness, a notion underlying many tasks in cognitive science and NLP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 181
                            }
                        ],
                        "text": "In this experiment, we examine whether the dependency-based models discussed in this article can be used for the sense ranking task, thereby assessing their potential for practical NLP tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 225
                            }
                        ],
                        "text": "Dependency-Based Construction of Semantic Space Models\nSebastian Pad\u00f3\u2217 Saarland University\nMirella Lapata\u2217\u2217 University of Edinburgh\nTraditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "The potential applications are many and varied both for cognitive science and NLP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "We evaluated our framework on tasks relevant for NLP and cognitive science and compared it against state-of-the-art models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "Syntax-based vector space models have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and Carroll 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 149
                            }
                        ],
                        "text": "\u2026a sense at random for each word type from its sense inventory and assuming that this is the first sense:\nRandomsr = 1|Wps| \u2211\nw \u2208Wps\n1 |S(w)| (22)\nLike McCarthy et al. (2004), we also assessed the word sense disambiguation potential (Accwsd) of the automatically acquired first senses for each word\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 158
                            }
                        ],
                        "text": "For example, we could experiment with more coarse-grained functions based on parts-of-speech or more fine-grained ones such as the relation\u2013word pairs used by McCarthy et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "We evaluate our framework on tasks relevant for cognitive science and NLP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 8
                            }
                        ],
                        "text": "Because McCarthy et al. (2004) use all available basis elements, their semantic space grows linearly with vocabulary (i.e., corpus) size."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 120
                            }
                        ],
                        "text": "They argue that the latter measure is more efficient for large scale WSD and use it exclusively in all subsequent work (McCarthy et al. 2004; Koeling, McCarthy, and Carroll 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "A number of NLP tasks could also benefit from the framework presented in this article."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "\u2026representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 119
                            }
                        ],
                        "text": "The total number of nouns after applying the frequency cutoffs was 2,75012 and the average sense ambiguity was 4.55\n12 McCarthy et al. (2004) use 2,595 nouns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "Our final experiment concentrates on unsupervised word sense disambiguation (WSD), thereby exploring the potential of the proposed framework for NLP applications requiring large scale semantic processing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 57
                            }
                        ],
                        "text": "Note that we have a slightly different set of nouns from McCarthy et al. (2004); this is due to the use of a different\nparser and a larger corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "McCarthy et al. (2004) use their ranking model to automatically infer the first senses of all nouns attested in SemCor, a subset of the Brown corpus containing 23,346 lemmas annotated with senses according to WordNet 1.6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "Vector space models of word co-occurrence have proved a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 236
                            }
                        ],
                        "text": "Examples include word sense discrimination (Lin 1998a; Sch\u00fctze 1998), automatic thesaurus construction (Grefenstette 1994; Curran and Moens 2002), automatic clustering, lexicon acquisition, and in general similarity-based approaches to NLP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "Experiments 2 and 3 examined the usefulness of our framework for NLP: we used our model to detect synonymy relations and to automatically acquire prevalent senses for polysemous words."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "The benchmark data set collected by Rubenstein and Goodenough (1965) is routinely used in NLP and cognitive science for development purposes\u2014for example, for evaluating automatic measures of semantic similarity (Resnik 1995; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003) or for exploring the parameter space of vector space models (McDonald 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 68
                            }
                        ],
                        "text": "An interesting observation is that our dependency model outperforms McCarthy et al. (2004) by a large margin (8.3"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 31
                            }
                        ],
                        "text": "We obtained results similar to McCarthy et al. (2004) on the sense ranking task and demonstrated that our model performs significantly better on WSD."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "The ability to identify the intended reading of a polysemous word (the word sense) in context is crucial for accomplishing many NLP tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 135
                            }
                        ],
                        "text": "Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique word, commonly referred to as \u201ctarget word,\u201d and each column represents a given linguistic context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "McCarthy et al. (2004) show that the annotation bottleneck can be avoided by inferring the first sense heuristic automatically from raw text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Finding predominant senses in untagged text"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66962602"
                        ],
                        "name": "G. Green",
                        "slug": "G.-Green",
                        "structuredName": {
                            "firstName": "Georgia",
                            "lastName": "Green",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Green"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 190
                            }
                        ],
                        "text": "\u2026hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 143986357,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "cd818e04b2778c6e3613ed265d647c83462a5616",
            "isKey": false,
            "numCitedBy": 355,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Semantics-and-Syntactic-Regularity-Green",
            "title": {
                "fragments": [],
                "text": "Semantics and Syntactic Regularity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126299280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9efffc63f81bf3f6cb6357ddc15e9cd9da75d16",
            "isKey": false,
            "numCitedBy": 27000,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-computations-Golub",
            "title": {
                "fragments": [],
                "text": "Matrix computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2912454"
                        ],
                        "name": "C. Fillmore",
                        "slug": "C.-Fillmore",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Fillmore",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fillmore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 175
                            }
                        ],
                        "text": "\u2026hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122540940,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "125af2d933dc3798140230d44c5872832884c52e",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Indirect-Object-Constructions-In-English-And-The-Of-Fillmore",
            "title": {
                "fragments": [],
                "text": "Indirect Object Constructions In English And The Ordering Of Transformations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34819990"
                        ],
                        "name": "E. Keenan",
                        "slug": "E.-Keenan",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Keenan",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Keenan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2727629"
                        ],
                        "name": "B. Comrie",
                        "slug": "B.-Comrie",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Comrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Comrie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 240
                            }
                        ],
                        "text": "It assigns a value of 1 to paths of length 1 and fractions to longer paths:\nvlength(\u03c0) = 1||\u03c0|| (11)\nA more linguistically informed path value function can be defined by taking into account the obliqueness hierarchy of grammatical relations (Keenan and Comrie 1977)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 204
                            }
                        ],
                        "text": "\u2026a value inversely proportional to its length, thus giving more weight to shorter paths corresponding to more direct relationships.\ngram-rel (vgram-rel, see Equation (12)) uses the obliqueness hierarchy (Keenan and Comrie 1977) to rank paths according to the salience of their grammatical relations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118251194,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "730e2bc78fa52b8feb7ef426505788324972cc2e",
            "isKey": false,
            "numCitedBy": 1178,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Noun-Phrase-Accessibility-and-Universal-Grammar-Keenan-Comrie",
            "title": {
                "fragments": [],
                "text": "Noun Phrase Accessibility and Universal Grammar"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 164
                            }
                        ],
                        "text": "These vary from simple edge-counting (Rada, Mili, and Bicknell 1989) to attempts to factor in peculiarities of the network structure by considering link direction (Hirst and St-Onge 1998), relative depth (Leacock and Chodorow 1998), and density (Agirre and Rigau 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 63192994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3f93c45e6ae2ad0e836de7141d612a398215cca",
            "isKey": false,
            "numCitedBy": 422,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Lexical-Chains-as-Representations-of-Context-for-of-Fellbaum-Miller",
            "title": {
                "fragments": [],
                "text": "Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3226331"
                        ],
                        "name": "C. Leacock",
                        "slug": "C.-Leacock",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Leacock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leacock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736799"
                        ],
                        "name": "M. Chodorow",
                        "slug": "M.-Chodorow",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Chodorow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Chodorow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 205
                            }
                        ],
                        "text": "These vary from simple edge-counting (Rada, Mili, and Bicknell 1989) to attempts to factor in peculiarities of the network structure by considering link direction (Hirst and St-Onge 1998), relative depth (Leacock and Chodorow 1998), and density (Agirre and Rigau 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59721988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54cd5ea7f987a23d663605640113859207490400",
            "isKey": false,
            "numCitedBy": 1053,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-local-context-and-wordnet-similarity-for-Leacock-Chodorow",
            "title": {
                "fragments": [],
                "text": "Combining local context and wordnet similarity for word sense identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680223"
                        ],
                        "name": "A. Smeaton",
                        "slug": "A.-Smeaton",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Smeaton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeaton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1759481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f0581f0cd8d35260c324466b4841632fb345346",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Natural-language-processing-and-information-Smeaton",
            "title": {
                "fragments": [],
                "text": "Natural language processing and information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2500077"
                        ],
                        "name": "Julie Weeds",
                        "slug": "Julie-Weeds",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Weeds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julie Weeds"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 22521075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb8b0c56bad2d3952ad1a71ed479b1679e0963eb",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 217,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Measures-and-applications-of-lexical-distributional-Weeds",
            "title": {
                "fragments": [],
                "text": "Measures and applications of lexical distributional similarity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428338"
                        ],
                        "name": "Freddy Y. Y. Choi",
                        "slug": "Freddy-Y.-Y.-Choi",
                        "structuredName": {
                            "firstName": "Freddy",
                            "lastName": "Choi",
                            "middleNames": [
                                "Y.",
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Freddy Y. Y. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400286782"
                        ],
                        "name": "P. Wiemer-Hastings",
                        "slug": "P.-Wiemer-Hastings",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Wiemer-Hastings",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wiemer-Hastings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47147237"
                        ],
                        "name": "Johanna D. Moore",
                        "slug": "Johanna-D.-Moore",
                        "structuredName": {
                            "firstName": "Johanna",
                            "lastName": "Moore",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johanna D. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 6
                            }
                        ],
                        "text": "In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 149
                            }
                        ],
                        "text": "\u2026of natural language processing (NLP) tasks, such as word sense discrimination (Sch\u00fctze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39184340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "405297d1cf45a2375a6a20294a5f28b5a4633cd0",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Semantic-Analysis-for-Text-Segmentation-Choi-Wiemer-Hastings",
            "title": {
                "fragments": [],
                "text": "Latent Semantic Analysis for Text Segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1968.Mathematical Structures of Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 202
                            }
                        ],
                        "text": "\u2026hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic and Cognition"
            },
            "venue": {
                "fragments": [],
                "text": "Semantic and Cognition"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A statistical 195 Downloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/coli.2007.33.2.161 by guest on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Matrix Computations. Johns Hopkins Series in the Mathematical Sciences"
            },
            "venue": {
                "fragments": [],
                "text": "Matrix Computations. Johns Hopkins Series in the Mathematical Sciences"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A statistical 195 Downloaded from http://www.mitpressjournals.org/doi/pdfplus/10.1162/coli.2007.33.2.161 by guest on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Fusion for Multi-Document Summarization: Paraphrasing and Generation.Ph.D"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 219
                            }
                        ],
                        "text": "\u2026hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Language Typology and Syntactic Description III: Grammatical Categories and the Lexicon"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic sense disambiguation: How to tell a pine cone from an ice cream cone"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1986 Special Interest Group in Documentation"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 175
                            }
                        ],
                        "text": "\u2026hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Indirect Object Constructions and the Ordering of Transformations"
            },
            "venue": {
                "fragments": [],
                "text": "Indirect Object Constructions and the Ordering of Transformations"
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "N:subj:V (null):lex-mod:N,N:conj:N (null):lex-mod:N,N:conj:N, N:lex-mod:(null) (null):lex-mod:N,N:gen:N (null):lex-mod:N,N:gen:N, N:lex-mod:(null) (null):lex-mod:N,N:mod:A (null"
            },
            "venue": {
                "fragments": [],
                "text": "lex-mod:N,N:mod:Pred (null):lex-mod:N,N:obj:V (null):lex-mod:N,N:subj:A (null):lex-mod:N,N:subj:V Prep:mod:N,N:lex-mod:(null) Prep:mod:N,N:nn:N V:obj:N,N:lex-mod:(null) V:obj:N,N:nn:N V:subj:N,N:lex-mod:(null) V:subj:N,N:nn:N Maximum contains all medium medium medium templates and: A:mod:A,A:mod:N,N"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Fusion for Multi-Document Summarization: Praphrasing and Generation"
            },
            "venue": {
                "fragments": [],
                "text": "Ph.D. thesis, Columbia University, New York."
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1989.Matrix Computations. Johns Hopkins Series in the Mathematical Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 71
                            }
                        ],
                        "text": "The path value function v assigns weights to paths, thus allowing linguistic knowledge to influence the construction of the space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 128
                            }
                        ],
                        "text": "On the other hand, attempts to generate more meaningful indexing terms for information retrieval (IR) using syntactic analysis (Salton and Smith 1989; Strzalkowski 1999; Henderson et al. 2002) have been largely unsuccessful."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the application of syntactic methodologies in automatic text indexing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th ACM SIGIR Conference"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases, our framework obtains results that are comparable or superior to the state of the art."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comprehending sentence structure"
            },
            "venue": {
                "fragments": [],
                "text": "Invitation to Cognitive Science"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Users Guide for the British National Corpus"
            },
            "venue": {
                "fragments": [],
                "text": "British National Corpus Consortium,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 114
                            }
                        ],
                        "text": "We can visualize the computation using a two-by-two contingency table whose four cells correspond to four events (Kilgarriff 2001):\nt \u00ac t b k l\n\u00ac b m n\nThe top left cell records the frequency k with which t and b co-occur (i.e., k corresponds to raw frequency counts)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparing corpora"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Corpus Linguistics"
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 56,
            "methodology": 42,
            "result": 26
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 103,
        "totalPages": 11
    },
    "page_url": "https://www.semanticscholar.org/paper/Dependency-Based-Construction-of-Semantic-Space-Pad\u00f3-Lapata/7441116c5b5a745708a9d7c5aa0ecf04e0c76c93?sort=total-citations"
}