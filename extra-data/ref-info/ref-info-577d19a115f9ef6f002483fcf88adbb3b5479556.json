{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 133
                            }
                        ],
                        "text": "In many applications, it would be more realistic to assume that there is some noise in the measurements (see e.g. (Hyv\u00e4rinen, 1998a; Hyv\u00e4rinen, 1999c)), which would mean adding a noise term in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 124
                            }
                        ],
                        "text": "Negentropy has the additional interesting property that it is invariant for invertible linear transformations (Comon, 1994; Hyv\u00e4rinen, 1999e)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1520136,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "551bb4142794dd682acf9a1159063158895e8214",
            "isKey": false,
            "numCitedBy": 1319,
            "numCiting": 165,
            "paperAbstract": {
                "fragments": [],
                "text": "A common problem encountered in such disciplines as statistics, data analysis, signal processing, and neural network research, is nding a suitable representation of multivariate data. For computational and conceptual simplicity, such a representation is often sought as a linear transformation of the original data. Well-known linear transformation methods include, for example, principal component analysis, factor analysis, and projection pursuit. A recently developed linear transformation method is independent component analysis (ICA), in which the desired representation is the one that minimizes the statistical dependence of the components of the representation. Such a representation seems to capture the essential structure of the data in many applications. In this paper, we survey the existing theory and methods for ICA."
            },
            "slug": "Survey-on-Independent-Component-Analysis-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Survey on Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper surveys the existing theory and methods for independent component analysis (ICA), in which the desired representation is the one that minimizes the statistical dependence of the components of the representation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120692531"
                        ],
                        "name": "Liuyue Wang",
                        "slug": "Liuyue-Wang",
                        "structuredName": {
                            "firstName": "Liuyue",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liuyue Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210204"
                        ],
                        "name": "R. Vig\u00e1rio",
                        "slug": "R.-Vig\u00e1rio",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vig\u00e1rio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vig\u00e1rio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768650"
                        ],
                        "name": "J. Joutsensalo",
                        "slug": "J.-Joutsensalo",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Joutsensalo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Joutsensalo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 310835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c93c897fd80f5246b839a2044798780cf2c5a77",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent component analysis (ICA) is a recently developed, useful extension of standard principal component analysis (PCA). The ICA model is utilized mainly in blind separation of unknown source signals from their linear mixtures. In this application only the source signals which correspond to the coefficients of the ICA expansion are of interest. In this paper, we propose neural structures related to multilayer feedforward networks for performing complete ICA. The basic ICA network consists of whitening, separation, and basis vector estimation layers. It can be used for both blind source separation and estimation of the basis vectors of ICA. We consider learning algorithms for each layer, and modify our previous nonlinear PCA type algorithms so that their separation capabilities are greatly improved. The proposed class of networks yields good results in test examples with both artificial and real-world data."
            },
            "slug": "A-class-of-neural-networks-for-independent-analysis-Karhunen-Oja",
            "title": {
                "fragments": [],
                "text": "A class of neural networks for independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes neural structures related to multilayer feedforward networks for performing complete independent component analysis (ICA) and modify the previous nonlinear PCA type algorithms so that their separation capabilities are greatly improved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 52
                            }
                        ],
                        "text": "To rigorously define ICA (Jutten and H\u00e9rault, 1991; Comon, 1994), we can use a statistical \u201clatent variables\u201d model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Negentropy has the additional interesting property that it is inv ariant for invertible linear transformations ( Comon, 1994;  Hyvarinen, 1999e)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 111
                            }
                        ],
                        "text": "Negentropy has the additional interesting property that it is invariant for invertible linear transformations (Comon, 1994; Hyv\u00e4rinen, 1999e)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16135158,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8d946c3eb1d1db376a89ad9342282163b5ae0930",
            "isKey": false,
            "numCitedBy": 5789,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent component analysis (ICA) is a statistical method for transforming an observed multidimensional random vector into components that are statistically as independent from each other as possible. In this paper, we use a combination of two different approaches for linear ICA: Comon's information-theoretic approach and the projection pursuit approach. Using maximum entropy approximations of differential entropy, we introduce a family of new contrast (objective) functions for ICA. These contrast functions enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions. The statistical properties of the estimators based on such contrast functions are analyzed under the assumption of the linear mixture model, and it is shown how to choose contrast functions that are robust and/or of minimum variance. Finally, we introduce simple fixed-point algorithms for practical optimization of the contrast functions. These algorithms optimize the contrast functions very fast and reliably."
            },
            "slug": "Fast-and-robust-fixed-point-algorithms-for-analysis-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Fast and robust fixed-point algorithms for independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Using maximum entropy approximations of differential entropy, a family of new contrast (objective) functions for ICA enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 152
                            }
                        ],
                        "text": "They need not be estimated with any great precision: in fact it is enough to estimate whether they are sub- or supergaussian (Cardoso and Laheld, 1996; Hyv\u00e4rinen and Oja, 1998; Lee et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8072151,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "bac464b9562ce85b1155883023f36b8e3e8ddba3",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis-by-general-nonlinear-Hyv\u00e4rinen-Oja",
            "title": {
                "fragments": [],
                "text": "Independent component analysis by general nonlinear Hebbian-like learning rules"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 118
                            }
                        ],
                        "text": "The variable y is assumed to be of zero mean and unit variance, and the functions Gi are some nonquadratic functions (Hyv\u00e4rinen, 1998b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 0
                            }
                        ],
                        "text": "(Hyv\u00e4rinen, 1998a; Hyv\u00e4rinen, 1999c)), which would mean adding a noise term in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 115
                            }
                        ],
                        "text": "In many applications, it would be more realistic to assume that there is some noise in the measurements (see e.g. (Hyv\u00e4rinen, 1998a; Hyv\u00e4rinen, 1999c)), which would mean adding a noise term in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 121
                            }
                        ],
                        "text": "To avoid the problems encountered with the preceding approximations of negentropy, new approximations were developed in (Hyv\u00e4rinen, 1998b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8366145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fa143dc7facbbc9be7b4b73ccf592588e77fa81",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis-in-the-presence-of-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Independent component analysis in the presence of Gaussian noise by maximizing joint likelihood"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11499343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdf197c1211d409006caabf66187604e4edea97a",
            "isKey": false,
            "numCitedBy": 218,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel approach for the problem of estimating the data model of independent component analysis (or blind source separation) in the presence of Gaussian noise is introduced. We define the Gaussian moments of a random variable as the expectations of the Gaussian function (and some related functions) with different scale parameters, and show how the Gaussian moments of a random variable can be estimated from noisy observations. This enables us to use Gaussian moments as one-unit contrast functions that have no asymptotic bias even in the presence of noise, and that are robust against outliers. To implement the maximization of the contrast functions based on Gaussian moments, a modification of the fixed-point (FastICA) algorithm is introduced."
            },
            "slug": "Gaussian-moments-for-noisy-independent-component-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Gaussian moments for noisy independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A novel approach for the problem of estimating the data model of independent component analysis (or blind source separation) in the presence of Gaussian noise is introduced and a modification of the fixed-point (FastICA) algorithm is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144288586"
                        ],
                        "name": "A. Back",
                        "slug": "A.-Back",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Back",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Back"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 872703,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "ba45192f771feb2e3a671cc641282351081c8113",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the application of a signal processing technique known as independent component analysis (ICA) or blind source separation to multivariate financial time series such as a portfolio of stocks. The key idea of ICA is to linearly map the observed multivariate time series into a new space of statistically independent components (ICs). We apply ICA to three years of daily returns of the 28 largest Japanese stocks and compare the results with those obtained using principal component analysis. The results indicate that the estimated ICs fall into two categories, (i) infrequent large shocks (responsible for the major changes in the stock prices), and (ii) frequent smaller fluctuations (contributing little to the overall level of the stocks). We show that the overall stock price can be reconstructed surprisingly well by using a small number of thresholded weighted ICs. In contrast, when using shocks derived from principal components instead of independent components, the reconstructed price is less similar to the original one. ICA is shown to be a potentially powerful method of analyzing and understanding driving mechanisms in financial time series. The application to portfolio optimization is described in Chin and Weigend (1998)."
            },
            "slug": "A-First-Application-of-Independent-Component-to-Back-Weigend",
            "title": {
                "fragments": [],
                "text": "A First Application of Independent Component Analysis to Extracting Structure from Stock Returns"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the overall stock price can be reconstructed surprisingly well by using a small number of thresholded weighted ICs, and when using shocks derived from principal components instead of independent components, the reconstructed price is less similar to the original one."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 177
                            }
                        ],
                        "text": "They need not be estimated with any great precision: in fact it is enough to estimate whether they are sub- or supergaussian (Cardoso and Laheld, 1996; Hyv\u00e4rinen and Oja, 1998; Lee et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207739442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642f41bc18b36dead2b85e45a93bcfb8379224a2",
            "isKey": false,
            "numCitedBy": 1710,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "An extension of the infomax algorithm of Bell and Sejnowski (1995) is presented that is able blindly to separate mixed signals with sub- and supergaussian source distributions. This was achieved by using a simple type of learning rule first derived by Girolami (1997) by choosing negentropy as a projection pursuit index. Parameterized probability distributions that have sub- and supergaussian regimes were used to derive a general learning rule that preserves the simple architecture proposed by Bell and Sejnowski (1995), is optimized using the natural gradient by Amari (1998), and uses the stability analysis of Cardoso and Laheld (1996) to switch between sub- and supergaussian regimes. We demonstrate that the extended infomax algorithm is able to separate 20 sources with a variety of source distributions easily. Applied to high-dimensional data from electroencephalographic recordings, it is effective at separating artifacts such as eye blinks and line noise from weaker electrical signals that arise from sources in the brain."
            },
            "slug": "Independent-Component-Analysis-Using-an-Extended-Lee-Girolami",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis Using an Extended Infomax Algorithm for Mixed Subgaussian and Supergaussian Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An extension of the infomax algorithm of Bell and Sejnowski (1995) is presented that is able blindly to separate mixed signals with sub- and supergaussian source distributions and is effective at separating artifacts such as eye blinks and line noise from weaker electrical signals that arise from sources in the brain."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6219133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca1d23be869380ac9e900578c601c2d1febcc0c9",
            "isKey": false,
            "numCitedBy": 2373,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-\u201cindependent-components\u201d-of-natural-scenes-are-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "The \u201cindependent components\u201d of natural scenes are edge filters"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57275136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ff80d4cdf8cc91964381a1d226dc730e0057146",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse coding is a method for finding a representation of data in which each of the components of the representation is only rarely significantlyactive. Such a representation isclosely related to independent component analysis (ICA) and has some neurophysiological plausibility. In this chapter, we show how sparse coding can be used for image denoising. We model the noise-free image data by independent component analysis and denoise a noisy image by maximum likelihood estimation of the noisy version of the ICA model. This leads to the application of a soft-thresholding (shrinkage) operator on the components of sparse coding. Our method is closely related to the method of wavelet shrinkage and coring methods, but it has the important benefit that the representation is determined solely by the statistical properties of the data. In fact, our method can be seen as a simple rederivation of the wavelet shrinkage method for image data, using just the basic principle of maximum likelihood estimation. On the other hand, it allows the method to adapt to different kinds of data sets."
            },
            "slug": "Image-Denoising-by-Sparse-Code-Shrinkage-Haykin-Kosko",
            "title": {
                "fragments": [],
                "text": "Image Denoising by Sparse Code Shrinkage"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This chapter shows how sparse coding can be used for image denoising and model the noise-free image data by independent component analysis and denoise a noisy image by maximum likelihood estimation of the noisy version of the ICA model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To rigorously define ICA ( Jutten and Herault, 1991;  Comon, 19 94), we can use a statistical \u201clatent variables\u201d model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 26
                            }
                        ],
                        "text": "To rigorously define ICA (Jutten and H\u00e9rault, 1991; Comon, 1994), we can use a statistical \u201clatent variables\u201d model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": false,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 82
                            }
                        ],
                        "text": "Another related contrast function was derived from a neural network viewpoint in (Bell and Sejnowski, 1995; Nadal and Parga, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8753,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14086981,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb54ea020644b6125ce97958f2747d3a1223d485",
            "isKey": false,
            "numCitedBy": 425,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse coding is a method for finding a representation of data in which each of the components of the representation is only rarely significantly active. Such a representation is closely related to redundancy reduction and independent component analysis, and has some neurophysiological plausibility. In this article, we show how sparse coding can be used for denoising. Using maximum likelihood estimation of nongaussian variables corrupted by gaussian noise, we show how to apply a soft-thresholding (shrinkage) operator on the components of sparse coding so as to reduce noise. Our method is closely related to the method of wavelet shrinkage, but it has the important benefit over wavelet methods that the representation is determined solely by the statistical properties of the data. The wavelet representation, on the other hand, relies heavily on certain mathematical properties (like self-similarity) that may be only weakly related to the properties of natural data."
            },
            "slug": "Sparse-Code-Shrinkage:-Denoising-of-Nongaussian-by-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Sparse Code Shrinkage: Denoising of Nongaussian Data by Maximum Likelihood Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article shows how sparse coding can be used for denoising, using maximum likelihood estimation of nongaussian variables corrupted by gaussian noise to apply a soft-thresholding (shrinkage) operator on the components of sparse coding so as to reduce noise."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583773"
                        ],
                        "name": "L. Parra",
                        "slug": "L.-Parra",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Parra",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Parra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 47
                            }
                        ],
                        "text": "Indeed, several authors, e.g., (Cardoso, 1997; Pearlmutter and Parra, 1997), proved the surprising result that the principle of network entropy maximization, or \u201cinfomax\u201d, is equivalent to maximum likelihood estimation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, this approach gives a rigorous justification for the heuristic principles used above."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9704838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cea9d59691410447bde0f39a028ffb3e21181a3",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In the square linear blind source separation problem, one must find a linear unmixing operator which can detangle the result xi(t) of mixing n unknown independent sources si(t) through an unknown n \u00d7 n mixing matrix A(t) of causal linear filters: xi = \u03a3j aij * sj. We cast the problem as one of maximum likelihood density estimation, and in that framework introduce an algorithm that searches for independent components using both temporal and spatial cues. We call the resulting algorithm \"Contextual ICA,\" after the (Bell and Sejnowski 1995) Infomax algorithm, which we show to be a special case of cICA. Because cICA can make use of the temporal structure of its input, it is able separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms."
            },
            "slug": "Maximum-Likelihood-Blind-Source-Separation:-A-of-Pearlmutter-Parra",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood Blind Source Separation: A Context-Sensitive Generalization of ICA"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The resulting algorithm is called cICA, after the (Bell and Sejnowski 1995) Infomax algorithm, which is able to separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The above version of FastICA could be compared with the stochastic gradient method for maximizing likelihood ( Amari et al., 1996;  Bell and Sejnowski, 1995; Cardoso and Laheld, 1996; Cichocki and Unbehauen, 1996):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, we give a version of FastICA that shows explicitly the connection to the well-known infomax or maximum likelihood algorithm introduced in ( Amari et al., 1996;  Bell and Sejnowski, 1995; Cardoso and Laheld, 1996; Cichocki and Unbehauen, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 118
                            }
                        ],
                        "text": "The variable y is assumed to be of zero mean and unit variance, and the functions Gi are some nonquadratic functions (Hyv\u00e4rinen, 1998b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 115
                            }
                        ],
                        "text": "In many applications, it would be more realistic to assume that there is some noise in the measurements (see e.g. (Hyv\u00e4rinen, 1998a; Hyv\u00e4rinen, 1999c)), which would mean adding a noise term in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1138,
                                "start": 111
                            }
                        ],
                        "text": "The convergence is cubic (or at least quadratic), under the assumption of the ICA data model (for a proof, see Hyv\u00e4rinen (1999a)). This is in contrast to ordinary ICA algorithms based on (stochastic) gradient descent methods, where the convergence is only linear. This means a very fast convergence, as has been confirmed by simulations and experiments on real data (see Giannakopoulos, Karhunen and Oja (1998)). 2. Contrary to gradient-based algorithms, there are no step size parameters to choose. This means that the algorithm is easy to use. 3. The algorithm finds directly independent components of (practically) any non-Gaussian distribution using any non-linearityg. This is in contrast to many algorithms, where some estimate of the probability distribution function has to be first available, and the non-linearity must be chosen accordingly. 4. The performance of the method can be optimized by choosing a suitable non-linearity g. In particular, one can obtain algorithms that are robust and/or of minimum variance. In fact, the two non-linearities in Eq. (40) have some optimal properties; for details see Hyva \u0308rinen (1999a). 5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 79
                            }
                        ],
                        "text": "This operator can be learned with a modification of the FastICA algorithm; see Hyv\u00e4rinen (1999d) for details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 121
                            }
                        ],
                        "text": "To avoid the problems encountered with the preceding approximations of negentropy, new approximations were developed in (Hyv\u00e4rinen, 1998b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 411,
                                "start": 111
                            }
                        ],
                        "text": "The convergence is cubic (or at least quadratic), under the assumption of the ICA data model (for a proof, see Hyv\u00e4rinen (1999a)). This is in contrast to ordinary ICA algorithms based on (stochastic) gradient descent methods, where the convergence is only linear. This means a very fast convergence, as has been confirmed by simulations and experiments on real data (see Giannakopoulos, Karhunen and Oja (1998))."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 111
                            }
                        ],
                        "text": "The convergence is cubic (or at least quadratic), under the assumption of the ICA data model (for a proof, see Hyv\u00e4rinen (1999a))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 17
                            }
                        ],
                        "text": "For details, see Hyv\u00e4rinen (1999b). In FastICA, convergence speed is optimized by the choice of the matrices diag( i) and diag( b i)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11359216,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "04528b1e4303170535abccf1c968ae16b6c1e397",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a first-order approximation of the density of maximum entropy for a continuous 1-D random variable, given a number of simple constraints. This results in a density expansion which is somewhat similar to the classical polynomial density expansions by Gram-Charlier and Edgeworth. Using this approximation of density, an approximation of 1-D differential entropy is derived. The approximation of entropy is both more exact and more robust against outliers than the classical approximation based on the polynomial density expansions, without being computationally more expensive. The approximation has applications, for example, in independent component analysis and projection pursuit."
            },
            "slug": "New-Approximations-of-Differential-Entropy-for-and-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "New Approximations of Differential Entropy for Independent Component Analysis and Projection Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A first-order approximation of the density of maximum entropy for a continuous 1-D random variable is derived, which results in a density expansion which is somewhat similar to the classical polynomial density expansions by Gram-Charlier and Edgeworth."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37367995"
                        ],
                        "name": "X. Giannakopoulos",
                        "slug": "X.-Giannakopoulos",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Giannakopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Giannakopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15288294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcca19c6d8f46158dcd1f508ed0a7f55d7ea17f5",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Several neural algorithms for Independent Component Analysis (ICA) have been introduced lately, but their computational properties have not yet been systematically studied. In this paper, we compare the accuracy, convergence speed, computational load, and other properties of five prominent neural or semi-neural ICA algorithms. The comparison reveals some interesting differences between the algorithms."
            },
            "slug": "An-Experimental-Comparison-of-Neural-ICA-Algorithms-Giannakopoulos-Karhunen",
            "title": {
                "fragments": [],
                "text": "An Experimental Comparison of Neural ICA Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper compares the accuracy, convergence speed, computational load, and other properties of five prominent neural or semi-neural ICA algorithms and reveals some interesting differences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2356353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b78626ce1a562c05b1c06f9c805e839f9760b9ab",
            "isKey": false,
            "numCitedBy": 20813,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiresolution representations are effective for analyzing the information content of images. The properties of the operator which approximates a signal at a given resolution were studied. It is shown that the difference of information between the approximation of a signal at the resolutions 2/sup j+1/ and 2/sup j/ (where j is an integer) can be extracted by decomposing this signal on a wavelet orthonormal basis of L/sup 2/(R/sup n/), the vector space of measurable, square-integrable n-dimensional functions. In L/sup 2/(R), a wavelet orthonormal basis is a family of functions which is built by dilating and translating a unique function psi (x). This decomposition defines an orthogonal multiresolution representation called a wavelet representation. It is computed with a pyramidal algorithm based on convolutions with quadrature mirror filters. Wavelet representation lies between the spatial and Fourier domains. For images, the wavelet representation differentiates several spatial orientations. The application of this representation to data compression in image coding, texture discrimination and fractal analysis is discussed. >"
            },
            "slug": "A-Theory-for-Multiresolution-Signal-Decomposition:-Mallat",
            "title": {
                "fragments": [],
                "text": "A Theory for Multiresolution Signal Decomposition: The Wavelet Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the difference of information between the approximation of a signal at the resolutions 2/sup j+1/ and 2/Sup j/ can be extracted by decomposing this signal on a wavelet orthonormal basis of L/sup 2/(R/sup n/), the vector space of measurable, square-integrable n-dimensional functions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28110514"
                        ],
                        "name": "N. Delfosse",
                        "slug": "N.-Delfosse",
                        "structuredName": {
                            "firstName": "Nathalie",
                            "lastName": "Delfosse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Delfosse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144667255"
                        ],
                        "name": "P. Loubaton",
                        "slug": "P.-Loubaton",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Loubaton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Loubaton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 24
                            }
                        ],
                        "text": "It is not hard to show (Delfosse and Loubaton, 1995) that the maxima are at the points when exactly one of the elements of vector z is zero and the other nonzero; because of the unit circle constraint, the nonzero element must be equal to 1 or -1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12894028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7a108b22336a90f728eb62cade867054c24481d",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-blind-separation-of-independent-sources:-A-Delfosse-Loubaton",
            "title": {
                "fragments": [],
                "text": "Adaptive blind separation of independent sources: A deflation approach"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732878"
                        ],
                        "name": "R. Unbehauen",
                        "slug": "R.-Unbehauen",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Unbehauen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Unbehauen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62325111,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "a01629e0d236c1fad21cfc38fa33b58d53ea5e70",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Two unsupervised, self-normalizing, adaptive learning algorithms are developed for robust blind identification and/or blind separation of independent source signals from a linear mixture of them. One of these algorithms is developed for on-line learning of a single-layer feed-forward neural network model and a second one for a feedback (fully recurrent) neural network model. The proposed algorithms are robust, efficient, fast and suitable for real-time implementations. Moreover, they ensure the separation of extremely weak or badly scaled stationary signals, as well as a successful separation even if the mixture matrix is very ill-conditioned (near singular). The performance of the proposed algorithms is illustrated by computer simulation experiments."
            },
            "slug": "Robust-neural-networks-with-on-line-learning-for-of-Cichocki-Unbehauen",
            "title": {
                "fragments": [],
                "text": "Robust neural networks with on-line learning for blind identification and blind separation of sources"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Two unsupervised, self-normalizing, adaptive learning algorithms are developed for robust blind identification and/or blind separation of independent source signals from a linear mixture of them and are suitable for real-time implementations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952058"
                        ],
                        "name": "S. Makeig",
                        "slug": "S.-Makeig",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Makeig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Makeig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144437435"
                        ],
                        "name": "T. Jung",
                        "slug": "T.-Jung",
                        "structuredName": {
                            "firstName": "Tzyy-Ping",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14080786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f61237db63fb1616fe2c9ff8a81d863a72500a37",
            "isKey": false,
            "numCitedBy": 1951,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the distance between the skull and brain and their different resistivities, electroencephalographic (EEG) data collected from any point on the human scalp includes activity generated within a large brain area. This spatial smearing of EEG data by volume conduction does not involve significant time delays, however, suggesting that the Independent Component Analysis (ICA) algorithm of Bell and Sejnowski [1] is suitable for performing blind source separation on EEG data. The ICA algorithm separates the problem of source identification from that of source localization. First results of applying the ICA algorithm to EEG and event-related potential (ERP) data collected during a sustained auditory detection task show: (1) ICA training is insensitive to different random seeds. (2) ICA may be used to segregate obvious artifactual EEG components (line and muscle noise, eye movements) from other sources. (3) ICA is capable of isolating overlapping EEG phenomena, including alpha and theta bursts and spatially-separable ERP components, to separate ICA channels. (4) Nonstationarities in EEG and behavioral state can be tracked using ICA via changes in the amount of residual correlation between ICA-filtered output channels."
            },
            "slug": "Independent-Component-Analysis-of-Data-Makeig-Bell",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis of Electroencephalographic Data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "First results of applying the ICA algorithm to EEG and event-related potential (ERP) data collected during a sustained auditory detection task show that ICA training is insensitive to different random seeds and ICA may be used to segregate obvious artifactual EEG components from other sources."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716124"
                        ],
                        "name": "Beate H. Laheld",
                        "slug": "Beate-H.-Laheld",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Laheld",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beate H. Laheld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17839672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8637f042e3d2a2d45de41566b4203646987a8424",
            "isKey": false,
            "numCitedBy": 1501,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Source separation consists of recovering a set of independent signals when only mixtures with unknown coefficients are observed. This paper introduces a class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called equivariant adaptive separation via independence (EASI). The EASI algorithms are based on the idea of serial updating. This specific form of matrix updates systematically yields algorithms with a simple structure for both real and complex mixtures. Most importantly, the performance of an EASI algorithm does not depend on the mixing matrix. In particular, convergence rates, stability conditions, and interference rejection levels depend only on the (normalized) distributions of the source signals. Closed-form expressions of these quantities are given via an asymptotic performance analysis. The theme of equivariance is stressed throughout the paper. The source separation problem has an underlying multiplicative structure. The parameter space forms a (matrix) multiplicative group. We explore the (favorable) consequences of this fact on implementation, performance, and optimization of EASI algorithms."
            },
            "slug": "Equivariant-adaptive-source-separation-Cardoso-Laheld",
            "title": {
                "fragments": [],
                "text": "Equivariant adaptive source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called EASI, which yields algorithms with a simple structure for both real and complex mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210204"
                        ],
                        "name": "R. Vig\u00e1rio",
                        "slug": "R.-Vig\u00e1rio",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vig\u00e1rio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vig\u00e1rio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2190275"
                        ],
                        "name": "V. Jousm\u00e4ki",
                        "slug": "V.-Jousm\u00e4ki",
                        "structuredName": {
                            "firstName": "Veikko",
                            "lastName": "Jousm\u00e4ki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Jousm\u00e4ki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32405184"
                        ],
                        "name": "M. H\u00e4m\u00e4l\u00e4inen",
                        "slug": "M.-H\u00e4m\u00e4l\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "H\u00e4m\u00e4l\u00e4inen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H\u00e4m\u00e4l\u00e4inen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873866"
                        ],
                        "name": "R. Hari",
                        "slug": "R.-Hari",
                        "structuredName": {
                            "firstName": "Riitta",
                            "lastName": "Hari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17626526,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "104473d1e7eeae9a31f274d1194ce10408d217eb",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We have studied the application of an independent component analysis (ICA) approach to the identification and possible removal of artifacts from a magnetoencephalographic (MEG) recording. This statistical technique separates components according to the kurtosis of their amplitude distributions over time, thus distinguishing between strictly periodical signals, and regularly and irregularly occurring signals. Many artifacts belong to the last category. In order to assess the effectiveness of the method, controlled artifacts were produced, which included saccadic eye movements and blinks, increased muscular tension due to biting and the presence of a digital watch inside the magnetically shielded room. The results demonstrate the capability of the method to identify and clearly isolate the produced artifacts."
            },
            "slug": "Independent-Component-Analysis-for-Identification-Vig\u00e1rio-Jousm\u00e4ki",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The results demonstrate the capability of the independent component analysis (ICA) method to identify and clearly isolate the produced artifacts."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210204"
                        ],
                        "name": "R. Vig\u00e1rio",
                        "slug": "R.-Vig\u00e1rio",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vig\u00e1rio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vig\u00e1rio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2538598"
                        ],
                        "name": "J. S\u00e4rel\u00e4",
                        "slug": "J.-S\u00e4rel\u00e4",
                        "structuredName": {
                            "firstName": "Jaakko",
                            "lastName": "S\u00e4rel\u00e4",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. S\u00e4rel\u00e4"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15342848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4013be99e08d5d981b6afec52ffda3990e6175d",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a novel approach to the problem of decomposition of auditory evoked fields (AEF), measured by magnetoencephalography (MEG), into basic components. This approach is based on independent component analysis (ICA), that separates components according to the kurtosis of their amplitude distribution over time. The fixed-point algorithm used extracts one independent component at a time, allowing the combination of a high resolution 122-channel whole-scalp neuromagnetometer, to a fast and very efficient implementation of ICA."
            },
            "slug": "Independent-Component-Analysis-in-Wave-of-Auditory-Vig\u00e1rio-S\u00e4rel\u00e4",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis in Wave Decomposition of Auditory Evoked Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A novel approach to the problem of decomposition of auditory evoked fields (AEF), measured by magnetoencephalography (MEG), into basic components is introduced, based on independent component analysis (ICA), that separates components according to the kurtosis of their amplitude distribution over time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039472"
                        ],
                        "name": "N. Parga",
                        "slug": "N.-Parga",
                        "structuredName": {
                            "firstName": "N\u00e9stor",
                            "lastName": "Parga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Parga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another related contrast function was derived from a neural network viewpoint in (Bell and Sejnowski, 1995;  Nadal and Parga, 1994 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 108
                            }
                        ],
                        "text": "Another related contrast function was derived from a neural network viewpoint in (Bell and Sejnowski, 1995; Nadal and Parga, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115302789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "698aedd44c51da829228e2c7d243960345efeb94",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the consequences of maximizing information transfer in a simple neural network (one input layer, one output layer), focusing on the case of nonlinear transfer functions. We assume that both receptive fields (synaptic efficacies) and transfer functions can be adapted to the environment. The main result is that, for bounded and invertible transfer functions, in the case of a vanishing additive output noise, and no input noise, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow. We also show that this result is valid for linear and, more generally, unbounded, transfer functions, provided optimization is performed under an additive constraint, i.e. which can be written as a sum of terms, each one being specific to one output neuron. Finally, we study the effect of a non-zero input noise. We find that, to first order in the input noise, assumed to be small in comparison with th..."
            },
            "slug": "Nonlinear-neurons-in-the-low-noise-limit:-a-code-5-Nadal-Parga",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low-noise limit: a factorial code maximizes information transfer Network 5"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The main result is that, for bounded and invertible transfer functions, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818391"
                        ],
                        "name": "G. Kerkyacharian",
                        "slug": "G.-Kerkyacharian",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Kerkyacharian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kerkyacharian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145897897"
                        ],
                        "name": "D. Picard",
                        "slug": "D.-Picard",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Picard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Picard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12737710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b20b4095236f461ab067d0c23bc9653d3d1f9c0",
            "isKey": false,
            "numCitedBy": 1647,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "Much recent effort has sought asymptotically minimax methods for recovering infinite dimensional objects-curves, densities, spectral densities, images-from noisy data. A now rich and complex body of work develops nearly or exactly minimax estimators for an array of interesting problems. Unfortunately, the results have rarely moved into practice, for a variety of reasons-among them being similarity to known methods, computational intractability and lack of spatial adaptivity. We discuss a method for curve estimation based on n noisy data: translate the empirical wavelet coefficients towards the origin by an amount \u221a(2 log n) /\u221an. The proposal differs from those in current use, is computationally practical and is spatially adaptive; it thus avoids several of the previous objections. Further, the method is nearly minimax both for a wide variety of loss functions-pointwise error, global error measured in L p -norms, pointwise and global error in estimation of derivatives-and for a wide range of smoothness classes, including standard Holder and Sobolev classes, and bounded variation. This is a much broader near optimality than anything previously proposed: we draw loose parallels with near optimality in robustness and also with the broad near eigenfunction properties of wavelets themselves. Finally, the theory underlying the method is interesting, as it exploits a correspondence between statistical questions and questions of optimal recovery and information-based complexity"
            },
            "slug": "Wavelet-Shrinkage:-Asymptopia-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Wavelet Shrinkage: Asymptopia?"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A method for curve estimation based on n noisy data: translate the empirical wavelet coefficients towards the origin by an amount \u221a(2 log n) /\u221an and draw loose parallels with near optimality in robustness and also with the broad near eigenfunction properties of wavelets themselves."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210204"
                        ],
                        "name": "R. Vig\u00e1rio",
                        "slug": "R.-Vig\u00e1rio",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vig\u00e1rio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vig\u00e1rio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17061385,
            "fieldsOfStudy": [
                "Computer Science",
                "Geology"
            ],
            "id": "133a230b7e5db7bde7752712123cf9160c1cbe90",
            "isKey": false,
            "numCitedBy": 593,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extraction-of-ocular-artefacts-from-EEG-using-Vig\u00e1rio",
            "title": {
                "fragments": [],
                "text": "Extraction of ocular artefacts from EEG using independent component analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Electroencephalography and clinical neurophysiology"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 108
                            }
                        ],
                        "text": "Another related contrast function was derived from a neural network viewpoint in (Bell and Sejnowski, 1995; Nadal and Parga, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "We will explain this approach here, and show that it leads to the same principle of finding most nongaussian directions as was described above."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 15816289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15e5ac0a7c3542ce58a67c43fa0e81398be2e5a7",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the consequences of maximizing information transfer in a simple neural network (one input layer, one output layer), focussing on the case of non linear transfer functions. We assume that both receptive elds (synaptic eecacies) and transfer functions can be adapted to the environment. The main result is that, for bounded and invertible transfer functions, in the case of a vanishing additive output noise, and no input noise, maximization of information (Linsker'sinfomax principle) leads to a factorial code-hence to the same solution as required by the redundancy reduction principle of Barlow. We show also that this result is valid for linear, more generally unbounded, transfer functions, provided optimization is performed under an additive constraint, that is which can be written as a sum of terms, each one being speciic to one output neuron. Finally we study the eeect of a non zero input noise. We nd that, at rst order in the input noise, assumed to be small as compared to the-small-output noise, the above results are still valid, provided the output noise is uncorrelated from one neuron to the other. P.A.C.S. 87.30 Biophysics of neurophysiological processes Short title: Information maximization with non linear neurons To appear in NETWORK INDEX: nadalparga.infomaxredred.ps.Z nadal@physique.ens.fr 19 pages Infomax applied to non linear neurons, in the low noise limit, leads to redundancy reduction."
            },
            "slug": "Non-linear-neurons-in-the-low-noise-limit-:-a-code-Nadal",
            "title": {
                "fragments": [],
                "text": "Non linear neurons in the low noise limit : a factorial code maximizes information transferJean"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The main result is that, for bounded and invertible transfer functions, maximization of information (Linsker'sinfomax principle) leads to a factorial code-hence to the same solution as required by the redundancy reduction principle of Barlow."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144616256"
                        ],
                        "name": "D. Pham",
                        "slug": "D.-Pham",
                        "structuredName": {
                            "firstName": "Dinh",
                            "lastName": "Pham",
                            "middleNames": [
                                "Tuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94375334"
                        ],
                        "name": "P. Garrat",
                        "slug": "P.-Garrat",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Garrat",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Garrat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 100
                            }
                        ],
                        "text": "It is possible to formulate directly the likelihood in the noise-free ICA model, which was done in (Pham et al., 1992), and then estimate the model by a maximum likelihood method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 78
                            }
                        ],
                        "text": "Denoting byW= (w1, ...,wn)T the matrix A\u22121, the log-likelihood takes the form (Pham et al., 1992):\nL= T\n\u2211 t=1\nn\n\u2211 i=1 log fi(wTi x(t))+T log |detW| (31)\nwhere the fi are the density functions of the si (here assumed to be known), and the x(t),t = 1, ...,T are the realizations of x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 21
                            }
                        ],
                        "text": "The classical method of approximating negentropy is using higher-order moments, for example as follows (Jones and Sibson, 1987):\nJ(y) \u2248 1 12 E{y3}2+ 1 48 kurt(y)2 (23)\nThe random variable y is assumed to be of zero mean and unit variance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15960752,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "04659300743b0a716154918340c6743d435de902",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Fig. 1. Absolute value of the crosstalk with respect to the number of samples (NS) used to estimate the cross cumulants. Each point is the average of 10 experiments. to estimate the cross-cumulants. Each point in Fig. 1 corresponds to the average over 10 experiments, in which the mixing matrix is randomly chosen: The matrix entries mC3 (i # j) are random numbers in the range [-1, +1]. With 500 samples, a residual crosstalk of about-20 dB is obtained. In the case of nonstationary signals, cross-cumulant estimation must be done on few samples and has a larger variance. Consequently, it can lead to more inaccurate estimation of the mixing matrix. We still obtained an interesting performance: a residual crosstalk of about-15 to-20 dB, with various signals (colored noise, speech) and statistics estimated over 500 samples. In this correspondence, we proved that the mixing matrix can be. estimated using fourth-ordercross-cumulants, for two mixtures of two non-Gaussian sources. Solutions are obtained by rooting a fourth-order polynomial equation. Using second-order cross-cumulants allows us to simplify the method; the solution is then obtained by rooting two second-order polynomial equations and gives the result if one source is Gaussian. The methods are then quite simple, but its roots are very sensitive to the accuracy of the estimated cumulants. In fact, this direct solution is less accurate than indirect methods, especially adaptive a l g o r i b s. Moreover, we restricted the study to the separation of two sources, and theoretical solutions for three sources or more seems not easily tractable. However, in the case of two mixtures of two sources, it may give a good starting point with a small computation cost for any adaptive algorithm. REFERENCES J.-F. Cardoso, \" Blind identification of independent signals, \" in Proc."
            },
            "slug": "Separation-of-a-mixture-of-independent-sources-a-Pham-Garrat",
            "title": {
                "fragments": [],
                "text": "Separation of a mixture of independent sources through a maximum likelihood approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proved that the mixing matrix can be estimated using fourth-ordercross-cumulants, for two mixtures of two non-Gaussian sources, and theoretical solutions for three sources or more seems not easily tractable."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3361799"
                        ],
                        "name": "E. Sorouchyari",
                        "slug": "E.-Sorouchyari",
                        "structuredName": {
                            "firstName": "Esfandiar",
                            "lastName": "Sorouchyari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sorouchyari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 26
                            }
                        ],
                        "text": "To rigorously define ICA (Jutten and H\u00e9rault, 1991; Comon, 1994), we can use a statistical \u201clatent variables\u201d model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37484086,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f72708fb94c1f097216bca5431c325e50f69037e",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-III:-Stability-Sorouchyari",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part III: Stability analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016914"
                        ],
                        "name": "J. Tukey",
                        "slug": "J.-Tukey",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tukey",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tukey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 20
                            }
                        ],
                        "text": "Projection pursuit (Friedman and Tukey, 1974; Friedman, 1987; Huber, 1985; Jones and Sibson, 1987) is a technique developed in statistics for finding \u201cinteresting\u201d projections of multidimensional data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7997450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e12d7b5498d251692d87abc3ee983c078fee7f5f",
            "isKey": false,
            "numCitedBy": 1652,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for the analysis of multivariate data is presented and is discussed in terms of specific examples. The algorithm seeks to find one-and two-dimensional linear projections of multivariate data that are relatively highly revealing."
            },
            "slug": "A-Projection-Pursuit-Algorithm-for-Exploratory-Data-Friedman-Tukey",
            "title": {
                "fragments": [],
                "text": "A Projection Pursuit Algorithm for Exploratory Data Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An algorithm for the analysis of multivariate data is presented and is discussed in terms of specific examples to find one-and two-dimensional linear projections of multivariable data that are relatively highly revealing."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2654304"
                        ],
                        "name": "R. Cristescu",
                        "slug": "R.-Cristescu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Cristescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cristescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798865"
                        ],
                        "name": "T. Ristaniemi",
                        "slug": "T.-Ristaniemi",
                        "structuredName": {
                            "firstName": "Tapani",
                            "lastName": "Ristaniemi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ristaniemi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768650"
                        ],
                        "name": "J. Joutsensalo",
                        "slug": "J.-Joutsensalo",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Joutsensalo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Joutsensalo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 352,
                                "start": 278
                            }
                        ],
                        "text": "But the number of parameters to be estimated is often so high that suitable blind source separation techniques taking into account the available prior knowledge provide a clear performance improvement over more traditional estimation techniques (Ristaniemi & Joutsensalo, 1999; Cristescu et al., 2000). A. Hyv\u00e4rinen, E. Oja / Neural Networks 13 (2000) 411\u2013430 428"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 301,
                                "start": 245
                            }
                        ],
                        "text": "But the number of parameters to be estimated is often so high that suitable blind source separation techniques taking into account the available prior knowledge provide a clear performance improvement over more traditional estimation techniques (Ristaniemi & Joutsensalo, 1999; Cristescu et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17489073,
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "id": "d2ae1adab909c9f4883b916773341ce3c5a7801e",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a method for timing acquisition in a code division multiple access (CDMA) communications system. The assumptions made are that the data signals corresponding to diierent users are statistically independent, and that a minor pilot sequence corresponding to the desired user is available. An independent component analysis (ICA) approach based on a fast xed-point algorithm is used for simultaneous demodulation of the symbol sequence and nding of synchronization information of the user of interest. No additional pilot signal is used for the timing acquisition. Our semi-blind method performs better than conventional matched lter detector and CMOE receiver."
            },
            "slug": "Delay-Estimation-in-CDMA-Communications-Using-A-Cristescu-Ristaniemi",
            "title": {
                "fragments": [],
                "text": "Delay Estimation in CDMA Communications Using A FastICA Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper proposes a semi-blind method for timing acquisition in a code division multiple access (CDMA) communications system that performs better than conventional matched matched lter detector and CMOE receiver."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40574103"
                        ],
                        "name": "S. Klinke",
                        "slug": "S.-Klinke",
                        "structuredName": {
                            "firstName": "Sigbert",
                            "lastName": "Klinke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Klinke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549124"
                        ],
                        "name": "J. Polzehl",
                        "slug": "J.-Polzehl",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Polzehl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Polzehl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 46
                            }
                        ],
                        "text": "Projection pursuit (Friedman and Tukey, 1974; Friedman, 1987; Huber, 1985; Jones and Sibson, 1987) is a technique developed in statistics for finding \u201cinteresting\u201d projections of multidimensional data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60628897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43d7cce568c4d6ec9cc2d95fe54dd5fa8f51e936",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "\u201cProjection Pursuit\u201d (PP) stands for a class of exploratory projection techniques. This class contains methods designed for analyzing high dimensional data using low-dimensional projections. The main idea is to describe \u201cinteresting\u201d projections by maximizing an objective function or projection pursuit index."
            },
            "slug": "Exploratory-Projection-Pursuit-Klinke-Polzehl",
            "title": {
                "fragments": [],
                "text": "Exploratory Projection Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "\u201cProjection Pursuit\u201d (PP) stands for a class of exploratory projection techniques that contains methods designed for analyzing high dimensional data using low-dimensional projections."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 911,
                                "start": 82
                            }
                        ],
                        "text": "Independent Component Analysis (ICA); see Hyv\u00e4rinen, Karhunen, and Oja (2001) and Cichocki and Amari (2002) is a novel statistical signal and data analysis method. It assumes a statistical model whereby the observed multivariate data x, typically given as a large database of samples, are assumed to be linear or nonlinear mixtures of some unknown latent variables. In the linear noise-free case the model is simply x = As. The mixing coefficients or elements of matrix A are also unknown. The latent variables si or elements of vector s are nongaussian and mutually independent, and they are called the independent components of the observed data. By ICA, these independent components, also called sources or factors, can be found. This problem is also called blind source separation because almost everything is unknown. Many practical algorithms exist such as the FastICA algorithm (Hyv\u00e4rinen and Oja (1997); Hyv\u00e4rinen (1999)), which is generally considered to be one of the best ones of the existing algorithms and is widely used by practitioners all over the world."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 82
                            }
                        ],
                        "text": "Independent Component Analysis (ICA); see Hyv\u00e4rinen, Karhunen, and Oja (2001) and Cichocki and Amari (2002) is a novel statistical signal and data analysis method."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61077157,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8fbfe61b6726477acb13183d46c4c6119389b62c",
            "isKey": false,
            "numCitedBy": 1441,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": "Find the secret to improve the quality of life by reading this adaptive blind signal and image processing. This is a kind of book that you need now. Besides, it can be your favorite book to read after having this book. Do you ask why? Well, this is a book that has different characteristic with others. You may not need to know who the author is, how well-known the work is. As wise word, never judge the words from who speaks, but make the words as your good value to your life."
            },
            "slug": "Adaptive-blind-signal-and-image-processing-Cichocki-Amari",
            "title": {
                "fragments": [],
                "text": "Adaptive blind signal and image processing"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Find the secret to improve the quality of life by reading this adaptive blind signal and image processing and make the words as your good value to your life."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 32
                            }
                        ],
                        "text": "Indeed, several authors, e.g., (Cardoso, 1997; Pearlmutter and Parra, 1997), proved the surprising result that the principle of network entropy maximization, or \u201cinfomax\u201d, is equivalent to maximum likelihood estimation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14149261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aaf352dc0b8c02e22bf0e11dc7bbcbed90e4f16f",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for the blind separation of sources can be derived from several different principles. This article shows that the infomax (information-maximization) principle is equivalent to the maximum likelihood. The application of the infomax principle to source separation consists of maximizing an output entropy."
            },
            "slug": "Infomax-and-maximum-likelihood-for-blind-source-Cardoso",
            "title": {
                "fragments": [],
                "text": "Infomax and maximum likelihood for blind source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article shows that the infomax (information-maximization) principle is equivalent to the maximum likelihood."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 36
                            }
                        ],
                        "text": "A typical example is the uniform distibution in eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 82
                            }
                        ],
                        "text": "The differential entropy H of a random vector y with density f (y) is defined as (Cover and Thomas, 1991; Papoulis, 1991):\nH(y) = \u2212 \u222b f (y) log f (y)dy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 23
                            }
                        ],
                        "text": "For a proof, see e.g. (Cover and Thomas, 1991; Papoulis, 1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 61
                            }
                        ],
                        "text": "An important property of mutual information (Papoulis, 1991; Cover and Thomas, 1991) is that we have for an invertible linear transformation y=Wx:\nI(y1,y2, ...,yn) =\u2211 i H(yi)\u2212H(x)\u2212 log|detW|."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 51
                            }
                        ],
                        "text": "For introductions on information theory, see e.g. (Cover and Thomas, 1991; Papoulis, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": true,
            "numCitedBy": 42785,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064699986"
                        ],
                        "name": "D. Chakrabarti",
                        "slug": "D.-Chakrabarti",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Chakrabarti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Chakrabarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068766142"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 132
                            }
                        ],
                        "text": "The FastICA is based on a fixed-point iteration scheme for finding a maximum of the nongaussianity of wTx, as measured in (25), see (Hyv\u00e4rinen and Oja, 1997; Hyv\u00e4rinen, 1999a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118274211,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "6ade6139ee56684cdf190f7f1212541fcb5ffb69",
            "isKey": false,
            "numCitedBy": 2270,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An apparatus for hydrolytic degradation of plastics in which plastic material is deposited into a tubular housing via a feed hopper. An elongated screw shaft has a first section in the form of a high pitch screw thread disposed below the feed hopper to receive and advance the material to a second section. The second section of the screw shaft is in the form of a lower pitch thread for compressing the plastic material and transferring it to a longer, third section in the form of kneading discs, from which material passes through an outlet nozzle section to a cyclone separator where trapped gases and liquid may be withdrawn. The tubular housing is vented upstream of the feed hopper and a water inlet pipe is disposed adjacent to the second section of the screw shaft, downstream of the feed hopper. The outlet nozzle section is provided with pressure measuring and regulating means and a liquid level measuring and regulating device."
            },
            "slug": "A-fast-fixed-point-algorithm-for-independent-Chakrabarti-Hoyer",
            "title": {
                "fragments": [],
                "text": "A fast fixed - point algorithm for independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2501863"
                        ],
                        "name": "D. Luenberger",
                        "slug": "D.-Luenberger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Luenberger",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Luenberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117941806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2bbea031af4e0aab292323c6dcd128050b26540",
            "isKey": false,
            "numCitedBy": 5460,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nEngineers must make decisions regarding the distribution of expensive resources in a manner that will be economically beneficial. This problem can be realistically formulated and logically analyzed with optimization theory. This book shows engineers how to use optimization theory to solve complex problems. Unifies the large field of optimization with a few geometric principles. Covers functional analysis with a minimum of mathematics. Contains problems that relate to the applications in the book."
            },
            "slug": "Optimization-by-Vector-Space-Methods-Luenberger",
            "title": {
                "fragments": [],
                "text": "Optimization by Vector Space Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book shows engineers how to use optimization theory to solve complex problems with a minimum of mathematics and unifies the large field of optimization with a few geometric principles."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 69
                            }
                        ],
                        "text": "The main problem is that kurtosis can be very sensitive to outliers (Huber, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 3
                            }
                        ],
                        "text": "In this approach that is an alternative to the model estimation approach, we define the ICA of a random vector x as an invertible transformation as in (6), where the matrixW is determined so that the mutual information of the transformed components si is minimized."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 62
                            }
                        ],
                        "text": "Projection pursuit (Friedman and Tukey, 1974; Friedman, 1987; Huber, 1985; Jones and Sibson, 1987) is a technique developed in statistics for finding \u201cinteresting\u201d projections of multidimensional data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 32
                            }
                        ],
                        "text": "More precisely, it is roughly equivalent to finding 1-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 29
                            }
                        ],
                        "text": "It has been argued by Huber (Huber, 1985) and by Jones and Sibson (Jones and Sibson, 1987) that the Gaussian distribution is the least interesting one, and that the most\ninteresting directions are those that show the least Gaussian distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Projection pursuit. The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Projection pursuit. The Annals of Statistics"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1991940"
                        ],
                        "name": "J. Proakis",
                        "slug": "J.-Proakis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Proakis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Proakis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 132
                            }
                        ],
                        "text": "The term log |detW| in the likelihood comes from the classic rule for (linearly) transforming random variables and their densities (Papoulis, 1991): In general, for any random vector x with density px and for any matrixW, the density of y=Wx is given by px(Wx)|detW|."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 20
                            }
                        ],
                        "text": "A typical example is the uniform distibution in eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 106
                            }
                        ],
                        "text": "The differential entropy H of a random vector y with density f (y) is defined as (Cover and Thomas, 1991; Papoulis, 1991):\nH(y) = \u2212 \u222b f (y) log f (y)dy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 47
                            }
                        ],
                        "text": "For a proof, see e.g. (Cover and Thomas, 1991; Papoulis, 1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 134
                            }
                        ],
                        "text": "To avoid the problems encountered with the preceding approximations of negentropy, new approximations were developed in (Hyv\u00e4rinen, 1998b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 45
                            }
                        ],
                        "text": "An important property of mutual information (Papoulis, 1991; Cover and Thomas, 1991) is that we have for an invertible linear transformation y=Wx:\nI(y1,y2, ...,yn) =\u2211 i H(yi)\u2212H(x)\u2212 log|detW|."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 75
                            }
                        ],
                        "text": "For introductions on information theory, see e.g. (Cover and Thomas, 1991; Papoulis, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2072334,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7eeb730524a8e980f91c923b8e1d026b17883e38",
            "isKey": true,
            "numCitedBy": 7904,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probability,-random-variables-and-stochastic-Proakis",
            "title": {
                "fragments": [],
                "text": "Probability, random variables and stochastic processes"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 103
                            }
                        ],
                        "text": "The classical method of approximating negentropy is using higher-order moments, for example as follows (Jones and Sibson, 1987): J(y) \u2248 1 12 E{y3}2+ 1 48 kurt(y)2 (23)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 66
                            }
                        ],
                        "text": "It has been argued by Huber (Huber, 1985) and by Jones and Sibson (Jones and Sibson, 1987) that the Gaussian distribution is the least interesting one, and that the most"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 75
                            }
                        ],
                        "text": "Projection pursuit (Friedman and Tukey, 1974; Friedman, 1987; Huber, 1985; Jones and Sibson, 1987) is a technique developed in statistics for finding \u201cinteresting\u201d projections of multidimensional data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 67
                            }
                        ],
                        "text": "It has been argued by Huber (Huber, 1985) and by Jones and Sibson (Jones and Sibson, 1987) that the Gaussian distribution is the least interesting one, and that the most\ninteresting directions are those that show the least Gaussian distribution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 104
                            }
                        ],
                        "text": "The classical method of approximating negentropy is using higher-order moments, for example as follows (Jones and Sibson, 1987):\nJ(y) \u2248 1 12 E{y3}2+ 1 48 kurt(y)2 (23)\nThe random variable y is assumed to be of zero mean and unit variance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1987).What is projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": "J. of the Royal Statistical Society,"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143929773"
                        ],
                        "name": "M. C. Jones",
                        "slug": "M.-C.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33734211"
                        ],
                        "name": "R. Sibson",
                        "slug": "R.-Sibson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Sibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sibson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125481163,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1ebb53a7e5cff86b2b42d1108a0fa81f571d8894",
            "isKey": false,
            "numCitedBy": 1404,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-projection-pursuit-Jones-Sibson",
            "title": {
                "fragments": [],
                "text": "What is projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98114104"
                        ],
                        "name": "S. Port",
                        "slug": "S.-Port",
                        "structuredName": {
                            "firstName": "Sidney",
                            "lastName": "Port",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Port"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122447472,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fe174df145cf8aadd1043d191916f01944308c41",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probability,-Random-Variables,-and-Stochastic-Port",
            "title": {
                "fragments": [],
                "text": "Probability, Random Variables, and Stochastic Processes\u2014Second Edition (Athanasios Papoulis)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Independent Component Analysis (ICA); see Hyv\u00e4rinen, Karhunen, and Oja (2001) and Cichocki and Amari (2002) is a novel statistical signal and data analysis method."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 220697840,
            "fieldsOfStudy": [],
            "id": "67ea293fc82fde1f47a253cd8062b01d7bbbd5a2",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6519466"
                        ],
                        "name": "A. Hyvarinen",
                        "slug": "A.-Hyvarinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyvarinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyvarinen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 133
                            }
                        ],
                        "text": "In many applications, it would be more realistic to assume that there is some noise in the measurements (see e.g. (Hyv\u00e4rinen, 1998a; Hyv\u00e4rinen, 1999c)), which would mean adding a noise term in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 124
                            }
                        ],
                        "text": "Negentropy has the additional interesting property that it is invariant for invertible linear transformations (Comon, 1994; Hyv\u00e4rinen, 1999e)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61272672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b66f79a854289997e04045adc4af150520c95e86",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SURVEY-OF-INDEPENDENT-COMPONENT-ANALYSIS-Hyvarinen",
            "title": {
                "fragments": [],
                "text": "SURVEY OF INDEPENDENT COMPONENT ANALYSIS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145061328"
                        ],
                        "name": "R. Gonz\u00e1lez",
                        "slug": "R.-Gonz\u00e1lez",
                        "structuredName": {
                            "firstName": "Rafael",
                            "lastName": "Gonz\u00e1lez",
                            "middleNames": [
                                "Corsino"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gonz\u00e1lez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50739223"
                        ],
                        "name": "P. Wintz",
                        "slug": "P.-Wintz",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Wintz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wintz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57769088,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "6b3f78ffac2d6a86a94f8f7e116008cc2aa31694",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Instructor's-manual-for-digital-image-processing-Gonz\u00e1lez-Wintz",
            "title": {
                "fragments": [],
                "text": "Instructor's manual for digital image processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114788213"
                        ],
                        "name": "D. Signorini",
                        "slug": "D.-Signorini",
                        "structuredName": {
                            "firstName": "DavidF.",
                            "lastName": "Signorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Signorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4137433"
                        ],
                        "name": "J. Slattery",
                        "slug": "J.-Slattery",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Slattery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058057862"
                        ],
                        "name": "S. Dodds",
                        "slug": "S.-Dodds",
                        "structuredName": {
                            "firstName": "Sally",
                            "lastName": "Dodds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dodds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51934565"
                        ],
                        "name": "V. Lane",
                        "slug": "V.-Lane",
                        "structuredName": {
                            "firstName": "V",
                            "lastName": "Lane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095059657"
                        ],
                        "name": "P. Littlejohns",
                        "slug": "P.-Littlejohns",
                        "structuredName": {
                            "firstName": "P",
                            "lastName": "Littlejohns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Littlejohns"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2878979,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "20b844e395355b40fa5940c61362ec40e56027aa",
            "isKey": false,
            "numCitedBy": 4703,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-Signorini-Slattery",
            "title": {
                "fragments": [],
                "text": "Neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "The Lancet"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2000591"
                        ],
                        "name": "K. Kiviluoto",
                        "slug": "K.-Kiviluoto",
                        "structuredName": {
                            "firstName": "Kimmo",
                            "lastName": "Kiviluoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kiviluoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34734823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8791864cc42401fb5bb5209a2acff94347a6a426",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-Component-Analysis-for-Parallel-Time-Kiviluoto-Oja",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis for Parallel Financial Time Series"
            },
            "venue": {
                "fragments": [],
                "text": "ICONIP"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 177
                            }
                        ],
                        "text": "They need not be estimated with any great precision: in fact it is enough to estimate whether they are sub- or supergaussian (Cardoso and Laheld, 1996; Hyv\u00e4rinen and Oja, 1998; Lee et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 221401721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "219ee1c7eee3142c8a4156733fbaacd5cc753ac6",
            "isKey": false,
            "numCitedBy": 302,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-Component-Analysis-Using-an-Extended-Lee-Girolami",
            "title": {
                "fragments": [],
                "text": "Independent Component Analysis Using an Extended Infomax Algorithm for Mixed Sub-Gaussian and Super-Gaussian Sources"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Comput."
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Infomax and maximum likelihood for s"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 152
                            }
                        ],
                        "text": "They need not be estimated with any great precision: in fact it is enough to estimate whether they are sub- or supergaussian (Cardoso and Laheld, 1996; Hyv\u00e4rinen and Oja, 1998; Lee et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 125
                            }
                        ],
                        "text": "They need not be estimated with any great precision: in fact it is enough to estimate whether the y ar sub- or supergaussian (Cardoso and Laheld, 1996; Hyv\u00e4rinen and Oja, 1998; Lee et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component ana"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spikes and bumps: Artefacts generated by independent component analysis with insufficient sample size"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Workshop on Independent Component Analysis and Signal Separation ( ICA \u2019 99 )"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 20
                            }
                        ],
                        "text": "Projection pursuit (Friedman and Tukey, 1974; Friedman, 1987; Huber, 1985; Jones and Sibson, 1987) is a technique developed in statistics for finding \u201cinteresting\u201d projections of multidimensional data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A projection pursuit algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The \u2019independent compone"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 133
                            }
                        ],
                        "text": "In many applications, it would be more realistic to assume that there is some noise in the measurements (see e.g. (Hyv\u00e4rinen, 1998a; Hyv\u00e4rinen, 1999c)), which would mean adding a noise term in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 124
                            }
                        ],
                        "text": "Negentropy has the additional interesting property that it is invariant for invertible linear transformations (Comon, 1994; Hyv\u00e4rinen, 1999e)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gaussian moments for noisy independ"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 247
                            }
                        ],
                        "text": "An example of a real-world communications application where blind separ ation techniques are useful is the separation of the user\u2019s own signal from the interfering other users\u2019 signals in CDMA (Code-Division Multiple Access) mobile communications (Ristaniemi and Joutsensalo, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 248
                            }
                        ],
                        "text": "But th e number of parameters to be estimated is often so high that suitable blind source separation techniques taki ng into account the available prior knowledge provide a clear performance improvement over more traditional estim ation techniques (Ristaniemi and Joutsensalo, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the performan"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 32
                            }
                        ],
                        "text": "Indeed, several authors, e.g., (Cardoso, 1997; Pearlmutter and Parra, 1997), proved the surprising result that the principle of network entropy maximization, or \u201cinfomax\u201d, is equivalent to maximum likelihood estimation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, this approach gives a rigorous justification for the heuristic principles used above."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Infomax and maximum likelihood for source separation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Letters on Signal Processing"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 181
                            }
                        ],
                        "text": "3 FastICA and maximum likelihood Finally, we give a version of FastICA that shows explicitly t he connection to the well-known infomax or maximum likelihood algorithm introduced in (Amari et al., 1996; Bel l and Sejnowski, 1995; Cardoso and Laheld, 1996; Cichocki and Unbehauen, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 110
                            }
                        ],
                        "text": "The above version of FastICA could be compared with the stoch astic gradient method for maximizing likelihood (Amari et al., 1996; Bell and Sejnowski, 1995; Cardoso a nd Laheld, 1996; Cichocki and Unbehauen, 1996):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new learni"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 92
                            }
                        ],
                        "text": "Moreover, dimension reduction prevents overlearning, which can sometimes be observed in ICA (Hyv\u00e4rinen et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spikes and bumps: Artefacts generated by independent component analysis with insufficient sample size"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int. Workshop on Independent Component Analysis and Signal Separation (ICA'99)"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 118
                            }
                        ],
                        "text": "The variable y is assumed to be of zero mean and unit variance, and the functions Gi are some nonquadratic functions (Hyv\u00e4rinen, 1998b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 115
                            }
                        ],
                        "text": "In many applications, it would be more realistic to assume that there is some noise in the measurements (see e.g. (Hyv\u00e4rinen, 1998a; Hyv\u00e4rinen, 1999c)), which would mean adding a noise term in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 121
                            }
                        ],
                        "text": "To avoid the problems encountered with the preceding approximations of negentropy, new approximations were developed in (Hyv\u00e4rinen, 1998b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New approximations of differential"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 26
                            }
                        ],
                        "text": "To rigorously define ICA (Jutten and H\u00e9rault, 1991; Comon, 1994), we can use a statistical \u201clatent variables\u201d model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of sourc"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 74
                            }
                        ],
                        "text": "More details on the experiments and their interpretation ca n be found in (Kiviluoto and Oja, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 3
                            }
                        ],
                        "text": "In (Kiviluoto and Oja, 1998), we applied ICA on a different pr oblem: the cashflow of several stores belonging to the same retail chain, trying to find the fundamental facto rs c mmon to all stores that affect the cashflow data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component ana"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 133
                            }
                        ],
                        "text": "In many applications, it would be more realistic to assume that there is some noise in the measurements (see e.g. (Hyv\u00e4rinen, 1998a; Hyv\u00e4rinen, 1999c)), which would mean adding a noise term in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 124
                            }
                        ],
                        "text": "Negentropy has the additional interesting property that it is invariant for invertible linear transformations (Comon, 1994; Hyv\u00e4rinen, 1999e)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Survey on independent component ana"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-linear neurons"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new learning algorithm for blind source separation"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 133
                            }
                        ],
                        "text": "In many applications, it would be more realistic to assume that there is some noise in the measurements (see e.g. (Hyv\u00e4rinen, 1998a; Hyv\u00e4rinen, 1999c)), which would mean adding a noise term in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 124
                            }
                        ],
                        "text": "Negentropy has the additional interesting property that it is invariant for invertible linear transformations (Comon, 1994; Hyv\u00e4rinen, 1999e)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast and robust fixed-point algorith"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 24
                            }
                        ],
                        "text": "It is not hard to show (Delfosse and Loubaton, 1995) that the maxima are at the points when exactly one of the elements of vector z is zero and the other nonzero; because of the unit circle constraint, the nonzero element must be equal to 1 or -1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 23
                            }
                        ],
                        "text": "It is not hard to show (Delfosse and Loubaton, 1995) that the m axi a are at the points when exactly one of the elements of vector z is zero and the other nonzero; because of the unit circle cons trai t, the nonzero element must be equal to 1 or -1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive blind separa"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Equivariant adaptive source"
            },
            "venue": {
                "fragments": [],
                "text": "separation.IEEE Transactions on Signal Processing"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 181
                            }
                        ],
                        "text": "3 FastICA and maximum likelihood Finally, we give a version of FastICA that shows explicitly t he connection to the well-known infomax or maximum likelihood algorithm introduced in (Amari et al., 1996; Bel l and Sejnowski, 1995; Cardoso and Laheld, 1996; Cichocki and Unbehauen, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 110
                            }
                        ],
                        "text": "The above version of FastICA could be compared with the stoch astic gradient method for maximizing likelihood (Amari et al., 1996; Bell and Sejnowski, 1995; Cardoso a nd Laheld, 1996; Cichocki and Unbehauen, 1996):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust neural networ"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 133
                            }
                        ],
                        "text": "The FastICA is based on a fixed-point iteration scheme for find ing a maximum of the nongaussianity of wTx, as measured in (25), see (Hyv\u00e4rinen and Oja, 1997; Hyv\u00e4rinen , 1999a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast fixed-point algorith"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extraction of ocular artifacts from EEG using independent component analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Electroenceph . Clin. Neurophysiol"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 68
                            }
                        ],
                        "text": "A better method is the recently introduced Wavelet Shrinkage method (Donoho et al., 1995) in which a transform based on wavelets is used, or methods based on median filtering (Gonzales and Wintz, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wavelet shrinkage: asymptopia? Journal of the Royal Statistical Society, Ser"
            },
            "venue": {
                "fragments": [],
                "text": "B, 57:301\u2013337."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 40
                            }
                        ],
                        "text": "According to the Kuhn-Tucker conditions (Luenberger, 1969), the optima of E{G(wTx)} under the constraint E{(wTx)2} = \u2225w\u22252 = 1 are obtained at points where E{xg(wTx)}\u2212\u03b2w= 0 (41)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1969).Optimization by Vector Space Methods. Wiley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the performance of blind source separation in CDMA downlink"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int. Workshop on Independent Component Analysis and Signal Separation (ICA'99)"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 52
                            }
                        ],
                        "text": "Imagine that you are in a room where two people are speaking simultaneously."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 52
                            }
                        ],
                        "text": "To rigorously define ICA (Jutten and H\u00e9rault, 1991; Comon, 1994), we can use a statistical \u201clatent variables\u201d model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 111
                            }
                        ],
                        "text": "Negentropy has the additional interesting property that it is invariant for invertible linear transformations (Comon, 1994; Hyv\u00e4rinen, 1999e)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis\u2014a new concept? Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Independent component analysis\u2014a new concept? Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 118
                            }
                        ],
                        "text": "The variable y is assumed to be of zero mean and unit variance, and the functions Gi are some nonquadratic functions (Hyv\u00e4rinen, 1998b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 115
                            }
                        ],
                        "text": "In many applications, it would be more realistic to assume that there is some noise in the measurements (see e.g. (Hyv\u00e4rinen, 1998a; Hyv\u00e4rinen, 1999c)), which would mean adding a noise term in the model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 121
                            }
                        ],
                        "text": "To avoid the problems encountered with the preceding approximations of negentropy, new approximations were developed in (Hyv\u00e4rinen, 1998b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis in t"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 34,
            "methodology": 26,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 78,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Independent-component-analysis:-algorithms-and-Hyv\u00e4rinen-Oja/577d19a115f9ef6f002483fcf88adbb3b5479556?sort=total-citations"
}