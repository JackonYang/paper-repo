{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 141
                            }
                        ],
                        "text": "Cybern., 68, 23-29 21\nA W\nu\ns\nfilters\nbasis functions causes\nimage patch, x image ensemble\nFigure 1: The Blind Linear Image Synthesis model (Olshausen & Field, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 30
                            }
                        ],
                        "text": "The starting point is that of Olshausen & Field 1996, depicted in Figure 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 104
                            }
                        ],
                        "text": "Some of these lters are Gabor-like and resemble those produced by thesparseness-maximisation network of Olshausen & Field (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 22
                            }
                        ],
                        "text": "Field's arguments led Olshausen & Field (1996), in work that motivatedour approach, to attempt to learn receptive elds by maximising sparseness."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9526302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e309e441a38ccee6456bd02e0f1e894e44180d53",
            "isKey": true,
            "numCitedBy": 618,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural images contain characteristic statistical regularities that set them apart from purely random images. Understanding what these regularities are can enable natural images to be coded more efficiently. In this paper, we describe some of the forms of structure that are contained in natural images, and we show how these are related to the response properties of neurons at early stages of the visual system. Many of the important forms of structure require higher-order (i.e. more than linear, pairwise) statistics to characterize, which makes models based on linear Hebbian learning, or principal components analysis, inappropriate for finding efficient codes for natural images. We suggest that a good objective for an efficient coding of natural scenes is to maximize the sparseness of the representation, and we show that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex."
            },
            "slug": "Natural-image-statistics-and-efficient-coding.-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Natural image statistics and efficient coding."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that a good objective for an efficient coding of natural Scenes is to maximize the sparseness of the representation, and it is shown that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 141
                            }
                        ],
                        "text": "Cybern., 68, 23-29 21\nA W\nu\ns\nfilters\nbasis functions causes\nimage patch, x image ensemble\nFigure 1: The Blind Linear Image Synthesis model (Olshausen & Field, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 30
                            }
                        ],
                        "text": "The starting point is that of Olshausen & Field 1996, depicted in Figure 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 104
                            }
                        ],
                        "text": "Some of these lters are Gabor-like and resemble those produced by thesparseness-maximisation network of Olshausen & Field (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 22
                            }
                        ],
                        "text": "Field's arguments led Olshausen & Field (1996), in work that motivatedour approach, to attempt to learn receptive elds by maximising sparseness."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15125241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44352b35791ceaad3439b8ccf165cc9b4978d801",
            "isKey": true,
            "numCitedBy": 31,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural images contain characteristic statistical regularities that set them apart from purely random images. Understanding what these regularities are can enable natural images to be coded more eeciently. In this paper, we describe some of the forms of structure that are contained in natural images, and we show how these are related to the response properties of neurons at early stages of the visual system. Many of the important forms of structure require higher-order (i.e., more than linear, pairwise) statistics to characterize, which makes models based on linear Hebbian learning, or principal components analysis, inappropriate for nding eecient codes for natural images. We suggest that a good objective for an eecient coding of natural scenes is to maximize the sparseness of the representation, and we show that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive elds similar to those in the primate striate cortex."
            },
            "slug": "Natural-Image-Statistics-and-Eecient-Coding-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Natural Image Statistics and Eecient Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that a good objective for an eecient coding of natural Scenes is to maximize the sparseness of the representation, and it is shown that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive elds similar to those in the primate striate cortex."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1600874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aeb37769d72999bcbfb0582b73607fd8d23f4545",
            "isKey": false,
            "numCitedBy": 3274,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images. The coefficients of such codes are represented by arrays of mechanisms that respond to local regions of space, spatial frequency, and orientation (Gabor-like transforms). For many classes of image, such codes will not be an efficient means of representing information. However, the results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy (e.g., correlation between the intensities of neighboring pixels) into first-order redundancy (i.e., the response distribution of the coefficients). Such coding produces a relatively high signal-to-noise ratio and permits information to be transmitted with only a subset of the total number of cells. These results support Barlow's theory that the goal of natural vision is to represent the information in the natural environment with minimal redundancy."
            },
            "slug": "Relations-between-the-statistics-of-natural-images-Field",
            "title": {
                "fragments": [],
                "text": "Relations between the statistics of natural images and the response properties of cortical cells."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy into first- order redundancy."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1650980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff1152582155acaa0e9d0ccbc900a4641504256d",
            "isKey": false,
            "numCitedBy": 1344,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of recent attempts have been made to describe early sensory coding in terms of a general information processing strategy. In this paper, two strategies are contrasted. Both strategies take advantage of the redundancy in the environment to produce more effective representations. The first is described as a compact coding scheme. A compact code performs a transform that allows the input to be represented with a reduced number of vectors (cells) with minimal RMS error. This approach has recently become popular in the neural network literature and is related to a process called Principal Components Analysis (PCA). A number of recent papers have suggested that the optimal compact code for representing natural scenes will have units with receptive field profiles much like those found in the retina and primary visual cortex. However, in this paper, it is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway. In contrast, it is proposed that the visual system is near to optimal in representing natural scenes only if optimality is defined in terms of sparse distributed coding. In a sparse distributed code, all cells in the code have an equal response probability across the class of images but have a low response probability for any single image. In such a code, the dimensionality is not reduced. Rather, the redundancy of the input is transformed into the redundancy of the firing pattern of cells. It is proposed that the signature for a sparse code is found in the fourth moment of the response distribution (i.e., the kurtosis). In measurements with 55 calibrated natural scenes, the kurtosis was found to peak when the bandwidths of the visual code matched those of cells in the mammalian visual cortex. Codes resembling wavelet transforms are proposed to be effective because the response histograms of such codes are sparse (i.e., show high kurtosis) when presented with natural scenes. It is proposed that the structure of the image that allows sparse coding is found in the phase spectrum of the image. It is suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet). Possible reasons for why sensory systems would evolve toward sparse coding are presented."
            },
            "slug": "What-Is-the-Goal-of-Sensory-Coding-Field",
            "title": {
                "fragments": [],
                "text": "What Is the Goal of Sensory Coding?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway and suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 127
                            }
                        ],
                        "text": "The matrix, W, is a decorrelating matrix when the covariance matrix of the output vector, u, satis es:huuT i = diagonal matrix (4) In general, there will be many W's which decorrelate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Principal Components Analysis (PCA) is the orthogonal solution to Eq.(4)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 228
                            }
                        ],
                        "text": "In the case where the decorrelating lters are of the local ZCA type(see Section 3), the noise model is required (Atick & Redlich 1990) to avoidcentre-surround receptive elds with peaks a single pixel wide, as in Fig.3b(see also Atick & Redlich 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "A variety of Hebbian feature-learning algorithms for decorrelation havebeen proposed (Linsker 1992, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990,Atick & Redlich 1993), but in the absense of particular external constraintsthe solutions to the decorrelation problem are non-unique (see Section 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 63
                            }
                        ],
                        "text": "WZ is related to the transformsdescribed by Goodall (1960) and Atick & Redlich (1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207599521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e1482c67fc0c96dbd1d190e5040ab113a53e544",
            "isKey": true,
            "numCitedBy": 101,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised developmental algorithm for linear maps is derived which reduces the pixel-entropy (using the measure introduced in previous work) at every update and thus removes pairwise correlations between pixels. Since the measure of pixel-entropy has only a global minimum the algorithm is guaranteed to converge to the minimum entropy map. Such optimal maps have recently been shown to possess cognitively desirable properties and are likely to be used by the nervous system to organize sensory information. The algorithm derived here turns out to be one proposed by Goodall for pairwise decorrelation. It is biologically plausible since in a neural network implementation it requires only data available locally to a neuron. In training over ensembles of two-dimensional input signals with the same spatial power spectrum as natural scenes, networks develop output neurons with center-surround receptive fields similar to those of ganglion cells in the retina. Some technical issues pertinent to developmental algorithms of this sort, such as symmetry fixing, are also discussed."
            },
            "slug": "Convergent-Algorithm-for-Sensory-Receptive-Field-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "Convergent Algorithm for Sensory Receptive Field Development"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An unsupervised developmental algorithm for linear maps is derived which reduces the pixel-entropy at every update and thus removes pairwise correlations between pixels, and is biologically plausible since in a neural network implementation it requires only data available locally to a neuron."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "The rst workalong these lines was by Linsker (1988) who rst proposed the `infomax' prin-ciple which underlies our own work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 240
                            }
                        ],
                        "text": "\u2026of classical V1 simple cell receptive elds (Hubel &Wiesel 1968), that they are local and oriented, are properties of the lters inFig.4, while failing to emerge (without external constraints) in many previousself-organising network models (Linsker 1988, Miller 1988, Atick & Redlich15\n1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 106
                            }
                        ],
                        "text": "4, while failing to emerge (without external constraints) in many previous self-organizing network models (Linsker, 1988; Miller, 1988; Atick & Redlich, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1527671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f",
            "isKey": false,
            "numCitedBy": 1417,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The emergence of a feature-analyzing function from the development rules of simple, multilayered networks is explored. It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory. The network studied is based on the visual system. These results are used to infer an information-theoretic principle that can be applied to the network as a whole, rather than a single cell. The organizing principle proposed is that the network connections develop in such a way as to maximize the amount of information that is preserved when signals are transformed at each processing stage, subject to certain constraints. The operation of this principle is illustrated for some simple cases.<<ETX>>"
            },
            "slug": "Self-organization-in-a-perceptual-network-Linsker",
            "title": {
                "fragments": [],
                "text": "Self-organization in a perceptual network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762240"
                        ],
                        "name": "W. Bialek",
                        "slug": "W.-Bialek",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bialek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bialek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829021"
                        ],
                        "name": "D. Ruderman",
                        "slug": "D.-Ruderman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ruderman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143860433"
                        ],
                        "name": "A. Zee",
                        "slug": "A.-Zee",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Zee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "The learning rate (proportionality constant in Eq.(13)) was set as follows: 21 sweeps at 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "The matrix, W, is then initialised to the identity matrix, and trained using the logistic function version of Eq.(13), in which Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Writing Eq.(13) in terms of individual weights, we have: wij / wij + \u0177iXk wkjuk (14) The weighted sum non-local term in this rule can be seen as the result of a simple backwards pass through the weights from the linear output vector, u, to the inputs, x, so that each weight `knows the in uence' of its input, xj."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "This is an appropriately scaled starting point for further training since infomax (Eq.(13)) on raw data, with the logistic function, yi = (1 + e ui) 1, produces a u-vector which approximately satis es huuT i = 4I."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "In summary, these simulations show that the lters found by the ICA algorithm of Eq.(13) with a logistic non-linearity are localised, oriented, and produce outputs distributions of very high kurtosis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "(11): W / @H(y) @W WTW = (I+ \u0177uT )W (13) This rule has the twin advantages over Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "Di erentiating, and using the quotient rule for matrices gives: V = (W 1) =W 1( W)W 1: (15) Inserting Eq.(13) and rearranging gives the learning rule for a feedback weight matrix: V / (I+V)(I+ \u0177uT ): (16) In terms of an individual feedback weight, vij, this rule is: vij / ij + vij + uj(\u0177i +Xk vik \u0177k) (17) where ij = 1 when i = j, otherwise 0."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15218126,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "aa5601be49c9450528dcbdbfc1a9fa05b55155fd",
            "isKey": true,
            "numCitedBy": 36,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate the problem of optimizing the sampling of natural images using an array of linear filters. Optimization of information capacity is constrained by the noise levels of the individual channels and by a penalty for the construction of long-range interconnections in the array. At low signal-to-noise ratios the optimal filter characteristics correspond to bound states of a Schrodinger equation in which the signal spectrum plays the role of the potential. The resulting optimal filters are remarkably similar to those observed in the mammalian visual cortex and the retinal ganglion cells of lower vertebrates. The observed scale invariance of natural images plays an essential role in this construction."
            },
            "slug": "Optimal-Sampling-of-Natural-Images:-A-Design-for-Bialek-Ruderman",
            "title": {
                "fragments": [],
                "text": "Optimal Sampling of Natural Images: A Design Principle for the Visual System"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The resulting optimal filters are remarkably similar to those observed in the mammalian visual cortex and the retinal ganglion cells of lower vertebrates."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742997"
                        ],
                        "name": "C. Fyfe",
                        "slug": "C.-Fyfe",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Fyfe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fyfe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34931109"
                        ],
                        "name": "R. Baddeley",
                        "slug": "R.-Baddeley",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Baddeley",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baddeley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8604471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5ae37805bc9e0291c4c2c64cb2435f91849de74",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Some recent work has investigated the dichotomy between compact coding using dimensionality reduction and sparse-distributed coding in the context of understanding biological information processing. We introduce an artificial neural network which self-organizes on the basis of simple Hebbian learning and negative feedback of activation and show that it is capable both of forming compact codings of data distributions and of identifying filters most sensitive to sparse-distributed codes. The network is extremely simple and its biological relevance is investigated via its response to a set of images which are typical of everyday life. However, an analysis of the network's identification of the filter for sparse coding reveals that this coding may not be globally optimal and that there exists an innate limiting factor which cannot be transcended."
            },
            "slug": "Finding-compact-and-sparse-distributed-of-visual-Fyfe-Baddeley",
            "title": {
                "fragments": [],
                "text": "Finding compact and sparse-distributed representations of visual images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An artificial neural network which self-organizes on the basis of simple Hebbian learning and negative feedback of activation is introduced and it is shown that it is capable both of forming compact codings of data distributions and of identifying filters most sensitive to sparse-distributed codes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34931109"
                        ],
                        "name": "R. Baddeley",
                        "slug": "R.-Baddeley",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Baddeley",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baddeley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "(2) when huuT i = I, then: WTW = hxxT i 1 (5) which clearly leaves freedom in the choice of W."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "There are, however, several special solutions to Eq.(5)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 19
                            }
                        ],
                        "text": "In a further study,Baddeley (1996) argued against kurtosis-maximisation, partly on the groundsthat it would produce lters which are two pixels wide."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "(6) into Eq.(5) and solving for W gives the PCA solution, WP : WP = D 12ET (7) This solution is unusual in that the lters (rows of WP ) are orthogonal, so that WWT = D 1, a scaling matrix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "If we force W to be symmetrical, so that WT =W, then the solution, WZ to Eq.(5) is: WZ = hxxT i 1=2: (8) Like most other decorrelating lters, and unlike PCA, the basis functions and the lters coming fromWZ will be di erent from each other, and neither will be orthogonal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16941334,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "114e7b8d0bfd716ee853c9656fbdc802b10dec09",
            "isKey": true,
            "numCitedBy": 59,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been independently proposed, by Barlow, Field, Intrator and co-workers, that the receptive fields of neurons in V1 are optimized to generate 'sparse', Kurtotic, or 'interesting' output probability distributions. We investigate the empirical evidence for this further and argue that filters can produce 'interesting' output distributions simply because natural images have variable local intensity variance. If the proposed filters have zero DC, then the probability distribution of filter outputs (and hence the output Kurtosis) is well predicted simply from these effects of variable local variance. This suggests that finding filters with high output Kurtosis does not necessarily signal interesting image structure. It is then argued that finding filters that maximize output Kurtosis generates filters that are incompatible with observed physiology. In particular the optimal difference-of-Gaussian (DOG) filter should have the smallest possible scale, an on-centre off-surround cell should have a negative DC, and that the ratio of centre width to surround width should approach unity. This is incompatible with the physiology. Further, it is also predicted that oriented filters should always be oriented in the vertical direction, and of all the filters tested, the filter with the highest output Kurtosis has the lowest signal-to-noise ratio (the filter is simply the difference of two neighbouring pixels). Whilst these observations are not incompatible with the brain using a sparse representation, it does argue that little significance should be placed on finding filters with highly Kurtotic output distributions. It is therefore argued that other constraints are required in order to understand the development of visual receptive fields."
            },
            "slug": "Searching-for-filters-with-'interesting'-output-an-Baddeley",
            "title": {
                "fragments": [],
                "text": "Searching for filters with 'interesting' output distributions: an uninteresting direction to explore?"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "It is argued that other constraints are required in order to understand the development of visual receptive fields and that filters can produce 'interesting' output distributions simply because natural images have variable local intensity variance."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5024927"
                        ],
                        "name": "D. Ferster",
                        "slug": "D.-Ferster",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ferster",
                            "middleNames": [
                                "L"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ferster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50260231"
                        ],
                        "name": "Sooyoung Chung",
                        "slug": "Sooyoung-Chung",
                        "structuredName": {
                            "firstName": "Sooyoung",
                            "lastName": "Chung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sooyoung Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6195298"
                        ],
                        "name": "H. Wheat",
                        "slug": "H.-Wheat",
                        "structuredName": {
                            "firstName": "H",
                            "lastName": "Wheat",
                            "middleNames": [
                                "S"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wheat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 123
                            }
                        ],
                        "text": "Nonetheless, recent evidence has been found for a feedforward origin tothe oriented properties of simple cells in the cat (Ferster et al. 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4367529,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "05206030c219c04d1d3eec510ad1521b6f85d9a7",
            "isKey": false,
            "numCitedBy": 544,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "MORE than 30 years after Hubel and Wiesel1 first described orientation selectivity in the mammalian visual cortex, the mechanism that gives rise to this property is still controversial. Hubel and Wiesel1 proposed a simple model for the origin of orientation tuning, in which the circularly symmetrical receptive fields of neurons in the lateral geniculate nucleus that excite a cortical simple cell are arranged in rows. Since this model was proposed, several experiments2\u20136 and neuronal simulations7,8 have suggested that the connectivity between the lateral geniculate nucleus and the cortex is not well organized in an orientation-specific fashion, and that orientation tuning arises instead from extensive interactions within the cortex. To test these models we have recorded visually evoked synaptic potentials in simple cells while cooling the cortex9, which largely inactivates the cortical network, but leaves geniculate synaptic input functional. We report that the orientation tuning of these potentials is almost unaffected by cooling the cortex, in agreement with Hubel and Wiesel's original proposal1."
            },
            "slug": "Orientation-selectivity-of-thalamic-input-to-simple-Ferster-Chung",
            "title": {
                "fragments": [],
                "text": "Orientation selectivity of thalamic input to simple cells of cat visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is reported that the orientation tuning of these potentials is almost unaffected by cooling the cortex, in agreement with Hubel and Wiesel's original proposal."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807117"
                        ],
                        "name": "T. Sanger",
                        "slug": "T.-Sanger",
                        "structuredName": {
                            "firstName": "Terence",
                            "lastName": "Sanger",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sanger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 123
                            }
                        ],
                        "text": "A variety of Hebbian feature-learning algorithms for decorrelation havebeen proposed (Linsker 1992, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990,Atick & Redlich 1993), but in the absense of particular external constraintsthe solutions to the decorrelation problem are non-unique (see Section 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 80
                            }
                        ],
                        "text": "Many contributions have emphasised the roles of decorrelation and PCA (Oja1989, Sanger 1989, Miller 1988, Hancock et al 1992, Foldiak 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10138295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "709b4bfc5198336ba5d70da987889a157f695c1e",
            "isKey": false,
            "numCitedBy": 1524,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-unsupervised-learning-in-a-single-layer-Sanger",
            "title": {
                "fragments": [],
                "text": "Optimal unsupervised learning in a single-layer linear feedforward neural network"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15214722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69d713c63575947e4ef78b4dc04ed3a686cc06a4",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In previous work (Olshausen \\& Field 1996), an algorithm was described for learning linear sparse codes which, when trained on natural images, produces a set of basis functions that are spatially localized, oriented, and bandpass (i.e., wavelet-like). This note shows how the algorithm may be interpreted within a maximum-likelihood framework. Several useful insights emerge from this connection: it makes explicit the relation to statistical independence (i.e., factorial coding), it shows a formal relationship to the algorithm of Bell and Sejnowski (1995), and it suggests how to adapt parameters that were previously fixed."
            },
            "slug": "Learning-linear,-sparse,-factorial-codes-Olshausen",
            "title": {
                "fragments": [],
                "text": "Learning linear, sparse, factorial codes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This note shows how the algorithm for learning linear sparse codes may be interpreted within a maximum-likelihood framework and makes explicit the relation to statistical independence and shows a formal relationship to Bell and Sejnowski's algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40000333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de86c61defbb8c259583074f3cf63afe13571ce1",
            "isKey": false,
            "numCitedBy": 856,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principal-components,-minor-components,-and-linear-Oja",
            "title": {
                "fragments": [],
                "text": "Principal components, minor components, and linear neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "For example, in the case of Eq.(2) when huuT i = I, then: WTW = hxxT i 1 (5) which clearly leaves freedom in the choice of W."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "(1) there may be no ICA solution, and (2) a given ICA algorithmmay not nd the solution even if it exists, since there are approximations involved."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "(2) The PCA basis functions (columns of AP , or rows of W T P { see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "Solving this gives u = (I +V) 1x, showing that V is just a coordinate transform of the W of Eq.(2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 233
                            }
                        ],
                        "text": "The linear image synthesis model is therefore given by: x = As: (1) The goal of a perceptual system, in this simpli ed framework, is to linearly transform the images, x, with a matrix of lters, W, so that the resulting vector: u =Wx (2) recovers the underlying causes, s, possibly in a di erent order, and rescaled."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 113
                            }
                        ],
                        "text": "In the case where the decorrelating lters are of the local ZCA type(see Section 3), the noise model is required (Atick & Redlich 1990) to avoidcentre-surround receptive elds with peaks a single pixel wide, as in Fig.3b(see also Atick & Redlich 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28154878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03128ecdd3c3dfb9752861d8555b97535e1cfc14",
            "isKey": true,
            "numCitedBy": 531,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a theory of the early processing in the mammalian visual pathway. The theory is formulated in the language of information theory and hypothesizes that the goal of this processing is to recode in order to reduce a generalized redundancy subject to a constraint that specifies the amount of average information preserved. In the limit of no noise, this theory becomes equivalent to Barlow's redundancy reduction hypothesis, but it leads to very different computational strategies when noise is present. A tractable approach for finding the optimal encoding is to solve the problem in successive stages where at each stage the optimization is performed within a restricted class of transfer functions. We explicitly find the solution for the class of encodings to which the parvocellular retinal processing belongs, namely linear and nondivergent transformations. The solution shows agreement with the experimentally observed transfer functions at all levels of signal to noise."
            },
            "slug": "Towards-a-Theory-of-Early-Visual-Processing-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "Towards a Theory of Early Visual Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A theory of the early processing in the mammalian visual pathway is proposed and the solution for the class of encodings to which the parvocellular retinal processing belongs, namely linear and nondivergent transformations is found."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8884630"
                        ],
                        "name": "L. Cooper",
                        "slug": "L.-Cooper",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Cooper",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094648"
                        ],
                        "name": "P. Munro",
                        "slug": "P.-Munro",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Munro",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Munro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 84
                            }
                        ],
                        "text": "He used an index emphasizing multimodal projections, and connected it with the BCM (Bienenstock et al., 1982) learning rule. Following up from this, Law & Cooper (1994) and Shouval (1995) used the BCM rule to selforganize oriented and somewhat localized receptive fields on an ensemble of natural images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 678,
                                "start": 84
                            }
                        ],
                        "text": "He used an index emphasizing multimodal projections, and connected it with the BCM (Bienenstock et al., 1982) learning rule. Following up from this, Law & Cooper (1994) and Shouval (1995) used the BCM rule to selforganize oriented and somewhat localized receptive fields on an ensemble of natural images. The BCM rule is a nonlinear Hebbian/anti-Hebbian mechanism. The nonlinearity undoubtedly contributes higher-order statistical information, but it is less clear, than in Olshausen's network or our own, how the nonlinearity contributes to the solution. Another principle, predictability minimization, has also been brought to bear on the problem by Schmidhuber et al. (1996). This approach attempts to ensure independence of one output from the others by moving its receptive field away from what is predictable (using a nonlinear \"lateral\" network) from the outputs of the others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 83
                            }
                        ],
                        "text": "He used an index emphasizing multimodal projections, and connected it with the BCM (Bienenstock et al., 1982) learning rule."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 917,
                                "start": 84
                            }
                        ],
                        "text": "He used an index emphasizing multimodal projections, and connected it with the BCM (Bienenstock et al., 1982) learning rule. Following up from this, Law & Cooper (1994) and Shouval (1995) used the BCM rule to selforganize oriented and somewhat localized receptive fields on an ensemble of natural images. The BCM rule is a nonlinear Hebbian/anti-Hebbian mechanism. The nonlinearity undoubtedly contributes higher-order statistical information, but it is less clear, than in Olshausen's network or our own, how the nonlinearity contributes to the solution. Another principle, predictability minimization, has also been brought to bear on the problem by Schmidhuber et al. (1996). This approach attempts to ensure independence of one output from the others by moving its receptive field away from what is predictable (using a nonlinear \"lateral\" network) from the outputs of the others. Finally, Harpur & Prager (1996) have formalized an inhibitory feedback network which also learns nonorthogonal oriented receptive fields."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1607496,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9f22cf81654dd50b95e65b86b1125cfe6803a67b",
            "isKey": true,
            "numCitedBy": 2695,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further."
            },
            "slug": "Theory-for-the-development-of-neuron-selectivity:-Bienenstock-Cooper",
            "title": {
                "fragments": [],
                "text": "Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework and a synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829021"
                        ],
                        "name": "D. Ruderman",
                        "slug": "D.-Ruderman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ruderman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruderman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 16
                            }
                        ],
                        "text": "As empasized by Ruderman (1994)and Field (1994), the general form of these histograms is double-exponential(e juij), or `sparse', meaning peaky with a long tail, when compared to a gaus-sian."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2793971,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "13206b6ba3711a14a56cf1599ecb08c16f49061e",
            "isKey": false,
            "numCitedBy": 912,
            "numCiting": 144,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently there has been a resurgence of interest in the properties of natural images. Their statistics are important not only in image compression but also for the study of sensory processing in biology, which can be viewed as satisfying certain \u2018design criteria\u2019. This review summarizes previous work on image statistics and presents our own data. Perhaps the most notable property of natural images is an invariance to scale. We present data to support this claim as well as evidence for a hierarchical invariance in natural scenes. These symmetries provide a powerful description of natural images as they greatly restrict the class of allowed distributions."
            },
            "slug": "The-statistics-of-natural-images-Ruderman",
            "title": {
                "fragments": [],
                "text": "The statistics of natural images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153281777"
                        ],
                        "name": "D. Marr",
                        "slug": "D.-Marr",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Marr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31857045"
                        ],
                        "name": "E. Hildreth",
                        "slug": "E.-Hildreth",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Hildreth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hildreth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 89
                            }
                        ],
                        "text": "Anthony J. Bell and Terrence J. SejnowskiHoward Hughes Medical InstituteComputational Neurobiology LaboratoryThe Salk Institute10010 N. Torrey Pines RoadLa Jolla, California 92037 AbstractField (1994) has suggested that neurons with line and edge selectiv-ities found in primary visual cortex of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 48
                            }
                        ],
                        "text": "Anthony J. Bell and Terrence J. SejnowskiHoward Hughes Medical InstituteComputational Neurobiology LaboratoryThe Salk Institute10010 N. Torrey Pines RoadLa Jolla, California 92037 AbstractField (1994) has suggested that neurons with line and edge selectiv-ities found in primary visual cortex of cats and monkeys form a sparse,distributed representaton of natural scenes, and Barlow (1989) has rea-soned that such responses should emerge from an unsupervised learningalgorithm that attempts to nd a factorial code of independent visualfeatures."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2150419,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9009c9685754346deb93f316144a9da1f70ffcd8",
            "isKey": false,
            "numCitedBy": 7031,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A theory of edge detection is presented. The analysis proceeds in two parts. (1) Intensity changes, which occur in a natural image over a wide range of scales, are detected separately at different scales. An appropriate filter for this purpose at a given scale is found to be the second derivative of a Gaussian, and it is shown that, provided some simple conditions are satisfied, these primary filters need not be orientation-dependent. Thus, intensity changes at a given scale are best detected by finding the zero values of \u22072G(x, y)* I(x, y) for image I, where G(x, y) is a two-dimensional Gaussian distribution and \u22072 is the Laplacian. The intensity changes thus discovered in each of the channels are then represented by oriented primitives called zero-crossing segments, and evidence is given that this representation is complete. (2) Intensity changes in images arise from surface discontinuities or from reflectance or illumination boundaries, and these all have the property that they are spatially localized. Because of this, the zero-crossing segments from the different channels are not independent, and rules are deduced for combining them into a description of the image. This description is called the raw primal sketch. The theory explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround \u22072G filters acting on the image forms the basis for a physiological model of simple cells (see Marr & Ullman 1979)."
            },
            "slug": "Theory-of-edge-detection-Marr-Hildreth",
            "title": {
                "fragments": [],
                "text": "Theory of edge detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The theory of edge detection explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround \u22072G filters acting on the image forms the basis for a physiological model of simple cells."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Royal Society of London. Series B. Biological Sciences"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 35
                            }
                        ],
                        "text": "The approach which we developed in Bell & Sejnowski (1995a) was tomaximise by stochastic gradient ascent the joint entropy,H[g(u)], of the lineartransform squashed by a sigmoidal function, g."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 61
                            }
                        ],
                        "text": "We refer the reader to these papers, to the twoabove, and to Bell & Sejnowski (1995a) for further background on ICA."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 293
                            }
                        ],
                        "text": "De ning yi = g(ui) to be the sigmoidally transformed output variables, the learning rule is then: W / @H(y) @W = E \"@ ln jJ j @W # (9) In this, E denotes expected value, y = [g(u1) : : : g(uN)]T , and jJ j is the absolute value of the determinant of the Jacobian matrix: J = det \" @yi @xj #ij (10) In stochastic gradient ascent we remove the expected value operator in Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 45
                            }
                        ],
                        "text": "(9), and then evaluate the gradient to give (Bell & Sejnowski 1995): W / [WT ] 1 + y\u0302xT (11)where y\u0302 = [y\u03021 : : : y\u0302N ]T , the elements of which depend on the nonlinearity asfollows: y\u0302i = @@yi @yi@ui = @@ui ln @yi@ui (12)Amari, Cichocki & Yang (1996) have proposed a modi cation of this rulewhich\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 131
                            }
                        ],
                        "text": "We show here that a new unsupervised learning algorithmthat is based on information maximisation, a non-linear `infomax' net-work (Bell and Sejnowski, 1995) when applied to an ensemble of naturalscenes, produces sets of visual lters that are localised and oriented."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 149
                            }
                        ],
                        "text": "\u2026algorithm is described here as an ICA1See the Proceedings of IEEE, 84, 4, April 1996 | a special issue on wavelets.2In a previous conference paper (Bell & Sejnowski 1995b), we also published a proof ofthis result, which ought to have referenced the equivalent proof by Nadal & Parga.6\nalgorithm, a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8756,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9271650,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "02f89cd1fd6f013a1a301a292936ff8fb06aff25",
            "isKey": false,
            "numCitedBy": 3420,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Two-dimensional spatial linear filters are constrained by general uncertainty relations that limit their attainable information resolution for orientation, spatial frequency, and two-dimensional (2D) spatial position. The theoretical lower limit for the joint entropy, or uncertainty, of these variables is achieved by an optimal 2D filter family whose spatial weighting functions are generated by exponentiated bivariate second-order polynomials with complex coefficients, the elliptic generalization of the one-dimensional elementary functions proposed in Gabor's famous theory of communication [J. Inst. Electr. Eng. 93, 429 (1946)]. The set includes filters with various orientation bandwidths, spatial-frequency bandwidths, and spatial dimensions, favoring the extraction of various kinds of information from an image. Each such filter occupies an irreducible quantal volume (corresponding to an independent datum) in a four-dimensional information hyperspace whose axes are interpretable as 2D visual space, orientation, and spatial frequency, and thus such a filter set could subserve an optimally efficient sampling of these variables. Evidence is presented that the 2D receptive-field profiles of simple cells in mammalian visual cortex are well described by members of this optimal 2D filter family, and thus such visual neurons could be said to optimize the general uncertainty relations for joint 2D-spatial-2D-spectral information resolution. The variety of their receptive-field dimensions and orientation and spatial-frequency bandwidths, and the correlations among these, reveal several underlying constraints, particularly in width/length aspect ratio and principal axis organization, suggesting a polar division of labor in occupying the quantal volumes of information hyperspace.(ABSTRACT TRUNCATED AT 250 WORDS)"
            },
            "slug": "Uncertainty-relation-for-resolution-in-space,-and-Daugman",
            "title": {
                "fragments": [],
                "text": "Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Evidence is presented that the 2D receptive-field profiles of simple cells in mammalian visual cortex are well described by members of this optimal 2D filter family, and thus such visual neurons could be said to optimize the general uncertainty relations for joint 2D-spatial-2D-spectral information resolution."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 86
                            }
                        ],
                        "text": "A variety of Hebbian feature-learning algorithms for decorrelation havebeen proposed (Linsker 1992, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990,Atick & Redlich 1993), but in the absense of particular external constraintsthe solutions to the decorrelation problem are non-unique (see Section 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42871496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "026e9b04bab73d3a34cd37f8b290f0c8f6da5f4e",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A network that develops to maximize the mutual information between its output and the signal portion of its input (which is admixed with noise) is useful for extracting salient input features, and may provide a model for aspects of biological neural network function. I describe a local synaptic Learning rule that performs stochastic gradient ascent in this information-theoretic quantity, for the case in which the input-output mapping is linear and the input signal and noise are multivariate gaussian. Feedforward connection strengths are modified by a Hebbian rule during a \"learning\" phase in which examples of input signal plus noise are presented to the network, and by an anti-Hebbian rule during an \"unlearning\" phase in which examples of noise alone are presented. Each recurrent lateral connection has two values of connection strength, one for each phase; these values are updated by an anti-Hebbian rule."
            },
            "slug": "Local-Synaptic-Learning-Rules-Suffice-to-Maximize-a-Linsker",
            "title": {
                "fragments": [],
                "text": "Local Synaptic Learning Rules Suffice to Maximize Mutual Information in a Linear Network"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A local synaptic Learning rule is described that performs stochastic gradient ascent in this information-theoretic quantity, for the case in which the input-output mapping is linear and the input signal and noise are multivariate gaussian."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064184853"
                        ],
                        "name": "C. C. Law",
                        "slug": "C.-C.-Law",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Law",
                            "middleNames": [
                                "Charles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. C. Law"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8884630"
                        ],
                        "name": "L. Cooper",
                        "slug": "L.-Cooper",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Cooper",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cooper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 25243472,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "486babc6bea3568bf1ef0ae8ef5d469c212bcb41",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bienenstock, Cooper, and Munro (BCM) theory of synaptic plasticity has successfully reproduced the development of orientation selectivity and ocular dominance in kitten visual cortex in normal, as well as deprived, visual environments. To better compare the consequences of this theory with experiment, previous abstractions of the visual environment are replaced in this work by real visual images with retinal processing. The visual environment is represented by 24 gray-scale natural images that are shifted across retinal fields. In this environment, the BCM neuron develops receptive fields similar to the fields of simple cells found in kitten striate cortex. These fields display adjacent excitatory and inhibitory bands when tested with spot stimuli, orientation selectivity when tested with bar stimuli, and spatial-frequency selectivity when tested with sinusoidal gratings. In addition, their development in various deprived visual environments agrees with experimental results."
            },
            "slug": "Formation-of-receptive-fields-in-realistic-visual-Law-Cooper",
            "title": {
                "fragments": [],
                "text": "Formation of receptive fields in realistic visual environments according to the Bienenstock, Cooper, and Munro (BCM) theory."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Bienenstock, Cooper, and Munro (BCM) theory of synaptic plasticity has successfully reproduced the development of orientation selectivity and ocular dominance in kitten visual cortex in normal, as well as deprived, visual environments."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160279"
                        ],
                        "name": "P. Hancock",
                        "slug": "P.-Hancock",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hancock",
                            "middleNames": [
                                "J.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hancock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34931109"
                        ],
                        "name": "R. Baddeley",
                        "slug": "R.-Baddeley",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Baddeley",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baddeley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708497"
                        ],
                        "name": "Leslie S. Smith",
                        "slug": "Leslie-S.-Smith",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Smith",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leslie S. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 96
                            }
                        ],
                        "text": "Both the classic experiments of Hubel &Wiesel (1968) on neurons in visual cor-tex, and several decades of theorising about feature detection in vision (Marr& Hildreth 1980), have left open the question most succinctly phrased by Bar-low & Tolhurst (1992) \\Why do we have edge detectors?\"."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 165
                            }
                        ],
                        "text": "Onepopular decorrelating solution is Principal Components Analysis (PCA) butthe principal components of natural scenes amount to a global spatial frequencyanalysis (Hancock et al 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 106
                            }
                        ],
                        "text": "Many contributions have emphasised the roles of decorrelation and PCA (Oja1989, Sanger 1989, Miller 1988, Hancock et al 1992, Foldiak 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9967699,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "7dcfa42cfe3b59becb441844b72558b361693608",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural net was used to analyse samples of natural images and text. For the natural images, components resemble derivatives of Gaussian operators, similar to those found in visual cortex and inferred from psychophysics. While the results from natural images do not depend on scale, those from text images are highly scale dependent. Convolution of one of the text components with an original image shows that it is sensitive to inter-word gaps."
            },
            "slug": "The-principal-components-of-natural-images-Hancock-Baddeley",
            "title": {
                "fragments": [],
                "text": "The principal components of natural images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 57
                            }
                        ],
                        "text": "This `In-2\ndependent Components Analysis' (ICA) problem (Comon 1994) is equivalentto Barlow's redundancy reduction problem, therefore if Barlow's reasoning iscorrect, we would expect the ICA solution to yield localised edge detectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 232
                            }
                        ],
                        "text": "An-other way to constrain the solution is to attempt to produce outputs which arenot just decorrelated, but statistically independent, the much stronger require-ment of Independent Components Analysis, or ICA (Jutten & Herault 1991,Comon 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "(8): fxg 2WZ(fxg hxi): (18) This removes both rst and second order statistics from the data, and makes the covariance matrix of x equal to 4I."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039472"
                        ],
                        "name": "N. Parga",
                        "slug": "N.-Parga",
                        "structuredName": {
                            "firstName": "N\u00e9stor",
                            "lastName": "Parga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Parga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115302789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "698aedd44c51da829228e2c7d243960345efeb94",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the consequences of maximizing information transfer in a simple neural network (one input layer, one output layer), focusing on the case of nonlinear transfer functions. We assume that both receptive fields (synaptic efficacies) and transfer functions can be adapted to the environment. The main result is that, for bounded and invertible transfer functions, in the case of a vanishing additive output noise, and no input noise, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow. We also show that this result is valid for linear and, more generally, unbounded, transfer functions, provided optimization is performed under an additive constraint, i.e. which can be written as a sum of terms, each one being specific to one output neuron. Finally, we study the effect of a non-zero input noise. We find that, to first order in the input noise, assumed to be small in comparison with th..."
            },
            "slug": "Nonlinear-neurons-in-the-low-noise-limit:-a-code-5-Nadal-Parga",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low-noise limit: a factorial code maximizes information transfer Network 5"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The main result is that, for bounded and invertible transfer functions, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120692531"
                        ],
                        "name": "Liuyue Wang",
                        "slug": "Liuyue-Wang",
                        "structuredName": {
                            "firstName": "Liuyue",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liuyue Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210204"
                        ],
                        "name": "R. Vig\u00e1rio",
                        "slug": "R.-Vig\u00e1rio",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Vig\u00e1rio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vig\u00e1rio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768650"
                        ],
                        "name": "J. Joutsensalo",
                        "slug": "J.-Joutsensalo",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Joutsensalo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Joutsensalo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 113
                            }
                        ],
                        "text": "A number of ap-proaches to ICA have some relations with the one we describe below, notablyCardoso & Laheld 1996, Karhunen et al 1996, Amari et al 1996, Cichocki etal 1994 and Pham et al 1992."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 220
                            }
                        ],
                        "text": "Therefore, by prewhitening x in this way, we can ensure thatthe subsequent transformation, u = Wx, to be learnt should approximate anorthonormal matrix (rotation without scaling), roughly satisfying the relationWTW = I (Karhunen et al 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 310835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c93c897fd80f5246b839a2044798780cf2c5a77",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent component analysis (ICA) is a recently developed, useful extension of standard principal component analysis (PCA). The ICA model is utilized mainly in blind separation of unknown source signals from their linear mixtures. In this application only the source signals which correspond to the coefficients of the ICA expansion are of interest. In this paper, we propose neural structures related to multilayer feedforward networks for performing complete ICA. The basic ICA network consists of whitening, separation, and basis vector estimation layers. It can be used for both blind source separation and estimation of the basis vectors of ICA. We consider learning algorithms for each layer, and modify our previous nonlinear PCA type algorithms so that their separation capabilities are greatly improved. The proposed class of networks yields good results in test examples with both artificial and real-world data."
            },
            "slug": "A-class-of-neural-networks-for-independent-analysis-Karhunen-Oja",
            "title": {
                "fragments": [],
                "text": "A class of neural networks for independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes neural structures related to multilayer feedforward networks for performing complete independent component analysis (ICA) and modify the previous nonlinear PCA type algorithms so that their separation capabilities are greatly improved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334226"
                        ],
                        "name": "D. Hubel",
                        "slug": "D.-Hubel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hubel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hubel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2629471"
                        ],
                        "name": "T. Wiesel",
                        "slug": "T.-Wiesel",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Wiesel",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wiesel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7136759,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c5f5311fa1f34159ab3a0a1d58da51cd0340a640",
            "isKey": false,
            "numCitedBy": 6319,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded."
            },
            "slug": "Receptive-fields-and-functional-architecture-of-Hubel-Wiesel",
            "title": {
                "fragments": [],
                "text": "Receptive fields and functional architecture of monkey striate cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light, with response properties very similar to those previously described in the cat."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039157"
                        ],
                        "name": "H. Shouval",
                        "slug": "H.-Shouval",
                        "structuredName": {
                            "firstName": "Harel",
                            "lastName": "Shouval",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shouval"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 48
                            }
                        ],
                        "text": "Following up from this, Law & Cooper (1994) and Shouval (1995) used theBCM rule to self-organise oriented and somewhat localised receptive elds onan ensemble of natural images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59959192,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "b6987fc1bc70314d6a4e9427e6381aeb38e6133d",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "iii Acknowledgments I would like to thank Professor Leon N Cooper for urging me to examine the eeect of introducing a more realistic visual environments, and for asking me to make sense of some of the network models, and see how they relate to the BCM model. I would like to thank him also for sharing with me his ability to gure out what is important in a complex detailed situation, I hope some of it may have rubbed oo on me. I like to express my appreciation to Professor Eitan Domany who was my advisor at the Weizmann Institute, and who rst introduced me to the world of scientiic research, from him I learned the importance of a good understanding of the fundamentals. I Would also like to thank all the members of the Institute of Brain and Neural Systems, and in particular to, Yong Liu with whom I cooperated a lot in the last three years, to Nathan Intrator and Mike Perrone for many fruitful discussions, to Charlie Law who was very helpful in setting up the natural images data base, and for Roger Blumberg for carefully reading a draft of this thesis. My thanks to Prooesors Bob Pelcovits David Gottlieb and to Igor Bukharev for help and comments on some of this work. I would express my deepest thanks to my family; Rutie who helped even in the hardest times, though she would have preferred I did something comprehensible, to Ofer cause he is the cutest, and most curious kid in the world, and most of all to my parents who brought me up with a lot of love, and who encouraged my curiosity from the earliest age I can remember."
            },
            "slug": "Formation-and-Organization-of-Receptive-fields,-an-Shouval",
            "title": {
                "fragments": [],
                "text": "Formation and Organization of Receptive fields, with an input Environment Composed of Natural Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This thesis aims to examine the importance of introducing a more realistic visual environments through the use of network models, and how they relate to the BCM model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334226"
                        ],
                        "name": "D. Hubel",
                        "slug": "D.-Hubel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hubel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hubel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2629471"
                        ],
                        "name": "T. Wiesel",
                        "slug": "T.-Wiesel",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Wiesel",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wiesel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13140630,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d1d186fe8eae26824a380f70454be04d6a756bef",
            "isKey": false,
            "numCitedBy": 895,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with the relationship between orientation columns, ocular\u2010dominance columns, the topographic mapping of visual fields onto cortex, and receptive\u2010field size and scatter. Although the orientation columns are an order of magnitude smaller than the ocular\u2010dominance columns, the horizontal distance corresponding to a complete cycle of orientation columns, representing a rotation through 180\u00b0, seems to be roughly the same size as a left\u2010plus\u2010right ocular dominance set, with a thickness of about 0.5\u20131 mm, independent of eccentricity at least out to 15\u00b0. We use the term hypercolumn to refer to a complete set of either type (180\u00b0, or left\u2010plus\u2010right eyes)."
            },
            "slug": "Uniformity-of-monkey-striate-cortex:-A-parallel-and-Hubel-Wiesel",
            "title": {
                "fragments": [],
                "text": "Uniformity of monkey striate cortex: A parallel relationship between field size, scatter, and magnification factor"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The term hypercolumn is used to refer to a complete set of either type (180\u00b0, or left\u2010plus\u2010right eyes), with implications for the topographic mapping of visual fields onto cortex, and receptive\u2010field size and scatter."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of comparative neurology"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15816289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15e5ac0a7c3542ce58a67c43fa0e81398be2e5a7",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the consequences of maximizing information transfer in a simple neural network (one input layer, one output layer), focussing on the case of non linear transfer functions. We assume that both receptive elds (synaptic eecacies) and transfer functions can be adapted to the environment. The main result is that, for bounded and invertible transfer functions, in the case of a vanishing additive output noise, and no input noise, maximization of information (Linsker'sinfomax principle) leads to a factorial code-hence to the same solution as required by the redundancy reduction principle of Barlow. We show also that this result is valid for linear, more generally unbounded, transfer functions, provided optimization is performed under an additive constraint, that is which can be written as a sum of terms, each one being speciic to one output neuron. Finally we study the eeect of a non zero input noise. We nd that, at rst order in the input noise, assumed to be small as compared to the-small-output noise, the above results are still valid, provided the output noise is uncorrelated from one neuron to the other. P.A.C.S. 87.30 Biophysics of neurophysiological processes Short title: Information maximization with non linear neurons To appear in NETWORK INDEX: nadalparga.infomaxredred.ps.Z nadal@physique.ens.fr 19 pages Infomax applied to non linear neurons, in the low noise limit, leads to redundancy reduction."
            },
            "slug": "Non-linear-neurons-in-the-low-noise-limit-:-a-code-Nadal",
            "title": {
                "fragments": [],
                "text": "Non linear neurons in the low noise limit : a factorial code maximizes information transferJean"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The main result is that, for bounded and invertible transfer functions, maximization of information (Linsker'sinfomax principle) leads to a factorial code-hence to the same solution as required by the redundancy reduction principle of Barlow."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8909922"
                        ],
                        "name": "N. Intrator",
                        "slug": "N.-Intrator",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Intrator",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Intrator"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 45
                            }
                        ],
                        "text": "This has led to feature-learning algorithms (Intrator 1992) with a `Projection Pursuit' (Huber 1985) avour, the most successful of which has been Olshausen & Field's (1996)demonstration of the self-organisation of local, oriented receptive elds usinga sparseness criterion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Intrator (1992) haspioneered the application of projection pursuit reasoning to feature extractionproblems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 28
                            }
                        ],
                        "text": "feature-learning algorithms (Intrator, 1992) with a \"projection pursuit\" (Huber, 1985) flavour, the most successful of which has been the Olshausen & Field"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15475544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "136fc611e49e5f3676265a288b78e473a752783b",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel unsupervised neural network for dimensionality reduction that seeks directions emphasizing multimodality is presented, and its connection to exploratory projection pursuit methods is discussed. This leads to a new statistical insight into the synaptic modification equations governing learning in Bienenstock, Cooper, and Munro (BCM) neurons (1982). The importance of a dimensionality reduction principle based solely on distinguishing features is demonstrated using a phoneme recognition experiment. The extracted features are compared with features extracted using a backpropagation network."
            },
            "slug": "Feature-Extraction-Using-an-Unsupervised-Neural-Intrator",
            "title": {
                "fragments": [],
                "text": "Feature Extraction Using an Unsupervised Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A novel unsupervised neural network for dimensionality reduction that seeks directions emphasizing multimodality is presented, and its connection to exploratory projection pursuit methods is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40468115"
                        ],
                        "name": "Joel L. Davis",
                        "slug": "Joel-L.-Davis",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Davis",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10395533,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4a1a91cdf12c63e66e79321120de2ff510355a21",
            "isKey": false,
            "numCitedBy": 665,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This book originated at a small and informal workshop held in December of 1992 in Idyllwild, a relatively secluded resort village situated amid forests in the San Jacinto Mountains above Palm Springs in Southern California. Eighteen colleagues from a broad range of disciplines, including biophysics, electrophysiology, neuroanatomy, psychophysics, clinical studies, mathematics and computer vision, discussed 'Large Scale Models of the Brain,' that is, theories and models that cover a broad range of phenomena, including early and late vision, various memory systems, selective attention, and the neuronal code underlying figure-ground segregation and awareness (for a brief summary of this meeting, see Stevens 1993). The bias in the selection of the speakers toward researchers in the area of visual perception reflects both the academic background of one of the organizers as well as the (relative) more mature status of vision compared with other modalities. This should not be surprising given the emphasis we humans place on'seeing' for orienting ourselves, as well as the intense scrutiny visual processes have received due to their obvious usefullness in military, industrial, and robotic applications. JMD"
            },
            "slug": "Large-Scale-Neuronal-Theories-of-the-Brain-Koch-Davis",
            "title": {
                "fragments": [],
                "text": "Large-Scale Neuronal Theories of the Brain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6059616"
                        ],
                        "name": "G. F. Harpur",
                        "slug": "G.-F.-Harpur",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Harpur",
                            "middleNames": [
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. F. Harpur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680968"
                        ],
                        "name": "R. Prager",
                        "slug": "R.-Prager",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Prager",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prager"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 9
                            }
                        ],
                        "text": "Finally, Harpur & Prager (1996) have formalised an inhibitoryfeedback network which also learns non-orthogonal oriented receptive elds.7.5 Biological signi cance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13911187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0590940043b88992c4417d239c01c282cd31309",
            "isKey": true,
            "numCitedBy": 117,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an unsupervised neural network which exhibits competition between units via inhibitory feedback. The operation is such as to minimize reconstruction error, both for individual patterns, and over the entire training set. A key difference from networks which perform principal components analysis, or one of its variants, is the ability to converge to non-orthogonal weight values. We discuss the network's operation in relation to the twin goals of maximizing information transfer and minimizing code entropy, and show how the assignment of prior probabilities to network outputs can help to reduce entropy. We present results from two binary coding problems, and from experiments with image coding."
            },
            "slug": "Development-of-low-entropy-coding-in-a-recurrent-Harpur-Prager",
            "title": {
                "fragments": [],
                "text": "Development of low entropy coding in a recurrent network."
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An unsupervised neural network which exhibits competition between units via inhibitory feedback is presented, and it is shown how the assignment of prior probabilities to network outputs can help to reduce entropy."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067720375"
                        ],
                        "name": "A. Bell",
                        "slug": "A.-Bell",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Bell",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "(9), and then evaluate the gradient to give (Bell & Sejnowski 1995): W / [WT ] 1 + \u0177xT (11) where \u0177 = [\u01771 : : : \u0177N ]T , the elements of which depend on the nonlinearity as follows: \u0177i = @ @yi @yi @ui = @ @ui ln @yi @ui (12) Amari, Cichocki & Yang (1996) have proposed a modi cation of this rule which utilises the natural gradient rather than the absolute gradient of H(y)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 86
                            }
                        ],
                        "text": "An earlier account of the application of these techniques tonatural sounds appears in Bell & Sejnowski (1996).2 Blind separation of natural images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "(13), in which Eq.(12) evaluates as: \u0177i = 1 2yi."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14707799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5241dd7d74602186dd65fe05435fc65eae797e4",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised learning algorithms paying attention only to second-order statistics ignore the phase structure (higher-order statistics) of signals, which contains all the informative temporal and spatial coincidences which we think of as 'features'. Here we discuss how an Independent Component Analysis (ICA) algorithm may be used to elucidate the higher-order structure of natural signals, yielding their independent basis functions. This is illustrated with the ICA transform of the sound of a fingernail tapping musically on a tooth. The resulting independent basis functions look like the sounds themselves, having similar temporal envelopes and the same musical pitches. Thus they reflect both the phase and frequency information inherent in the data."
            },
            "slug": "Learning-the-higher-order-structure-of-a-natural-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning the higher-order structure of a natural sound."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "How an Independent Component Analysis algorithm may be used to elucidate the higher-order structure of natural signals, yielding their independent basis functions, illustrated with the ICA transform of the sound of a fingernail tapping musically on a tooth."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094223803"
                        ],
                        "name": "Terrence J. SejnowskiComputational",
                        "slug": "Terrence-J.-SejnowskiComputational",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "SejnowskiComputational",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terrence J. SejnowskiComputational"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "(9), and then evaluate the gradient to give (Bell & Sejnowski 1995): W / [WT ] 1 + \u0177xT (11) where \u0177 = [\u01771 : : : \u0177N ]T , the elements of which depend on the nonlinearity as follows: \u0177i = @ @yi @yi @ui = @ @ui ln @yi @ui (12) Amari, Cichocki & Yang (1996) have proposed a modi cation of this rule which utilises the natural gradient rather than the absolute gradient of H(y)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "(11): W / @H(y) @W WTW = (I+ \u0177uT )W (13) This rule has the twin advantages over Eq.(11) of avoiding the matrix inverse, and of converging several orders of magnitude more quickly, for data, x, that is not prewhitened."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "It amounts to multiplying the absolute gradient by WTW, giving, in our case, the following altered version of Eq.(11): W / @H(y) @W WTW = (I+ \u0177uT )W (13) This rule has the twin advantages over Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16640812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eac703036e525a3de0b2a17315208d0760f3d036",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Blind separation is an information theoretic problem , and we have proposed an information theoretic 'sigmoid-based' solution [2]. Here we elaborate on several aspects of that solution. Firstly, we argue that the separation matrix may be exactly found by maximis-ing the joint entropy of the random vector resulting from a linear transformation of the mixtures followed by sigmoidal non-linearities which are the cumulative density functions of the 'unknown' sources. Secondly, we present the learning rule for performing this max-imisation. Thirdly, we discuss the role of prior knowledge of the c.d.f.'s of the sources in customising the learning rule. We argue that sigmoid-based methods are better able to make use of this prior knowledge than cumulant-based methods, because the optimal non-linearity they should use is just an estimate of the source c.d.f. We also suggest that they may have the edge in terms of robustness and speed of convergence. Improvements in convergence speed have been facilitated by the introduction of pre-whitening of the mixture data. An example result demonstrating this is the perfect separation of ten artificially mixed audio signals in 10 seconds of workstation computing time (4 to prewhiten and 6 to separate). Statistically independent sources propagating in a medium are subject to several forms of distortion and interference. They may be (1) mixed with other sources (2) mixed with time delayed versions of themselves , and (3) time-delayed. The mixing may be linear or non-linear. The inversion of these three forms of scrambling without any knowledge of their form may be called blznd szgnal processzng, or blznd zdentzficatzon. When the mixing is linear, we usually refer to (1) as the problem of blind separation [4], (2) as the problem of blind deconvolution, and (3) as the problem of blind time alignment. These problems are znformation theoretzc problems in the sense that we are dealing with the removal of statistical dependencies introduced by the medium, and the correct measure of statistical dependency is mutual information (see below). In the most general information theoretic formalism, no special status is given to noise introduced by the medium or the sensors. It is regarded as another 'source' to be separated out. It cannot be assumed to be characterised only by second-order statistics (gaussian). In fact, if we are lucky (and we usually are), it will not be gaussian, for it is the higher-order statistics which characterise a signal as independent and enable it \u2026"
            },
            "slug": "Fast-blind-separation-based-on-information-theory-Bell-SejnowskiComputational",
            "title": {
                "fragments": [],
                "text": "Fast blind separation based on information theory"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that the separation matrix may be exactly found by maximis-ing the joint entropy of the random vector resulting from a linear transformation of the mixtures followed by sigmoidal non-linearities which are the cumulative density functions of the 'unknown' sources."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983976"
                        ],
                        "name": "M. Eldracher",
                        "slug": "M.-Eldracher",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Eldracher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Eldracher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51073406"
                        ],
                        "name": "Bernhard Foltin",
                        "slug": "Bernhard-Foltin",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Foltin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Foltin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16154391,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bf2479e607ff547012b54dab3f35dc01613ef86",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Predictability minimization (PMSchmidhuber 1992) exhibits various intuitive and theoretical advantages over many other methods for unsupervised redundancy reduction. So far, however, there have not been any serious practical applications of PM. In this paper, we apply semilinear PM to static real world images and find that without a teacher and without any significant preprocessing, the system automatically learns to generate distributed representations based on well-known feature detectors, such as orientation-sensitive edge detectors and off-centeron-surround detectors, thus extracting simple features related to those considered useful for image preprocessing and compression."
            },
            "slug": "Semilinear-Predictability-Minimization-Produces-Schmidhuber-Eldracher",
            "title": {
                "fragments": [],
                "text": "Semilinear Predictability Minimization Produces Well-Known Feature Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper applies semilinear PM to static real world images and finds that without a teacher and without any significant preprocessing, the system automatically learns to generate distributed representations based on well-known feature detectors, thus extracting simple features related to those considered useful for image preprocessing and compression."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770190"
                        ],
                        "name": "K. Torkkola",
                        "slug": "K.-Torkkola",
                        "structuredName": {
                            "firstName": "Kari",
                            "lastName": "Torkkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Torkkola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 134
                            }
                        ],
                        "text": "Such arecurrent ICA system has been further developed for recovering sources whichhave been linearly convolved with temporal lters by Torkkola (1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5749256,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "17318d51a624933fbf7c6dbada7da9e3b850bc00",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Blind separation of independent sources from their convolutive mixtures is a problem in many real world multi-sensor applications. In this paper we present a solution to this problem based on the information maximization principle, which was proposed by Bell and Sejnowski (1995) for the case of blind separation of instantaneous mixtures. We present a feedback network architecture capable of coping with convolutive mixtures, and we derive the adaptation equations for the adaptive filters in the network by maximizing the information transferred through the network. Examples using speech signals are presented to illustrate the algorithm."
            },
            "slug": "Blind-separation-of-convolved-sources-based-on-Torkkola",
            "title": {
                "fragments": [],
                "text": "Blind separation of convolved sources based on information maximization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a feedback network architecture capable of coping with convolutive mixtures, and derives the adaptation equations for the adaptive filters in the network by maximizing the information transferred through the network."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "(1) there may be no ICA solution, and (2) a given ICA algorithmmay not nd the solution even if it exists, since there are approximations involved."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "The linear image synthesis model is therefore given by: x = As: (1) The goal of a perceptual system, in this simpli ed framework, is to linearly transform the images, x, with a matrix of lters, W, so that the resulting vector: u =Wx (2) recovers the underlying causes, s, possibly in a di erent order, and rescaled."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "The linear image synthesismodel is therefore given by: x = As: (1)The goal of a perceptual system, in this simpli ed framework, is to linearlytransform the images, x, with a matrix of lters, W, so that the resultingvector: u =Wx (2)recovers the underlying causes, s, possibly in a di erent order, and rescaled."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "properties: (1) The PCA lters de ne orthogonal directions in the vector space of the image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 154
                            }
                        ],
                        "text": "\u2026/ [WT ] 1 + y\u0302xT (11)where y\u0302 = [y\u03021 : : : y\u0302N ]T , the elements of which depend on the nonlinearity asfollows: y\u0302i = @@yi @yi@ui = @@ui ln @yi@ui (12)Amari, Cichocki & Yang (1996) have proposed a modi cation of this rulewhich utilises the natural gradient rather than the absolute gradient of H(y)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": true,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 210
                            }
                        ],
                        "text": "An-other way to constrain the solution is to attempt to produce outputs which arenot just decorrelated, but statistically independent, the much stronger require-ment of Independent Components Analysis, or ICA (Jutten & Herault 1991,Comon 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 218
                            }
                        ],
                        "text": "We have previously demon-strated the ability of this non-linear information maximisation process (Bell& Sejnowski 1995) to nd statistically independent components to solve theproblem of separating mixed audio sources (Jutten & Herault 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": false,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2161090"
                        ],
                        "name": "S. Laughlin",
                        "slug": "S.-Laughlin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Laughlin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Laughlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8991866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "420c02bdc487338dbda64feb7491dcf9fb412f17",
            "isKey": false,
            "numCitedBy": 904,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The contrast-response function of a class of first order intemeurons in the fly's compound eye approximates to the cumulative probability distribution of contrast levels in natural scenes. Elementary information theory shows that this matching enables the neurons to encode contrast fluctuations most efficiently."
            },
            "slug": "A-Simple-Coding-Procedure-Enhances-a-Neuron's-Laughlin",
            "title": {
                "fragments": [],
                "text": "A Simple Coding Procedure Enhances a Neuron's Information Capacity"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The contrast-response function of a class of first order intemeurons in the fly's compound eye approximates to the cumulative probability distribution of contrast levels in natural scenes, showing that this matching enables the neurons to encode contrast fluctuations most efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "Zeitschrift fur Naturforschung. Section C, Biosciences"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716124"
                        ],
                        "name": "Beate H. Laheld",
                        "slug": "Beate-H.-Laheld",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Laheld",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beate H. Laheld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "(13) and rearranging gives the learning rule for a feedback weight matrix: V / (I+V)(I+ \u0177uT ): (16) In terms of an individual feedback weight, vij, this rule is: vij / ij + vij + uj(\u0177i +Xk vik \u0177k) (17) where ij = 1 when i = j, otherwise 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17839672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8637f042e3d2a2d45de41566b4203646987a8424",
            "isKey": false,
            "numCitedBy": 1501,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Source separation consists of recovering a set of independent signals when only mixtures with unknown coefficients are observed. This paper introduces a class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called equivariant adaptive separation via independence (EASI). The EASI algorithms are based on the idea of serial updating. This specific form of matrix updates systematically yields algorithms with a simple structure for both real and complex mixtures. Most importantly, the performance of an EASI algorithm does not depend on the mixing matrix. In particular, convergence rates, stability conditions, and interference rejection levels depend only on the (normalized) distributions of the source signals. Closed-form expressions of these quantities are given via an asymptotic performance analysis. The theme of equivariance is stressed throughout the paper. The source separation problem has an underlying multiplicative structure. The parameter space forms a (matrix) multiplicative group. We explore the (favorable) consequences of this fact on implementation, performance, and optimization of EASI algorithms."
            },
            "slug": "Equivariant-adaptive-source-separation-Cardoso-Laheld",
            "title": {
                "fragments": [],
                "text": "Equivariant adaptive source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A class of adaptive algorithms for source separation that implements an adaptive version of equivariant estimation and is henceforth called EASI, which yields algorithms with a simple structure for both real and complex mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 375,
                                "start": 372
                            }
                        ],
                        "text": "De ning yi = g(ui) to be the sigmoidally transformed output variables, the learning rule is then: W / @H(y) @W = E \"@ ln jJ j @W # (9) In this, E denotes expected value, y = [g(u1) : : : g(uN)]T , and jJ j is the absolute value of the determinant of the Jacobian matrix: J = det \" @yi @xj #ij (10) In stochastic gradient ascent we remove the expected value operator in Eq.(9), and then evaluate the gradient to give (Bell & Sejnowski 1995): W / [WT ] 1 + \u0177xT (11) where \u0177 = [\u01771 : : : \u0177N ]T , the elements of which depend on the nonlinearity as follows: \u0177i = @ @yi @yi @ui = @ @ui ln @yi @ui (12) Amari, Cichocki & Yang (1996) have proposed a modi cation of this rule which utilises the natural gradient rather than the absolute gradient of H(y)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "De ning yi = g(ui) to be the sigmoidally transformed output variables, the learning rule is then: W / @H(y) @W = E \"@ ln jJ j @W # (9) In this, E denotes expected value, y = [g(u1) : : : g(uN)]T , and jJ j is the absolute value of the determinant of the Jacobian matrix: J = det \" @yi @xj #ij (10) In stochastic gradient ascent we remove the expected value operator in Eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 150
                            }
                        ],
                        "text": "\u2026matrix, WI ,with `true' biophysical parameters, we prefer to imagine that potentially realbiophysical self-organisational processes (see for example Bell (1992)) occurin local spatial media where the feedforward and the feedback of informationare tightly functionally coupled, and where some\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10783447,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5bedc70649b766d3b6ef0d4c58e2a268e9c381d6",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Ion channels are the dynamical systems of the nervous system. Their distribution within the membrane governs not only communication of information between neurons, but also how that information is integrated within the cell. Here, an argument is presented for an 'anti-Hebbian' rule for changing the distribution of voltage-dependent ion channels in order to flatten voltage curvatures in dendrites. Simulations show that this rule can account for the self-organisation of dynamical receptive field properties such as resonance and direction selectivity. It also creates the conditions for the faithful conduction within the cell of signals to which the cell has been exposed. Various possible cellular implementations of such a learning rule are proposed, including activity-dependent migration of channel proteins in the plane of the membrane."
            },
            "slug": "Self-organization-in-Real-Neurons:-Anti-Hebb-in-Bell",
            "title": {
                "fragments": [],
                "text": "Self-organization in Real Neurons: Anti-Hebb in 'Channel Space'?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulations show that an 'anti-Hebbian' rule for changing the distribution of voltage-dependent ion channels in order to flatten voltage curvatures in dendrites can account for the self-organisation of dynamical receptive field properties such as resonance and direction selectivity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732878"
                        ],
                        "name": "R. Unbehauen",
                        "slug": "R.-Unbehauen",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Unbehauen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Unbehauen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8616881"
                        ],
                        "name": "E. Rummert",
                        "slug": "E.-Rummert",
                        "structuredName": {
                            "firstName": "Elmar",
                            "lastName": "Rummert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rummert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 197
                            }
                        ],
                        "text": "(13) and rearranging gives the learning rule for a feedback weight matrix: V / (I+V)(I+ \u0177uT ): (16) In terms of an individual feedback weight, vij, this rule is: vij / ij + vij + uj(\u0177i +Xk vik \u0177k) (17) where ij = 1 when i = j, otherwise 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62303138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efd6ac0de538af10c943b69b502a7bbeeca95e10",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a novel, efficient, self-normalising, unsupervised adaptive learning algorithm for the on-line (real-time) separation of statistically independent unknown source signals from a linear mixture of them. In contrast to the known algorithms the new algorithm allows the separation (or extraction) of extremely badly scaled signals (i.e. some or even all of the source and/or sensor signals can be very weak). Moreover, the mixing matrix can be very ill-conditioned. >"
            },
            "slug": "Robust-learning-algorithm-for-blind-separation-of-Cichocki-Unbehauen",
            "title": {
                "fragments": [],
                "text": "Robust learning algorithm for blind separation of signals"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A novel, efficient, self-normalising, unsupervised adaptive learning algorithm for the on-line (real-time) separation of statistically independent unknown source signals from a linear mixture of them."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144616256"
                        ],
                        "name": "D. Pham",
                        "slug": "D.-Pham",
                        "structuredName": {
                            "firstName": "Dinh",
                            "lastName": "Pham",
                            "middleNames": [
                                "Tuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94375334"
                        ],
                        "name": "P. Garrat",
                        "slug": "P.-Garrat",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Garrat",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Garrat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15960752,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "04659300743b0a716154918340c6743d435de902",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Fig. 1. Absolute value of the crosstalk with respect to the number of samples (NS) used to estimate the cross cumulants. Each point is the average of 10 experiments. to estimate the cross-cumulants. Each point in Fig. 1 corresponds to the average over 10 experiments, in which the mixing matrix is randomly chosen: The matrix entries mC3 (i # j) are random numbers in the range [-1, +1]. With 500 samples, a residual crosstalk of about-20 dB is obtained. In the case of nonstationary signals, cross-cumulant estimation must be done on few samples and has a larger variance. Consequently, it can lead to more inaccurate estimation of the mixing matrix. We still obtained an interesting performance: a residual crosstalk of about-15 to-20 dB, with various signals (colored noise, speech) and statistics estimated over 500 samples. In this correspondence, we proved that the mixing matrix can be. estimated using fourth-ordercross-cumulants, for two mixtures of two non-Gaussian sources. Solutions are obtained by rooting a fourth-order polynomial equation. Using second-order cross-cumulants allows us to simplify the method; the solution is then obtained by rooting two second-order polynomial equations and gives the result if one source is Gaussian. The methods are then quite simple, but its roots are very sensitive to the accuracy of the estimated cumulants. In fact, this direct solution is less accurate than indirect methods, especially adaptive a l g o r i b s. Moreover, we restricted the study to the separation of two sources, and theoretical solutions for three sources or more seems not easily tractable. However, in the case of two mixtures of two sources, it may give a good starting point with a small computation cost for any adaptive algorithm. REFERENCES J.-F. Cardoso, \" Blind identification of independent signals, \" in Proc."
            },
            "slug": "Separation-of-a-mixture-of-independent-sources-a-Pham-Garrat",
            "title": {
                "fragments": [],
                "text": "Separation of a mixture of independent sources through a maximum likelihood approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proved that the mixing matrix can be estimated using fourth-ordercross-cumulants, for two mixtures of two non-Gaussian sources, and theoretical solutions for three sources or more seems not easily tractable."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391863648"
                        ],
                        "name": "M. Goodall",
                        "slug": "M.-Goodall",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Goodall",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goodall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 44
                            }
                        ],
                        "text": "WZ is related to the transformsdescribed by Goodall (1960) and Atick & Redlich (1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4266472,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2a77eeb1b608548dbb92690f36358ee07fec2af5",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "STOCHASTIC nets have been proposed by several authors1\u20133 as models of cognitive activity. So far, these have been dominated by heuristic constraints designed to show some analogy with the nervous system or merely for ease of computation. An adequate mathematical theory has, as was recognized by Uttley4, to show how such a device can learn relations."
            },
            "slug": "Performance-of-a-Stochastic-Net-Goodall",
            "title": {
                "fragments": [],
                "text": "Performance of a Stochastic Net"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An adequate mathematical theory has been recognized to show how aSTOCHASTIC nets can learn relations, as was recognized by Uttley4."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3236630"
                        ],
                        "name": "R. Lambert",
                        "slug": "R.-Lambert",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Lambert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lambert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6741849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee115cafbbd6483b88a9694b92e6e84a50a9c9f6",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the difficult problem of separating multiple speakers with multiple microphones in a real room. We combine the work of Torkkola and Amari, Cichocki and Yang, to give Natural Gradient information maximisation rules for recurrent (IIR) networks, blindly adjusting delays, separating and deconvolving mixed signals. While they work well on simulated data, these rules fail in real rooms which usually involve non-minimum phase transfer functions, not-invertible using stable IIR filters. An approach that sidesteps this problem is to perform infomax on a feedforward architecture in the frequency domain (Lambert 1996). We demonstrate real-room separation of two natural signals using this approach."
            },
            "slug": "Blind-Separation-of-Delayed-and-Convolved-Sources-Lee-Bell",
            "title": {
                "fragments": [],
                "text": "Blind Separation of Delayed and Convolved Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work combines the work of Torkkola and Amari, Cichocki and Yang, to give Natural Gradient information maximisation rules for recurrent (IIR) networks, blindly adjusting delays, separating and deconvolving mixed signals."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 83
                            }
                        ],
                        "text": "The mathematical framework for analysing such `coincidences' isInformation Theory (Cover & Thomas 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 81
                            }
                        ],
                        "text": "Correlation-based models of neural development, inNeuroscience and Connectionist Theory, M. Gluck & D. Rumelhart, eds.,267-353, Lawrence Erlbaum, Hillsdale, NJ[43] Nadal J-P."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "Symp. on Nonlinear Theory and Applications,Las Vegas, Dec. 1995[12] Bell A.J. & Sejnowski T.J. 1996."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Theory of edge detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Theory for the de-velopment of neuron selectivity: orientation speci city and binocular in-teraction in visual cortex, J. Neurosci., 2, 1, 32-48."
                    },
                    "intents": []
                }
            ],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": true,
            "numCitedBy": 42792,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143929773"
                        ],
                        "name": "M. C. Jones",
                        "slug": "M.-C.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33734211"
                        ],
                        "name": "R. Sibson",
                        "slug": "R.-Sibson",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Sibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sibson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 89
                            }
                        ],
                        "text": "This has led to feature-learning algorithms (Intrator 1992) with a `Projection Pursuit' (Huber 1985) avour, the most successful of which has been Olshausen & Field's (1996)demonstration of the self-organisation of local, oriented receptive elds usinga sparseness criterion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 111
                            }
                        ],
                        "text": "Sparseness, as captured by the kurtosis, is one projection index often mentioned in projection pursuit methods (Huber, 1985), which look in multivariate data for directions with \"interesting\" distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 113
                            }
                        ],
                        "text": "Sparseness, as captured by the kurtosis, is one projection index often men-tioned in projection pursuit methods (Huber, 1985), which look in multivari-ate data for directions with `interesting' distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 73
                            }
                        ],
                        "text": "feature-learning algorithms (Intrator, 1992) with a \"projection pursuit\" (Huber, 1985) flavour, the most successful of which has been the Olshausen & Field"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125481163,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1ebb53a7e5cff86b2b42d1108a0fa81f571d8894",
            "isKey": true,
            "numCitedBy": 1404,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-projection-pursuit-Jones-Sibson",
            "title": {
                "fragments": [],
                "text": "What is projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144246983"
                        ],
                        "name": "H. Barlow",
                        "slug": "H.-Barlow",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Barlow",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barlow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 89
                            }
                        ],
                        "text": "Field (1987, 1994) has argued for theimportance of sparse, or `Minimum Entropy', coding (Barlow 1994), in whicheach feature detector is activated as rarely as possible."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "(5) is: WZ = hxxT i 1=2: (8) Like most other decorrelating lters, and unlike PCA, the basis functions and the lters coming fromWZ will be di erent from each other, and neither will be orthogonal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "The training set was `sphered' by subtracting the mean and multiplying by two times the local symmetrical (zero-phase) whitening lter of Eq.(8): fxg 2WZ(fxg hxi): (18) This removes both rst and second order statistics from the data, and makes the covariance matrix of x equal to 4I."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38039315,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a793fcb70cbbfbcbf69330593136fce84054f11b",
            "isKey": true,
            "numCitedBy": 164,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-the-computational-goal-of-the-neocortex-Barlow",
            "title": {
                "fragments": [],
                "text": "What is the computational goal of the neocortex"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 141
                            }
                        ],
                        "text": "Cybern., 68, 23-29 21\nA W\nu\ns\nfilters\nbasis functions causes\nimage patch, x image ensemble\nFigure 1: The Blind Linear Image Synthesis model (Olshausen & Field, 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 30
                            }
                        ],
                        "text": "The starting point is that of Olshausen & Field 1996, depicted in Figure 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 104
                            }
                        ],
                        "text": "Some of these lters are Gabor-like and resemble those produced by thesparseness-maximisation network of Olshausen & Field (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 22
                            }
                        ],
                        "text": "Field's arguments led Olshausen & Field (1996), in work that motivatedour approach, to attempt to learn receptive elds by maximising sparseness."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Natural image statistics and e cient  coding, Network: Computation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Systems,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 106
                            }
                        ],
                        "text": "4, while failing to emerge (without external constraints) in many previous self-organizing network models (Linsker, 1988; Miller, 1988; Atick & Redlich, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 93
                            }
                        ],
                        "text": "Many contributions have emphasised the roles of decorrelation and PCA (Oja1989, Sanger 1989, Miller 1988, Hancock et al 1992, Foldiak 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 70
                            }
                        ],
                        "text": "Many contributions have emphasized the roles of decorrelation and PCA (Oja, 1989; Sanger, 1989; Miller, 1988; Hancock et al., 1992; F61difik, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 254
                            }
                        ],
                        "text": "\u2026of classical V1 simple cell receptive elds (Hubel &Wiesel 1968), that they are local and oriented, are properties of the lters inFig.4, while failing to emerge (without external constraints) in many previousself-organising network models (Linsker 1988, Miller 1988, Atick & Redlich15\n1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 100
                            }
                        ],
                        "text": "A variety of Hebbian feature-learning algorithms for decorrelation havebeen proposed (Linsker 1992, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990,Atick & Redlich 1993), but in the absense of particular external constraintsthe solutions to the decorrelation problem are non-unique (see Section 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Correlation-based models of neural development"
            },
            "venue": {
                "fragments": [],
                "text": "Gluck, M. & Rumelhart, D. (Eds), Neuroscience and connectionist theory (pp. 267-353). Hillsdale, NJ: Lawrence Erlbaum."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 83
                            }
                        ],
                        "text": "The mathematical framework for analysing such `coincidences' isInformation Theory (Cover & Thomas 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 217
                            }
                        ],
                        "text": "The original (unsphered) data was then transformed by all three decorrelating transforms, and for each the kurtosis of each of the 144 lters was calculated, according to the formula: Ki = h(ui huii)4i hu2i hu2i ii2 3 (19) Then the mean kurtosis for each lter type (ICA, PCA, ZCA) was calculated, averaging over all lters and input data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 81
                            }
                        ],
                        "text": "Correlation-based models of neural development, inNeuroscience and Connectionist Theory, M. Gluck & D. Rumelhart, eds.,267-353, Lawrence Erlbaum, Hillsdale, NJ[43] Nadal J-P."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "Symp. on Nonlinear Theory and Applications,Las Vegas, Dec. 1995[12] Bell A.J. & Sejnowski T.J. 1996."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Theory of edge detection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Theory for the de-velopment of neuron selectivity: orientation speci city and binocular in-teraction in visual cortex, J. Neurosci., 2, 1, 32-48."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elements of information theory, John  Wiley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3133875"
                        ],
                        "name": "C. Blakemore",
                        "slug": "C.-Blakemore",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Blakemore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Blakemore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1488922565"
                        ],
                        "name": "K. Adler",
                        "slug": "K.-Adler",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Adler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Adler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95339213"
                        ],
                        "name": "M. Pointon",
                        "slug": "M.-Pointon",
                        "structuredName": {
                            "firstName": "Marcia",
                            "lastName": "Pointon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pointon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60661455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b77ff97f6629175e0b4321d426506f0e26946de",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Vision:-Coding-and-Efficiency-Blakemore-Adler",
            "title": {
                "fragments": [],
                "text": "Vision: Coding and Efficiency"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120692531"
                        ],
                        "name": "Liuyue Wang",
                        "slug": "Liuyue-Wang",
                        "structuredName": {
                            "firstName": "Liuyue",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liuyue Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768650"
                        ],
                        "name": "J. Joutsensalo",
                        "slug": "J.-Joutsensalo",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Joutsensalo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Joutsensalo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59755188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "202e7213a5c0a0ef3ee53bf4c651140ce9e8c66e",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Estimation-of-Basis-Vectors-in-Independent-Karhunen-Wang",
            "title": {
                "fragments": [],
                "text": "Neural Estimation of Basis Vectors in Independent Component Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98782082"
                        ],
                        "name": "J. Benes",
                        "slug": "J.-Benes",
                        "structuredName": {
                            "firstName": "Jir\u00ed",
                            "lastName": "Benes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Benes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16819094,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "c54c3b6f602ca674169d660151dcd72fd14d7b95",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-neural-networks-Benes",
            "title": {
                "fragments": [],
                "text": "On neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Kybernetika"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Why do you have edge detectors?"
            },
            "venue": {
                "fragments": [],
                "text": "Why do you have edge detectors?"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 48
                            }
                        ],
                        "text": "Following up from this, Law & Cooper (1994) and Shouval (1995) used theBCM rule to self-organise oriented and somewhat localised receptive elds onan ensemble of natural images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 138
                            }
                        ],
                        "text": "Such a recurrent ICA system has been further developed for recovering sources which have been linearly convolved with temporal filters by Torkkola (1996) and Lee et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Formation and organisation of receptive fields"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 123
                            }
                        ],
                        "text": "A variety of Hebbian feature-learning algorithms for decorrelation havebeen proposed (Linsker 1992, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990,Atick & Redlich 1993), but in the absense of particular external constraintsthe solutions to the decorrelation problem are non-unique (see Section 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 80
                            }
                        ],
                        "text": "Many contributions have emphasised the roles of decorrelation and PCA (Oja1989, Sanger 1989, Miller 1988, Hancock et al 1992, Foldiak 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal unsupervised learning in a single-layer net-  work"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature detection in biological and artificial vision systems Vision: coding and efficiency"
            },
            "venue": {
                "fragments": [],
                "text": "Feature detection in biological and artificial vision systems Vision: coding and efficiency"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Why do you have edge detectors? Optical society of America: Technical Digest"
            },
            "venue": {
                "fragments": [],
                "text": "Why do you have edge detectors? Optical society of America: Technical Digest"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature detection in biological and artificial vision systems"
            },
            "venue": {
                "fragments": [],
                "text": "Vision : coding and efficiency"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature detection in biological and articial vision systems Vision: coding and eeciency, Camb. Univ. Press 16 Equivariant adaptive source separation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. on Signal Proc"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 24
                            }
                        ],
                        "text": "Following up from this, Law & Cooper (1994) and Shouval (1995) used theBCM rule to self-organise oriented and somewhat localised receptive elds onan ensemble of natural images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Formation of receptive elds in realistic visual environments according to the Bienestock"
            },
            "venue": {
                "fragments": [],
                "text": "Cooper and Munro (BCM) theory, Proc. Natl. Acad. Sci, USA, 91"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 24
                            }
                        ],
                        "text": "Following up from this, Law & Cooper (1994) and Shouval (1995) used theBCM rule to self-organise oriented and somewhat localised receptive elds onan ensemble of natural images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Formation of receptive fields in realistic visual environments according to the Bienestock"
            },
            "venue": {
                "fragments": [],
                "text": "Cooper and Munro (BCM) theory. Proceedings of the National Academy of Sciences USA"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 56
                            }
                        ],
                        "text": "This point is brought out more fully in a recent report (Olshausen, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning linear, sparse, factorial codes, MIT AI-memo No. 1580"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 150
                            }
                        ],
                        "text": "\u2026matrix, WI ,with `true' biophysical parameters, we prefer to imagine that potentially realbiophysical self-organisational processes (see for example Bell (1992)) occurin local spatial media where the feedforward and the feedback of informationare tightly functionally coupled, and where some\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self - organisation in real neurons : antiHebb in Channel space ?"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in neural information processing systems"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 254
                            }
                        ],
                        "text": "\u2026of classical V1 simple cell receptive elds (Hubel &Wiesel 1968), that they are local and oriented, are properties of the lters inFig.4, while failing to emerge (without external constraints) in many previousself-organising network models (Linsker 1988, Miller 1988, Atick & Redlich15\n1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 100
                            }
                        ],
                        "text": "A variety of Hebbian feature-learning algorithms for decorrelation havebeen proposed (Linsker 1992, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990,Atick & Redlich 1993), but in the absense of particular external constraintsthe solutions to the decorrelation problem are non-unique (see Section 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 93
                            }
                        ],
                        "text": "Many contributions have emphasised the roles of decorrelation and PCA (Oja1989, Sanger 1989, Miller 1988, Hancock et al 1992, Foldiak 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Correlation-based models of neural development Non-linear neurons in the low noise limit: a factorial code maximises information transfer"
            },
            "venue": {
                "fragments": [],
                "text": "Nadal J-P. & Parga N. Network"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 83
                            }
                        ],
                        "text": "Linsker's approach, and that of Atick &Redlich (1990), Bialek et al (1991) and van Hateren (1992) uses the second11\norder (covariance matrix) approximation of the required information theoreticquantities, and generally assumes gaussian signal and gaussian noise, in whichcase the second order\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A theory of maximising sensory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 45
                            }
                        ],
                        "text": "Wz is related to the transforms described by Goodall (1960) and Atick & Redlich (1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Finding compact and sparsedistributed representations of visual"
            },
            "venue": {
                "fragments": [],
                "text": "images. Network,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 89
                            }
                        ],
                        "text": "This has led to feature-learning algorithms (Intrator 1992) with a `Projection Pursuit' (Huber 1985) avour, the most successful of which has been Olshausen & Field's (1996)demonstration of the self-organisation of local, oriented receptive elds usinga sparseness criterion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 113
                            }
                        ],
                        "text": "Sparseness, as captured by the kurtosis, is one projection index often men-tioned in projection pursuit methods (Huber, 1985), which look in multivari-ate data for directions with `interesting' distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Projection pursuit, The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Projection pursuit, The Annals of Statistics"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 150
                            }
                        ],
                        "text": "\u2026matrix, WI ,with `true' biophysical parameters, we prefer to imagine that potentially realbiophysical self-organisational processes (see for example Bell (1992)) occurin local spatial media where the feedforward and the feedback of informationare tightly functionally coupled, and where some\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-organisation in real neurons: Anti-Hebb inin`channel space?"
            },
            "venue": {
                "fragments": [],
                "text": "Moody J. et al (eds) Advances in Neural Information Processing Systems"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "The rst workalong these lines was by Linsker (1988) who rst proposed the `infomax' prin-ciple which underlies our own work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 240
                            }
                        ],
                        "text": "\u2026of classical V1 simple cell receptive elds (Hubel &Wiesel 1968), that they are local and oriented, are properties of the lters inFig.4, while failing to emerge (without external constraints) in many previousself-organising network models (Linsker 1988, Miller 1988, Atick & Redlich15\n1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-organization in a perceptual"
            },
            "venue": {
                "fragments": [],
                "text": "network. Computer,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "Di erentiating, and using the quotient rule for matrices gives: V = (W 1) =W 1( W)W 1: (15) Inserting Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature detection in biological and arti-  cial vision systems, in Blakemore C. (ed.) Vision: coding and e ciency,  Camb"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 210
                            }
                        ],
                        "text": "An-other way to constrain the solution is to attempt to produce outputs which arenot just decorrelated, but statistically independent, the much stronger require-ment of Independent Components Analysis, or ICA (Jutten & Herault 1991,Comon 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 218
                            }
                        ],
                        "text": "We have previously demon-strated the ability of this non-linear information maximisation process (Bell& Sejnowski 1995) to nd statistically independent components to solve theproblem of separating mixed audio sources (Jutten & Herault 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: an adap-  tive algorithm based on neuromimetic architecture, Signal processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Separation of a mixture of inde-  pendent sources through a maximum likelihood approach"
            },
            "venue": {
                "fragments": [],
                "text": "in Proc. EU-  SIPCO,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-linear neurons in the low noise limit: a  factorial code maximises information"
            },
            "venue": {
                "fragments": [],
                "text": "transfer. Network,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of convolved sources based on information maximisation A theory of maximising sensory information"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Workshop on Neural Networks and Signal Processing"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 123
                            }
                        ],
                        "text": "A variety of Hebbian feature-learning algorithms for decorrelation havebeen proposed (Linsker 1992, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990,Atick & Redlich 1993), but in the absense of particular external constraintsthe solutions to the decorrelation problem are non-unique (see Section 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 70
                            }
                        ],
                        "text": "Many contributions have emphasized the roles of decorrelation and PCA (Oja, 1989; Sanger, 1989; Miller, 1988; Hancock et al., 1992; F61difik, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 80
                            }
                        ],
                        "text": "Many contributions have emphasised the roles of decorrelation and PCA (Oja1989, Sanger 1989, Miller 1988, Hancock et al 1992, Foldiak 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal unsupervised learning in a single-layer network"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 45
                            }
                        ],
                        "text": "This has led to feature-learning algorithms (Intrator 1992) with a `Projection Pursuit' (Huber 1985) avour, the most successful of which has been Olshausen & Field's (1996)demonstration of the self-organisation of local, oriented receptive elds usinga sparseness criterion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Intrator (1992) haspioneered the application of projection pursuit reasoning to feature extractionproblems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature extraction using an unsupervised neural net-  work"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 557,
                                "start": 233
                            }
                        ],
                        "text": "Another way to constrain the solution is to attempt to produce outputs which are not just decorrelated, but statistically independent, the much stronger requirement of Independent Components Analysis, or ICA (Jutten & Hrrault, 1991; Comon, 1994). The ui are independent when their probability distribution, fu, factorizes as follows: f u (u) = Hi fuiui, equivalently, when there is zero mutual information between them: I(ui,uj) = O, V i # j. A number of approaches to ICA have some relations with the one we describe below, notably Cardoso & Laheld (1996), Karhunen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 581,
                                "start": 233
                            }
                        ],
                        "text": "Another way to constrain the solution is to attempt to produce outputs which are not just decorrelated, but statistically independent, the much stronger requirement of Independent Components Analysis, or ICA (Jutten & Hrrault, 1991; Comon, 1994). The ui are independent when their probability distribution, fu, factorizes as follows: f u (u) = Hi fuiui, equivalently, when there is zero mutual information between them: I(ui,uj) = O, V i # j. A number of approaches to ICA have some relations with the one we describe below, notably Cardoso & Laheld (1996), Karhunen et al. (1996), Amari et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature detection in biological and artificial vision systems. In Blakemore, C. (Ed.), Vision: coding and efficiency"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind deconvolution. New Jersey: PrenticeHall"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 539,
                                "start": 156
                            }
                        ],
                        "text": "cessed with a whitening/lowpass filter, our algorithm yielded basis functions which were localized multiscale Gabor patches qualitively similar to those in Olshausen's Fig. 4. Part of the difference in our results is therefore attributable to different preprocessing techniques. Further discussion and comparison of these two approaches is deferred to the section entitled: Sparseness. Figure 5 shows the result of analysing the distributions (image histograms) produced by each of the three filter types. As emphasized by Ruderman (1994) and Field (1994), the general form of these histograms is doubleexponential (exp-]ui[), or \"sparse\", meaning peaky with a long tail, when compared with a gaussian."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Non-linear neurons in the low noise limit: a factorial code maximises information"
            },
            "venue": {
                "fragments": [],
                "text": "transfer. Network,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 113
                            }
                        ],
                        "text": "A variety of Hebbian feature-learning algorithms for decorrelation havebeen proposed (Linsker 1992, Miller 1988, Oja 1989, Sanger 1989, Foldiak 1990,Atick & Redlich 1993), but in the absense of particular external constraintsthe solutions to the decorrelation problem are non-unique (see Section 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 70
                            }
                        ],
                        "text": "Many contributions have emphasized the roles of decorrelation and PCA (Oja, 1989; Sanger, 1989; Miller, 1988; Hancock et al., 1992; F61difik, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 71
                            }
                        ],
                        "text": "Many contributions have emphasised the roles of decorrelation and PCA (Oja1989, Sanger 1989, Miller 1988, Hancock et al 1992, Foldiak 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks, principal components and linear neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind Deconvolution, Prentice-Hall, New Jersey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 19
                            }
                        ],
                        "text": "In a further study,Baddeley (1996) argued against kurtosis-maximisation, partly on the groundsthat it would produce lters which are two pixels wide."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Searching for lters with \\interesting\" output distributions: an uninteresting direction to explore?, Network"
            },
            "venue": {
                "fragments": [],
                "text": "Searching for lters with \\interesting\" output distributions: an uninteresting direction to explore?, Network"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 16
                            }
                        ],
                        "text": "As empasized by Ruderman (1994)and Field (1994), the general form of these histograms is double-exponential(e juij), or `sparse', meaning peaky with a long tail, when compared to a gaus-sian."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The statistics of natural images, Network: Compu-  tation in"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Systems,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 79
                            }
                        ],
                        "text": "Linsker's approach, and that of Atick &Redlich (1990), Bialek et al (1991) and van Hateren (1992) uses the second11\norder (covariance matrix) approximation of the required information theoreticquantities, and generally assumes gaussian signal and gaussian noise, in whichcase the second order\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A theory of maximising sensory information"
            },
            "venue": {
                "fragments": [],
                "text": "Biol. Cybern"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "A PCA matrix, WP , was calculated from Eq.(7)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "(5) and solving for W gives the PCA solution, WP : WP = D 12ET (7) This solution is unusual in that the lters (rows of WP ) are orthogonal, so that WWT = D 1, a scaling matrix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Why do you have edge detectors?  Optical society of America"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Digest,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "(13) in terms of individual weights, we have: wij / wij + \u0177iXk wkjuk (14) The weighted sum non-local term in this rule can be seen as the result of a simple backwards pass through the weights from the linear output vector, u, to the inputs, x, so that each weight `knows the in uence' of its input, xj."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory for the de-  velopment of neuron selectivity: orientation speci city and binocular in-  teraction in visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "J. Neurosci.,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, a new concept? Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Independent component analysis, a new concept? Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 26
                            }
                        ],
                        "text": "The approach developed in Bell & Sejnowski (1995a) was to maximize by stochastic gradient ascent the joint entropy, H[g(u)], of the linear transform squashed by a sigmoidal function, g."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 150
                            }
                        ],
                        "text": "\u2026matrix, WI ,with `true' biophysical parameters, we prefer to imagine that potentially realbiophysical self-organisational processes (see for example Bell (1992)) occurin local spatial media where the feedforward and the feedback of informationare tightly functionally coupled, and where some\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-organisation in real neurons: anti-Hebb in Channel space? In Moody, J"
            },
            "venue": {
                "fragments": [],
                "text": "et al. (Eds) Advances in neural information processing systems (Vol. 4, pp. 59-66). MorganKaufmann."
            },
            "year": 1992
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 46,
            "methodology": 30,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 90,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/The-\u201cindependent-components\u201d-of-natural-scenes-are-Bell-Sejnowski/ca1d23be869380ac9e900578c601c2d1febcc0c9?sort=total-citations"
}