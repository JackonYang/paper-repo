{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34793898"
                        ],
                        "name": "H. Yu",
                        "slug": "H.-Yu",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Yu",
                            "middleNames": [
                                "Heather"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145136049"
                        ],
                        "name": "W. Wolf",
                        "slug": "W.-Wolf",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Wolf",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Wolf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62633650,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b0b3890878a4c6e723b207c8c66a59e3b75fb374",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of scenic image classification is presented in the paper. On considering the specific nature of this problem, we propose a statistically data-based method, the Hidden Markov Model, to solve this problem. We segment an image and use the sequence of segments as the definition of the image; we then train a HMM on a test set of sequences/images to establish a classification. We present preliminary results on the use of a 1D HMM for classification of images as either indoor or outdoor."
            },
            "slug": "Scenic-classification-methods-for-image-and-video-Yu-Wolf",
            "title": {
                "fragments": [],
                "text": "Scenic classification methods for image and video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A statistically data-based method is proposed, the Hidden Markov Model, to solve the problem of scenic image classification, where an image is segmented and the sequence of segments is used as the definition of the image."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13687267"
                        ],
                        "name": "Tanweer Kabir",
                        "slug": "Tanweer-Kabir",
                        "structuredName": {
                            "firstName": "Tanweer",
                            "lastName": "Kabir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tanweer Kabir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2170732251"
                        ],
                        "name": "Fang Liu",
                        "slug": "Fang-Liu",
                        "structuredName": {
                            "firstName": "Fang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fang Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6884754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cdfb65cb9d565f831fe11d048c8958c2c5e40f4",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The Brodatz Album has become the de facto standard for evaluating texture algorithms, with hundreds of studies having been applied to small sets of its images. The authors compare two powerful recognition algorithms, principal components analysis and multiscale autoregressive models, by evaluating them on a 999-image database derived from the entire Brodatz Album. The variety of homogeneous and nonhomogeneous images studied is thus nearly an order of magnitude larger than has been compared before, giving one snapshot of the state of the art in real-time texture recognition.<<ETX>>"
            },
            "slug": "Real-time-recognition-with-the-entire-Brodatz-Picard-Kabir",
            "title": {
                "fragments": [],
                "text": "Real-time recognition with the entire Brodatz texture database"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The authors compare two powerful recognition algorithms, principal components analysis and multiscale autoregressive models, by evaluating them on a 999-image database derived from the entire Brodatz Album, giving one snapshot of the state of the art in real-time texture recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4723637"
                        ],
                        "name": "J. Mao",
                        "slug": "J.-Mao",
                        "structuredName": {
                            "firstName": "Jianchang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11686184,
            "fieldsOfStudy": [
                "Mathematics",
                "Environmental Science",
                "Computer Science"
            ],
            "id": "0b1c14ccf1aad87f215bfa5c6678d975d44ffb3a",
            "isKey": false,
            "numCitedBy": 795,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Texture-classification-and-segmentation-using-Mao-Jain",
            "title": {
                "fragments": [],
                "text": "Texture classification and segmentation using multiresolution simultaneous autoregressive models"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087139"
                        ],
                        "name": "M. Gorkani",
                        "slug": "M.-Gorkani",
                        "structuredName": {
                            "firstName": "Monika",
                            "lastName": "Gorkani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gorkani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "Gorkani and Picard [5] discriminate between photos of city scenes and photos of landscape scenes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3643,
                                "start": 3625
                            }
                        ],
                        "text": "Introduction\nThe scene classi cation problem is one of the holy grail challenges of computer vision Given an arbitrary pho tograph we would like to describe what type of seman tic scene it depicts Currently very little work has been done in this area probably because the problem is very di cult and also because there is no agreed upon scene description language Most computer vision research in volves low level image analysis that rarely tries to bridge the gap to semantic scene description\nOur purpose is to show how one particular seman tic scene description problem can be approached The task is to determine whether a consumer photograph depicts an indoor or an outdoor scene This problem is relatively unambiguous and is motivated by several practical applications\nThere certainly exist photographs for which the indoor outdoor distinction is unclear Examples include shots made through a window with visible window edges photographs of paintings and extreme close ups of faces Fortunately such compositions are rare in con sumer photographs Our database of images was\nThis work was supported in part by Kodak NEC and Hewlett\nPackard Labs\nclassi ed by two independent people and only nineteen out of these were labeled as unclear and omitted from this study\nThe applications of this problem are interesting Knowledge about the scene enables more intelligent im age processing For example when lm is developed and prints are made from the negatives the exposure and color is automatically adjusted Unfortunately the automatic correction does not take into account the con tent of the photograph If the machine could distin guish indoor from outdoor images it could adjust these classes di erently rather than adjusting everything to wards one ideal exposure and color This observation can also be applied to image scanners photocopiers fax machines image processing software etc\nAnother important application is image retrieval Let s say we would like to nd a beach scene A help ful step would be to limit the search to outdoor scenes Unfortunately this is not possible even in state of the art image retrieval systems such as QBIC Virage and VisualSEEk These systems are based mainly on color histograms and primitive texture measures The user builds a query by selecting colors from a palette a texture from a chart and then indicates how to weight the color versus the texture Unfortunately it is di cult for a user to know how to weight the di erent features to get a beach scene The systems level of abstraction is much too low\nQuery by image example enables the user to select one image and nd other similar images making it eas ier to specify the relevant color and texture query Most systems still require the user to select weights for the di erent features An exception to this is FourEyes which can learn the relevant feature combination based on several positive and negative examples In an initial quick attempt to teach FourEyes to solve the indoor outdoor classi cation problem using whole images with no speci c subblock guidance we did not meet with sig ni cant success Although FourEyes can learn any clas si cation it was not very e cient on this one probably due to the noisiness of the two classes being considered\nIn this work we propose a di erent classi cation ap proach that exploits the same idea in FourEyes of non linearly combining features from multiple models but does so in a di erent way This new way is successful for accurately distinguishing indoor from outdoor scenes\nBackground\nSeveral attempts at recognizing high level scene proper ties using low level features have been made Gorkani and Picard discriminate between photos of city scenes and photos of landscape scenes They use a multiscale steerable pyramid to nd dominant orientations in subblocks of the image The image is classi ed as a city scene if enough subblocks have strong dominant verti cal orientation or alternatively medium strong vertical orientation and also horizontal orientation\nYiu uses the same dominant orientation features and also color information to classify indoor and outdoor scenes She uses nearest neighbor and support vector machine classi ers The former classi er is better at color the latter at dominant orientation Yiu reports accuracies similar to those in our work but they are believed to have a high variance because they were not thoroughly evaluated with a leave one out method and a much smaller database of only images was used Furthermore the texture features used here give sig ni cantly better results than her dominant orientation detector The work here also takes advantage of a spa tial tessellation of the image which we found provides a signi cant gain in performance\nInstead of building a speci c scene class detector Lipson describes a general scene query approach Scenes are described by graphs representing relations between image regions The relationships include rel ative color spatial location and highpass frequency content Unfortunately the templates have to be con structed manually for each scene layout These tem plates are also quite speci c which makes them ne for limited special cases such as sky over mountain over lake but di cult for the case considered here of cap turing a broad concept like an outdoor scene\nYu learns a statistical template from examples She computes vector quantized color histograms for sub blocks of the image Then she trains a one dimensional hidden Markov model along vertical or horizontal seg ments of speci c scene layouts such as sky mountain river scenes Unfortunately the one dimensional model cannot describe spatial relationships well and a two dimensional generalization such as Markov random elds is desirable but raises many new kinds of prob lems\nFeatures\nImage Database\nThe image database in these experiments consists of consumer photographs collected and labeled by Kodak They depict typical family and vacation scenes and are taken by many di erent individuals at all dif ferent times of they year The database is quite diverse and includes snow bright sun sea sunset night and silhouette scenes Image types not in our database can be easily added to the training set without changing any algorithms\nThe images were hand labeled by two independent people not the authors resulting in labeled as outdoor and labeled as indoor We have excluded images which were labeled as am biguous All the images in the set have landscape ori entation and are right side up The full resolution of the images was but for most experiments we used half or quarter resolution as will be described later The images originally came in bit color but were quantized down to bit color At the same time we performed basic color balancing according to steps provided by Kodak which simply clipped the top and bottom of the intensity levels shifted the histograms to the middle and stretched them to occupy the bit range\nA baseline experiment\nThe problem is quite challenging A naive approach such as a traditional color histogram will not give good classi cation performance To illustrate this we com puted bin histograms uniformly spaced for each RGB channel concatenated them into a feature vec tor and applied a nearest neighbor classi er The dis tance between feature vectors was measured using the Euclidean norm The resulting leave one out classi cation performance was This number is only somewhat better than just guessing that each image is outdoor which would be correct Nevertheless a very similar color histogram is used at the heart of most image retrieval systems\nBelow we describe and evaluate several methods which perform much better These included more so phisticated features and classi ers which tessellate the image into subregions and combine the results from dif ferent features and di erent spatial regions to result in signi cantly improved performance\nThe features\nWe have used three types of features one each for color texture and frequency information These features were\ncomputed both for the whole image and for each sub block of a image tessellation\nThe color feature is a color histogram and has bins per channel like our baseline However the three channels come from the Ohta color space The color axes of this space are the largest eigenvectors of the RGB space found through principal components anal ysis of a large selection of natural images This yields\nI R G B\nI R B\nI R G B\nThe advantage of the Ohta color space is that the color channels are approximately decorrelated which makes it a good choice for computing per channel histograms The change of color spaces from RGB to Ohta raises the performance of color histogram based recognition to\nMoreover instead of using the Euclidean norm for measuring distances between histograms we use the his togram intersection norm It measures the amount of overlap between corresponding buckets in the two his tograms h and h and is de ned as\ndist h h\nNX\ni\nh i min h i h i\nWhen both the Ohta color space and histogram in tersection is used the classi cation rises to cor rect The intersection norm is better than the Euclidean norm possibly because it penalizes linear error as op posed to squared error reducing sensitivity to outliers In the rest of the paper we exclusively apply the Ohta color space with histogram intersection\nThe texture features are computed using the multiresolution simultaneous autoregressive model MSAR These are among the best texture features bench marked on the Brodatz album The model constructs the best linear predictor of a pixel based on a noncausal neighborhood The features are the weights of the predictor Three di erent neighborhoods at scales and are used and the weights are concatenated to yield a dimensional vector as in The Maha lanobis norm is used to measure feature vector distance covariances are estimated from several subwindow es timates We extracted these features from gray scale images at two resolutions half and quarter using a suitable antialiasing lter\nThe MSAR classi cation is and correct at half and quarter resolutions respectively which is sig ni cantly better than the best color classi cation This\nis surprising since the MSAR feature presupposes a sin gle texture characterized by a second order stationary autoregressive process whereas a typical image con sists of many textures and is de nitely nonstationary As we shall see we can do even better by dividing the image into subblocks and computing the features separately over each block\nThe frequency features are obtained by rst calcu lating the D DFT magnitude and then taking the D DCT The rst step is shift invariant and for periodic textures it shows a regular pattern of peaks fundamen tal and harmonic frequencies The second step replaces all related frequencies by one coe cient All computa tions are done over pixel blocks and the results are averaged over the image region also producing co variances used for the Mahalanobis distance metric\nClassi cation\nThe performance numbers so far refer to nearest neigh bor classi cation of features computed on the whole im age Unfortunately this method cannot exploit local properties of the image e g blue sky at the top Never theless most image retrieval systems use features com puted for the whole image thus the numbers here are useful for comparisons The results are summarized in Table for a K nearest neighbor classi er We have also tried a layer neural network with sigmoid nonlinear ities but training was slow and the results were worse than for the nearest neighbor algorithm when compared on the color histogram features\nTo allow local and spatial properties to improve the classi cation we divided the image into subblocks and computed the features separately within them Now the question arises how to classify these blocks One possibility would be to concatenate feature vector from all subblocks of an image and apply a classi er to this vector The problem is that such a feature is very high dimensional e g It is di cult to estimate covariances for such a large vector and we en counter general curse of dimensionality problems\nInstead we chose to pursue a multi stage classi ca tion approach classifying the subblocks independently and then performing another classi cation on these an swers Figure This is reminiscent of stacking except that the subblock classi ers here were trained on their own data Not surprisingly the individual sub block classi ers are less accurate than a whole image classi er Ideally we would keep a con dence or prob abilistic value associated with each subblock classi ca tion as opposed to the binary decision in or out shown in Figure In theory the mixture of experts method applied below takes care of this case this will be described later For now Table shows the results\nTable Whole image classi cation results using k nearest neighbor The best result in each row is marked with\nFeature k k k k k RGB histogram euclidean RGB histogram intersection Ohta histogram euclidean Ohta histogram intersection MSAR quarter resolution MSAR half resolution DCT half resolution\nof the use of the k nearest neighbor classi er on the color features where each subblock is compared to all subblocks in the database regardless of spatial location excluding subblocks from the same image\nWhen the results of the subblocks are combined the classi cation can be greatly improved Three ways to combine the features were systematically tried a simple majority classi er that assigns the label for the image to the most common class label among the sub blocks a one layer neural net and a Mixture of Experts classi er The rst method was evaluated using the leave one out method and was found to give good results Table The other two methods because of their long training time were only evaluated with a few runs of leave out training on of the data and testing on the other In these limited tests which are subject to higher variance than the leave one out test method we got slightly better results than the majority classi er but not signi cantly better\nThe one layer neural net can give us information about what subblocks are important for the classi ca tion task The net has a sigmoid nonlinearity at the output and optimizes the cross entropy cost function between the network output and the true class By ob serving the weights learned by the network we found that it favors especially the top row but also the lower center of the image for classi cation gure the sub blocks were classi ed using color histograms In some images sky occurs in this region and is perhaps easier to classify correctly and hence is heavily weighted\nThe mixture of experts classifer is similar to but more exible than the above neural network It learns experts for speci c subproblems The experts are se lected depending on the input data and each expert can weight the data di erently For example if the top of the image is classi ed as outdoor an expert can weight it more heavily than if the top of the image is classi ed as indoor The technique is similar to softly clus tering the data and assigning a set of weights for each\n0\n0.05\n0.1\n0.15\nx subblock\ny su\nbb lo\nck\nWeights in stacked regression\n0.5 1 1.5 2 2.5 3 3.5 4 4.5\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nFigure Weights of subblocks in image classi cation The larger the weights the brighter the square\ncluster however the clustering and weight assignment occur simultaneously Somewhat disappointingly the classi cation results are approximately the same as for the neural network Moreover we are required to set additional parameters the number of experts and be careful to avoid over tting Thus we judged that the additional e ort was not justi ed\nMultiple feature combination\nSo far we have combined multiple subblock classi ca tions but these were all based on a single image feature To gain robustness we can use multiple image features simultaneously One common way to do this is to con catenate di erent feature vectors into a longer vector Unfortunately this step increases the dimensionality of the problem and requires a metric which is simulta neously good for e g color histograms and MSAR The relationship between two features is almost certainly not linear so such a metric is di cult to construct There\nTexture Texture classification\nColor Color classification\nCombined classification\ninout\nininin in\nin in\nin in\nin in in\nin\nout\nout\nin\nininin in\nout in in\nout in\nout in in\nin\nin\nin\nIn\nFigure Two stage classi cation combining color and texture\nTable K nearest neighbor classi cations on subblocks The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Majority classi er based on k nearest neighbor The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Combined feature classi er k MSAR fea tures were measured at half resolution\nFeature Performance Color MSAR Color DCT MSAR DCT Color MSAR DCT\nTable Confusion matrix for color MSAR combination\nTrue Class Classi ed as indoor outdoor\nindoor outdoor\nfore we think this approach is a mistake even though it is commonly used by researchers working in content based retrieval\nThere is a way to combine feature vectors just by concatenation by rst translating all the features into a common language This was done in the FourEyes system by using the common language of clusters Our common language is di erent the subblock classes as signed by the k nearest neighbor classifer In other words we simply concatenate the subblock classi ca tions based on di erent image features and then do a second stage classi cation In the second stage we get signi cantly improved results by applying the majority classi er to the combined vectors Table\nDiscussion\nThe best classi cation results were generally obtained by combining color features with texture features Both the MSAR and DCT based features capture shift invariant intensity variations over a range of scales so combining them does not provide as much gain as com bining color with one of them\nThe color MSAR combination gives us the best re sult correct measured using leave one out cross validation The confusion matrix for this result Ta ble shows that it is approximately equally likely to mistakenly label indoor images as being outdoor or vice versa The proportion of indoor to outdoor images in the database is vs which is fairly balanced\nFigure shows several correctly classi ed images by the combined color and MSAR algorithm see http www media mit edu szummer caivd for color ver sions of the images These images were incorrectly la beled when using only color information The color algo rithm easily mistakes photos containing green or navy\nblue as outdoor images Conversely it often mistakes photos containing white areas e g snow scenes and brown colors as indoor images The texture feature dis regards color and the combination gives the right an swer\nFigures and show samples of images that were misclassi ed by the combined color and MSAR algo rithm Some of the missclassi ed indoor images contain green plants Christmas trees or green walls since green is a typical color of outdoor images Another di cult indoor image is a picture of the top of a shelf and the ceiling which looks blue under ash light like sky The missclassi ed outdoor images are often night time ash photographs White outdoor walls and hazy white sky are also sometimes mistaken to be indoor probably be cause they are very common in indoor scenes Close ups are always challenging because they are dominated by one object and provide little background\nIt is tempting to believe that outdoor images can be easily classi ed by building a blue sky detector A quick look at the database disspells this myth at least for amateur photographs only about one in ve outdoor images have clear blue sky in most outdoor images the sky is not visible or is cloudy white or gray These cloudy colors can unfortunately be produced by ash light as well making them di cult to use for discrimi nation\nConclusions\nWe have shown how high level scene properties can be inferred from low level image features The indoor outdoor classi cation problem is only one example of a high level scene property and we believe that many other properties can be inferred in a similar way Since people often reason in terms of semantic image proper ties it is important for vision systems to extract them\nWe found that it is quite di cult to predict the performance of a feature or feature combination often combining two weaker features with a k nearest neigh bor classi er consistently produced better results than a single good feature Moreover relatively simple clas si ers k nearest neighbors performed better than the more sophisticated neural networks and mixture of ex pert classi ers These empirical results suggest that a theoretical investigation should be undertaken in an ef fort to better understand the relative merits of these methods\nNevertheless we believe that performance will scale well to larger databases of consumer photography After a thorough examination we settled for simple but ro bust classi ers that require few parameter settings Of course it is always possible to devise scenes that will\nfool any system However our system can always be provided with more ground truth for new image types which is likely to increase the performance on such im ages In the domain of consumer photography we have used a large enough sample to show that accurate clas si cation is possible\nAcknowledgement\nThe authors wish to thank Thomas Minka for help with the FourEyes software and Bob Gray at Kodak for sug gestions Portions of the research in this paper use the Kodak Image Research Database This work was sup ported in part by Kodak NEC and Hewlett Packard Labs\nReferences\nMyron Flickner Harpreet Sawhney et al Query by image and video content The QBIC system IEEE Computer Sept\nAmarnath Gupta and Ramesh Jain Visual infor mation retrieval Communications of the ACM http www virage com research htm vir cacm pdf\nJ R Smith and S F Chang Visualseek a fully automated content based image query system In ACM Multimedia pages Nov\nT P Minka and R W Picard Interactive learn ing using a !society of models In Proceedings of CVPR pages San Francisco CA June IEEE Computer Society\nMonika Gorkani and Rosalind W Picard Tex ture orientation for sorting photos at a glance In Proc Int Conf Pat Rec volume I pages Jerusalem Israel Oct\nElaine C Yiu Image classi cation using color cues and texture orientation Master s thesis MIT dept EECS\nPamela R Lipson Context and Con guration Based Scene Classi cation PhD thesis MIT EECS dept\nHong Heather Yu and Wayne Wolf Scenic clas si cation methods for image and video databases In Proc SPIE Digital Image Storage and Archiv ing systems pages http www ee princeton edu heathery\nY I Ohta T Kanade and T Sakai Color infor mation for region segmentation Comp Graph and Img Proc\nMichael Swain and Dana Ballard Color indexing Int J of Comp Vis\nJianchang Mao and Anil K Jain Texture classi cation and segmentation using multiresolution si multaneous autoregressive models Pattern Recog nition\nRosalind W Picard Tanweer Kabir and Fang Liu Real time recognition with the entire Brodatz tex ture database In Proc IEEE Conf on Computer Vision and Pattern Recognition pages New York June MIT Media Lab Perceptual Computing TR\nLeo Breiman Stacked regression ftp ftp stat berkeley edu pub users breiman stacked abstract\nMichael I Jordan and Robert A Jacobs Hierar chical mixtures of experts and the EM algorithm Neural Computation\nFigure out of missclassi ed outdoor images combined color and MSAR classi er Outdoor ash photos in dusk or at night are especially di cult as are scenes with white regions walls hazy sky Close ups are also challenging since they are dominated by one object and do not provide much context The other outdoor images in the database were correctly labelled\nFigure out of missclassi ed indoor images combined color and MSAR classi er Plants Christmas trees green walls and brown oors are sometimes mistakenly thought to belong to outdoor scenes The other indoor images were correctly classi ed"
                    },
                    "intents": []
                }
            ],
            "corpusId": 18920889,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c21738e116aeabca1e523f612610605718ae00ff",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Investigates a measure of \"dominant perceived orientation\" that has been developed to match the output of a human study involving 40 subjects. The results of this measure are compared with humans analyzing seven \"teaser\" images to test its effectiveness for finding perceptually dominant orientations. The use of low-level orientation is then applied to a \"quick search\" problem important in image database applications. Since both pigeons and humans are able to perform coarse classification of certain kinds of scenes, e.g., city from country, without taking time or brain-power to solve the image understanding problem, the authors conjecture that the collective behavior of low-level textural features such as orientation may be doing most of the work. The authors demonstrate a simple test of global multiscale orientation for quickly searching a database of vacation photos for likely \"city/suburb\" shots. The orientation features achieve agreement with human classification in 91 out of 98 of the scenes."
            },
            "slug": "Texture-orientation-for-sorting-photos-\"at-a-Gorkani-Picard",
            "title": {
                "fragments": [],
                "text": "Texture orientation for sorting photos \"at a glance\""
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors demonstrate a simple test of global multiscale orientation for quickly searching a database of vacation photos for likely \"city/suburb\" shots and find the orientation features achieve agreement with human classification in 91 out of 98 of the scenes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 12th International Conference on Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722219"
                        ],
                        "name": "Y. Ohta",
                        "slug": "Y.-Ohta",
                        "structuredName": {
                            "firstName": "Yuichi",
                            "lastName": "Ohta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ohta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46599154"
                        ],
                        "name": "T. Sakai",
                        "slug": "T.-Sakai",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Sakai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sakai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121890149,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ab67b9d0da50e251a4f7e42370540547b891ceb1",
            "isKey": false,
            "numCitedBy": 975,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Color-information-for-region-segmentation-Ohta-Kanade",
            "title": {
                "fragments": [],
                "text": "Color information for region segmentation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118965263"
                        ],
                        "name": "John R. Smith",
                        "slug": "John-R.-Smith",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Smith",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John R. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "Unfortunately, this is not possible even in state-of-theart image retrieval systems such as QBIC [1], Virage [2] and VisualSEEk [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2161,
                                "start": 2151
                            }
                        ],
                        "text": "Introduction\nThe scene classi cation problem is one of the holy grail challenges of computer vision Given an arbitrary pho tograph we would like to describe what type of seman tic scene it depicts Currently very little work has been done in this area probably because the problem is very di cult and also because there is no agreed upon scene description language Most computer vision research in volves low level image analysis that rarely tries to bridge the gap to semantic scene description\nOur purpose is to show how one particular seman tic scene description problem can be approached The task is to determine whether a consumer photograph depicts an indoor or an outdoor scene This problem is relatively unambiguous and is motivated by several practical applications\nThere certainly exist photographs for which the indoor outdoor distinction is unclear Examples include shots made through a window with visible window edges photographs of paintings and extreme close ups of faces Fortunately such compositions are rare in con sumer photographs Our database of images was\nThis work was supported in part by Kodak NEC and Hewlett\nPackard Labs\nclassi ed by two independent people and only nineteen out of these were labeled as unclear and omitted from this study\nThe applications of this problem are interesting Knowledge about the scene enables more intelligent im age processing For example when lm is developed and prints are made from the negatives the exposure and color is automatically adjusted Unfortunately the automatic correction does not take into account the con tent of the photograph If the machine could distin guish indoor from outdoor images it could adjust these classes di erently rather than adjusting everything to wards one ideal exposure and color This observation can also be applied to image scanners photocopiers fax machines image processing software etc\nAnother important application is image retrieval Let s say we would like to nd a beach scene A help ful step would be to limit the search to outdoor scenes Unfortunately this is not possible even in state of the art image retrieval systems such as QBIC Virage and VisualSEEk These systems are based mainly on color histograms and primitive texture measures The user builds a query by selecting colors from a palette a texture from a chart and then indicates how to weight the color versus the texture Unfortunately it is di cult for a user to know how to weight the di erent features to get a beach scene The systems level of abstraction is much too low\nQuery by image example enables the user to select one image and nd other similar images making it eas ier to specify the relevant color and texture query Most systems still require the user to select weights for the di erent features An exception to this is FourEyes which can learn the relevant feature combination based on several positive and negative examples In an initial quick attempt to teach FourEyes to solve the indoor outdoor classi cation problem using whole images with no speci c subblock guidance we did not meet with sig ni cant success Although FourEyes can learn any clas si cation it was not very e cient on this one probably due to the noisiness of the two classes being considered\nIn this work we propose a di erent classi cation ap proach that exploits the same idea in FourEyes of non linearly combining features from multiple models but does so in a di erent way This new way is successful for accurately distinguishing indoor from outdoor scenes\nBackground\nSeveral attempts at recognizing high level scene proper ties using low level features have been made Gorkani and Picard discriminate between photos of city scenes and photos of landscape scenes They use a multiscale steerable pyramid to nd dominant orientations in subblocks of the image The image is classi ed as a city scene if enough subblocks have strong dominant verti cal orientation or alternatively medium strong vertical orientation and also horizontal orientation\nYiu uses the same dominant orientation features and also color information to classify indoor and outdoor scenes She uses nearest neighbor and support vector machine classi ers The former classi er is better at color the latter at dominant orientation Yiu reports accuracies similar to those in our work but they are believed to have a high variance because they were not thoroughly evaluated with a leave one out method and a much smaller database of only images was used Furthermore the texture features used here give sig ni cantly better results than her dominant orientation detector The work here also takes advantage of a spa tial tessellation of the image which we found provides a signi cant gain in performance\nInstead of building a speci c scene class detector Lipson describes a general scene query approach Scenes are described by graphs representing relations between image regions The relationships include rel ative color spatial location and highpass frequency content Unfortunately the templates have to be con structed manually for each scene layout These tem plates are also quite speci c which makes them ne for limited special cases such as sky over mountain over lake but di cult for the case considered here of cap turing a broad concept like an outdoor scene\nYu learns a statistical template from examples She computes vector quantized color histograms for sub blocks of the image Then she trains a one dimensional hidden Markov model along vertical or horizontal seg ments of speci c scene layouts such as sky mountain river scenes Unfortunately the one dimensional model cannot describe spatial relationships well and a two dimensional generalization such as Markov random elds is desirable but raises many new kinds of prob lems\nFeatures\nImage Database\nThe image database in these experiments consists of consumer photographs collected and labeled by Kodak They depict typical family and vacation scenes and are taken by many di erent individuals at all dif ferent times of they year The database is quite diverse and includes snow bright sun sea sunset night and silhouette scenes Image types not in our database can be easily added to the training set without changing any algorithms\nThe images were hand labeled by two independent people not the authors resulting in labeled as outdoor and labeled as indoor We have excluded images which were labeled as am biguous All the images in the set have landscape ori entation and are right side up The full resolution of the images was but for most experiments we used half or quarter resolution as will be described later The images originally came in bit color but were quantized down to bit color At the same time we performed basic color balancing according to steps provided by Kodak which simply clipped the top and bottom of the intensity levels shifted the histograms to the middle and stretched them to occupy the bit range\nA baseline experiment\nThe problem is quite challenging A naive approach such as a traditional color histogram will not give good classi cation performance To illustrate this we com puted bin histograms uniformly spaced for each RGB channel concatenated them into a feature vec tor and applied a nearest neighbor classi er The dis tance between feature vectors was measured using the Euclidean norm The resulting leave one out classi cation performance was This number is only somewhat better than just guessing that each image is outdoor which would be correct Nevertheless a very similar color histogram is used at the heart of most image retrieval systems\nBelow we describe and evaluate several methods which perform much better These included more so phisticated features and classi ers which tessellate the image into subregions and combine the results from dif ferent features and di erent spatial regions to result in signi cantly improved performance\nThe features\nWe have used three types of features one each for color texture and frequency information These features were\ncomputed both for the whole image and for each sub block of a image tessellation\nThe color feature is a color histogram and has bins per channel like our baseline However the three channels come from the Ohta color space The color axes of this space are the largest eigenvectors of the RGB space found through principal components anal ysis of a large selection of natural images This yields\nI R G B\nI R B\nI R G B\nThe advantage of the Ohta color space is that the color channels are approximately decorrelated which makes it a good choice for computing per channel histograms The change of color spaces from RGB to Ohta raises the performance of color histogram based recognition to\nMoreover instead of using the Euclidean norm for measuring distances between histograms we use the his togram intersection norm It measures the amount of overlap between corresponding buckets in the two his tograms h and h and is de ned as\ndist h h\nNX\ni\nh i min h i h i\nWhen both the Ohta color space and histogram in tersection is used the classi cation rises to cor rect The intersection norm is better than the Euclidean norm possibly because it penalizes linear error as op posed to squared error reducing sensitivity to outliers In the rest of the paper we exclusively apply the Ohta color space with histogram intersection\nThe texture features are computed using the multiresolution simultaneous autoregressive model MSAR These are among the best texture features bench marked on the Brodatz album The model constructs the best linear predictor of a pixel based on a noncausal neighborhood The features are the weights of the predictor Three di erent neighborhoods at scales and are used and the weights are concatenated to yield a dimensional vector as in The Maha lanobis norm is used to measure feature vector distance covariances are estimated from several subwindow es timates We extracted these features from gray scale images at two resolutions half and quarter using a suitable antialiasing lter\nThe MSAR classi cation is and correct at half and quarter resolutions respectively which is sig ni cantly better than the best color classi cation This\nis surprising since the MSAR feature presupposes a sin gle texture characterized by a second order stationary autoregressive process whereas a typical image con sists of many textures and is de nitely nonstationary As we shall see we can do even better by dividing the image into subblocks and computing the features separately over each block\nThe frequency features are obtained by rst calcu lating the D DFT magnitude and then taking the D DCT The rst step is shift invariant and for periodic textures it shows a regular pattern of peaks fundamen tal and harmonic frequencies The second step replaces all related frequencies by one coe cient All computa tions are done over pixel blocks and the results are averaged over the image region also producing co variances used for the Mahalanobis distance metric\nClassi cation\nThe performance numbers so far refer to nearest neigh bor classi cation of features computed on the whole im age Unfortunately this method cannot exploit local properties of the image e g blue sky at the top Never theless most image retrieval systems use features com puted for the whole image thus the numbers here are useful for comparisons The results are summarized in Table for a K nearest neighbor classi er We have also tried a layer neural network with sigmoid nonlinear ities but training was slow and the results were worse than for the nearest neighbor algorithm when compared on the color histogram features\nTo allow local and spatial properties to improve the classi cation we divided the image into subblocks and computed the features separately within them Now the question arises how to classify these blocks One possibility would be to concatenate feature vector from all subblocks of an image and apply a classi er to this vector The problem is that such a feature is very high dimensional e g It is di cult to estimate covariances for such a large vector and we en counter general curse of dimensionality problems\nInstead we chose to pursue a multi stage classi ca tion approach classifying the subblocks independently and then performing another classi cation on these an swers Figure This is reminiscent of stacking except that the subblock classi ers here were trained on their own data Not surprisingly the individual sub block classi ers are less accurate than a whole image classi er Ideally we would keep a con dence or prob abilistic value associated with each subblock classi ca tion as opposed to the binary decision in or out shown in Figure In theory the mixture of experts method applied below takes care of this case this will be described later For now Table shows the results\nTable Whole image classi cation results using k nearest neighbor The best result in each row is marked with\nFeature k k k k k RGB histogram euclidean RGB histogram intersection Ohta histogram euclidean Ohta histogram intersection MSAR quarter resolution MSAR half resolution DCT half resolution\nof the use of the k nearest neighbor classi er on the color features where each subblock is compared to all subblocks in the database regardless of spatial location excluding subblocks from the same image\nWhen the results of the subblocks are combined the classi cation can be greatly improved Three ways to combine the features were systematically tried a simple majority classi er that assigns the label for the image to the most common class label among the sub blocks a one layer neural net and a Mixture of Experts classi er The rst method was evaluated using the leave one out method and was found to give good results Table The other two methods because of their long training time were only evaluated with a few runs of leave out training on of the data and testing on the other In these limited tests which are subject to higher variance than the leave one out test method we got slightly better results than the majority classi er but not signi cantly better\nThe one layer neural net can give us information about what subblocks are important for the classi ca tion task The net has a sigmoid nonlinearity at the output and optimizes the cross entropy cost function between the network output and the true class By ob serving the weights learned by the network we found that it favors especially the top row but also the lower center of the image for classi cation gure the sub blocks were classi ed using color histograms In some images sky occurs in this region and is perhaps easier to classify correctly and hence is heavily weighted\nThe mixture of experts classifer is similar to but more exible than the above neural network It learns experts for speci c subproblems The experts are se lected depending on the input data and each expert can weight the data di erently For example if the top of the image is classi ed as outdoor an expert can weight it more heavily than if the top of the image is classi ed as indoor The technique is similar to softly clus tering the data and assigning a set of weights for each\n0\n0.05\n0.1\n0.15\nx subblock\ny su\nbb lo\nck\nWeights in stacked regression\n0.5 1 1.5 2 2.5 3 3.5 4 4.5\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nFigure Weights of subblocks in image classi cation The larger the weights the brighter the square\ncluster however the clustering and weight assignment occur simultaneously Somewhat disappointingly the classi cation results are approximately the same as for the neural network Moreover we are required to set additional parameters the number of experts and be careful to avoid over tting Thus we judged that the additional e ort was not justi ed\nMultiple feature combination\nSo far we have combined multiple subblock classi ca tions but these were all based on a single image feature To gain robustness we can use multiple image features simultaneously One common way to do this is to con catenate di erent feature vectors into a longer vector Unfortunately this step increases the dimensionality of the problem and requires a metric which is simulta neously good for e g color histograms and MSAR The relationship between two features is almost certainly not linear so such a metric is di cult to construct There\nTexture Texture classification\nColor Color classification\nCombined classification\ninout\nininin in\nin in\nin in\nin in in\nin\nout\nout\nin\nininin in\nout in in\nout in\nout in in\nin\nin\nin\nIn\nFigure Two stage classi cation combining color and texture\nTable K nearest neighbor classi cations on subblocks The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Majority classi er based on k nearest neighbor The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Combined feature classi er k MSAR fea tures were measured at half resolution\nFeature Performance Color MSAR Color DCT MSAR DCT Color MSAR DCT\nTable Confusion matrix for color MSAR combination\nTrue Class Classi ed as indoor outdoor\nindoor outdoor\nfore we think this approach is a mistake even though it is commonly used by researchers working in content based retrieval\nThere is a way to combine feature vectors just by concatenation by rst translating all the features into a common language This was done in the FourEyes system by using the common language of clusters Our common language is di erent the subblock classes as signed by the k nearest neighbor classifer In other words we simply concatenate the subblock classi ca tions based on di erent image features and then do a second stage classi cation In the second stage we get signi cantly improved results by applying the majority classi er to the combined vectors Table\nDiscussion\nThe best classi cation results were generally obtained by combining color features with texture features Both the MSAR and DCT based features capture shift invariant intensity variations over a range of scales so combining them does not provide as much gain as com bining color with one of them\nThe color MSAR combination gives us the best re sult correct measured using leave one out cross validation The confusion matrix for this result Ta ble shows that it is approximately equally likely to mistakenly label indoor images as being outdoor or vice versa The proportion of indoor to outdoor images in the database is vs which is fairly balanced\nFigure shows several correctly classi ed images by the combined color and MSAR algorithm see http www media mit edu szummer caivd for color ver sions of the images These images were incorrectly la beled when using only color information The color algo rithm easily mistakes photos containing green or navy\nblue as outdoor images Conversely it often mistakes photos containing white areas e g snow scenes and brown colors as indoor images The texture feature dis regards color and the combination gives the right an swer\nFigures and show samples of images that were misclassi ed by the combined color and MSAR algo rithm Some of the missclassi ed indoor images contain green plants Christmas trees or green walls since green is a typical color of outdoor images Another di cult indoor image is a picture of the top of a shelf and the ceiling which looks blue under ash light like sky The missclassi ed outdoor images are often night time ash photographs White outdoor walls and hazy white sky are also sometimes mistaken to be indoor probably be cause they are very common in indoor scenes Close ups are always challenging because they are dominated by one object and provide little background\nIt is tempting to believe that outdoor images can be easily classi ed by building a blue sky detector A quick look at the database disspells this myth at least for amateur photographs only about one in ve outdoor images have clear blue sky in most outdoor images the sky is not visible or is cloudy white or gray These cloudy colors can unfortunately be produced by ash light as well making them di cult to use for discrimi nation\nConclusions\nWe have shown how high level scene properties can be inferred from low level image features The indoor outdoor classi cation problem is only one example of a high level scene property and we believe that many other properties can be inferred in a similar way Since people often reason in terms of semantic image proper ties it is important for vision systems to extract them\nWe found that it is quite di cult to predict the performance of a feature or feature combination often combining two weaker features with a k nearest neigh bor classi er consistently produced better results than a single good feature Moreover relatively simple clas si ers k nearest neighbors performed better than the more sophisticated neural networks and mixture of ex pert classi ers These empirical results suggest that a theoretical investigation should be undertaken in an ef fort to better understand the relative merits of these methods\nNevertheless we believe that performance will scale well to larger databases of consumer photography After a thorough examination we settled for simple but ro bust classi ers that require few parameter settings Of course it is always possible to devise scenes that will\nfool any system However our system can always be provided with more ground truth for new image types which is likely to increase the performance on such im ages In the domain of consumer photography we have used a large enough sample to show that accurate clas si cation is possible\nAcknowledgement\nThe authors wish to thank Thomas Minka for help with the FourEyes software and Bob Gray at Kodak for sug gestions Portions of the research in this paper use the Kodak Image Research Database This work was sup ported in part by Kodak NEC and Hewlett Packard Labs\nReferences\nMyron Flickner Harpreet Sawhney et al Query by image and video content The QBIC system IEEE Computer Sept\nAmarnath Gupta and Ramesh Jain Visual infor mation retrieval Communications of the ACM http www virage com research htm vir cacm pdf\nJ R Smith and S F Chang Visualseek a fully automated content based image query system In ACM Multimedia pages Nov\nT P Minka and R W Picard Interactive learn ing using a !society of models In Proceedings of CVPR pages San Francisco CA June IEEE Computer Society\nMonika Gorkani and Rosalind W Picard Tex ture orientation for sorting photos at a glance In Proc Int Conf Pat Rec volume I pages Jerusalem Israel Oct\nElaine C Yiu Image classi cation using color cues and texture orientation Master s thesis MIT dept EECS\nPamela R Lipson Context and Con guration Based Scene Classi cation PhD thesis MIT EECS dept\nHong Heather Yu and Wayne Wolf Scenic clas si cation methods for image and video databases In Proc SPIE Digital Image Storage and Archiv ing systems pages http www ee princeton edu heathery\nY I Ohta T Kanade and T Sakai Color infor mation for region segmentation Comp Graph and Img Proc\nMichael Swain and Dana Ballard Color indexing Int J of Comp Vis\nJianchang Mao and Anil K Jain Texture classi cation and segmentation using multiresolution si multaneous autoregressive models Pattern Recog nition\nRosalind W Picard Tanweer Kabir and Fang Liu Real time recognition with the entire Brodatz tex ture database In Proc IEEE Conf on Computer Vision and Pattern Recognition pages New York June MIT Media Lab Perceptual Computing TR\nLeo Breiman Stacked regression ftp ftp stat berkeley edu pub users breiman stacked abstract\nMichael I Jordan and Robert A Jacobs Hierar chical mixtures of experts and the EM algorithm Neural Computation\nFigure out of missclassi ed outdoor images combined color and MSAR classi er Outdoor ash photos in dusk or at night are especially di cult as are scenes with white regions walls hazy sky Close ups are also challenging since they are dominated by one object and do not provide much context The other outdoor images in the database were correctly labelled\nFigure out of missclassi ed indoor images combined color and MSAR classi er Plants Christmas trees green walls and brown oors are sometimes mistakenly thought to belong to outdoor scenes The other indoor images were correctly classi ed"
                    },
                    "intents": []
                }
            ],
            "corpusId": 17464838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa8af1c28f8ab2011339627bf8e13e267c57abdb",
            "isKey": true,
            "numCitedBy": 2203,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a highly functional prototype system for searching by visual features in an image database. The VisualSEEk system is novel in that the user forms the queries by diagramming spatial arrangements of color regions. The system nds the images that contain the most similar arrangements of similar regions. Prior to the queries, the system automatically extracts and indexes salient color regions from the images. By utilizing e cient indexing techniques for color information, region sizes and absolute and relative spatial locations, a wide variety of complex joint color/spatial queries may be computed."
            },
            "slug": "VisualSEEk:-a-fully-automated-content-based-image-Smith-Chang",
            "title": {
                "fragments": [],
                "text": "VisualSEEk: a fully automated content-based image query system"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The VisualSEEk system is novel in that the user forms the queries by diagramming spatial arrangements of color regions by utilizing color information, region sizes and absolute and relative spatial locations."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '96"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712991"
                        ],
                        "name": "M. Flickner",
                        "slug": "M.-Flickner",
                        "structuredName": {
                            "firstName": "Myron",
                            "lastName": "Flickner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Flickner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733393"
                        ],
                        "name": "H. Sawhney",
                        "slug": "H.-Sawhney",
                        "structuredName": {
                            "firstName": "Harpreet",
                            "lastName": "Sawhney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sawhney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152883679"
                        ],
                        "name": "J. Ashley",
                        "slug": "J.-Ashley",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Ashley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ashley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391129943"
                        ],
                        "name": "Qian Huang",
                        "slug": "Qian-Huang",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786444"
                        ],
                        "name": "B. Dom",
                        "slug": "B.-Dom",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Dom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087139"
                        ],
                        "name": "M. Gorkani",
                        "slug": "M.-Gorkani",
                        "structuredName": {
                            "firstName": "Monika",
                            "lastName": "Gorkani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gorkani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39311329"
                        ],
                        "name": "J. Hafner",
                        "slug": "J.-Hafner",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Hafner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hafner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499047"
                        ],
                        "name": "Denis Lee",
                        "slug": "Denis-Lee",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denis Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143867341"
                        ],
                        "name": "D. Petkovic",
                        "slug": "D.-Petkovic",
                        "structuredName": {
                            "firstName": "Dragutin",
                            "lastName": "Petkovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Petkovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144028064"
                        ],
                        "name": "David Steele",
                        "slug": "David-Steele",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Steele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70341848"
                        ],
                        "name": "P. Yanker",
                        "slug": "P.-Yanker",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Yanker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Yanker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Unfortunately, this is not possible even in state-of-theart image retrieval systems such as QBIC [1], Virage [2] and VisualSEEk [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2139,
                                "start": 2135
                            }
                        ],
                        "text": "Introduction\nThe scene classi cation problem is one of the holy grail challenges of computer vision Given an arbitrary pho tograph we would like to describe what type of seman tic scene it depicts Currently very little work has been done in this area probably because the problem is very di cult and also because there is no agreed upon scene description language Most computer vision research in volves low level image analysis that rarely tries to bridge the gap to semantic scene description\nOur purpose is to show how one particular seman tic scene description problem can be approached The task is to determine whether a consumer photograph depicts an indoor or an outdoor scene This problem is relatively unambiguous and is motivated by several practical applications\nThere certainly exist photographs for which the indoor outdoor distinction is unclear Examples include shots made through a window with visible window edges photographs of paintings and extreme close ups of faces Fortunately such compositions are rare in con sumer photographs Our database of images was\nThis work was supported in part by Kodak NEC and Hewlett\nPackard Labs\nclassi ed by two independent people and only nineteen out of these were labeled as unclear and omitted from this study\nThe applications of this problem are interesting Knowledge about the scene enables more intelligent im age processing For example when lm is developed and prints are made from the negatives the exposure and color is automatically adjusted Unfortunately the automatic correction does not take into account the con tent of the photograph If the machine could distin guish indoor from outdoor images it could adjust these classes di erently rather than adjusting everything to wards one ideal exposure and color This observation can also be applied to image scanners photocopiers fax machines image processing software etc\nAnother important application is image retrieval Let s say we would like to nd a beach scene A help ful step would be to limit the search to outdoor scenes Unfortunately this is not possible even in state of the art image retrieval systems such as QBIC Virage and VisualSEEk These systems are based mainly on color histograms and primitive texture measures The user builds a query by selecting colors from a palette a texture from a chart and then indicates how to weight the color versus the texture Unfortunately it is di cult for a user to know how to weight the di erent features to get a beach scene The systems level of abstraction is much too low\nQuery by image example enables the user to select one image and nd other similar images making it eas ier to specify the relevant color and texture query Most systems still require the user to select weights for the di erent features An exception to this is FourEyes which can learn the relevant feature combination based on several positive and negative examples In an initial quick attempt to teach FourEyes to solve the indoor outdoor classi cation problem using whole images with no speci c subblock guidance we did not meet with sig ni cant success Although FourEyes can learn any clas si cation it was not very e cient on this one probably due to the noisiness of the two classes being considered\nIn this work we propose a di erent classi cation ap proach that exploits the same idea in FourEyes of non linearly combining features from multiple models but does so in a di erent way This new way is successful for accurately distinguishing indoor from outdoor scenes\nBackground\nSeveral attempts at recognizing high level scene proper ties using low level features have been made Gorkani and Picard discriminate between photos of city scenes and photos of landscape scenes They use a multiscale steerable pyramid to nd dominant orientations in subblocks of the image The image is classi ed as a city scene if enough subblocks have strong dominant verti cal orientation or alternatively medium strong vertical orientation and also horizontal orientation\nYiu uses the same dominant orientation features and also color information to classify indoor and outdoor scenes She uses nearest neighbor and support vector machine classi ers The former classi er is better at color the latter at dominant orientation Yiu reports accuracies similar to those in our work but they are believed to have a high variance because they were not thoroughly evaluated with a leave one out method and a much smaller database of only images was used Furthermore the texture features used here give sig ni cantly better results than her dominant orientation detector The work here also takes advantage of a spa tial tessellation of the image which we found provides a signi cant gain in performance\nInstead of building a speci c scene class detector Lipson describes a general scene query approach Scenes are described by graphs representing relations between image regions The relationships include rel ative color spatial location and highpass frequency content Unfortunately the templates have to be con structed manually for each scene layout These tem plates are also quite speci c which makes them ne for limited special cases such as sky over mountain over lake but di cult for the case considered here of cap turing a broad concept like an outdoor scene\nYu learns a statistical template from examples She computes vector quantized color histograms for sub blocks of the image Then she trains a one dimensional hidden Markov model along vertical or horizontal seg ments of speci c scene layouts such as sky mountain river scenes Unfortunately the one dimensional model cannot describe spatial relationships well and a two dimensional generalization such as Markov random elds is desirable but raises many new kinds of prob lems\nFeatures\nImage Database\nThe image database in these experiments consists of consumer photographs collected and labeled by Kodak They depict typical family and vacation scenes and are taken by many di erent individuals at all dif ferent times of they year The database is quite diverse and includes snow bright sun sea sunset night and silhouette scenes Image types not in our database can be easily added to the training set without changing any algorithms\nThe images were hand labeled by two independent people not the authors resulting in labeled as outdoor and labeled as indoor We have excluded images which were labeled as am biguous All the images in the set have landscape ori entation and are right side up The full resolution of the images was but for most experiments we used half or quarter resolution as will be described later The images originally came in bit color but were quantized down to bit color At the same time we performed basic color balancing according to steps provided by Kodak which simply clipped the top and bottom of the intensity levels shifted the histograms to the middle and stretched them to occupy the bit range\nA baseline experiment\nThe problem is quite challenging A naive approach such as a traditional color histogram will not give good classi cation performance To illustrate this we com puted bin histograms uniformly spaced for each RGB channel concatenated them into a feature vec tor and applied a nearest neighbor classi er The dis tance between feature vectors was measured using the Euclidean norm The resulting leave one out classi cation performance was This number is only somewhat better than just guessing that each image is outdoor which would be correct Nevertheless a very similar color histogram is used at the heart of most image retrieval systems\nBelow we describe and evaluate several methods which perform much better These included more so phisticated features and classi ers which tessellate the image into subregions and combine the results from dif ferent features and di erent spatial regions to result in signi cantly improved performance\nThe features\nWe have used three types of features one each for color texture and frequency information These features were\ncomputed both for the whole image and for each sub block of a image tessellation\nThe color feature is a color histogram and has bins per channel like our baseline However the three channels come from the Ohta color space The color axes of this space are the largest eigenvectors of the RGB space found through principal components anal ysis of a large selection of natural images This yields\nI R G B\nI R B\nI R G B\nThe advantage of the Ohta color space is that the color channels are approximately decorrelated which makes it a good choice for computing per channel histograms The change of color spaces from RGB to Ohta raises the performance of color histogram based recognition to\nMoreover instead of using the Euclidean norm for measuring distances between histograms we use the his togram intersection norm It measures the amount of overlap between corresponding buckets in the two his tograms h and h and is de ned as\ndist h h\nNX\ni\nh i min h i h i\nWhen both the Ohta color space and histogram in tersection is used the classi cation rises to cor rect The intersection norm is better than the Euclidean norm possibly because it penalizes linear error as op posed to squared error reducing sensitivity to outliers In the rest of the paper we exclusively apply the Ohta color space with histogram intersection\nThe texture features are computed using the multiresolution simultaneous autoregressive model MSAR These are among the best texture features bench marked on the Brodatz album The model constructs the best linear predictor of a pixel based on a noncausal neighborhood The features are the weights of the predictor Three di erent neighborhoods at scales and are used and the weights are concatenated to yield a dimensional vector as in The Maha lanobis norm is used to measure feature vector distance covariances are estimated from several subwindow es timates We extracted these features from gray scale images at two resolutions half and quarter using a suitable antialiasing lter\nThe MSAR classi cation is and correct at half and quarter resolutions respectively which is sig ni cantly better than the best color classi cation This\nis surprising since the MSAR feature presupposes a sin gle texture characterized by a second order stationary autoregressive process whereas a typical image con sists of many textures and is de nitely nonstationary As we shall see we can do even better by dividing the image into subblocks and computing the features separately over each block\nThe frequency features are obtained by rst calcu lating the D DFT magnitude and then taking the D DCT The rst step is shift invariant and for periodic textures it shows a regular pattern of peaks fundamen tal and harmonic frequencies The second step replaces all related frequencies by one coe cient All computa tions are done over pixel blocks and the results are averaged over the image region also producing co variances used for the Mahalanobis distance metric\nClassi cation\nThe performance numbers so far refer to nearest neigh bor classi cation of features computed on the whole im age Unfortunately this method cannot exploit local properties of the image e g blue sky at the top Never theless most image retrieval systems use features com puted for the whole image thus the numbers here are useful for comparisons The results are summarized in Table for a K nearest neighbor classi er We have also tried a layer neural network with sigmoid nonlinear ities but training was slow and the results were worse than for the nearest neighbor algorithm when compared on the color histogram features\nTo allow local and spatial properties to improve the classi cation we divided the image into subblocks and computed the features separately within them Now the question arises how to classify these blocks One possibility would be to concatenate feature vector from all subblocks of an image and apply a classi er to this vector The problem is that such a feature is very high dimensional e g It is di cult to estimate covariances for such a large vector and we en counter general curse of dimensionality problems\nInstead we chose to pursue a multi stage classi ca tion approach classifying the subblocks independently and then performing another classi cation on these an swers Figure This is reminiscent of stacking except that the subblock classi ers here were trained on their own data Not surprisingly the individual sub block classi ers are less accurate than a whole image classi er Ideally we would keep a con dence or prob abilistic value associated with each subblock classi ca tion as opposed to the binary decision in or out shown in Figure In theory the mixture of experts method applied below takes care of this case this will be described later For now Table shows the results\nTable Whole image classi cation results using k nearest neighbor The best result in each row is marked with\nFeature k k k k k RGB histogram euclidean RGB histogram intersection Ohta histogram euclidean Ohta histogram intersection MSAR quarter resolution MSAR half resolution DCT half resolution\nof the use of the k nearest neighbor classi er on the color features where each subblock is compared to all subblocks in the database regardless of spatial location excluding subblocks from the same image\nWhen the results of the subblocks are combined the classi cation can be greatly improved Three ways to combine the features were systematically tried a simple majority classi er that assigns the label for the image to the most common class label among the sub blocks a one layer neural net and a Mixture of Experts classi er The rst method was evaluated using the leave one out method and was found to give good results Table The other two methods because of their long training time were only evaluated with a few runs of leave out training on of the data and testing on the other In these limited tests which are subject to higher variance than the leave one out test method we got slightly better results than the majority classi er but not signi cantly better\nThe one layer neural net can give us information about what subblocks are important for the classi ca tion task The net has a sigmoid nonlinearity at the output and optimizes the cross entropy cost function between the network output and the true class By ob serving the weights learned by the network we found that it favors especially the top row but also the lower center of the image for classi cation gure the sub blocks were classi ed using color histograms In some images sky occurs in this region and is perhaps easier to classify correctly and hence is heavily weighted\nThe mixture of experts classifer is similar to but more exible than the above neural network It learns experts for speci c subproblems The experts are se lected depending on the input data and each expert can weight the data di erently For example if the top of the image is classi ed as outdoor an expert can weight it more heavily than if the top of the image is classi ed as indoor The technique is similar to softly clus tering the data and assigning a set of weights for each\n0\n0.05\n0.1\n0.15\nx subblock\ny su\nbb lo\nck\nWeights in stacked regression\n0.5 1 1.5 2 2.5 3 3.5 4 4.5\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nFigure Weights of subblocks in image classi cation The larger the weights the brighter the square\ncluster however the clustering and weight assignment occur simultaneously Somewhat disappointingly the classi cation results are approximately the same as for the neural network Moreover we are required to set additional parameters the number of experts and be careful to avoid over tting Thus we judged that the additional e ort was not justi ed\nMultiple feature combination\nSo far we have combined multiple subblock classi ca tions but these were all based on a single image feature To gain robustness we can use multiple image features simultaneously One common way to do this is to con catenate di erent feature vectors into a longer vector Unfortunately this step increases the dimensionality of the problem and requires a metric which is simulta neously good for e g color histograms and MSAR The relationship between two features is almost certainly not linear so such a metric is di cult to construct There\nTexture Texture classification\nColor Color classification\nCombined classification\ninout\nininin in\nin in\nin in\nin in in\nin\nout\nout\nin\nininin in\nout in in\nout in\nout in in\nin\nin\nin\nIn\nFigure Two stage classi cation combining color and texture\nTable K nearest neighbor classi cations on subblocks The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Majority classi er based on k nearest neighbor The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Combined feature classi er k MSAR fea tures were measured at half resolution\nFeature Performance Color MSAR Color DCT MSAR DCT Color MSAR DCT\nTable Confusion matrix for color MSAR combination\nTrue Class Classi ed as indoor outdoor\nindoor outdoor\nfore we think this approach is a mistake even though it is commonly used by researchers working in content based retrieval\nThere is a way to combine feature vectors just by concatenation by rst translating all the features into a common language This was done in the FourEyes system by using the common language of clusters Our common language is di erent the subblock classes as signed by the k nearest neighbor classifer In other words we simply concatenate the subblock classi ca tions based on di erent image features and then do a second stage classi cation In the second stage we get signi cantly improved results by applying the majority classi er to the combined vectors Table\nDiscussion\nThe best classi cation results were generally obtained by combining color features with texture features Both the MSAR and DCT based features capture shift invariant intensity variations over a range of scales so combining them does not provide as much gain as com bining color with one of them\nThe color MSAR combination gives us the best re sult correct measured using leave one out cross validation The confusion matrix for this result Ta ble shows that it is approximately equally likely to mistakenly label indoor images as being outdoor or vice versa The proportion of indoor to outdoor images in the database is vs which is fairly balanced\nFigure shows several correctly classi ed images by the combined color and MSAR algorithm see http www media mit edu szummer caivd for color ver sions of the images These images were incorrectly la beled when using only color information The color algo rithm easily mistakes photos containing green or navy\nblue as outdoor images Conversely it often mistakes photos containing white areas e g snow scenes and brown colors as indoor images The texture feature dis regards color and the combination gives the right an swer\nFigures and show samples of images that were misclassi ed by the combined color and MSAR algo rithm Some of the missclassi ed indoor images contain green plants Christmas trees or green walls since green is a typical color of outdoor images Another di cult indoor image is a picture of the top of a shelf and the ceiling which looks blue under ash light like sky The missclassi ed outdoor images are often night time ash photographs White outdoor walls and hazy white sky are also sometimes mistaken to be indoor probably be cause they are very common in indoor scenes Close ups are always challenging because they are dominated by one object and provide little background\nIt is tempting to believe that outdoor images can be easily classi ed by building a blue sky detector A quick look at the database disspells this myth at least for amateur photographs only about one in ve outdoor images have clear blue sky in most outdoor images the sky is not visible or is cloudy white or gray These cloudy colors can unfortunately be produced by ash light as well making them di cult to use for discrimi nation\nConclusions\nWe have shown how high level scene properties can be inferred from low level image features The indoor outdoor classi cation problem is only one example of a high level scene property and we believe that many other properties can be inferred in a similar way Since people often reason in terms of semantic image proper ties it is important for vision systems to extract them\nWe found that it is quite di cult to predict the performance of a feature or feature combination often combining two weaker features with a k nearest neigh bor classi er consistently produced better results than a single good feature Moreover relatively simple clas si ers k nearest neighbors performed better than the more sophisticated neural networks and mixture of ex pert classi ers These empirical results suggest that a theoretical investigation should be undertaken in an ef fort to better understand the relative merits of these methods\nNevertheless we believe that performance will scale well to larger databases of consumer photography After a thorough examination we settled for simple but ro bust classi ers that require few parameter settings Of course it is always possible to devise scenes that will\nfool any system However our system can always be provided with more ground truth for new image types which is likely to increase the performance on such im ages In the domain of consumer photography we have used a large enough sample to show that accurate clas si cation is possible\nAcknowledgement\nThe authors wish to thank Thomas Minka for help with the FourEyes software and Bob Gray at Kodak for sug gestions Portions of the research in this paper use the Kodak Image Research Database This work was sup ported in part by Kodak NEC and Hewlett Packard Labs\nReferences\nMyron Flickner Harpreet Sawhney et al Query by image and video content The QBIC system IEEE Computer Sept\nAmarnath Gupta and Ramesh Jain Visual infor mation retrieval Communications of the ACM http www virage com research htm vir cacm pdf\nJ R Smith and S F Chang Visualseek a fully automated content based image query system In ACM Multimedia pages Nov\nT P Minka and R W Picard Interactive learn ing using a !society of models In Proceedings of CVPR pages San Francisco CA June IEEE Computer Society\nMonika Gorkani and Rosalind W Picard Tex ture orientation for sorting photos at a glance In Proc Int Conf Pat Rec volume I pages Jerusalem Israel Oct\nElaine C Yiu Image classi cation using color cues and texture orientation Master s thesis MIT dept EECS\nPamela R Lipson Context and Con guration Based Scene Classi cation PhD thesis MIT EECS dept\nHong Heather Yu and Wayne Wolf Scenic clas si cation methods for image and video databases In Proc SPIE Digital Image Storage and Archiv ing systems pages http www ee princeton edu heathery\nY I Ohta T Kanade and T Sakai Color infor mation for region segmentation Comp Graph and Img Proc\nMichael Swain and Dana Ballard Color indexing Int J of Comp Vis\nJianchang Mao and Anil K Jain Texture classi cation and segmentation using multiresolution si multaneous autoregressive models Pattern Recog nition\nRosalind W Picard Tanweer Kabir and Fang Liu Real time recognition with the entire Brodatz tex ture database In Proc IEEE Conf on Computer Vision and Pattern Recognition pages New York June MIT Media Lab Perceptual Computing TR\nLeo Breiman Stacked regression ftp ftp stat berkeley edu pub users breiman stacked abstract\nMichael I Jordan and Robert A Jacobs Hierar chical mixtures of experts and the EM algorithm Neural Computation\nFigure out of missclassi ed outdoor images combined color and MSAR classi er Outdoor ash photos in dusk or at night are especially di cult as are scenes with white regions walls hazy sky Close ups are also challenging since they are dominated by one object and do not provide much context The other outdoor images in the database were correctly labelled\nFigure out of missclassi ed indoor images combined color and MSAR classi er Plants Christmas trees green walls and brown oors are sometimes mistakenly thought to belong to outdoor scenes The other indoor images were correctly classi ed"
                    },
                    "intents": []
                }
            ],
            "corpusId": 110716,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "dc139f901c869f80b54b41f89d5b7f35c7dfa3c7",
            "isKey": true,
            "numCitedBy": 4258,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on ways to extend and improve query methods for image databases is widespread. We have developed the QBIC (Query by Image Content) system to explore content-based retrieval methods. QBIC allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information. Two key properties of QBIC are (1) its use of image and video content-computable properties of color, texture, shape and motion of images, videos and their objects-in the queries, and (2) its graphical query language, in which queries are posed by drawing, selecting and other graphical means. This article describes the QBIC system and demonstrates its query capabilities. QBIC technology is part of several IBM products. >"
            },
            "slug": "Query-by-Image-and-Video-Content:-The-QBIC-System-Flickner-Sawhney",
            "title": {
                "fragments": [],
                "text": "Query by Image and Video Content: The QBIC System"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The QBIC system is described and its query capabilities are demonstrated, which allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14646846,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dccf7738dcfd578f023f4831fb5eca7e1449c753",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Digital library access is driven by features but features are often context dependent and noisy and their relevance for a query is not always obvious This paper describes an approach for utilizing many data dependent user dependent and task dependent features in a semi automated tool Instead of requiring universal similarity measures or manual selection of relevant features the approach provides a learning algorithm for selecting and combining groupings of the data where groupings can be induced by highly spe cialized and context dependent features The se lection process is guided by a rich example based interaction with the user The inherent com binatorics of using multiple features is reduced by a multistage grouping generation weighting and collection process The stages closest to the user are trained fastest and slowly propagate their adaptations back to earlier stages The weighting stage adapts the collection stage s search space across uses so that in later interactions good groupings are found given few examples from the user Described is an interactive time imple mentation of this architecture for semi automatic within image segmentation and across image la beling driven by concurrently active color mod els texture models or manually provided group ings Issues for digital libraries Digital libraries of images video and sound are a rich area for pattern recognition research They also introduce a host of new problems and requirements since the range of possible queries is immense and requires the utilization of many spe cialized features Also systems for retrieval browsing and annotation i e classifying regions often must perform with only a small number of examples from a user i e an insuf cient amount of training data by traditional requirements Thus the area is doubly exciting since it presents the eld of pattern recognition with new challenges while beckoning in new applications One important issue for digital libraries is nding good models and similarity measures for comparing database en tries A part of this di culty is that feature extraction and comparison methods are highly data dependent see Figure This work was supported in part by BT PLC Hewlett Packard Labs and NEC for an example with texture Similarity measures are also user and task dependent as demonstrated by Figure Un fortunately these dependencies are not at this point under stood well enough especially by the typical digital library user to permit careful selection of the optimal measure be forehand Note that the multi resolution simultaneous auto regressive MRSAR model of which fares poorly com pared to the shift invariant eigenvector EV model in the above two examples scores clearly above the EV model on the standard Brodatz database On the same test data but for a perceptually motivated similarity criteria based on periodicity directionality and randomness both the EV and MRSAR models are beat by a new Wold based model Attempts to use intuitive texture features like coarseness contrast and directionality are appropri ate in some cases but do not fully determine all the qualities people might use in judging similarity Thus an a priori opti mal context dependent selection among similarity measures either by human or computer seems unlikely Next the scope of queries that databases need to address is immense Current computational solutions attempt to of fer location of perceptual content nd round red objects and objective content nd pictures of people in Boston Desirable queries also extend to subjective content give me a scene of a romantic forest task speci c content I need something with open space to place text collaborative con tent show me pictures children like and more An swering such queries requires a variety of features or meta data to be attached to the data in a digital library some of which may not be computable directly from the data The implication for algorithms is that they cannot rely on one model or one small set of carefully picked features but will have to drink from a veritable feature hydrant from which only a few drops may be relevant for the query Finally there is a signi cant need for semi automated ver sus fully automated tools Human computer synergy can make ill de ned tasks manageable and has the power to over come many of the problems of current pattern recognition tools An important application of semi automated tools is to assist the population of a database viz the creation of metadata A crucial technical issue for such tools is the selec tion and combination of existing features which features are most useful for a given query or annotation how should they be combined and which combinations are useful for the sys tem to remember so that it gets smarter with increased use This last point is important since not only are the queries immensely variable but the amount of training data i e ex amples provided by a user of what they do and don t want available at any instant is usually limited Hence a tool should strive to improve its generalization ability"
            },
            "slug": "Interactive-Learning-Using-a-\"Society-of-Models\"-Minka-Picard",
            "title": {
                "fragments": [],
                "text": "Interactive Learning Using a \"Society of Models\""
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper describes an approach for utilizing many data dependent user dependent and task dependent features in a semi automated tool instead of requiring universal similarity measures or manual selection of relevant features the approach provides a learning algorithm for selecting and combining groupings of the data."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 1996"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": false,
            "numCitedBy": 2136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical mixtures of experts and the EM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722619"
                        ],
                        "name": "Amarnath Gupta",
                        "slug": "Amarnath-Gupta",
                        "structuredName": {
                            "firstName": "Amarnath",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amarnath Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938740"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 15984449,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "45ce9d12be58f15f5a1ff3325a67d57f1f04732e",
            "isKey": false,
            "numCitedBy": 482,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "New updated! The latest book from a very famous author finally comes out. Book of visual information retrieval, as an amazing reference becomes what you need to get. What's for is this book? Are you still thinking for what the book is? Well, this is what you probably will get. You should have made proper choices for your better life. Book, as a source that may involve the facts, opinion, literature, religion, and many others are the great friends to join with."
            },
            "slug": "Visual-information-retrieval-Gupta-Jain",
            "title": {
                "fragments": [],
                "text": "Visual information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Book of visual information retrieval, as an amazing reference becomes what you need to get, and book, as a source that may involve the facts, opinion, literature, religion, and many others are the great friends to join with."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "Unfortunately, this is not possible even in state-of-theart image retrieval systems such as QBIC [1], Virage [2] and VisualSEEk [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2146,
                                "start": 2140
                            }
                        ],
                        "text": "Introduction\nThe scene classi cation problem is one of the holy grail challenges of computer vision Given an arbitrary pho tograph we would like to describe what type of seman tic scene it depicts Currently very little work has been done in this area probably because the problem is very di cult and also because there is no agreed upon scene description language Most computer vision research in volves low level image analysis that rarely tries to bridge the gap to semantic scene description\nOur purpose is to show how one particular seman tic scene description problem can be approached The task is to determine whether a consumer photograph depicts an indoor or an outdoor scene This problem is relatively unambiguous and is motivated by several practical applications\nThere certainly exist photographs for which the indoor outdoor distinction is unclear Examples include shots made through a window with visible window edges photographs of paintings and extreme close ups of faces Fortunately such compositions are rare in con sumer photographs Our database of images was\nThis work was supported in part by Kodak NEC and Hewlett\nPackard Labs\nclassi ed by two independent people and only nineteen out of these were labeled as unclear and omitted from this study\nThe applications of this problem are interesting Knowledge about the scene enables more intelligent im age processing For example when lm is developed and prints are made from the negatives the exposure and color is automatically adjusted Unfortunately the automatic correction does not take into account the con tent of the photograph If the machine could distin guish indoor from outdoor images it could adjust these classes di erently rather than adjusting everything to wards one ideal exposure and color This observation can also be applied to image scanners photocopiers fax machines image processing software etc\nAnother important application is image retrieval Let s say we would like to nd a beach scene A help ful step would be to limit the search to outdoor scenes Unfortunately this is not possible even in state of the art image retrieval systems such as QBIC Virage and VisualSEEk These systems are based mainly on color histograms and primitive texture measures The user builds a query by selecting colors from a palette a texture from a chart and then indicates how to weight the color versus the texture Unfortunately it is di cult for a user to know how to weight the di erent features to get a beach scene The systems level of abstraction is much too low\nQuery by image example enables the user to select one image and nd other similar images making it eas ier to specify the relevant color and texture query Most systems still require the user to select weights for the di erent features An exception to this is FourEyes which can learn the relevant feature combination based on several positive and negative examples In an initial quick attempt to teach FourEyes to solve the indoor outdoor classi cation problem using whole images with no speci c subblock guidance we did not meet with sig ni cant success Although FourEyes can learn any clas si cation it was not very e cient on this one probably due to the noisiness of the two classes being considered\nIn this work we propose a di erent classi cation ap proach that exploits the same idea in FourEyes of non linearly combining features from multiple models but does so in a di erent way This new way is successful for accurately distinguishing indoor from outdoor scenes\nBackground\nSeveral attempts at recognizing high level scene proper ties using low level features have been made Gorkani and Picard discriminate between photos of city scenes and photos of landscape scenes They use a multiscale steerable pyramid to nd dominant orientations in subblocks of the image The image is classi ed as a city scene if enough subblocks have strong dominant verti cal orientation or alternatively medium strong vertical orientation and also horizontal orientation\nYiu uses the same dominant orientation features and also color information to classify indoor and outdoor scenes She uses nearest neighbor and support vector machine classi ers The former classi er is better at color the latter at dominant orientation Yiu reports accuracies similar to those in our work but they are believed to have a high variance because they were not thoroughly evaluated with a leave one out method and a much smaller database of only images was used Furthermore the texture features used here give sig ni cantly better results than her dominant orientation detector The work here also takes advantage of a spa tial tessellation of the image which we found provides a signi cant gain in performance\nInstead of building a speci c scene class detector Lipson describes a general scene query approach Scenes are described by graphs representing relations between image regions The relationships include rel ative color spatial location and highpass frequency content Unfortunately the templates have to be con structed manually for each scene layout These tem plates are also quite speci c which makes them ne for limited special cases such as sky over mountain over lake but di cult for the case considered here of cap turing a broad concept like an outdoor scene\nYu learns a statistical template from examples She computes vector quantized color histograms for sub blocks of the image Then she trains a one dimensional hidden Markov model along vertical or horizontal seg ments of speci c scene layouts such as sky mountain river scenes Unfortunately the one dimensional model cannot describe spatial relationships well and a two dimensional generalization such as Markov random elds is desirable but raises many new kinds of prob lems\nFeatures\nImage Database\nThe image database in these experiments consists of consumer photographs collected and labeled by Kodak They depict typical family and vacation scenes and are taken by many di erent individuals at all dif ferent times of they year The database is quite diverse and includes snow bright sun sea sunset night and silhouette scenes Image types not in our database can be easily added to the training set without changing any algorithms\nThe images were hand labeled by two independent people not the authors resulting in labeled as outdoor and labeled as indoor We have excluded images which were labeled as am biguous All the images in the set have landscape ori entation and are right side up The full resolution of the images was but for most experiments we used half or quarter resolution as will be described later The images originally came in bit color but were quantized down to bit color At the same time we performed basic color balancing according to steps provided by Kodak which simply clipped the top and bottom of the intensity levels shifted the histograms to the middle and stretched them to occupy the bit range\nA baseline experiment\nThe problem is quite challenging A naive approach such as a traditional color histogram will not give good classi cation performance To illustrate this we com puted bin histograms uniformly spaced for each RGB channel concatenated them into a feature vec tor and applied a nearest neighbor classi er The dis tance between feature vectors was measured using the Euclidean norm The resulting leave one out classi cation performance was This number is only somewhat better than just guessing that each image is outdoor which would be correct Nevertheless a very similar color histogram is used at the heart of most image retrieval systems\nBelow we describe and evaluate several methods which perform much better These included more so phisticated features and classi ers which tessellate the image into subregions and combine the results from dif ferent features and di erent spatial regions to result in signi cantly improved performance\nThe features\nWe have used three types of features one each for color texture and frequency information These features were\ncomputed both for the whole image and for each sub block of a image tessellation\nThe color feature is a color histogram and has bins per channel like our baseline However the three channels come from the Ohta color space The color axes of this space are the largest eigenvectors of the RGB space found through principal components anal ysis of a large selection of natural images This yields\nI R G B\nI R B\nI R G B\nThe advantage of the Ohta color space is that the color channels are approximately decorrelated which makes it a good choice for computing per channel histograms The change of color spaces from RGB to Ohta raises the performance of color histogram based recognition to\nMoreover instead of using the Euclidean norm for measuring distances between histograms we use the his togram intersection norm It measures the amount of overlap between corresponding buckets in the two his tograms h and h and is de ned as\ndist h h\nNX\ni\nh i min h i h i\nWhen both the Ohta color space and histogram in tersection is used the classi cation rises to cor rect The intersection norm is better than the Euclidean norm possibly because it penalizes linear error as op posed to squared error reducing sensitivity to outliers In the rest of the paper we exclusively apply the Ohta color space with histogram intersection\nThe texture features are computed using the multiresolution simultaneous autoregressive model MSAR These are among the best texture features bench marked on the Brodatz album The model constructs the best linear predictor of a pixel based on a noncausal neighborhood The features are the weights of the predictor Three di erent neighborhoods at scales and are used and the weights are concatenated to yield a dimensional vector as in The Maha lanobis norm is used to measure feature vector distance covariances are estimated from several subwindow es timates We extracted these features from gray scale images at two resolutions half and quarter using a suitable antialiasing lter\nThe MSAR classi cation is and correct at half and quarter resolutions respectively which is sig ni cantly better than the best color classi cation This\nis surprising since the MSAR feature presupposes a sin gle texture characterized by a second order stationary autoregressive process whereas a typical image con sists of many textures and is de nitely nonstationary As we shall see we can do even better by dividing the image into subblocks and computing the features separately over each block\nThe frequency features are obtained by rst calcu lating the D DFT magnitude and then taking the D DCT The rst step is shift invariant and for periodic textures it shows a regular pattern of peaks fundamen tal and harmonic frequencies The second step replaces all related frequencies by one coe cient All computa tions are done over pixel blocks and the results are averaged over the image region also producing co variances used for the Mahalanobis distance metric\nClassi cation\nThe performance numbers so far refer to nearest neigh bor classi cation of features computed on the whole im age Unfortunately this method cannot exploit local properties of the image e g blue sky at the top Never theless most image retrieval systems use features com puted for the whole image thus the numbers here are useful for comparisons The results are summarized in Table for a K nearest neighbor classi er We have also tried a layer neural network with sigmoid nonlinear ities but training was slow and the results were worse than for the nearest neighbor algorithm when compared on the color histogram features\nTo allow local and spatial properties to improve the classi cation we divided the image into subblocks and computed the features separately within them Now the question arises how to classify these blocks One possibility would be to concatenate feature vector from all subblocks of an image and apply a classi er to this vector The problem is that such a feature is very high dimensional e g It is di cult to estimate covariances for such a large vector and we en counter general curse of dimensionality problems\nInstead we chose to pursue a multi stage classi ca tion approach classifying the subblocks independently and then performing another classi cation on these an swers Figure This is reminiscent of stacking except that the subblock classi ers here were trained on their own data Not surprisingly the individual sub block classi ers are less accurate than a whole image classi er Ideally we would keep a con dence or prob abilistic value associated with each subblock classi ca tion as opposed to the binary decision in or out shown in Figure In theory the mixture of experts method applied below takes care of this case this will be described later For now Table shows the results\nTable Whole image classi cation results using k nearest neighbor The best result in each row is marked with\nFeature k k k k k RGB histogram euclidean RGB histogram intersection Ohta histogram euclidean Ohta histogram intersection MSAR quarter resolution MSAR half resolution DCT half resolution\nof the use of the k nearest neighbor classi er on the color features where each subblock is compared to all subblocks in the database regardless of spatial location excluding subblocks from the same image\nWhen the results of the subblocks are combined the classi cation can be greatly improved Three ways to combine the features were systematically tried a simple majority classi er that assigns the label for the image to the most common class label among the sub blocks a one layer neural net and a Mixture of Experts classi er The rst method was evaluated using the leave one out method and was found to give good results Table The other two methods because of their long training time were only evaluated with a few runs of leave out training on of the data and testing on the other In these limited tests which are subject to higher variance than the leave one out test method we got slightly better results than the majority classi er but not signi cantly better\nThe one layer neural net can give us information about what subblocks are important for the classi ca tion task The net has a sigmoid nonlinearity at the output and optimizes the cross entropy cost function between the network output and the true class By ob serving the weights learned by the network we found that it favors especially the top row but also the lower center of the image for classi cation gure the sub blocks were classi ed using color histograms In some images sky occurs in this region and is perhaps easier to classify correctly and hence is heavily weighted\nThe mixture of experts classifer is similar to but more exible than the above neural network It learns experts for speci c subproblems The experts are se lected depending on the input data and each expert can weight the data di erently For example if the top of the image is classi ed as outdoor an expert can weight it more heavily than if the top of the image is classi ed as indoor The technique is similar to softly clus tering the data and assigning a set of weights for each\n0\n0.05\n0.1\n0.15\nx subblock\ny su\nbb lo\nck\nWeights in stacked regression\n0.5 1 1.5 2 2.5 3 3.5 4 4.5\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nFigure Weights of subblocks in image classi cation The larger the weights the brighter the square\ncluster however the clustering and weight assignment occur simultaneously Somewhat disappointingly the classi cation results are approximately the same as for the neural network Moreover we are required to set additional parameters the number of experts and be careful to avoid over tting Thus we judged that the additional e ort was not justi ed\nMultiple feature combination\nSo far we have combined multiple subblock classi ca tions but these were all based on a single image feature To gain robustness we can use multiple image features simultaneously One common way to do this is to con catenate di erent feature vectors into a longer vector Unfortunately this step increases the dimensionality of the problem and requires a metric which is simulta neously good for e g color histograms and MSAR The relationship between two features is almost certainly not linear so such a metric is di cult to construct There\nTexture Texture classification\nColor Color classification\nCombined classification\ninout\nininin in\nin in\nin in\nin in in\nin\nout\nout\nin\nininin in\nout in in\nout in\nout in in\nin\nin\nin\nIn\nFigure Two stage classi cation combining color and texture\nTable K nearest neighbor classi cations on subblocks The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Majority classi er based on k nearest neighbor The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Combined feature classi er k MSAR fea tures were measured at half resolution\nFeature Performance Color MSAR Color DCT MSAR DCT Color MSAR DCT\nTable Confusion matrix for color MSAR combination\nTrue Class Classi ed as indoor outdoor\nindoor outdoor\nfore we think this approach is a mistake even though it is commonly used by researchers working in content based retrieval\nThere is a way to combine feature vectors just by concatenation by rst translating all the features into a common language This was done in the FourEyes system by using the common language of clusters Our common language is di erent the subblock classes as signed by the k nearest neighbor classifer In other words we simply concatenate the subblock classi ca tions based on di erent image features and then do a second stage classi cation In the second stage we get signi cantly improved results by applying the majority classi er to the combined vectors Table\nDiscussion\nThe best classi cation results were generally obtained by combining color features with texture features Both the MSAR and DCT based features capture shift invariant intensity variations over a range of scales so combining them does not provide as much gain as com bining color with one of them\nThe color MSAR combination gives us the best re sult correct measured using leave one out cross validation The confusion matrix for this result Ta ble shows that it is approximately equally likely to mistakenly label indoor images as being outdoor or vice versa The proportion of indoor to outdoor images in the database is vs which is fairly balanced\nFigure shows several correctly classi ed images by the combined color and MSAR algorithm see http www media mit edu szummer caivd for color ver sions of the images These images were incorrectly la beled when using only color information The color algo rithm easily mistakes photos containing green or navy\nblue as outdoor images Conversely it often mistakes photos containing white areas e g snow scenes and brown colors as indoor images The texture feature dis regards color and the combination gives the right an swer\nFigures and show samples of images that were misclassi ed by the combined color and MSAR algo rithm Some of the missclassi ed indoor images contain green plants Christmas trees or green walls since green is a typical color of outdoor images Another di cult indoor image is a picture of the top of a shelf and the ceiling which looks blue under ash light like sky The missclassi ed outdoor images are often night time ash photographs White outdoor walls and hazy white sky are also sometimes mistaken to be indoor probably be cause they are very common in indoor scenes Close ups are always challenging because they are dominated by one object and provide little background\nIt is tempting to believe that outdoor images can be easily classi ed by building a blue sky detector A quick look at the database disspells this myth at least for amateur photographs only about one in ve outdoor images have clear blue sky in most outdoor images the sky is not visible or is cloudy white or gray These cloudy colors can unfortunately be produced by ash light as well making them di cult to use for discrimi nation\nConclusions\nWe have shown how high level scene properties can be inferred from low level image features The indoor outdoor classi cation problem is only one example of a high level scene property and we believe that many other properties can be inferred in a similar way Since people often reason in terms of semantic image proper ties it is important for vision systems to extract them\nWe found that it is quite di cult to predict the performance of a feature or feature combination often combining two weaker features with a k nearest neigh bor classi er consistently produced better results than a single good feature Moreover relatively simple clas si ers k nearest neighbors performed better than the more sophisticated neural networks and mixture of ex pert classi ers These empirical results suggest that a theoretical investigation should be undertaken in an ef fort to better understand the relative merits of these methods\nNevertheless we believe that performance will scale well to larger databases of consumer photography After a thorough examination we settled for simple but ro bust classi ers that require few parameter settings Of course it is always possible to devise scenes that will\nfool any system However our system can always be provided with more ground truth for new image types which is likely to increase the performance on such im ages In the domain of consumer photography we have used a large enough sample to show that accurate clas si cation is possible\nAcknowledgement\nThe authors wish to thank Thomas Minka for help with the FourEyes software and Bob Gray at Kodak for sug gestions Portions of the research in this paper use the Kodak Image Research Database This work was sup ported in part by Kodak NEC and Hewlett Packard Labs\nReferences\nMyron Flickner Harpreet Sawhney et al Query by image and video content The QBIC system IEEE Computer Sept\nAmarnath Gupta and Ramesh Jain Visual infor mation retrieval Communications of the ACM http www virage com research htm vir cacm pdf\nJ R Smith and S F Chang Visualseek a fully automated content based image query system In ACM Multimedia pages Nov\nT P Minka and R W Picard Interactive learn ing using a !society of models In Proceedings of CVPR pages San Francisco CA June IEEE Computer Society\nMonika Gorkani and Rosalind W Picard Tex ture orientation for sorting photos at a glance In Proc Int Conf Pat Rec volume I pages Jerusalem Israel Oct\nElaine C Yiu Image classi cation using color cues and texture orientation Master s thesis MIT dept EECS\nPamela R Lipson Context and Con guration Based Scene Classi cation PhD thesis MIT EECS dept\nHong Heather Yu and Wayne Wolf Scenic clas si cation methods for image and video databases In Proc SPIE Digital Image Storage and Archiv ing systems pages http www ee princeton edu heathery\nY I Ohta T Kanade and T Sakai Color infor mation for region segmentation Comp Graph and Img Proc\nMichael Swain and Dana Ballard Color indexing Int J of Comp Vis\nJianchang Mao and Anil K Jain Texture classi cation and segmentation using multiresolution si multaneous autoregressive models Pattern Recog nition\nRosalind W Picard Tanweer Kabir and Fang Liu Real time recognition with the entire Brodatz tex ture database In Proc IEEE Conf on Computer Vision and Pattern Recognition pages New York June MIT Media Lab Perceptual Computing TR\nLeo Breiman Stacked regression ftp ftp stat berkeley edu pub users breiman stacked abstract\nMichael I Jordan and Robert A Jacobs Hierar chical mixtures of experts and the EM algorithm Neural Computation\nFigure out of missclassi ed outdoor images combined color and MSAR classi er Outdoor ash photos in dusk or at night are especially di cult as are scenes with white regions walls hazy sky Close ups are also challenging since they are dominated by one object and do not provide much context The other outdoor images in the database were correctly labelled\nFigure out of missclassi ed indoor images combined color and MSAR classi er Plants Christmas trees green walls and brown oors are sometimes mistakenly thought to belong to outdoor scenes The other indoor images were correctly classi ed"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visual infor-  mation retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Communications of the ACM,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4776,
                                "start": 4770
                            }
                        ],
                        "text": "Introduction\nThe scene classi cation problem is one of the holy grail challenges of computer vision Given an arbitrary pho tograph we would like to describe what type of seman tic scene it depicts Currently very little work has been done in this area probably because the problem is very di cult and also because there is no agreed upon scene description language Most computer vision research in volves low level image analysis that rarely tries to bridge the gap to semantic scene description\nOur purpose is to show how one particular seman tic scene description problem can be approached The task is to determine whether a consumer photograph depicts an indoor or an outdoor scene This problem is relatively unambiguous and is motivated by several practical applications\nThere certainly exist photographs for which the indoor outdoor distinction is unclear Examples include shots made through a window with visible window edges photographs of paintings and extreme close ups of faces Fortunately such compositions are rare in con sumer photographs Our database of images was\nThis work was supported in part by Kodak NEC and Hewlett\nPackard Labs\nclassi ed by two independent people and only nineteen out of these were labeled as unclear and omitted from this study\nThe applications of this problem are interesting Knowledge about the scene enables more intelligent im age processing For example when lm is developed and prints are made from the negatives the exposure and color is automatically adjusted Unfortunately the automatic correction does not take into account the con tent of the photograph If the machine could distin guish indoor from outdoor images it could adjust these classes di erently rather than adjusting everything to wards one ideal exposure and color This observation can also be applied to image scanners photocopiers fax machines image processing software etc\nAnother important application is image retrieval Let s say we would like to nd a beach scene A help ful step would be to limit the search to outdoor scenes Unfortunately this is not possible even in state of the art image retrieval systems such as QBIC Virage and VisualSEEk These systems are based mainly on color histograms and primitive texture measures The user builds a query by selecting colors from a palette a texture from a chart and then indicates how to weight the color versus the texture Unfortunately it is di cult for a user to know how to weight the di erent features to get a beach scene The systems level of abstraction is much too low\nQuery by image example enables the user to select one image and nd other similar images making it eas ier to specify the relevant color and texture query Most systems still require the user to select weights for the di erent features An exception to this is FourEyes which can learn the relevant feature combination based on several positive and negative examples In an initial quick attempt to teach FourEyes to solve the indoor outdoor classi cation problem using whole images with no speci c subblock guidance we did not meet with sig ni cant success Although FourEyes can learn any clas si cation it was not very e cient on this one probably due to the noisiness of the two classes being considered\nIn this work we propose a di erent classi cation ap proach that exploits the same idea in FourEyes of non linearly combining features from multiple models but does so in a di erent way This new way is successful for accurately distinguishing indoor from outdoor scenes\nBackground\nSeveral attempts at recognizing high level scene proper ties using low level features have been made Gorkani and Picard discriminate between photos of city scenes and photos of landscape scenes They use a multiscale steerable pyramid to nd dominant orientations in subblocks of the image The image is classi ed as a city scene if enough subblocks have strong dominant verti cal orientation or alternatively medium strong vertical orientation and also horizontal orientation\nYiu uses the same dominant orientation features and also color information to classify indoor and outdoor scenes She uses nearest neighbor and support vector machine classi ers The former classi er is better at color the latter at dominant orientation Yiu reports accuracies similar to those in our work but they are believed to have a high variance because they were not thoroughly evaluated with a leave one out method and a much smaller database of only images was used Furthermore the texture features used here give sig ni cantly better results than her dominant orientation detector The work here also takes advantage of a spa tial tessellation of the image which we found provides a signi cant gain in performance\nInstead of building a speci c scene class detector Lipson describes a general scene query approach Scenes are described by graphs representing relations between image regions The relationships include rel ative color spatial location and highpass frequency content Unfortunately the templates have to be con structed manually for each scene layout These tem plates are also quite speci c which makes them ne for limited special cases such as sky over mountain over lake but di cult for the case considered here of cap turing a broad concept like an outdoor scene\nYu learns a statistical template from examples She computes vector quantized color histograms for sub blocks of the image Then she trains a one dimensional hidden Markov model along vertical or horizontal seg ments of speci c scene layouts such as sky mountain river scenes Unfortunately the one dimensional model cannot describe spatial relationships well and a two dimensional generalization such as Markov random elds is desirable but raises many new kinds of prob lems\nFeatures\nImage Database\nThe image database in these experiments consists of consumer photographs collected and labeled by Kodak They depict typical family and vacation scenes and are taken by many di erent individuals at all dif ferent times of they year The database is quite diverse and includes snow bright sun sea sunset night and silhouette scenes Image types not in our database can be easily added to the training set without changing any algorithms\nThe images were hand labeled by two independent people not the authors resulting in labeled as outdoor and labeled as indoor We have excluded images which were labeled as am biguous All the images in the set have landscape ori entation and are right side up The full resolution of the images was but for most experiments we used half or quarter resolution as will be described later The images originally came in bit color but were quantized down to bit color At the same time we performed basic color balancing according to steps provided by Kodak which simply clipped the top and bottom of the intensity levels shifted the histograms to the middle and stretched them to occupy the bit range\nA baseline experiment\nThe problem is quite challenging A naive approach such as a traditional color histogram will not give good classi cation performance To illustrate this we com puted bin histograms uniformly spaced for each RGB channel concatenated them into a feature vec tor and applied a nearest neighbor classi er The dis tance between feature vectors was measured using the Euclidean norm The resulting leave one out classi cation performance was This number is only somewhat better than just guessing that each image is outdoor which would be correct Nevertheless a very similar color histogram is used at the heart of most image retrieval systems\nBelow we describe and evaluate several methods which perform much better These included more so phisticated features and classi ers which tessellate the image into subregions and combine the results from dif ferent features and di erent spatial regions to result in signi cantly improved performance\nThe features\nWe have used three types of features one each for color texture and frequency information These features were\ncomputed both for the whole image and for each sub block of a image tessellation\nThe color feature is a color histogram and has bins per channel like our baseline However the three channels come from the Ohta color space The color axes of this space are the largest eigenvectors of the RGB space found through principal components anal ysis of a large selection of natural images This yields\nI R G B\nI R B\nI R G B\nThe advantage of the Ohta color space is that the color channels are approximately decorrelated which makes it a good choice for computing per channel histograms The change of color spaces from RGB to Ohta raises the performance of color histogram based recognition to\nMoreover instead of using the Euclidean norm for measuring distances between histograms we use the his togram intersection norm It measures the amount of overlap between corresponding buckets in the two his tograms h and h and is de ned as\ndist h h\nNX\ni\nh i min h i h i\nWhen both the Ohta color space and histogram in tersection is used the classi cation rises to cor rect The intersection norm is better than the Euclidean norm possibly because it penalizes linear error as op posed to squared error reducing sensitivity to outliers In the rest of the paper we exclusively apply the Ohta color space with histogram intersection\nThe texture features are computed using the multiresolution simultaneous autoregressive model MSAR These are among the best texture features bench marked on the Brodatz album The model constructs the best linear predictor of a pixel based on a noncausal neighborhood The features are the weights of the predictor Three di erent neighborhoods at scales and are used and the weights are concatenated to yield a dimensional vector as in The Maha lanobis norm is used to measure feature vector distance covariances are estimated from several subwindow es timates We extracted these features from gray scale images at two resolutions half and quarter using a suitable antialiasing lter\nThe MSAR classi cation is and correct at half and quarter resolutions respectively which is sig ni cantly better than the best color classi cation This\nis surprising since the MSAR feature presupposes a sin gle texture characterized by a second order stationary autoregressive process whereas a typical image con sists of many textures and is de nitely nonstationary As we shall see we can do even better by dividing the image into subblocks and computing the features separately over each block\nThe frequency features are obtained by rst calcu lating the D DFT magnitude and then taking the D DCT The rst step is shift invariant and for periodic textures it shows a regular pattern of peaks fundamen tal and harmonic frequencies The second step replaces all related frequencies by one coe cient All computa tions are done over pixel blocks and the results are averaged over the image region also producing co variances used for the Mahalanobis distance metric\nClassi cation\nThe performance numbers so far refer to nearest neigh bor classi cation of features computed on the whole im age Unfortunately this method cannot exploit local properties of the image e g blue sky at the top Never theless most image retrieval systems use features com puted for the whole image thus the numbers here are useful for comparisons The results are summarized in Table for a K nearest neighbor classi er We have also tried a layer neural network with sigmoid nonlinear ities but training was slow and the results were worse than for the nearest neighbor algorithm when compared on the color histogram features\nTo allow local and spatial properties to improve the classi cation we divided the image into subblocks and computed the features separately within them Now the question arises how to classify these blocks One possibility would be to concatenate feature vector from all subblocks of an image and apply a classi er to this vector The problem is that such a feature is very high dimensional e g It is di cult to estimate covariances for such a large vector and we en counter general curse of dimensionality problems\nInstead we chose to pursue a multi stage classi ca tion approach classifying the subblocks independently and then performing another classi cation on these an swers Figure This is reminiscent of stacking except that the subblock classi ers here were trained on their own data Not surprisingly the individual sub block classi ers are less accurate than a whole image classi er Ideally we would keep a con dence or prob abilistic value associated with each subblock classi ca tion as opposed to the binary decision in or out shown in Figure In theory the mixture of experts method applied below takes care of this case this will be described later For now Table shows the results\nTable Whole image classi cation results using k nearest neighbor The best result in each row is marked with\nFeature k k k k k RGB histogram euclidean RGB histogram intersection Ohta histogram euclidean Ohta histogram intersection MSAR quarter resolution MSAR half resolution DCT half resolution\nof the use of the k nearest neighbor classi er on the color features where each subblock is compared to all subblocks in the database regardless of spatial location excluding subblocks from the same image\nWhen the results of the subblocks are combined the classi cation can be greatly improved Three ways to combine the features were systematically tried a simple majority classi er that assigns the label for the image to the most common class label among the sub blocks a one layer neural net and a Mixture of Experts classi er The rst method was evaluated using the leave one out method and was found to give good results Table The other two methods because of their long training time were only evaluated with a few runs of leave out training on of the data and testing on the other In these limited tests which are subject to higher variance than the leave one out test method we got slightly better results than the majority classi er but not signi cantly better\nThe one layer neural net can give us information about what subblocks are important for the classi ca tion task The net has a sigmoid nonlinearity at the output and optimizes the cross entropy cost function between the network output and the true class By ob serving the weights learned by the network we found that it favors especially the top row but also the lower center of the image for classi cation gure the sub blocks were classi ed using color histograms In some images sky occurs in this region and is perhaps easier to classify correctly and hence is heavily weighted\nThe mixture of experts classifer is similar to but more exible than the above neural network It learns experts for speci c subproblems The experts are se lected depending on the input data and each expert can weight the data di erently For example if the top of the image is classi ed as outdoor an expert can weight it more heavily than if the top of the image is classi ed as indoor The technique is similar to softly clus tering the data and assigning a set of weights for each\n0\n0.05\n0.1\n0.15\nx subblock\ny su\nbb lo\nck\nWeights in stacked regression\n0.5 1 1.5 2 2.5 3 3.5 4 4.5\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nFigure Weights of subblocks in image classi cation The larger the weights the brighter the square\ncluster however the clustering and weight assignment occur simultaneously Somewhat disappointingly the classi cation results are approximately the same as for the neural network Moreover we are required to set additional parameters the number of experts and be careful to avoid over tting Thus we judged that the additional e ort was not justi ed\nMultiple feature combination\nSo far we have combined multiple subblock classi ca tions but these were all based on a single image feature To gain robustness we can use multiple image features simultaneously One common way to do this is to con catenate di erent feature vectors into a longer vector Unfortunately this step increases the dimensionality of the problem and requires a metric which is simulta neously good for e g color histograms and MSAR The relationship between two features is almost certainly not linear so such a metric is di cult to construct There\nTexture Texture classification\nColor Color classification\nCombined classification\ninout\nininin in\nin in\nin in\nin in in\nin\nout\nout\nin\nininin in\nout in in\nout in\nout in in\nin\nin\nin\nIn\nFigure Two stage classi cation combining color and texture\nTable K nearest neighbor classi cations on subblocks The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Majority classi er based on k nearest neighbor The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Combined feature classi er k MSAR fea tures were measured at half resolution\nFeature Performance Color MSAR Color DCT MSAR DCT Color MSAR DCT\nTable Confusion matrix for color MSAR combination\nTrue Class Classi ed as indoor outdoor\nindoor outdoor\nfore we think this approach is a mistake even though it is commonly used by researchers working in content based retrieval\nThere is a way to combine feature vectors just by concatenation by rst translating all the features into a common language This was done in the FourEyes system by using the common language of clusters Our common language is di erent the subblock classes as signed by the k nearest neighbor classifer In other words we simply concatenate the subblock classi ca tions based on di erent image features and then do a second stage classi cation In the second stage we get signi cantly improved results by applying the majority classi er to the combined vectors Table\nDiscussion\nThe best classi cation results were generally obtained by combining color features with texture features Both the MSAR and DCT based features capture shift invariant intensity variations over a range of scales so combining them does not provide as much gain as com bining color with one of them\nThe color MSAR combination gives us the best re sult correct measured using leave one out cross validation The confusion matrix for this result Ta ble shows that it is approximately equally likely to mistakenly label indoor images as being outdoor or vice versa The proportion of indoor to outdoor images in the database is vs which is fairly balanced\nFigure shows several correctly classi ed images by the combined color and MSAR algorithm see http www media mit edu szummer caivd for color ver sions of the images These images were incorrectly la beled when using only color information The color algo rithm easily mistakes photos containing green or navy\nblue as outdoor images Conversely it often mistakes photos containing white areas e g snow scenes and brown colors as indoor images The texture feature dis regards color and the combination gives the right an swer\nFigures and show samples of images that were misclassi ed by the combined color and MSAR algo rithm Some of the missclassi ed indoor images contain green plants Christmas trees or green walls since green is a typical color of outdoor images Another di cult indoor image is a picture of the top of a shelf and the ceiling which looks blue under ash light like sky The missclassi ed outdoor images are often night time ash photographs White outdoor walls and hazy white sky are also sometimes mistaken to be indoor probably be cause they are very common in indoor scenes Close ups are always challenging because they are dominated by one object and provide little background\nIt is tempting to believe that outdoor images can be easily classi ed by building a blue sky detector A quick look at the database disspells this myth at least for amateur photographs only about one in ve outdoor images have clear blue sky in most outdoor images the sky is not visible or is cloudy white or gray These cloudy colors can unfortunately be produced by ash light as well making them di cult to use for discrimi nation\nConclusions\nWe have shown how high level scene properties can be inferred from low level image features The indoor outdoor classi cation problem is only one example of a high level scene property and we believe that many other properties can be inferred in a similar way Since people often reason in terms of semantic image proper ties it is important for vision systems to extract them\nWe found that it is quite di cult to predict the performance of a feature or feature combination often combining two weaker features with a k nearest neigh bor classi er consistently produced better results than a single good feature Moreover relatively simple clas si ers k nearest neighbors performed better than the more sophisticated neural networks and mixture of ex pert classi ers These empirical results suggest that a theoretical investigation should be undertaken in an ef fort to better understand the relative merits of these methods\nNevertheless we believe that performance will scale well to larger databases of consumer photography After a thorough examination we settled for simple but ro bust classi ers that require few parameter settings Of course it is always possible to devise scenes that will\nfool any system However our system can always be provided with more ground truth for new image types which is likely to increase the performance on such im ages In the domain of consumer photography we have used a large enough sample to show that accurate clas si cation is possible\nAcknowledgement\nThe authors wish to thank Thomas Minka for help with the FourEyes software and Bob Gray at Kodak for sug gestions Portions of the research in this paper use the Kodak Image Research Database This work was sup ported in part by Kodak NEC and Hewlett Packard Labs\nReferences\nMyron Flickner Harpreet Sawhney et al Query by image and video content The QBIC system IEEE Computer Sept\nAmarnath Gupta and Ramesh Jain Visual infor mation retrieval Communications of the ACM http www virage com research htm vir cacm pdf\nJ R Smith and S F Chang Visualseek a fully automated content based image query system In ACM Multimedia pages Nov\nT P Minka and R W Picard Interactive learn ing using a !society of models In Proceedings of CVPR pages San Francisco CA June IEEE Computer Society\nMonika Gorkani and Rosalind W Picard Tex ture orientation for sorting photos at a glance In Proc Int Conf Pat Rec volume I pages Jerusalem Israel Oct\nElaine C Yiu Image classi cation using color cues and texture orientation Master s thesis MIT dept EECS\nPamela R Lipson Context and Con guration Based Scene Classi cation PhD thesis MIT EECS dept\nHong Heather Yu and Wayne Wolf Scenic clas si cation methods for image and video databases In Proc SPIE Digital Image Storage and Archiv ing systems pages http www ee princeton edu heathery\nY I Ohta T Kanade and T Sakai Color infor mation for region segmentation Comp Graph and Img Proc\nMichael Swain and Dana Ballard Color indexing Int J of Comp Vis\nJianchang Mao and Anil K Jain Texture classi cation and segmentation using multiresolution si multaneous autoregressive models Pattern Recog nition\nRosalind W Picard Tanweer Kabir and Fang Liu Real time recognition with the entire Brodatz tex ture database In Proc IEEE Conf on Computer Vision and Pattern Recognition pages New York June MIT Media Lab Perceptual Computing TR\nLeo Breiman Stacked regression ftp ftp stat berkeley edu pub users breiman stacked abstract\nMichael I Jordan and Robert A Jacobs Hierar chical mixtures of experts and the EM algorithm Neural Computation\nFigure out of missclassi ed outdoor images combined color and MSAR classi er Outdoor ash photos in dusk or at night are especially di cult as are scenes with white regions walls hazy sky Close ups are also challenging since they are dominated by one object and do not provide much context The other outdoor images in the database were correctly labelled\nFigure out of missclassi ed indoor images combined color and MSAR classi er Plants Christmas trees green walls and brown oors are sometimes mistakenly thought to belong to outdoor scenes The other indoor images were correctly classi ed"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "Instead of building a speci c scene class detector, Lipson [7] describes a general scene query approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Context and Con guration  Based Scene Classi cation"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "The texture features are computed using the multiresolution, simultaneous autoregressive model (MSAR) [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9396,
                                "start": 9392
                            }
                        ],
                        "text": "Introduction\nThe scene classi cation problem is one of the holy grail challenges of computer vision Given an arbitrary pho tograph we would like to describe what type of seman tic scene it depicts Currently very little work has been done in this area probably because the problem is very di cult and also because there is no agreed upon scene description language Most computer vision research in volves low level image analysis that rarely tries to bridge the gap to semantic scene description\nOur purpose is to show how one particular seman tic scene description problem can be approached The task is to determine whether a consumer photograph depicts an indoor or an outdoor scene This problem is relatively unambiguous and is motivated by several practical applications\nThere certainly exist photographs for which the indoor outdoor distinction is unclear Examples include shots made through a window with visible window edges photographs of paintings and extreme close ups of faces Fortunately such compositions are rare in con sumer photographs Our database of images was\nThis work was supported in part by Kodak NEC and Hewlett\nPackard Labs\nclassi ed by two independent people and only nineteen out of these were labeled as unclear and omitted from this study\nThe applications of this problem are interesting Knowledge about the scene enables more intelligent im age processing For example when lm is developed and prints are made from the negatives the exposure and color is automatically adjusted Unfortunately the automatic correction does not take into account the con tent of the photograph If the machine could distin guish indoor from outdoor images it could adjust these classes di erently rather than adjusting everything to wards one ideal exposure and color This observation can also be applied to image scanners photocopiers fax machines image processing software etc\nAnother important application is image retrieval Let s say we would like to nd a beach scene A help ful step would be to limit the search to outdoor scenes Unfortunately this is not possible even in state of the art image retrieval systems such as QBIC Virage and VisualSEEk These systems are based mainly on color histograms and primitive texture measures The user builds a query by selecting colors from a palette a texture from a chart and then indicates how to weight the color versus the texture Unfortunately it is di cult for a user to know how to weight the di erent features to get a beach scene The systems level of abstraction is much too low\nQuery by image example enables the user to select one image and nd other similar images making it eas ier to specify the relevant color and texture query Most systems still require the user to select weights for the di erent features An exception to this is FourEyes which can learn the relevant feature combination based on several positive and negative examples In an initial quick attempt to teach FourEyes to solve the indoor outdoor classi cation problem using whole images with no speci c subblock guidance we did not meet with sig ni cant success Although FourEyes can learn any clas si cation it was not very e cient on this one probably due to the noisiness of the two classes being considered\nIn this work we propose a di erent classi cation ap proach that exploits the same idea in FourEyes of non linearly combining features from multiple models but does so in a di erent way This new way is successful for accurately distinguishing indoor from outdoor scenes\nBackground\nSeveral attempts at recognizing high level scene proper ties using low level features have been made Gorkani and Picard discriminate between photos of city scenes and photos of landscape scenes They use a multiscale steerable pyramid to nd dominant orientations in subblocks of the image The image is classi ed as a city scene if enough subblocks have strong dominant verti cal orientation or alternatively medium strong vertical orientation and also horizontal orientation\nYiu uses the same dominant orientation features and also color information to classify indoor and outdoor scenes She uses nearest neighbor and support vector machine classi ers The former classi er is better at color the latter at dominant orientation Yiu reports accuracies similar to those in our work but they are believed to have a high variance because they were not thoroughly evaluated with a leave one out method and a much smaller database of only images was used Furthermore the texture features used here give sig ni cantly better results than her dominant orientation detector The work here also takes advantage of a spa tial tessellation of the image which we found provides a signi cant gain in performance\nInstead of building a speci c scene class detector Lipson describes a general scene query approach Scenes are described by graphs representing relations between image regions The relationships include rel ative color spatial location and highpass frequency content Unfortunately the templates have to be con structed manually for each scene layout These tem plates are also quite speci c which makes them ne for limited special cases such as sky over mountain over lake but di cult for the case considered here of cap turing a broad concept like an outdoor scene\nYu learns a statistical template from examples She computes vector quantized color histograms for sub blocks of the image Then she trains a one dimensional hidden Markov model along vertical or horizontal seg ments of speci c scene layouts such as sky mountain river scenes Unfortunately the one dimensional model cannot describe spatial relationships well and a two dimensional generalization such as Markov random elds is desirable but raises many new kinds of prob lems\nFeatures\nImage Database\nThe image database in these experiments consists of consumer photographs collected and labeled by Kodak They depict typical family and vacation scenes and are taken by many di erent individuals at all dif ferent times of they year The database is quite diverse and includes snow bright sun sea sunset night and silhouette scenes Image types not in our database can be easily added to the training set without changing any algorithms\nThe images were hand labeled by two independent people not the authors resulting in labeled as outdoor and labeled as indoor We have excluded images which were labeled as am biguous All the images in the set have landscape ori entation and are right side up The full resolution of the images was but for most experiments we used half or quarter resolution as will be described later The images originally came in bit color but were quantized down to bit color At the same time we performed basic color balancing according to steps provided by Kodak which simply clipped the top and bottom of the intensity levels shifted the histograms to the middle and stretched them to occupy the bit range\nA baseline experiment\nThe problem is quite challenging A naive approach such as a traditional color histogram will not give good classi cation performance To illustrate this we com puted bin histograms uniformly spaced for each RGB channel concatenated them into a feature vec tor and applied a nearest neighbor classi er The dis tance between feature vectors was measured using the Euclidean norm The resulting leave one out classi cation performance was This number is only somewhat better than just guessing that each image is outdoor which would be correct Nevertheless a very similar color histogram is used at the heart of most image retrieval systems\nBelow we describe and evaluate several methods which perform much better These included more so phisticated features and classi ers which tessellate the image into subregions and combine the results from dif ferent features and di erent spatial regions to result in signi cantly improved performance\nThe features\nWe have used three types of features one each for color texture and frequency information These features were\ncomputed both for the whole image and for each sub block of a image tessellation\nThe color feature is a color histogram and has bins per channel like our baseline However the three channels come from the Ohta color space The color axes of this space are the largest eigenvectors of the RGB space found through principal components anal ysis of a large selection of natural images This yields\nI R G B\nI R B\nI R G B\nThe advantage of the Ohta color space is that the color channels are approximately decorrelated which makes it a good choice for computing per channel histograms The change of color spaces from RGB to Ohta raises the performance of color histogram based recognition to\nMoreover instead of using the Euclidean norm for measuring distances between histograms we use the his togram intersection norm It measures the amount of overlap between corresponding buckets in the two his tograms h and h and is de ned as\ndist h h\nNX\ni\nh i min h i h i\nWhen both the Ohta color space and histogram in tersection is used the classi cation rises to cor rect The intersection norm is better than the Euclidean norm possibly because it penalizes linear error as op posed to squared error reducing sensitivity to outliers In the rest of the paper we exclusively apply the Ohta color space with histogram intersection\nThe texture features are computed using the multiresolution simultaneous autoregressive model MSAR These are among the best texture features bench marked on the Brodatz album The model constructs the best linear predictor of a pixel based on a noncausal neighborhood The features are the weights of the predictor Three di erent neighborhoods at scales and are used and the weights are concatenated to yield a dimensional vector as in The Maha lanobis norm is used to measure feature vector distance covariances are estimated from several subwindow es timates We extracted these features from gray scale images at two resolutions half and quarter using a suitable antialiasing lter\nThe MSAR classi cation is and correct at half and quarter resolutions respectively which is sig ni cantly better than the best color classi cation This\nis surprising since the MSAR feature presupposes a sin gle texture characterized by a second order stationary autoregressive process whereas a typical image con sists of many textures and is de nitely nonstationary As we shall see we can do even better by dividing the image into subblocks and computing the features separately over each block\nThe frequency features are obtained by rst calcu lating the D DFT magnitude and then taking the D DCT The rst step is shift invariant and for periodic textures it shows a regular pattern of peaks fundamen tal and harmonic frequencies The second step replaces all related frequencies by one coe cient All computa tions are done over pixel blocks and the results are averaged over the image region also producing co variances used for the Mahalanobis distance metric\nClassi cation\nThe performance numbers so far refer to nearest neigh bor classi cation of features computed on the whole im age Unfortunately this method cannot exploit local properties of the image e g blue sky at the top Never theless most image retrieval systems use features com puted for the whole image thus the numbers here are useful for comparisons The results are summarized in Table for a K nearest neighbor classi er We have also tried a layer neural network with sigmoid nonlinear ities but training was slow and the results were worse than for the nearest neighbor algorithm when compared on the color histogram features\nTo allow local and spatial properties to improve the classi cation we divided the image into subblocks and computed the features separately within them Now the question arises how to classify these blocks One possibility would be to concatenate feature vector from all subblocks of an image and apply a classi er to this vector The problem is that such a feature is very high dimensional e g It is di cult to estimate covariances for such a large vector and we en counter general curse of dimensionality problems\nInstead we chose to pursue a multi stage classi ca tion approach classifying the subblocks independently and then performing another classi cation on these an swers Figure This is reminiscent of stacking except that the subblock classi ers here were trained on their own data Not surprisingly the individual sub block classi ers are less accurate than a whole image classi er Ideally we would keep a con dence or prob abilistic value associated with each subblock classi ca tion as opposed to the binary decision in or out shown in Figure In theory the mixture of experts method applied below takes care of this case this will be described later For now Table shows the results\nTable Whole image classi cation results using k nearest neighbor The best result in each row is marked with\nFeature k k k k k RGB histogram euclidean RGB histogram intersection Ohta histogram euclidean Ohta histogram intersection MSAR quarter resolution MSAR half resolution DCT half resolution\nof the use of the k nearest neighbor classi er on the color features where each subblock is compared to all subblocks in the database regardless of spatial location excluding subblocks from the same image\nWhen the results of the subblocks are combined the classi cation can be greatly improved Three ways to combine the features were systematically tried a simple majority classi er that assigns the label for the image to the most common class label among the sub blocks a one layer neural net and a Mixture of Experts classi er The rst method was evaluated using the leave one out method and was found to give good results Table The other two methods because of their long training time were only evaluated with a few runs of leave out training on of the data and testing on the other In these limited tests which are subject to higher variance than the leave one out test method we got slightly better results than the majority classi er but not signi cantly better\nThe one layer neural net can give us information about what subblocks are important for the classi ca tion task The net has a sigmoid nonlinearity at the output and optimizes the cross entropy cost function between the network output and the true class By ob serving the weights learned by the network we found that it favors especially the top row but also the lower center of the image for classi cation gure the sub blocks were classi ed using color histograms In some images sky occurs in this region and is perhaps easier to classify correctly and hence is heavily weighted\nThe mixture of experts classifer is similar to but more exible than the above neural network It learns experts for speci c subproblems The experts are se lected depending on the input data and each expert can weight the data di erently For example if the top of the image is classi ed as outdoor an expert can weight it more heavily than if the top of the image is classi ed as indoor The technique is similar to softly clus tering the data and assigning a set of weights for each\n0\n0.05\n0.1\n0.15\nx subblock\ny su\nbb lo\nck\nWeights in stacked regression\n0.5 1 1.5 2 2.5 3 3.5 4 4.5\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nFigure Weights of subblocks in image classi cation The larger the weights the brighter the square\ncluster however the clustering and weight assignment occur simultaneously Somewhat disappointingly the classi cation results are approximately the same as for the neural network Moreover we are required to set additional parameters the number of experts and be careful to avoid over tting Thus we judged that the additional e ort was not justi ed\nMultiple feature combination\nSo far we have combined multiple subblock classi ca tions but these were all based on a single image feature To gain robustness we can use multiple image features simultaneously One common way to do this is to con catenate di erent feature vectors into a longer vector Unfortunately this step increases the dimensionality of the problem and requires a metric which is simulta neously good for e g color histograms and MSAR The relationship between two features is almost certainly not linear so such a metric is di cult to construct There\nTexture Texture classification\nColor Color classification\nCombined classification\ninout\nininin in\nin in\nin in\nin in in\nin\nout\nout\nin\nininin in\nout in in\nout in\nout in in\nin\nin\nin\nIn\nFigure Two stage classi cation combining color and texture\nTable K nearest neighbor classi cations on subblocks The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Majority classi er based on k nearest neighbor The best result in each row is marked with\nFeature k k k k k Ohta histogram intersection MSAR half resol MSAR quarter resol DCT half resol\nTable Combined feature classi er k MSAR fea tures were measured at half resolution\nFeature Performance Color MSAR Color DCT MSAR DCT Color MSAR DCT\nTable Confusion matrix for color MSAR combination\nTrue Class Classi ed as indoor outdoor\nindoor outdoor\nfore we think this approach is a mistake even though it is commonly used by researchers working in content based retrieval\nThere is a way to combine feature vectors just by concatenation by rst translating all the features into a common language This was done in the FourEyes system by using the common language of clusters Our common language is di erent the subblock classes as signed by the k nearest neighbor classifer In other words we simply concatenate the subblock classi ca tions based on di erent image features and then do a second stage classi cation In the second stage we get signi cantly improved results by applying the majority classi er to the combined vectors Table\nDiscussion\nThe best classi cation results were generally obtained by combining color features with texture features Both the MSAR and DCT based features capture shift invariant intensity variations over a range of scales so combining them does not provide as much gain as com bining color with one of them\nThe color MSAR combination gives us the best re sult correct measured using leave one out cross validation The confusion matrix for this result Ta ble shows that it is approximately equally likely to mistakenly label indoor images as being outdoor or vice versa The proportion of indoor to outdoor images in the database is vs which is fairly balanced\nFigure shows several correctly classi ed images by the combined color and MSAR algorithm see http www media mit edu szummer caivd for color ver sions of the images These images were incorrectly la beled when using only color information The color algo rithm easily mistakes photos containing green or navy\nblue as outdoor images Conversely it often mistakes photos containing white areas e g snow scenes and brown colors as indoor images The texture feature dis regards color and the combination gives the right an swer\nFigures and show samples of images that were misclassi ed by the combined color and MSAR algo rithm Some of the missclassi ed indoor images contain green plants Christmas trees or green walls since green is a typical color of outdoor images Another di cult indoor image is a picture of the top of a shelf and the ceiling which looks blue under ash light like sky The missclassi ed outdoor images are often night time ash photographs White outdoor walls and hazy white sky are also sometimes mistaken to be indoor probably be cause they are very common in indoor scenes Close ups are always challenging because they are dominated by one object and provide little background\nIt is tempting to believe that outdoor images can be easily classi ed by building a blue sky detector A quick look at the database disspells this myth at least for amateur photographs only about one in ve outdoor images have clear blue sky in most outdoor images the sky is not visible or is cloudy white or gray These cloudy colors can unfortunately be produced by ash light as well making them di cult to use for discrimi nation\nConclusions\nWe have shown how high level scene properties can be inferred from low level image features The indoor outdoor classi cation problem is only one example of a high level scene property and we believe that many other properties can be inferred in a similar way Since people often reason in terms of semantic image proper ties it is important for vision systems to extract them\nWe found that it is quite di cult to predict the performance of a feature or feature combination often combining two weaker features with a k nearest neigh bor classi er consistently produced better results than a single good feature Moreover relatively simple clas si ers k nearest neighbors performed better than the more sophisticated neural networks and mixture of ex pert classi ers These empirical results suggest that a theoretical investigation should be undertaken in an ef fort to better understand the relative merits of these methods\nNevertheless we believe that performance will scale well to larger databases of consumer photography After a thorough examination we settled for simple but ro bust classi ers that require few parameter settings Of course it is always possible to devise scenes that will\nfool any system However our system can always be provided with more ground truth for new image types which is likely to increase the performance on such im ages In the domain of consumer photography we have used a large enough sample to show that accurate clas si cation is possible\nAcknowledgement\nThe authors wish to thank Thomas Minka for help with the FourEyes software and Bob Gray at Kodak for sug gestions Portions of the research in this paper use the Kodak Image Research Database This work was sup ported in part by Kodak NEC and Hewlett Packard Labs\nReferences\nMyron Flickner Harpreet Sawhney et al Query by image and video content The QBIC system IEEE Computer Sept\nAmarnath Gupta and Ramesh Jain Visual infor mation retrieval Communications of the ACM http www virage com research htm vir cacm pdf\nJ R Smith and S F Chang Visualseek a fully automated content based image query system In ACM Multimedia pages Nov\nT P Minka and R W Picard Interactive learn ing using a !society of models In Proceedings of CVPR pages San Francisco CA June IEEE Computer Society\nMonika Gorkani and Rosalind W Picard Tex ture orientation for sorting photos at a glance In Proc Int Conf Pat Rec volume I pages Jerusalem Israel Oct\nElaine C Yiu Image classi cation using color cues and texture orientation Master s thesis MIT dept EECS\nPamela R Lipson Context and Con guration Based Scene Classi cation PhD thesis MIT EECS dept\nHong Heather Yu and Wayne Wolf Scenic clas si cation methods for image and video databases In Proc SPIE Digital Image Storage and Archiv ing systems pages http www ee princeton edu heathery\nY I Ohta T Kanade and T Sakai Color infor mation for region segmentation Comp Graph and Img Proc\nMichael Swain and Dana Ballard Color indexing Int J of Comp Vis\nJianchang Mao and Anil K Jain Texture classi cation and segmentation using multiresolution si multaneous autoregressive models Pattern Recog nition\nRosalind W Picard Tanweer Kabir and Fang Liu Real time recognition with the entire Brodatz tex ture database In Proc IEEE Conf on Computer Vision and Pattern Recognition pages New York June MIT Media Lab Perceptual Computing TR\nLeo Breiman Stacked regression ftp ftp stat berkeley edu pub users breiman stacked abstract\nMichael I Jordan and Robert A Jacobs Hierar chical mixtures of experts and the EM algorithm Neural Computation\nFigure out of missclassi ed outdoor images combined color and MSAR classi er Outdoor ash photos in dusk or at night are especially di cult as are scenes with white regions walls hazy sky Close ups are also challenging since they are dominated by one object and do not provide much context The other outdoor images in the database were correctly labelled\nFigure out of missclassi ed indoor images combined color and MSAR classi er Plants Christmas trees green walls and brown oors are sometimes mistakenly thought to belong to outdoor scenes The other indoor images were correctly classi ed"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Texture classi-  cation and segmentation using multiresolution si-  multaneous autoregressive models"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recog-  nition,"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52549908"
                        ],
                        "name": "Elaine C. Yiu",
                        "slug": "Elaine-C.-Yiu",
                        "structuredName": {
                            "firstName": "Elaine",
                            "lastName": "Yiu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elaine C. Yiu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "Yiu [6] uses the same dominant orientation features and also color information to classify indoor and outdoor scenes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56537438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e45f91dc8ac9ae7b3293e6adc4804e540f4ead6e",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Image-classification-using-color-cues-and-texture-Yiu",
            "title": {
                "fragments": [],
                "text": "Image classification using color cues and texture orientation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41186627,
            "fieldsOfStudy": [],
            "id": "a196394a31f83c515cb8a691e1d6386b628edd43",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interactive learning with a \"Society of Models\""
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207657152,
            "fieldsOfStudy": [],
            "id": "7bcdf9e7c9d072d94be325f8a0d3e7db60ac1b6c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical Mixtures of Experts and the EM Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierar - chical mixtures of experts and the EM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "However, the three channels come from the Ohta color space [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Color infor-  mation for region segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "Comp. Graph. and  Img. Proc.,"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "Moreover, instead of using the Euclidean norm for measuring distances between histograms, we use the histogram intersection norm [lo]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "of Comp"
            },
            "venue": {
                "fragments": [],
                "text": "Vis., (1):ll-32,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image classiication using color cues and texture orientation. Master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "Image classiication using color cues and texture orientation. Master's thesis"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image classiication using color cues and texture orientation. Master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "Image classiication using color cues and texture orientation. Master's thesis"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stacked regression. ftp: //ftp.stat.berkeley.edu/pub/users/breiman/ stacked"
            },
            "venue": {
                "fragments": [],
                "text": "Stacked regression. ftp: //ftp.stat.berkeley.edu/pub/users/breiman/ stacked"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "Three di erent neighborhoods at scales 2, 3, and 4 are used, and the weights are concatenated to yield a 15-dimensional vector, as in [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "These are among the best texture features bench-marked on the Brodatz album [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real-time recognition with the entire Brodatz tex-  ture database"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. IEEE Conf. on Computer  Vision and Pattern Recognition,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Texture classication and segmentation using multiresolution simultaneous autoregressive m o d els"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Context and Connguration Based Scene Classiication"
            },
            "venue": {
                "fragments": [],
                "text": "Context and Connguration Based Scene Classiication"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stacked regression"
            },
            "venue": {
                "fragments": [],
                "text": "Stacked regression"
            },
            "year": 1994
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Indoor-outdoor-image-classification-Szummer-Picard/0f45a46dedadf599c12874b22645d596205ed8d5?sort=total-citations"
}