{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109919504"
                        ],
                        "name": "Yiru Zhao",
                        "slug": "Yiru-Zhao",
                        "structuredName": {
                            "firstName": "Yiru",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiru Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738364"
                        ],
                        "name": "Bing Deng",
                        "slug": "Bing-Deng",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088079750"
                        ],
                        "name": "Chen Shen",
                        "slug": "Chen-Shen",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108923831"
                        ],
                        "name": "Yao Liu",
                        "slug": "Yao-Liu",
                        "structuredName": {
                            "firstName": "Yao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46385957"
                        ],
                        "name": "Hongtao Lu",
                        "slug": "Hongtao-Lu",
                        "structuredName": {
                            "firstName": "Hongtao",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongtao Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Some of these methods include Convolutional autoencoder [13], spatio-temporal autoencoder [5], 3D Convnet AE [27], and Temporally-coherent Sparse Coding StackedRNN [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207746234,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fef6f1e04fa64f2f26ac9f01cd143dd19e549790",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Anomalous events detection in real-world video scenes is a challenging problem due to the complexity of \"anomaly\" as well as the cluttered backgrounds, objects and motions in the scenes. Most existing methods use hand-crafted features in local spatial regions to identify anomalies. In this paper, we propose a novel model called Spatio-Temporal AutoEncoder (ST AutoEncoder or STAE), which utilizes deep neural networks to learn video representation automatically and extracts features from both spatial and temporal dimensions by performing 3-dimensional convolutions. In addition to the reconstruction loss used in existing typical autoencoders, we introduce a weight-decreasing prediction loss for generating future frames, which enhances the motion feature learning in videos. Since most anomaly detection datasets are restricted to appearance anomalies or unnatural motion anomalies, we collected a new challenging dataset comprising a set of real-world traffic surveillance videos. Several experiments are performed on both the public benchmarks and our traffic dataset, which show that our proposed method remarkably outperforms the state-of-the-art approaches."
            },
            "slug": "Spatio-Temporal-AutoEncoder-for-Video-Anomaly-Zhao-Deng",
            "title": {
                "fragments": [],
                "text": "Spatio-Temporal AutoEncoder for Video Anomaly Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel model called Spatio-Temporal AutoEncoding (ST AutoEncoder or STAE), which utilizes deep neural networks to learn video representation automatically and extracts features from both spatial and temporal dimensions by performing 3-dimensional convolutions, which enhances the motion feature learning in videos."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144742695"
                        ],
                        "name": "Dan Xu",
                        "slug": "Dan-Xu",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117857570"
                        ],
                        "name": "Yan Yan",
                        "slug": "Yan-Yan",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40811261"
                        ],
                        "name": "E. Ricci",
                        "slug": "E.-Ricci",
                        "structuredName": {
                            "firstName": "Elisa",
                            "lastName": "Ricci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ricci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703601"
                        ],
                        "name": "N. Sebe",
                        "slug": "N.-Sebe",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Sebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sebe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "These features are usually extracted from whole frames [5, 13, 18, 20, 25], localized on a grid of image patches [24], or concentrated on pre-identified regions [6, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "Traditional approaches consider video frames as separate data samples and model them using one-class classification methods, such as one-class SVM [25] and mixture of probabilistic PCA [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3949944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5366a704ffa3b41aacd385f3c087ec3fd566934",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Detecting-anomalous-events-in-videos-by-learning-of-Xu-Yan",
            "title": {
                "fragments": [],
                "text": "Detecting anomalous events in videos by learning deep representations of appearance and motion"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053795564"
                        ],
                        "name": "Yong Shean Chong",
                        "slug": "Yong-Shean-Chong",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Chong",
                            "middleNames": [
                                "Shean"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Shean Chong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9201065"
                        ],
                        "name": "Yong Haur Tay",
                        "slug": "Yong-Haur-Tay",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Tay",
                            "middleNames": [
                                "Haur"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Haur Tay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "These features are usually extracted from whole frames [5, 13, 18, 20, 25], localized on a grid of image patches [24], or concentrated on pre-identified regions [6, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Some of these methods include Convolutional autoencoder [13], spatio-temporal autoencoder [5], 3D Convnet AE [27], and Temporally-coherent Sparse Coding StackedRNN [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2012925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "527cc8cd2af06a9ac2e5cded806bab5c3faad9cf",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an efficient method for detecting anomalies in videos. Recent applications of convolutional neural networks have shown promises of convolutional layers for object detection and recognition, especially in images. However, convolutional neural networks are supervised and require labels as learning signals. We propose a spatiotemporal architecture for anomaly detection in videos including crowded scenes. Our architecture includes two main components, one for spatial feature representation, and one for learning the temporal evolution of the spatial features. Experimental results on Avenue, Subway and UCSD benchmarks confirm that the detection accuracy of our method is comparable to state-of-the-art methods at a considerable speed of up to 140 fps."
            },
            "slug": "Abnormal-Event-Detection-in-Videos-using-Chong-Tay",
            "title": {
                "fragments": [],
                "text": "Abnormal Event Detection in Videos using Spatiotemporal Autoencoder"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a spatiotemporal architecture for anomaly detection in videos including crowded scenes that includes two main components, one for spatial feature representation, and one for learning the temporal evolution of the spatial features."
            },
            "venue": {
                "fragments": [],
                "text": "ISNN"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117934369"
                        ],
                        "name": "Wen Liu",
                        "slug": "Wen-Liu",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074878"
                        ],
                        "name": "Weixin Luo",
                        "slug": "Weixin-Luo",
                        "structuredName": {
                            "firstName": "Weixin",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weixin Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35180251"
                        ],
                        "name": "Dongze Lian",
                        "slug": "Dongze-Lian",
                        "structuredName": {
                            "firstName": "Dongze",
                            "lastName": "Lian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongze Lian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702868"
                        ],
                        "name": "Shenghua Gao",
                        "slug": "Shenghua-Gao",
                        "structuredName": {
                            "firstName": "Shenghua",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shenghua Gao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "These features are usually extracted from whole frames [5, 13, 18, 20, 25], localized on a grid of image patches [24], or concentrated on pre-identified regions [6, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[18] proposed to use the prediction of optical flow in their temporal coherent loss, effectively filtering out parts of the noise in pixel appearance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 260
                            }
                        ],
                        "text": "Model regularization When training autoencoder style models for anomaly detection, a major challenge is that even if the model learns to generate normal data perfectly, there is still no guarantee that the model will produce high errors for abnormal sequences [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3865699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a6acba7fb2aad1299fcf35701417e063d410ed4",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Anomaly detection in videos refers to the identification of events that do not conform to expected behavior. However, almost all existing methods tackle the problem by minimizing the reconstruction errors of training data, which cannot guarantee a larger reconstruction error for an abnormal event. In this paper, we propose to tackle the anomaly detection problem within a video prediction framework. To the best of our knowledge, this is the first work that leverages the difference between a predicted future frame and its ground truth to detect an abnormal event. To predict a future frame with higher quality for normal events, other than the commonly used appearance (spatial) constraints on intensity and gradient, we also introduce a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and this is the first work that introduces a temporal constraint into the video prediction task. Such spatial and motion constraints facilitate the future frame prediction for normal events, and consequently facilitate to identify those abnormal events that do not conform the expectation. Extensive experiments on both a toy dataset and some publicly available datasets validate the effectiveness of our method in terms of robustness to the uncertainty in normal events and the sensitivity to abnormal events. All codes are released in https://github.com/StevenLiuWen/ano_pred_cvpr2018."
            },
            "slug": "Future-Frame-Prediction-for-Anomaly-Detection-A-New-Liu-Luo",
            "title": {
                "fragments": [],
                "text": "Future Frame Prediction for Anomaly Detection - A New Baseline"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes to tackle the anomaly detection problem within a video prediction framework by introducing a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and is the first work that introduces a temporal constraint into the video prediction task."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152302063"
                        ],
                        "name": "Mahmudul Hasan",
                        "slug": "Mahmudul-Hasan",
                        "structuredName": {
                            "firstName": "Mahmudul",
                            "lastName": "Hasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mahmudul Hasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119675494"
                        ],
                        "name": "Jonghyun Choi",
                        "slug": "Jonghyun-Choi",
                        "structuredName": {
                            "firstName": "Jonghyun",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonghyun Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144660077"
                        ],
                        "name": "J. Neumann",
                        "slug": "J.-Neumann",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404727582"
                        ],
                        "name": "A. Roy-Chowdhury",
                        "slug": "A.-Roy-Chowdhury",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Roy-Chowdhury",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Roy-Chowdhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 168
                            }
                        ],
                        "text": "To understand how the detection of anomalies is made by all models, we visually compare in Figure 4 the map of anomaly scores produced by MPED-RNN to those produced by Conv-AE [13] and Liu et al. [18]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "To understand how the detection of anomalies is made by all models, we visually compare in Figure 4 the map of anomaly scores produced by MPED-RNN to those produced by Conv-AE [13] and Liu et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Anomaly score map of Conv-AE [13], Liu et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "With less than a hundred dimensions per frame on average, equal to a small fraction of the popular visual features for anomaly detection (ResNet features of 2048 [13], AlexNet fc7 of 4096 [14]), skeleton features still provide equal or better performance than current state-of-the-art methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 114
                            }
                        ],
                        "text": "On HR-Avenue, we achieved a frame-level ROC AUC of 0.863, against 0.862 and 0.848 achieved by Liu et al. [18] and Conv-AE [13], respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "These features are usually extracted from whole frames [5, 13, 18, 20, 25], localized on a grid of image patches [24], or concentrated on pre-identified regions [6, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Some of these methods include Convolutional autoencoder [13], spatio-temporal autoencoder [5], 3D Convnet AE [27], and Temporally-coherent Sparse Coding StackedRNN [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2429016,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97e7c94a78ae17cfb90848c1cfca8c431082a7b2",
            "isKey": true,
            "numCitedBy": 536,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Perceiving meaningful activities in a long video sequence is a challenging problem due to ambiguous definition of 'meaningfulness' as well as clutters in the scene. We approach this problem by learning a generative model for regular motion patterns (termed as regularity) using multiple sources with very limited supervision. Specifically, we propose two methods that are built upon the autoencoders for their ability to work with little to no supervision. We first leverage the conventional handcrafted spatio-temporal local features and learn a fully connected autoencoder on them. Second, we build a fully convolutional feed-forward autoencoder to learn both the local features and the classifiers as an end-to-end learning framework. Our model can capture the regularities from multiple datasets. We evaluate our methods in both qualitative and quantitative ways - showing the learned regularity of videos in various aspects and demonstrating competitive performance on anomaly detection datasets as an application."
            },
            "slug": "Learning-Temporal-Regularity-in-Video-Sequences-Hasan-Choi",
            "title": {
                "fragments": [],
                "text": "Learning Temporal Regularity in Video Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work proposes two methods that are built upon the autoencoders for their ability to work with little to no supervision, and builds a fully convolutional feed-forward autoencoder to learn both the local features and the classifiers as an end-to-end learning framework."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410122921"
                        ],
                        "name": "Yuan Yuan",
                        "slug": "Yuan-Yuan",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389269"
                        ],
                        "name": "Jianwu Fang",
                        "slug": "Jianwu-Fang",
                        "structuredName": {
                            "firstName": "Jianwu",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianwu Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346762"
                        ],
                        "name": "Qi Wang",
                        "slug": "Qi-Wang",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [26], object trajectories were used to guide the pooling of the visual features so that interesting areas are paid more attention to."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4863545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13ae3c8afef5a0d6f4c9e684da9fc1fa96caaeb6",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Abnormal behavior detection in crowd scenes is continuously a challenge in the field of computer vision. For tackling this problem, this paper starts from a novel structure modeling of crowd behavior. We first propose an informative structural context descriptor (SCD) for describing the crowd individual, which originally introduces the potential energy function of particle's interforce in solid-state physics to intuitively conduct vision contextual cueing. For computing the crowd SCD variation effectively, we then design a robust multi-object tracker to associate the targets in different frames, which employs the incremental analytical ability of the 3-D discrete cosine transform (DCT). By online spatial-temporal analyzing the SCD variation of the crowd, the abnormality is finally localized. Our contribution mainly lies on three aspects: 1) the new exploration of abnormal detection from structure modeling where the motion difference between individuals is computed by a novel selective histogram of optical flow that makes the proposed method can deal with more kinds of anomalies; 2) the SCD description that can effectively represent the relationship among the individuals; and 3) the 3-D DCT multi-object tracker that can robustly associate the limited number of (instead of all) targets which makes the tracking analysis in high density crowd situation feasible. Experimental results on several publicly available crowd video datasets verify the effectiveness of the proposed method."
            },
            "slug": "Online-Anomaly-Detection-in-Crowd-Scenes-via-Yuan-Fang",
            "title": {
                "fragments": [],
                "text": "Online Anomaly Detection in Crowd Scenes via Structure Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes an informative structural context descriptor for describing the crowd individual, which originally introduces the potential energy function of particle's interforce in solid-state physics to intuitively conduct vision contextual cueing, and designs a robust multi-object tracker to associate the targets in different frames."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Cybernetics"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48493294"
                        ],
                        "name": "V. Mahadevan",
                        "slug": "V.-Mahadevan",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Mahadevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mahadevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34346779"
                        ],
                        "name": "Weixin Li",
                        "slug": "Weixin-Li",
                        "structuredName": {
                            "firstName": "Weixin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weixin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099718992"
                        ],
                        "name": "V. Bhalodia",
                        "slug": "V.-Bhalodia",
                        "structuredName": {
                            "firstName": "Viral",
                            "lastName": "Bhalodia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Bhalodia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699559"
                        ],
                        "name": "N. Vasconcelos",
                        "slug": "N.-Vasconcelos",
                        "structuredName": {
                            "firstName": "Nuno",
                            "lastName": "Vasconcelos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Vasconcelos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "It prevents us from trying our method on UCSD Ped1/Ped2 [21], another popular dataset whose video quality is too low to detect skeletons."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206591190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d3f0d47449c7db37d1bae3b70db2928610a8db7",
            "isKey": false,
            "numCitedBy": 1106,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel framework for anomaly detection in crowded scenes is presented. Three properties are identified as important for the design of a localized video representation suitable for anomaly detection in such scenes: 1) joint modeling of appearance and dynamics of the scene, and the abilities to detect 2) temporal, and 3) spatial abnormalities. The model for normal crowd behavior is based on mixtures of dynamic textures and outliers under this model are labeled as anomalies. Temporal anomalies are equated to events of low-probability, while spatial anomalies are handled using discriminant saliency. An experimental evaluation is conducted with a new dataset of crowded scenes, composed of 100 video sequences and five well defined abnormality categories. The proposed representation is shown to outperform various state of the art anomaly detection techniques."
            },
            "slug": "Anomaly-detection-in-crowded-scenes-Mahadevan-Li",
            "title": {
                "fragments": [],
                "text": "Anomaly detection in crowded scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A novel framework for anomaly detection in crowded scenes is presented and the proposed representation is shown to outperform various state of the art anomaly detection techniques."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109202643"
                        ],
                        "name": "Jaechul Kim",
                        "slug": "Jaechul-Kim",
                        "structuredName": {
                            "firstName": "Jaechul",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaechul Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 561197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "731a2844c5af6b072d3b404ecabbb488cdad9d46",
            "isKey": false,
            "numCitedBy": 347,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a space-time Markov random field (MRF) model to detect abnormal activities in video. The nodes in the MRF graph correspond to a grid of local regions in the video frames, and neighboring nodes in both space and time are associated with links. To learn normal patterns of activity at each local node, we capture the distribution of its typical optical flow with a mixture of probabilistic principal component analyzers. For any new optical flow patterns detected in incoming video clips, we use the learned model and MRF graph to compute a maximum a posteriori estimate of the degree of normality at each local node. Further, we show how to incrementally update the current model parameters as new video observations stream in, so that the model can efficiently adapt to visual context changes over a long period of time. Experimental results on surveillance videos show that our space-time MRF model robustly detects abnormal activities both in a local and global sense: not only does it accurately localize the atomic abnormal activities in a crowded video, but at the same time it captures the global-level abnormalities caused by irregular interactions between local activities."
            },
            "slug": "Observe-locally,-infer-globally:-A-space-time-MRF-Kim-Grauman",
            "title": {
                "fragments": [],
                "text": "Observe locally, infer globally: A space-time MRF for detecting abnormal activities with incremental updates"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experimental results on surveillance videos show that the space-time MRF model robustly detects abnormal activities both in a local and global sense: not only does it accurately localize the atomic abnormal activities in a crowded video, but at the same time it captures the global-level abnormalities caused by irregular interactions between local activities."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145966503"
                        ],
                        "name": "H. Vu",
                        "slug": "H.-Vu",
                        "structuredName": {
                            "firstName": "Hung",
                            "lastName": "Vu",
                            "middleNames": [
                                "Thanh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Vu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314511"
                        ],
                        "name": "T. Nguyen",
                        "slug": "T.-Nguyen",
                        "structuredName": {
                            "firstName": "Tu",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Dinh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144081637"
                        ],
                        "name": "A. Travers",
                        "slug": "A.-Travers",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Travers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Travers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143761093"
                        ],
                        "name": "S. Venkatesh",
                        "slug": "S.-Venkatesh",
                        "structuredName": {
                            "firstName": "Svetha",
                            "lastName": "Venkatesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Venkatesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749657"
                        ],
                        "name": "Dinh Q. Phung",
                        "slug": "Dinh-Q.-Phung",
                        "structuredName": {
                            "firstName": "Dinh",
                            "lastName": "Phung",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dinh Q. Phung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "These features are usually extracted from whole frames [5, 13, 18, 20, 25], localized on a grid of image patches [24], or concentrated on pre-identified regions [6, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11488355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b674c44c190bb3813715773a3fb06378ef8ff284",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Automated detection of abnormal events in video surveillance is an important task in research and practical applications. This is, however, a challenging problem due to the growing collection of data without the knowledge of what to be defined as \u201cabnormal\u201d, and the expensive feature engineering procedure. In this paper we introduce a unified framework for anomaly detection in video based on the restricted Boltzmann machine (\\(\\text {RBM}\\)), a recent powerful method for unsupervised learning and representation learning. Our proposed system works directly on the image pixels rather than hand-crafted features, it learns new representations for data in a completely unsupervised manner without the need for labels, and then reconstructs the data to recognize the locations of abnormal events based on the reconstruction errors. More importantly, our approach can be deployed in both offline and streaming settings, in which trained parameters of the model are fixed in offline setting whilst are updated incrementally with video data arriving in a stream. Experiments on three publicly benchmark video datasets show that our proposed method can detect and localize the abnormalities at pixel level with better accuracy than those of baselines, and achieve competitive performance compared with state-of-the-art approaches. Moreover, as RBM belongs to a wider class of deep generative models, our framework lays the groundwork towards a more powerful deep unsupervised abnormality detection framework."
            },
            "slug": "Energy-Based-Localized-Anomaly-Detection-in-Video-Vu-Nguyen",
            "title": {
                "fragments": [],
                "text": "Energy-Based Localized Anomaly Detection in Video Surveillance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified framework for anomaly detection in video based on the restricted Boltzmann machine, a recent powerful method for unsupervised learning and representation learning, that can detect and localize the abnormalities at pixel level with better accuracy than those of baselines, and achieve competitive performance compared with state-of-the-art approaches."
            },
            "venue": {
                "fragments": [],
                "text": "PAKDD"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148342"
                        ],
                        "name": "S. Co\u015far",
                        "slug": "S.-Co\u015far",
                        "structuredName": {
                            "firstName": "Serhan",
                            "lastName": "Co\u015far",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Co\u015far"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39046383"
                        ],
                        "name": "Giuseppe Donatiello",
                        "slug": "Giuseppe-Donatiello",
                        "structuredName": {
                            "firstName": "Giuseppe",
                            "lastName": "Donatiello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Giuseppe Donatiello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706154"
                        ],
                        "name": "V. Bogorny",
                        "slug": "V.-Bogorny",
                        "structuredName": {
                            "firstName": "Vania",
                            "lastName": "Bogorny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Bogorny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39846670"
                        ],
                        "name": "C. G\u00e1rate",
                        "slug": "C.-G\u00e1rate",
                        "structuredName": {
                            "firstName": "Carolina",
                            "lastName": "G\u00e1rate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G\u00e1rate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708083"
                        ],
                        "name": "L. Alvares",
                        "slug": "L.-Alvares",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Alvares",
                            "middleNames": [
                                "Ot\u00e1vio"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Alvares"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144103389"
                        ],
                        "name": "F. Br\u00e9mond",
                        "slug": "F.-Br\u00e9mond",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Br\u00e9mond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Br\u00e9mond"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 161
                            }
                        ],
                        "text": "These features are usually extracted from whole frames [5, 13, 18, 20, 25], localized on a grid of image patches [24], or concentrated on pre-identified regions [6, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13266485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65e8364db21ed72de50d9afe60026d2a22962746",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a unified approach for abnormal behavior detection and group behavior analysis in video scenes. Existing approaches for abnormal behavior detection do either use trajectory-based or pixel-based methods. Unlike these approaches, we propose an integrated pipeline that incorporates the output of object trajectory analysis and pixel-based analysis for abnormal behavior inference. This enables to detect abnormal behaviors related to speed and direction of object trajectories, as well as complex behaviors related to finer motion of each object. By applying our approach on three different data sets, we show that our approach is able to detect several types of abnormal group behaviors with less number of false alarms compared with existing approaches."
            },
            "slug": "Toward-Abnormal-Trajectory-and-Event-Detection-in-Co\u015far-Donatiello",
            "title": {
                "fragments": [],
                "text": "Toward Abnormal Trajectory and Event Detection in Video Surveillance"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper proposes an integrated pipeline that incorporates the output of object trajectory analysis and pixel-based analysis for abnormal behavior inference and shows that this approach is able to detect several types of abnormal group behaviors with less number of false alarms compared with existing approaches."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3401864"
                        ],
                        "name": "Ryota Hinami",
                        "slug": "Ryota-Hinami",
                        "structuredName": {
                            "firstName": "Ryota",
                            "lastName": "Hinami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryota Hinami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144025741"
                        ],
                        "name": "Tao Mei",
                        "slug": "Tao-Mei",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Mei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Mei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700567"
                        ],
                        "name": "S. Satoh",
                        "slug": "S.-Satoh",
                        "structuredName": {
                            "firstName": "Shin\u2019ichi",
                            "lastName": "Satoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satoh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 161
                            }
                        ],
                        "text": "These features are usually extracted from whole frames [5, 13, 18, 20, 25], localized on a grid of image patches [24], or concentrated on pre-identified regions [6, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] proposed to use object, attribute, and action detection labels to understand the reason of abnormality scores."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 188
                            }
                        ],
                        "text": "With less than a hundred dimensions per frame on average, equal to a small fraction of the popular visual features for anomaly detection (ResNet features of 2048 [13], AlexNet fc7 of 4096 [14]), skeleton features still provide equal or better performance than current state-of-the-art methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20635214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "094ac7510d1723cb9c2da01db47291322aa29025",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of joint detection and recounting of abnormal events in videos. Recounting of abnormal events, i.e., explaining why they are judged to be abnormal, is an unexplored but critical task in video surveillance, because it helps human observers quickly judge if they are false alarms or not. To describe the events in the human-understandable form for event recounting, learning generic knowledge about visual concepts (e.g., object and action) is crucial. Although convolutional neural networks (CNNs) have achieved promising results in learning such concepts, it remains an open question as to how to effectively use CNNs for abnormal event detection, mainly due to the environment-dependent nature of the anomaly detection. In this paper, we tackle this problem by integrating a generic CNN model and environment-dependent anomaly detectors. Our approach first learns CNN with multiple visual tasks to exploit semantic information that is useful for detecting and recounting abnormal events. By appropriately plugging the model into anomaly detectors, we can detect and recount abnormal events while taking advantage of the discriminative power of CNNs. Our approach outperforms the state-of-the-art on Avenue and UCSD Ped2 benchmarks for abnormal event detection and also produces promising results of abnormal event recounting."
            },
            "slug": "Joint-Detection-and-Recounting-of-Abnormal-Events-Hinami-Mei",
            "title": {
                "fragments": [],
                "text": "Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper addresses the problem of joint detection and recounting of abnormal events in videos by integrating a generic CNN model and environment-dependent anomaly detectors and produces promising results of abnormal event recounting."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705557"
                        ],
                        "name": "Katerina Fragkiadaki",
                        "slug": "Katerina-Fragkiadaki",
                        "structuredName": {
                            "firstName": "Katerina",
                            "lastName": "Fragkiadaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katerina Fragkiadaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736651"
                        ],
                        "name": "S. Levine",
                        "slug": "S.-Levine",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Levine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2986395"
                        ],
                        "name": "Panna Felsen",
                        "slug": "Panna-Felsen",
                        "structuredName": {
                            "firstName": "Panna",
                            "lastName": "Felsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Panna Felsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "ft = ( xt, y i t ) [10, 23], implicitly merging the global and local factors together."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "To bridge this gap, in [10, 23] the input of the RNN is extended to the skeleton joint locations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "Recently, more effort has been invested into unsupervised learning of human motion in social settings [1, 12, 23] and single pose configuration [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 128024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ec7433aeb4777e7d5c903920ae945e5429d3bc4",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoiding drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units [31]."
            },
            "slug": "Recurrent-Network-Models-for-Human-Dynamics-Fragkiadaki-Levine",
            "title": {
                "fragments": [],
                "text": "Recurrent Network Models for Human Dynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The Encoder-Recurrent-Decoder (ERD) model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers that extends previous Long Short Term Memory models in the literature to jointly learn representations and their dynamics."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074878"
                        ],
                        "name": "Weixin Luo",
                        "slug": "Weixin-Luo",
                        "structuredName": {
                            "firstName": "Weixin",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weixin Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117934369"
                        ],
                        "name": "Wen Liu",
                        "slug": "Wen-Liu",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702868"
                        ],
                        "name": "Shenghua Gao",
                        "slug": "Shenghua-Gao",
                        "structuredName": {
                            "firstName": "Shenghua",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shenghua Gao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "These features are usually extracted from whole frames [5, 13, 18, 20, 25], localized on a grid of image patches [24], or concentrated on pre-identified regions [6, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "Some of these methods include Convolutional autoencoder [13], spatio-temporal autoencoder [5], 3D Convnet AE [27], and Temporally-coherent Sparse Coding StackedRNN [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "The ShanghaiTech Campus dataset [20] is considered one of the most comprehensive and realistic datasets for video anomaly detection currently available."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "We evaluate our method on two datasets for video anomaly detection: ShanghaiTech Campus [20] and CUHK Avenue [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19052864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99dff291f260b3cc3ff190106b0c2e3e685223a4",
            "isKey": true,
            "numCitedBy": 268,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Motivated by the capability of sparse coding based anomaly detection, we propose a Temporally-coherent Sparse Coding (TSC) where we enforce similar neighbouring frames be encoded with similar reconstruction coefficients. Then we map the TSC with a special type of stacked Recurrent Neural Network (sRNN). By taking advantage of sRNN in learning all parameters simultaneously, the nontrivial hyper-parameter selection to TSC can be avoided, meanwhile with a shallow sRNN, the reconstruction coefficients can be inferred within a forward pass, which reduces the computational cost for learning sparse coefficients. The contributions of this paper are two-fold: i) We propose a TSC, which can be mapped to a sRNN which facilitates the parameter optimization and accelerates the anomaly prediction. ii) We build a very large dataset which is even larger than the summation of all existing dataset for anomaly detection in terms of both the volume of data and the diversity of scenes. Extensive experiments on both a toy dataset and real datasets demonstrate that our TSC based and sRNN based method consistently outperform existing methods, which validates the effectiveness of our method."
            },
            "slug": "A-Revisit-of-Sparse-Coding-Based-Anomaly-Detection-Luo-Liu",
            "title": {
                "fragments": [],
                "text": "A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A Temporally-coherent Sparse Coding which can be mapped to a sRNN which facilitates the parameter optimization and accelerates the anomaly prediction and builds a very large dataset which is even larger than the summation of all existing dataset for anomaly detection in terms of both the volume of data and the diversity of scenes."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111867908"
                        ],
                        "name": "Yong Du",
                        "slug": "Yong-Du",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46956675"
                        ],
                        "name": "Y. Fu",
                        "slug": "Y.-Fu",
                        "structuredName": {
                            "firstName": "Yun",
                            "lastName": "Fu",
                            "middleNames": [
                                "Raymond"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693997"
                        ],
                        "name": "Liang Wang",
                        "slug": "Liang-Wang",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Wang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7] proposed to divide the skeleton joints into five parts, which are jointly modeled in a five-branch bidirectional neural network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 127
                            }
                        ],
                        "text": "It has been applied in multiple computer vision applications, mostly with supervised learning tasks such as action recognition [7] and person reidentification [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17938825,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de420f342b15087515d9750a6fccec1909173d96",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Motion characteristics of human actions can be represented by the position variation of skeleton joints. Traditional approaches generally extract the spatial-temporal representation of the skeleton sequences with well-designed hand-crafted features. In this paper, in order to recognize actions according to the relative motion between the limbs and the trunk, we propose an end-to-end hierarchical RNN for skeleton-based action recognition. We divide human skeleton into five main parts in terms of the human physical structure, and then feed them to five independent subnets for local feature extraction. After the following hierarchical feature fusion and extraction from local to global, dimensions of the final temporal dynamics representations are reduced to the same number of action categories in the corresponding data set through a single-layer perceptron. In addition, the output of the perceptron is temporally accumulated as the input of a softmax layer for classification. Random scale and rotation transformations are employed to improve the robustness during training. We compare with five other deep RNN variants derived from our model in order to verify the effectiveness of the proposed network. In addition, we compare with several other methods on motion capture and Kinect data sets. Furthermore, we evaluate the robustness of our model trained with random scale and rotation transformations for a multiview problem. Experimental results demonstrate that our model achieves the state-of-the-art performance with high computational efficiency."
            },
            "slug": "Representation-Learning-of-Temporal-Dynamics-for-Du-Fu",
            "title": {
                "fragments": [],
                "text": "Representation Learning of Temporal Dynamics for Skeleton-Based Action Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results demonstrate that this model achieves the state-of-the-art performance with high computational efficiency and the robustness of the model trained with random scale and rotation transformations for a multiview problem is evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304525"
                        ],
                        "name": "Alexandre Alahi",
                        "slug": "Alexandre-Alahi",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Alahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandre Alahi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957685"
                        ],
                        "name": "Kratarth Goel",
                        "slug": "Kratarth-Goel",
                        "structuredName": {
                            "firstName": "Kratarth",
                            "lastName": "Goel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kratarth Goel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34066479"
                        ],
                        "name": "Vignesh Ramanathan",
                        "slug": "Vignesh-Ramanathan",
                        "structuredName": {
                            "firstName": "Vignesh",
                            "lastName": "Ramanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vignesh Ramanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364487"
                        ],
                        "name": "Alexandre Robicquet",
                        "slug": "Alexandre-Robicquet",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Robicquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandre Robicquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702137"
                        ],
                        "name": "S. Savarese",
                        "slug": "S.-Savarese",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Savarese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Savarese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 102
                            }
                        ],
                        "text": "Recently, more effort has been invested into unsupervised learning of human motion in social settings [1, 12, 23] and single pose configuration [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 246
                            }
                        ],
                        "text": "Regarding feature representation and modeling, apart from traditional methods that rely on hand-crafted local features and state machines, several recent works proposed to use interacting recurrent networks for motion modeling in social settings [1, 12]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9854676,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e11a020f0d2942d09127daf1ce7e658d3bf67291",
            "isKey": false,
            "numCitedBy": 1551,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrians follow different trajectories to avoid obstacles and accommodate fellow pedestrians. Any autonomous vehicle navigating such a scene should be able to foresee the future positions of pedestrians and accordingly adjust its path to avoid collisions. This problem of trajectory prediction can be viewed as a sequence generation task, where we are interested in predicting the future trajectory of people based on their past positions. Following the recent success of Recurrent Neural Network (RNN) models for sequence prediction tasks, we propose an LSTM model which can learn general human movement and predict their future trajectories. This is in contrast to traditional approaches which use hand-crafted functions such as Social forces. We demonstrate the performance of our method on several public datasets. Our model outperforms state-of-the-art methods on some of these datasets. We also analyze the trajectories predicted by our model to demonstrate the motion behaviour learned by our model."
            },
            "slug": "Social-LSTM:-Human-Trajectory-Prediction-in-Crowded-Alahi-Goel",
            "title": {
                "fragments": [],
                "text": "Social LSTM: Human Trajectory Prediction in Crowded Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes an LSTM model which can learn general human movement and predict their future trajectories and outperforms state-of-the-art methods on some of these datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830034"
                        ],
                        "name": "Cewu Lu",
                        "slug": "Cewu-Lu",
                        "structuredName": {
                            "firstName": "Cewu",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cewu Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788070"
                        ],
                        "name": "Jianping Shi",
                        "slug": "Jianping-Shi",
                        "structuredName": {
                            "firstName": "Jianping",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianping Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729056"
                        ],
                        "name": "Jiaya Jia",
                        "slug": "Jiaya-Jia",
                        "structuredName": {
                            "firstName": "Jiaya",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaya Jia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6070091,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "869b17632ed4f19f93b3b58dcaa9f0b8e92108f3",
            "isKey": false,
            "numCitedBy": 602,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Speedy abnormal event detection meets the growing demand to process an enormous number of surveillance videos. Based on inherent redundancy of video structures, we propose an efficient sparse combination learning framework. It achieves decent performance in the detection phase without compromising result quality. The short running time is guaranteed because the new method effectively turns the original complicated problem to one in which only a few costless small-scale least square optimization steps are involved. Our method reaches high detection rates on benchmark datasets at a speed of 140-150 frames per second on average when computing on an ordinary desktop PC using MATLAB."
            },
            "slug": "Abnormal-Event-Detection-at-150-FPS-in-MATLAB-Lu-Shi",
            "title": {
                "fragments": [],
                "text": "Abnormal Event Detection at 150 FPS in MATLAB"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "An efficient sparse combination learning framework based on inherent redundancy of video structures achieves decent performance in the detection phase without compromising result quality and reaches high detection rates on benchmark datasets at a speed of 140-150 frames per second on average."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47060433"
                        ],
                        "name": "Zhe Cao",
                        "slug": "Zhe-Cao",
                        "structuredName": {
                            "firstName": "Zhe",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhe Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145386542"
                        ],
                        "name": "T. Simon",
                        "slug": "T.-Simon",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Simon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797981"
                        ],
                        "name": "Shih-En Wei",
                        "slug": "Shih-En-Wei",
                        "structuredName": {
                            "firstName": "Shih-En",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-En Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774867"
                        ],
                        "name": "Yaser Sheikh",
                        "slug": "Yaser-Sheikh",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Sheikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaser Sheikh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 87
                            }
                        ],
                        "text": "However, the reliability of these skeleton detection modules are constantly increasing [4, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16224674,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e8db1519245426f3a78752a3d8360484f4626b1",
            "isKey": false,
            "numCitedBy": 3820,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency."
            },
            "slug": "Realtime-Multi-person-2D-Pose-Estimation-Using-Part-Cao-Simon",
            "title": {
                "fragments": [],
                "text": "Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This work presents an approach to efficiently detect the 2D pose of multiple people in an image using a nonparametric representation, which it refers to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9335567"
                        ],
                        "name": "Amani Elaoud",
                        "slug": "Amani-Elaoud",
                        "structuredName": {
                            "firstName": "Amani",
                            "lastName": "Elaoud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amani Elaoud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747242"
                        ],
                        "name": "W. Barhoumi",
                        "slug": "W.-Barhoumi",
                        "structuredName": {
                            "firstName": "Walid",
                            "lastName": "Barhoumi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Barhoumi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2641251"
                        ],
                        "name": "Hassen Drira",
                        "slug": "Hassen-Drira",
                        "structuredName": {
                            "firstName": "Hassen",
                            "lastName": "Drira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hassen Drira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684187"
                        ],
                        "name": "E. Zagrouba",
                        "slug": "E.-Zagrouba",
                        "structuredName": {
                            "firstName": "Ezzeddine",
                            "lastName": "Zagrouba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Zagrouba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "It has been applied in multiple computer vision applications, mostly with supervised learning tasks such as action recognition [7] and person reidentification [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10471678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de49115bd6ea876231837822afca6c653cf1033",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we are interested in people re-identification using skeleton information provided by a consumer RGB-D sensor. We perform the modelling and the analysis of human motion by focusing on 3D human joints given by skeletons. In fact, the motion dynamic is modeled by projecting skeleton information on Grassmann manifold. Moreover, in order to define the identity of a test trajectory, we compare it against a labeled trajectory database while using an unsupervised similarity assessment procedure. Indeed, the main contribution of this work resides in the introduced distance that combines temporal information as well as global and local geometrical ones. Realized experiments on standard datasets prove that the proposed method performs accurately even though it does not assume any prior knowledge."
            },
            "slug": "Analysis-of-Skeletal-Shape-Trajectories-for-Person-Elaoud-Barhoumi",
            "title": {
                "fragments": [],
                "text": "Analysis of Skeletal Shape Trajectories for Person Re-Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The main contribution of this work resides in the introduced distance that combines temporal information as well as global and local geometrical ones in the motion dynamic of human motion."
            },
            "venue": {
                "fragments": [],
                "text": "ACIVS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40940512"
                        ],
                        "name": "Jun Liu",
                        "slug": "Jun-Liu",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096527"
                        ],
                        "name": "G. Wang",
                        "slug": "G.-Wang",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7667912"
                        ],
                        "name": "Ling-yu Duan",
                        "slug": "Ling-yu-Duan",
                        "structuredName": {
                            "firstName": "Ling-yu",
                            "lastName": "Duan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ling-yu Duan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2844804"
                        ],
                        "name": "Kamila Abdiyeva",
                        "slug": "Kamila-Abdiyeva",
                        "structuredName": {
                            "firstName": "Kamila",
                            "lastName": "Abdiyeva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kamila Abdiyeva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711097"
                        ],
                        "name": "A. Kot",
                        "slug": "A.-Kot",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Kot",
                            "middleNames": [
                                "Chichung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kot"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ion in video scenes is an important factor for studying social behavior. It has been applied in multiple computer vision applications, mostly with supervised learning tasks such as action recognition [8, 19] and person re-identi\ufb01cation [9]. Recently, more effort has been invested into unsupervised learning of human motion in social settings [1, 13, 25] and single pose con\ufb01guration [11]. In this work, we "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3225725,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5202b026976b9057300eb30ee1804f8ae30a4c42",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "Human action recognition in 3D skeleton sequences has attracted a lot of research attention. Recently, long short-term memory (LSTM) networks have shown promising performance in this task due to their strengths in modeling the dependencies and dynamics in sequential data. As not all skeletal joints are informative for action recognition, and the irrelevant joints often bring noise which can degrade the performance, we need to pay more attention to the informative ones. However, the original LSTM network does not have explicit attention ability. In this paper, we propose a new class of LSTM network, global context-aware attention LSTM, for skeleton-based action recognition, which is capable of selectively focusing on the informative joints in each frame by using a global context memory cell. To further improve the attention capability, we also introduce a recurrent attention mechanism, with which the attention performance of our network can be enhanced progressively. Besides, a two-stream framework, which leverages coarse-grained attention and fine-grained attention, is also introduced. The proposed method achieves state-of-the-art performance on five challenging datasets for skeleton-based action recognition."
            },
            "slug": "Skeleton-Based-Human-Action-Recognition-With-Global-Liu-Wang",
            "title": {
                "fragments": [],
                "text": "Skeleton-Based Human Action Recognition With Global Context-Aware Attention LSTM Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new class of L STM network is proposed, global context-aware attention LSTM, for skeleton-based action recognition, which is capable of selectively focusing on the informative joints in each frame by using a global context memory cell."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51033208"
                        ],
                        "name": "Bingbin Liu",
                        "slug": "Bingbin-Liu",
                        "structuredName": {
                            "firstName": "Bingbin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bingbin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34149749"
                        ],
                        "name": "S. Yeung",
                        "slug": "S.-Yeung",
                        "structuredName": {
                            "firstName": "Serena",
                            "lastName": "Yeung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yeung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34613203"
                        ],
                        "name": "Edward Chou",
                        "slug": "Edward-Chou",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Chou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38485317"
                        ],
                        "name": "De-An Huang",
                        "slug": "De-An-Huang",
                        "structuredName": {
                            "firstName": "De-An",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "De-An Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9200530"
                        ],
                        "name": "Juan Carlos Niebles",
                        "slug": "Juan-Carlos-Niebles",
                        "structuredName": {
                            "firstName": "Juan Carlos",
                            "lastName": "Niebles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Carlos Niebles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 101
                            }
                        ],
                        "text": "It also reflects the current trend of architectures being modular, with multiple independent modules [2, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52953290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73d1b35cd28befe845fcb60a3fed67c9fb7793ad",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "A major challenge in computer vision is scaling activity understanding to the long tail of complex activities without requiring collecting large quantities of data for new actions. The task of video retrieval using natural language descriptions seeks to address this through rich, unconstrained supervision about complex activities. However, while this formulation offers hope of leveraging underlying compositional structure in activity descriptions, existing approaches typically do not explicitly model compositional reasoning. In this work, we introduce an approach for explicitly and dynamically reasoning about compositional natural language descriptions of activity in videos. We take a modular neural network approach that, given a natural language query, extracts the semantic structure to assemble a compositional neural network layout and corresponding network modules. We show that this approach is able to achieve state-of-the-art results on the DiDeMo video retrieval dataset."
            },
            "slug": "Temporal-Modular-Networks-for-Retrieving-Complex-in-Liu-Yeung",
            "title": {
                "fragments": [],
                "text": "Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work takes a modular neural network approach that, given a natural language query, extracts the semantic structure to assemble a compositional neural network layout and corresponding network modules and shows that it is able to achieve state-of-the-art results on the DiDeMo video retrieval dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144543406"
                        ],
                        "name": "Ruben Villegas",
                        "slug": "Ruben-Villegas",
                        "structuredName": {
                            "firstName": "Ruben",
                            "lastName": "Villegas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruben Villegas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768964"
                        ],
                        "name": "Jimei Yang",
                        "slug": "Jimei-Yang",
                        "structuredName": {
                            "firstName": "Jimei",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimei Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8299168"
                        ],
                        "name": "Yuliang Zou",
                        "slug": "Yuliang-Zou",
                        "structuredName": {
                            "firstName": "Yuliang",
                            "lastName": "Zou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuliang Zou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144832576"
                        ],
                        "name": "Sungryull Sohn",
                        "slug": "Sungryull-Sohn",
                        "structuredName": {
                            "firstName": "Sungryull",
                            "lastName": "Sohn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sungryull Sohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10668384"
                        ],
                        "name": "Xunyu Lin",
                        "slug": "Xunyu-Lin",
                        "structuredName": {
                            "firstName": "Xunyu",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xunyu Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "ft = ( xt, y i t ) [10, 23], implicitly merging the global and local factors together."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "To bridge this gap, in [10, 23] the input of the RNN is extended to the skeleton joint locations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 102
                            }
                        ],
                        "text": "Recently, more effort has been invested into unsupervised learning of human motion in social settings [1, 12, 23] and single pose configuration [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15117981,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f230cacc511b17b491bf3d90015bbbf85b9ef6af",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a hierarchical approach for making long-term predictions of future frames. To avoid inherent compounding errors in recursive pixel-level prediction, we propose to first estimate high-level structure in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, we construct the future frames without having to observe any of the pixel-level predictions. Long-term video prediction is difficult to perform by recurrently observing the predicted frames because the small errors in pixel space exponentially amplify as predictions are made deeper into the future. Our approach prevents pixel-level error propagation from happening by removing the need to observe the predicted frames. Our model is built with a combination of LSTM and analogy based encoder-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art."
            },
            "slug": "Learning-to-Generate-Long-term-Future-via-Villegas-Yang",
            "title": {
                "fragments": [],
                "text": "Learning to Generate Long-term Future via Hierarchical Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This model is built with a combination of LSTM and analogy based encoder-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively, which prevents pixel-level error propagation from happening by removing the need to observe the predicted frames."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2711409"
                        ],
                        "name": "Elman Mansimov",
                        "slug": "Elman-Mansimov",
                        "structuredName": {
                            "firstName": "Elman",
                            "lastName": "Mansimov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elman Mansimov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11699847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d",
            "isKey": false,
            "numCitedBy": 1975,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations (\"percepts\") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance."
            },
            "slug": "Unsupervised-Learning-of-Video-Representations-Srivastava-Mansimov",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Video Representations using LSTMs"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This work uses Long Short Term Memory networks to learn representations of video sequences and evaluates the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25445698"
                        ],
                        "name": "Agrim Gupta",
                        "slug": "Agrim-Gupta",
                        "structuredName": {
                            "firstName": "Agrim",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Agrim Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702137"
                        ],
                        "name": "S. Savarese",
                        "slug": "S.-Savarese",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Savarese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Savarese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304525"
                        ],
                        "name": "Alexandre Alahi",
                        "slug": "Alexandre-Alahi",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Alahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandre Alahi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 102
                            }
                        ],
                        "text": "Recently, more effort has been invested into unsupervised learning of human motion in social settings [1, 12, 23] and single pose configuration [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 246
                            }
                        ],
                        "text": "Regarding feature representation and modeling, apart from traditional methods that rely on hand-crafted local features and state machines, several recent works proposed to use interacting recurrent networks for motion modeling in social settings [1, 12]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4461350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49c076bbc21ab76720b610ab3840c15ce3dc4e6c",
            "isKey": false,
            "numCitedBy": 857,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity."
            },
            "slug": "Social-GAN:-Socially-Acceptable-Trajectories-with-Gupta-Johnson",
            "title": {
                "fragments": [],
                "text": "Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people, and outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122851212"
                        ],
                        "name": "Haoshu Fang",
                        "slug": "Haoshu-Fang",
                        "structuredName": {
                            "firstName": "Haoshu",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haoshu Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483323"
                        ],
                        "name": "S. Xie",
                        "slug": "S.-Xie",
                        "structuredName": {
                            "firstName": "Shuqin",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5068280"
                        ],
                        "name": "Yu-Wing Tai",
                        "slug": "Yu-Wing-Tai",
                        "structuredName": {
                            "firstName": "Yu-Wing",
                            "lastName": "Tai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Wing Tai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830034"
                        ],
                        "name": "Cewu Lu",
                        "slug": "Cewu-Lu",
                        "structuredName": {
                            "firstName": "Cewu",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cewu Lu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6529517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c70c6dafc7276177225f4604cb285db07881aa6f",
            "isKey": false,
            "numCitedBy": 779,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve 76:7 mAP on the MPII (multi person) dataset[3]. Our model and source codes are made publicly available."
            },
            "slug": "RMPE:-Regional-Multi-person-Pose-Estimation-Fang-Xie",
            "title": {
                "fragments": [],
                "text": "RMPE: Regional Multi-person Pose Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes and can achieve 76:7 mAP on the MPII (multi person) dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159726"
                        ],
                        "name": "David Bau",
                        "slug": "David-Bau",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Bau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Bau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145291669"
                        ],
                        "name": "Bolei Zhou",
                        "slug": "Bolei-Zhou",
                        "structuredName": {
                            "firstName": "Bolei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bolei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "This limitation can be amplified through processing in deep neural networks [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 378410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "744464cd6fa8341633cd3b5d378faab18a3b543a",
            "isKey": false,
            "numCitedBy": 899,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems. We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power."
            },
            "slug": "Network-Dissection:-Quantifying-Interpretability-of-Bau-Zhou",
            "title": {
                "fragments": [],
                "text": "Network Dissection: Quantifying Interpretability of Deep Visual Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work uses the proposed Network Dissection method to test the hypothesis that interpretability is an axis-independent property of the representation space, then applies the method to compare the latent representations of various networks when trained to solve different classification problems."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112400"
                        ],
                        "name": "Jacob Andreas",
                        "slug": "Jacob-Andreas",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Andreas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Andreas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 101
                            }
                        ],
                        "text": "It also reflects the current trend of architectures being modular, with multiple independent modules [2, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5276660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21c99706bb26e9012bfb4d8d48009a3d45af59b2",
            "isKey": false,
            "numCitedBy": 733,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual question answering is fundamentally compositional in nature-a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes."
            },
            "slug": "Neural-Module-Networks-Andreas-Rohrbach",
            "title": {
                "fragments": [],
                "text": "Neural Module Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.)."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3035541"
                        ],
                        "name": "Klaus Greff",
                        "slug": "Klaus-Greff",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Greff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klaus Greff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100612"
                        ],
                        "name": "R. Srivastava",
                        "slug": "R.-Srivastava",
                        "structuredName": {
                            "firstName": "Rupesh",
                            "lastName": "Srivastava",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2865775"
                        ],
                        "name": "J. Koutn\u00edk",
                        "slug": "J.-Koutn\u00edk",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Koutn\u00edk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koutn\u00edk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714059"
                        ],
                        "name": "Bas R. Steunebrink",
                        "slug": "Bas-R.-Steunebrink",
                        "structuredName": {
                            "firstName": "Bas",
                            "lastName": "Steunebrink",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bas R. Steunebrink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "We use Gated Recurrent Units (GRU) [4] in every segment of MPED-RNN for its simplicity and similar performance to LSTM [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "This structure is similar to the composite LSTM autoencoder (LSTM AE) of Srivastava et al. [22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "However, unlike LSTM AE, MPED-RNN does not only model the dynamics of each individual component, but also the interdependencies between them through a cross-branch message-passing mechanism."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3356463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081",
            "isKey": true,
            "numCitedBy": 3295,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ( $\\approx 15$  years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment."
            },
            "slug": "LSTM:-A-Search-Space-Odyssey-Greff-Srivastava",
            "title": {
                "fragments": [],
                "text": "LSTM: A Search Space Odyssey"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling, and observes that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks and Learning Systems"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3310376"
                        ],
                        "name": "A. Zimek",
                        "slug": "A.-Zimek",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Zimek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zimek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26011531"
                        ],
                        "name": "Erich Schubert",
                        "slug": "Erich-Schubert",
                        "structuredName": {
                            "firstName": "Erich",
                            "lastName": "Schubert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erich Schubert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688561"
                        ],
                        "name": "H. Kriegel",
                        "slug": "H.-Kriegel",
                        "structuredName": {
                            "firstName": "Hans-Peter",
                            "lastName": "Kriegel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kriegel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "Unfortunately, pixel-based features are high-dimensional unstructured signals sensitive to noise, that mask important information about the scene [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6724536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b65fc8f5e7329f0476bc7280f0ef6b91a8c8484b",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 164,
            "paperAbstract": {
                "fragments": [],
                "text": "High\u2010dimensional data in Euclidean space pose special challenges to data mining algorithms. These challenges are often indiscriminately subsumed under the term \u2018curse of dimensionality\u2019, more concrete aspects being the so\u2010called \u2018distance concentration effect\u2019, the presence of irrelevant attributes concealing relevant information, or simply efficiency issues. In about just the last few years, the task of unsupervised outlier detection has found new specialized solutions for tackling high\u2010dimensional data in Euclidean space. These approaches fall under mainly two categories, namely considering or not considering subspaces (subsets of attributes) for the definition of outliers. The former are specifically addressing the presence of irrelevant attributes, the latter do consider the presence of irrelevant attributes implicitly at best but are more concerned with general issues of efficiency and effectiveness. Nevertheless, both types of specialized outlier detection algorithms tackle challenges specific to high\u2010dimensional data. In this survey article, we discuss some important aspects of the \u2018curse of dimensionality\u2019 in detail and survey specialized algorithms for outlier detection from both categories. \u00a9 2012 Wiley Periodicals, Inc. Statistical Analysis and Data Mining, 2012"
            },
            "slug": "A-survey-on-unsupervised-outlier-detection-in-data-Zimek-Schubert",
            "title": {
                "fragments": [],
                "text": "A survey on unsupervised outlier detection in high\u2010dimensional numerical data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This survey article discusses some important aspects of the \u2018curse of dimensionality\u2019 in detail and surveys specialized algorithms for outlier detection from both categories."
            },
            "venue": {
                "fragments": [],
                "text": "Stat. Anal. Data Min."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158246"
                        ],
                        "name": "Bart van Merrienboer",
                        "slug": "Bart-van-Merrienboer",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Merrienboer",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bart van Merrienboer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1854385"
                        ],
                        "name": "\u00c7aglar G\u00fcl\u00e7ehre",
                        "slug": "\u00c7aglar-G\u00fcl\u00e7ehre",
                        "structuredName": {
                            "firstName": "\u00c7aglar",
                            "lastName": "G\u00fcl\u00e7ehre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c7aglar G\u00fcl\u00e7ehre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335364"
                        ],
                        "name": "Dzmitry Bahdanau",
                        "slug": "Dzmitry-Bahdanau",
                        "structuredName": {
                            "firstName": "Dzmitry",
                            "lastName": "Bahdanau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dzmitry Bahdanau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076086"
                        ],
                        "name": "Fethi Bougares",
                        "slug": "Fethi-Bougares",
                        "structuredName": {
                            "firstName": "Fethi",
                            "lastName": "Bougares",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fethi Bougares"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5590763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b544dfe355a5070b60986319a3f51fb45d1348e",
            "isKey": false,
            "numCitedBy": 15050,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases."
            },
            "slug": "Learning-Phrase-Representations-using-RNN-for-Cho-Merrienboer",
            "title": {
                "fragments": [],
                "text": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Qualitatively, the proposed RNN Encoder\u2010Decoder model learns a semantically and syntactically meaningful representation of linguistic phrases."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2941401"
                        ],
                        "name": "H. Kuhn",
                        "slug": "H.-Kuhn",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Kuhn",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kuhn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 241
                            }
                        ],
                        "text": "To track the skeletons across a video, we combined sparse optical flow with the detected skeletons to assign similarity scores between pairs of skeletons in neighboring frames, and solved the assignment problem using the Hungarian algorithm [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9426884,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "b6a0f30260302a2001da9999096cfdd89bc1f7fb",
            "isKey": false,
            "numCitedBy": 4506,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper has been presented with the Best Paper Award. It will appear in print in Volume 52, No. 1, February 2005."
            },
            "slug": "The-Hungarian-method-for-the-assignment-problem-Kuhn",
            "title": {
                "fragments": [],
                "text": "The Hungarian Method for the Assignment Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper has always been one of my favorite \u201cchildren,\u201d combining as it does elements of the duality of linear programming and combinatorial tools from graph theory."
            },
            "venue": {
                "fragments": [],
                "text": "50 Years of Integer Programming"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Viral Bhalodia , and Nuno Vasconcelos . Anomaly detection in crowded scenes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "To detect skeletons in the videos, we utilized Alpha Pose [10] to independently detect skeletons in each video frame."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 47
                            }
                        ],
                        "text": "To detect skeletons in the videos, we utilized Alpha Pose [9] to independently detect skeletons in each video frame."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 87
                            }
                        ],
                        "text": "However, the reliability of these skeleton detection modules are constantly increasing [4, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "RMPE: Regional multiperson pose estimation"
            },
            "venue": {
                "fragments": [],
                "text": "In IEEE International Conference on Computer Vision,"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Viral Bhalodia, and Nuno Vasconcelos. Anomaly detection in crowded scenes"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 14
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Regularity-in-Skeleton-Trajectories-for-in-Morais-Le/db20d81d40243d66ff90f11b5c6f058d43d3701f?sort=total-citations"
}