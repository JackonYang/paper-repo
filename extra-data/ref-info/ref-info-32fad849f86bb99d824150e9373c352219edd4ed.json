{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304025"
                        ],
                        "name": "Chiraz BenAbdelkader",
                        "slug": "Chiraz-BenAbdelkader",
                        "structuredName": {
                            "firstName": "Chiraz",
                            "lastName": "BenAbdelkader",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chiraz BenAbdelkader"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "Recent work by Ghanem and Davis [10] tackles detecting abandoned baggage by comparing the temporal template of the person before approaching a Region of Interest (ROI) and after leaving it."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "Later work by Benabdelkader and Davis [6] expanded the work of Haritaoglu et."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 14
                            }
                        ],
                        "text": "Later work by Benabdelkader and Davis [6] expanded the work of Haritaoglu et al. by dividing the person\u2019s body horizontally into three slices."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9608181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b25c2dec07054851d1a1dc087884e9575e3def9",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method to detect instances of a walking person carrying an object seen from a stationary camera. We take a correspondence-free motion-based recognition approach, that exploits known shape and periodicity cues of the human silhouette shape. Specifically, we subdivide the binary silhouette into four horizontal segments, and analyze the temporal behavior of the bounding box width over each segment. We posit that the periodicity and amplitudes of these time series satisfy certain criteria for a natural walking person, and deviations therefrom are an indication that the person might be carrying an object. The method is tested on 41 360/spl times/240 color outdoor sequences of people walking and carrying objects at various poses and camera viewpoints. A correct detection rate of 85% and a false alarm rate of 12% are obtained."
            },
            "slug": "Detection-of-people-carrying-objects-:-a-approach-BenAbdelkader-Davis",
            "title": {
                "fragments": [],
                "text": "Detection of people carrying objects : a motion-based recognition approach"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A correspondence-free motion-based recognition approach is taken, that exploits known shape and periodicity cues of the human silhouette shape, and subdivides the binary silhouette into four horizontal segments, and analyzes the temporal behavior of the bounding box width over each segment."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Fifth IEEE International Conference on Automatic Face Gesture Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2494983"
                        ],
                        "name": "N. Ghanem",
                        "slug": "N.-Ghanem",
                        "structuredName": {
                            "firstName": "Nagia",
                            "lastName": "Ghanem",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ghanem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Recent work by Ghanem and Davis [10] tackles detecting abandoned baggage by comparing the temporal template of the person before approaching a Region of Interest (ROI) and after leaving it."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 32
                            }
                        ],
                        "text": "Later work by Benabdelkader and Davis [6] expanded the work of Haritaoglu et al. by dividing the person\u2019s body horizontally into three slices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 68
                            }
                        ],
                        "text": "Our method uses the temporal template but differs from earlier work [1,10] by matching the generated temporal template against an exemplar temporal template generated offline from a 3D model of a walking person."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6596868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c7be9265439688b895a655b896fb66130fd25e2",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a machine learning approach to detect changes in human appearance between instances of the same person that may be taken with different cameras, but over short periods of time. For each video sequence of the person, we approximately align each frame in the sequence and then generate a set of features that captures the differences between the two sequences. The features are the occupancy difference map, the codeword frequency difference map (based on a vector quantization of the set of colors and frequencies) at each aligned pixel and the histogram intersection map. A boosting technique is then applied to learn the most discriminative set of features, and these features are then used to train a support vector machine classifier to recognize significant appearance changes. We apply our approach to the problem of left package detection. We train the classifiers on a laboratory database of videos in which people are seen with and without common articles that people carry - backpacks and suitcases. We test the approach on some real airport video sequences. Moving to the real world videos requires addressing additional problems, including the view selection problem and the frame selection problem."
            },
            "slug": "Human-Appearance-Change-Detection-Ghanem-Davis",
            "title": {
                "fragments": [],
                "text": "Human Appearance Change Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A machine learning approach to detect changes in human appearance between instances of the same person that may be taken with different cameras, but over short periods of time, and applies it to the problem of left package detection."
            },
            "venue": {
                "fragments": [],
                "text": "14th International Conference on Image Analysis and Processing (ICIAP 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2798041"
                        ],
                        "name": "I. Haritaoglu",
                        "slug": "I.-Haritaoglu",
                        "structuredName": {
                            "firstName": "Ismail",
                            "lastName": "Haritaoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Haritaoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145952419"
                        ],
                        "name": "Ross Cutler",
                        "slug": "Ross-Cutler",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Cutler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross Cutler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298122"
                        ],
                        "name": "D. Harwood",
                        "slug": "D.-Harwood",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Harwood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harwood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "We have re-implemented the earlier state of the art method [1] and demonstrate a substantial improvement in performance for the new method on the challenging PETS2006 dataset [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "In common with earlier work [1,2] on the same problem, the method starts by averaging aligned foreground regions of a walking pedestrian to produce a representation of motion and shape (known as a temporal template) that has some immunity to noise in foreground segmentations and phase of the walking cycle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 176
                            }
                        ],
                        "text": "Backpack then distinguishes between blobs representing carried objects and those being parts of limbs by analyzing the periodicity of the horizontal projection histograms (See [1] for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 68
                            }
                        ],
                        "text": "Our method uses the temporal template but differs from earlier work [1,10] by matching the generated temporal template against an exemplar temporal template generated offline from a 3D model of a walking person."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Backpack assumes the frequency of an asymmetric blob that represents a limb is numerically comparable to that of the full body."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 247
                            }
                        ],
                        "text": "We assume a static background and address errors in foreground segmentations due to noise and partial occlusions, by aligning and averaging segmentations to generate a so-called \u2018temporal-template\u2019 - this representation was originally proposed in [1] for the same application."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 13
                            }
                        ],
                        "text": "The Backpack [1,2] system detects the presence of carried objects from short video sequences of pedestrians (typically lasting a few seconds) by assuming the pedestrian\u2019s silhouette is symmetric when a bag is not being carried, and that people exhibit periodic motion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 39
                            }
                        ],
                        "text": "From our own evaluation, errors in the Backpack method arise from four sources."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 74
                            }
                        ],
                        "text": "We compare our re-implementation of Backpack as specified in their papers [1,2] with our proposed method (Section 3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2490945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad2e0f8d4cbd329bb393303d91ac66c89f3ab139",
            "isKey": true,
            "numCitedBy": 135,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We described a video-rate surveillance algorithm to detect and track people from a stationary camera, and to determine if they are carrying objects or moving unencumbered. The contribution of the paper is the shape analysis algorithm that both determines if a person is carrying an object and segments the object from the person so that it can be tracked, e.g., during an exchange of objects between two people. As the object is segmented an appearance model of the object is constructed. The method combines periodic motion estimation with static symmetry analysis of the silhouettes of a person in each frame of the sequence. Experimental results demonstrate robustness and real-time performance of the proposed algorithm."
            },
            "slug": "Backpack:-detection-of-people-carrying-objects-Haritaoglu-Cutler",
            "title": {
                "fragments": [],
                "text": "Backpack: Detection of People Carrying Objects Using Silhouettes"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A video-rate surveillance algorithm to detect and track people from a stationary camera, and to determine if they are carrying objects or moving unencumbered is described."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34767834"
                        ],
                        "name": "D. Magee",
                        "slug": "D.-Magee",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Magee",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Magee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "a generic tracker [14] to retrieve foreground segmentations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13206303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ceed7b394e5e39bcda31e5ac84b5681ba97cb8b",
            "isKey": false,
            "numCitedBy": 265,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Tracking-multiple-vehicles-using-foreground,-and-Magee",
            "title": {
                "fragments": [],
                "text": "Tracking multiple vehicles using foreground, background and motion models"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070926374"
                        ],
                        "name": "M. Dimitrijevic",
                        "slug": "M.-Dimitrijevic",
                        "structuredName": {
                            "firstName": "Miodrag",
                            "lastName": "Dimitrijevic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dimitrijevic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689738"
                        ],
                        "name": "V. Lepetit",
                        "slug": "V.-Lepetit",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Lepetit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lepetit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Several exemplars, corresponding to different views of a walking person, were generated from reusable silhouettes used successfully for pose detection [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "A set of exemplars for eight viewing directions was created using the dataset of silhouettes gathered at the Swiss Federal Institute of Technology (EPFL) [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 163
                            }
                        ],
                        "text": "This dataset is comprised of all the silhouettes of the mapped Maya model, and has previously been used for pose detection, 3D reconstruction and gait recognition [11,13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "We would like to thank Miodrag Dimitrijevic at the CVLAB, EPFL and his colleagues for providing the dataset of silhouettes used in our research."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2185019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3d02cc840a037a0c97a3c995972eee93b6f69bd",
            "isKey": true,
            "numCitedBy": 86,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Human-body-pose-detection-using-Bayesian-templates-Dimitrijevic-Lepetit",
            "title": {
                "fragments": [],
                "text": "Human body pose detection using Bayesian spatio-temporal templates"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2798041"
                        ],
                        "name": "I. Haritaoglu",
                        "slug": "I.-Haritaoglu",
                        "structuredName": {
                            "firstName": "Ismail",
                            "lastName": "Haritaoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Haritaoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298122"
                        ],
                        "name": "D. Harwood",
                        "slug": "D.-Harwood",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Harwood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harwood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our method starts by creating the temporal template from a sequence of tracked pedestrians as proposed by Haritaoglu et al. [ 2 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Backpack [1, 2 ] system detects the presence of carried objects from short video sequences of pedestrians (typically lasting a few seconds) by assuming the pedestrian\u2019s silhouette is symmetric when a bag is not being carried, and that people exhibit periodic motion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In common with earlier work [1, 2 ] on the same problem, the method starts by averaging aligned foreground regions of a walking pedestrian to produce a representation of motion and shape (known as a temporal template )t hat has some immunity to noise in foreground segmentations and phase of the walking cycle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We compare our re-implementation of Backpack as specified in their papers [1, 2 ] with our proposed method (Section 3). To ensure fair comparison, we use the same temporal templates as the input for both methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6837802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "518597d91ed49c28f5cf3f0a0b05609568b7e084",
            "isKey": true,
            "numCitedBy": 2784,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "W/sup 4/ is a real time visual surveillance system for detecting and tracking multiple people and monitoring their activities in an outdoor environment. It operates on monocular gray-scale video imagery, or on video imagery from an infrared camera. W/sup 4/ employs a combination of shape analysis and tracking to locate people and their parts (head, hands, feet, torso) and to create models of people's appearance so that they can be tracked through interactions such as occlusions. It can determine whether a foreground region contains multiple people and can segment the region into its constituent people and track them. W/sup 4/ can also determine whether people are carrying objects, and can segment objects from their silhouettes, and construct appearance models for them so they can be identified in subsequent frames. W/sup 4/ can recognize events between people and objects, such as depositing an object, exchanging bags, or removing an object. It runs at 25 Hz for 320/spl times/240 resolution images on a 400 MHz dual-Pentium II PC."
            },
            "slug": "W4:-Real-Time-Surveillance-of-People-and-Their-Haritaoglu-Harwood",
            "title": {
                "fragments": [],
                "text": "W4: Real-Time Surveillance of People and Their Activities"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "W/sup 4/ employs a combination of shape analysis and tracking to locate people and their parts and to create models of people's appearance so that they can be tracked through interactions such as occlusions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074819081"
                        ],
                        "name": "A. Fossati",
                        "slug": "A.-Fossati",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Fossati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fossati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070926374"
                        ],
                        "name": "M. Dimitrijevic",
                        "slug": "M.-Dimitrijevic",
                        "structuredName": {
                            "firstName": "Miodrag",
                            "lastName": "Dimitrijevic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dimitrijevic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689738"
                        ],
                        "name": "V. Lepetit",
                        "slug": "V.-Lepetit",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Lepetit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lepetit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 163
                            }
                        ],
                        "text": "This dataset is comprised of all the silhouettes of the mapped Maya model, and has previously been used for pose detection, 3D reconstruction and gait recognition [11,13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8587499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4454103bdbd4b9c8a5ded6c32a8eac3d3bf486f3",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We combine detection and tracking techniques to achieve robust 3-D motion recovery of people seen from arbitrary viewpoints by a single and potentially moving camera. We rely on detecting key postures, which can be done reliably, using a motion model to infer 3-D poses between consecutive detections, and finally refining them over the whole sequence using a generative model. We demonstrate our approach in the case of people walking against cluttered backgrounds and filmed using a moving camera, which precludes the use of simple background subtraction techniques. In this case, the easy-to-detect posture is the one that occurs at the end of each step when people have their legs furthest apart."
            },
            "slug": "Bridging-the-Gap-between-Detection-and-Tracking-for-Fossati-Dimitrijevic",
            "title": {
                "fragments": [],
                "text": "Bridging the Gap between Detection and Tracking for 3D Monocular Video-Based Motion Capture"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work combines detection and tracking techniques to achieve robust 3-D motion recovery of people seen from arbitrary viewpoints by a single and potentially moving camera, and relies on detecting key postures, which can be done reliably, using a motion model to infer3-D poses between consecutive detections, and refining them over the whole sequence using a generative model."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143719920"
                        ],
                        "name": "D. Tao",
                        "slug": "D.-Tao",
                        "structuredName": {
                            "firstName": "Dacheng",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67180560"
                        ],
                        "name": "Xuelong Li",
                        "slug": "Xuelong-Li",
                        "structuredName": {
                            "firstName": "Xuelong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuelong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144555237"
                        ],
                        "name": "S. Maybank",
                        "slug": "S.-Maybank",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Maybank",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Maybank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748808"
                        ],
                        "name": "Xindong Wu",
                        "slug": "Xindong-Wu",
                        "structuredName": {
                            "firstName": "Xindong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xindong Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] tries to detect pedestrians carrying heavy objects by performing gait analysis using General Tensor Discriminant Analysis (GTDA), and was tested on the USF HumanID gait analysis dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1630778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a0870355a4d40fbf31a9f5f7ebc11119eeede4e",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "A person\u2019s gait changes when he or she is carrying an object such as a bag, suitcase or rucksack. As a result, human identification and tracking are made more difficult because the averaged gait image is too simple to represent the carrying status. Therefore, in this paper we first introduce a set of Gabor based human gait appearance models, because Gabor functions are similar to the receptive field profiles in the mammalian cortical simple cells. The very high dimensionality of the feature space makes training difficult. In order to solve this problem we propose a general tensor discriminant analysis (GTDA), which seamlessly incorporates the object (Gabor based human gait appearance model) structure information as a natural constraint. GTDA differs from the previous tensor based discriminant analysis methods in that the training converges. Existing methods fail to converge in the training stage. This makes them unsuitable for practical tasks. Experiments are carried out on the USF baseline data set to recognize a human\u2019s ID from the gait silhouette. The proposed Gabor gait incorporated with GTDA is demonstrated to significantly outperform the existing appearance-based methods."
            },
            "slug": "Human-Carrying-Status-in-Visual-Surveillance-Tao-Li",
            "title": {
                "fragments": [],
                "text": "Human Carrying Status in Visual Surveillance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A general tensor discriminant analysis (GTDA), which seamlessly incorporates the object (Gabor based human gait appearance model) structure information as a natural constraint, is proposed and demonstrated to significantly outperform the existing appearance-based methods."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40506509"
                        ],
                        "name": "Weiming Hu",
                        "slug": "Weiming-Hu",
                        "structuredName": {
                            "firstName": "Weiming",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiming Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145917105"
                        ],
                        "name": "Min Hu",
                        "slug": "Min-Hu",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146287687"
                        ],
                        "name": "Xue Zhou",
                        "slug": "Xue-Zhou",
                        "structuredName": {
                            "firstName": "Xue",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xue Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143874948"
                        ],
                        "name": "T. Tan",
                        "slug": "T.-Tan",
                        "structuredName": {
                            "firstName": "Tieniu",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3269455"
                        ],
                        "name": "Jianguang Lou",
                        "slug": "Jianguang-Lou",
                        "structuredName": {
                            "firstName": "Jianguang",
                            "lastName": "Lou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguang Lou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144555237"
                        ],
                        "name": "S. Maybank",
                        "slug": "S.-Maybank",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Maybank",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Maybank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13545630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95b94feff775ae574b9c7dc94865044f9ab6d91f",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual surveillance using multiple cameras has attracted increasing interest in recent years. Correspondence between multiple cameras is one of the most important and basic problems which visual surveillance using multiple cameras brings. In this paper, we propose a simple and robust method, based on principal axes of people, to match people across multiple cameras. The correspondence likelihood reflecting the similarity of pairs of principal axes of people is constructed according to the relationship between \"ground-points\" of people detected in each camera view and the intersections of principal axes detected in different camera views and transformed to the same view. Our method has the following desirable properties; 1) camera calibration is not needed; 2) accurate motion detection and segmentation are less critical due to the robustness of the principal axis-based feature to noise; 3) based on the fused data derived from correspondence results, positions of people in each camera view can be accurately located even when the people are partially occluded in all views. The experimental results on several real video sequences from outdoor environments have demonstrated the effectiveness, efficiency, and robustness of our method."
            },
            "slug": "Principal-axis-based-correspondence-between-cameras-Hu-Hu",
            "title": {
                "fragments": [],
                "text": "Principal axis-based correspondence between multiple cameras for people tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A simple and robust method, based on principal axes of people, to match people across multiple cameras, according to the relationship between \"ground-points\" of people detected in each camera view and the intersections of principal axes detected in different camera views and transformed to the same view."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2798041"
                        ],
                        "name": "I. Haritaoglu",
                        "slug": "I.-Haritaoglu",
                        "structuredName": {
                            "firstName": "Ismail",
                            "lastName": "Haritaoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Haritaoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298122"
                        ],
                        "name": "D. Harwood",
                        "slug": "D.-Harwood",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Harwood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harwood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "It may be possible to reduce this source of error by positioning the major axis in other ways, for example forcing it to pass through the centroid of the head [4] or the ground point of the person walking [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43111790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "baa728ae305da6871023626d5f50ea9503a59bdd",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Hydra is a real-time system for detecting and tracking multiple people when they appear in a group. We describe the computational models employed by Hydra to track multiple people before, during and after occlusion. Hydra combines a silhouette-based shape model, a motion model, and correlation-based matching methods to classify whether or not a foreground region contains multiple people, and to segment the region into its constituent people and track them. Experimental results demonstrate robustness and real-time performance of the algorithm."
            },
            "slug": "Hydra:-multiple-people-detection-and-tracking-using-Haritaoglu-Harwood",
            "title": {
                "fragments": [],
                "text": "Hydra: multiple people detection and tracking using silhouettes"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work describes the computational models employed by Hydra to track multiple people before, during and after occlusion, and demonstrates robustness and real-time performance of the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 10th International Conference on Image Analysis and Processing"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2938485"
                        ],
                        "name": "H. Nanda",
                        "slug": "H.-Nanda",
                        "structuredName": {
                            "firstName": "Harsh",
                            "lastName": "Nanda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nanda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2171459131"
                        ],
                        "name": "C. Benabdelkedar",
                        "slug": "C.-Benabdelkedar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Benabdelkedar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Benabdelkedar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069324532"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] detect pedestrians carrying objects as outliers of a model for an unencumbered pedestrian obtained in a supervised learning procedure based on a three layer neural network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8210430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0c3456a547fb82cd748a71a3979e9ce3c229e49",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an example-based approach to learn a given class of complex shapes, and recognize instances of that shape with outliers. The system consists of a two-layer custom-designed neural network. We apply this approach to the recognition of pedestrians carrying objects from a single camera. The system is able to capture and model an ample range of pedestrian shapes at varying poses and camera orientations, and achieves a 90% correct recognition rate."
            },
            "slug": "Modelling-pedestrian-shapes-for-outlier-detection:-Nanda-Benabdelkedar",
            "title": {
                "fragments": [],
                "text": "Modelling pedestrian shapes for outlier detection: a neural net based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper presents an example-based approach to learn a given class of complex shapes, and recognize instances of that shape with outliers, and applies this approach to the recognition of pedestrians carrying objects from a single camera."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE IV2003 Intelligent Vehicles Symposium. Proceedings (Cat. No.03TH8683)"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11698,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38087653"
                        ],
                        "name": "A. Branca",
                        "slug": "A.-Branca",
                        "structuredName": {
                            "firstName": "Antonella",
                            "lastName": "Branca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Branca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532503"
                        ],
                        "name": "Marco Leo",
                        "slug": "Marco-Leo",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Leo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Leo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071975"
                        ],
                        "name": "G. Attolico",
                        "slug": "G.-Attolico",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Attolico",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Attolico"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49498161"
                        ],
                        "name": "A. Distante",
                        "slug": "A.-Distante",
                        "structuredName": {
                            "firstName": "Arcangelo",
                            "lastName": "Distante",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Distante"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7] try to identify intruders in archaeological sites."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36697485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a50162ba3f64cd5ef2f351a1c7adb8e6bf0e72a1",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Our application context is the visual surveillance of archeological sites. In this context the main aim is to detect the presence of people and to scan them in order to recognize intruders on the basis of their gestures. Since an intruder needs some utensils indispensable to perform the illegal actions of excavating on the ancient ruins, intruder detection involves first of all to ascertain if a person is carrying some objects and then recognizing the kind. In this paper we concentrate on the recognition of the objects carried by the detected moving persons. An example-based learning technique is used to first detect people and successively to scan them to recognize the possible carried objects. The patterns to be analysed are represented through the approximation coefficients of their three level wavelet decomposition. Pattern classification is performed through a supervised three layer neural network."
            },
            "slug": "Detection-of-objects-carried-by-people-Branca-Leo",
            "title": {
                "fragments": [],
                "text": "Detection of objects carried by people"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper focuses on the recognition of the objects carried by the detected moving persons and an example-based learning technique is used to first detect people and successively to scan them to recognize the possible carried objects."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. International Conference on Image Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922280"
                        ],
                        "name": "O. Veksler",
                        "slug": "O.-Veksler",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Veksler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Veksler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "We use the max-flow algorithm, proposed in [16], and its publically available implementation, to minimize the energy function (Equation 7)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2430892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3120324069ec20eed853d3f9bbbceb32e4173b93",
            "isKey": false,
            "numCitedBy": 3914,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address the problem of minimizing a large class of energy functions that occur in early vision. The major restriction is that the energy function's smoothness term must only involve pairs of pixels. We propose two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed. The first move we consider is an /spl alpha/-/spl beta/-swap: for a pair of labels /spl alpha/,/spl beta/, this move exchanges the labels between an arbitrary set of pixels labeled a and another arbitrary set labeled /spl beta/. Our first algorithm generates a labeling such that there is no swap move that decreases the energy. The second move we consider is an /spl alpha/-expansion: for a label a, this move assigns an arbitrary set of pixels the label /spl alpha/. Our second algorithm, which requires the smoothness term to be a metric, generates a labeling such that there is no expansion move that decreases the energy. Moreover, this solution is within a known factor of the global minimum. We experimentally demonstrate the effectiveness of our approach on image restoration, stereo and motion."
            },
            "slug": "Fast-approximate-energy-minimization-via-graph-cuts-Boykov-Veksler",
            "title": {
                "fragments": [],
                "text": "Fast approximate energy minimization via graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed, and generates a labeling such that there is no expansion move that decreases the energy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "The measure of overlap is defined by Equation 4 [15]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16328598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae0c928816d7ca1ea3213b5739db4141fed7d4b5",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1.2 Detection task For each of the four classes predict the bounding boxes of each object of that class in a test image (if any). Each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn. To be considered a correct detection, the area of overlap ao between the predicted bounding box Bp and ground truth bounding box Bgt must exceed 50% by the formula: ao = area(Bp \u2229 Bgt) area(Bp \u222a Bgt) (1)"
            },
            "slug": "The-PASCAL-Visual-Object-Classes-Challenge-2005-Kit-Everingham",
            "title": {
                "fragments": [],
                "text": "The PASCAL Visual Object Classes Challenge 2005 Development Kit"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "One class predicts the bounding boxes of each object of that class in a test image (if any) and each bounding box should be output with an associated real-valued confidence of the detection so that a precision/recall curve can be drawn."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218687"
                        ],
                        "name": "P. Rousseeuw",
                        "slug": "P.-Rousseeuw",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rousseeuw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rousseeuw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "The more computationally expensive Least Median of Squares (LMedS) estimator [12] gave similar results."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 33519033,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6677822795fd4aecb2c2cd262f3f3a6cca65295d",
            "isKey": false,
            "numCitedBy": 3539,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Classical least squares regression consists of minimizing the sum of the squared residuals. Many authors have produced more robust versions of this estimator by replacing the square by something else, such as the absolute value. In this article a different approach is introduced in which the sum is replaced by the median of the squared residuals. The resulting estimator can resist the effect of nearly 50% of contamination in the data. In the special case of simple regression, it corresponds to finding the narrowest strip covering half of the observations. Generalizations are possible to multivariate location, orthogonal regression, and hypothesis testing in linear models."
            },
            "slug": "Least-Median-of-Squares-Regression-Rousseeuw",
            "title": {
                "fragments": [],
                "text": "Least Median of Squares Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145089978"
                        ],
                        "name": "D. Damen",
                        "slug": "D.-Damen",
                        "structuredName": {
                            "firstName": "Dima",
                            "lastName": "Damen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Damen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967104"
                        ],
                        "name": "David C. Hogg",
                        "slug": "David-C.-Hogg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hogg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David C. Hogg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64711781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd116435b6f93e803e8db708ad4d0bce71499982",
            "isKey": false,
            "numCitedBy": 1554,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Vision-and-Pattern-Recognition-(CVPR)-Damen-Hogg",
            "title": {
                "fragments": [],
                "text": "Computer Vision and Pattern Recognition (CVPR)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45232003,
            "fieldsOfStudy": [],
            "id": "a57520b68e73b5e1fc3668b443daf74ebe957cc7",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast Approximate Energy Minimization via Graph Cuts"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 175
                            }
                        ],
                        "text": "We have re-implemented the earlier state of the art method [1] and demonstrate a substantial improvement in performance for the new method on the challenging PETS2006 dataset [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Int"
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Performance Evaluation of Tracking and Surveillance (PETS). IEEE, New York"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Int. Workshop on Performance Evaluation of Tracking and Surveillance (PETS)"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Int. Workshop on Performance Evaluation of Tracking and Surveillance (PETS)"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Workshop on Performance Evaluation of Tracking and Surveillance (PETS)"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Int"
            },
            "year": 2006
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 9,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 21,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Detecting-Carried-Objects-in-Short-Video-Sequences-Damen-Hogg/32fad849f86bb99d824150e9373c352219edd4ed?sort=total-citations"
}