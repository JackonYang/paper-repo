{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2651937"
                        ],
                        "name": "Yinggong Zhao",
                        "slug": "Yinggong-Zhao",
                        "structuredName": {
                            "firstName": "Yinggong",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinggong Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372307"
                        ],
                        "name": "Victoria Fossum",
                        "slug": "Victoria-Fossum",
                        "structuredName": {
                            "firstName": "Victoria",
                            "lastName": "Fossum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victoria Fossum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 20
                            }
                        ],
                        "text": "We should note that Vaswani et al. (2013) implements a method called Noise Contrastive Estimation (NCE) that is also used to train selfnormalized NNLMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3065236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71480da09af638260801af1db8eff6acb4e1122f",
            "isKey": false,
            "numCitedBy": 265,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu."
            },
            "slug": "Decoding-with-Large-Scale-Neural-Language-Models-Vaswani-Zhao",
            "title": {
                "fragments": [],
                "text": "Decoding with Large-Scale Neural Language Models Improves Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This work develops a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and incorporates it into a machine translation system both by reranking k-best lists and by direct integration into the decoder."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2325985"
                        ],
                        "name": "Michael Auli",
                        "slug": "Michael-Auli",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Auli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Auli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1947267"
                        ],
                        "name": "Michel Galley",
                        "slug": "Michel-Galley",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Galley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michel Galley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2596310"
                        ],
                        "name": "Chris Quirk",
                        "slug": "Chris-Quirk",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Quirk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Quirk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Auli et al. (2013) use a fixed continuous-space source representation, obtained from LDA (Blei et al., 2003) or a source-only NNLM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5552894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "167ad306d84cca2455bc50eb833454de9f2dcd02",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets."
            },
            "slug": "Joint-Language-and-Translation-Modeling-with-Neural-Auli-Galley",
            "title": {
                "fragments": [],
                "text": "Joint Language and Translation Modeling with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work presents a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words which shows competitive accuracy compared to the traditional channel model features."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49337181"
                        ],
                        "name": "Wei Wang",
                        "slug": "Wei-Wang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 56
                            }
                        ],
                        "text": ", 2013) \u2022 7 sparse feature types, totaling 50k features (Chiang et al., 2009) \u2022 LM adaptation (Snover et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 146
                            }
                        ],
                        "text": "\u2026(Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 7 sparse feature types, totaling 50k features\n(Chiang et al., 2009) \u2022 LM adaptation (Snover et al., 2008)\nWe also perform 1000-best rescoring with the following features:\n\u2022 5-gram\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3544821,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57b3f8df7020b67df71a96974adef8d5282ed396",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and our syntax-based translation system. On a large-scale Chinese-English translation task, we obtain statistically significant improvements of +1.5 Bleu and + 1.1 Bleu, respectively. We analyze the impact of the new features and the performance of the learning algorithm."
            },
            "slug": "11,001-New-Features-for-Statistical-Machine-Chiang-Knight",
            "title": {
                "fragments": [],
                "text": "11,001 New Features for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "On a large-scale Chinese-English translation task, the Margin Infused Relaxed Algorithm is used to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase-based translation system and the syntax-basedtranslation system."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784529"
                        ],
                        "name": "H. Le",
                        "slug": "H.-Le",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Le",
                            "middleNames": [
                                "Son"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311059"
                        ],
                        "name": "A. Allauzen",
                        "slug": "A.-Allauzen",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Allauzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Allauzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846431"
                        ],
                        "name": "Fran\u00e7ois Yvon",
                        "slug": "Fran\u00e7ois-Yvon",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Yvon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7ois Yvon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14810278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1401b98a6eb0b0ddaf5bdfd559ed1766434446b8",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. For lack of sufficient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture. In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance."
            },
            "slug": "Continuous-Space-Translation-Models-with-Neural-Le-Allauzen",
            "title": {
                "fragments": [],
                "text": "Continuous Space Translation Models with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Several continuous space translation models are explored, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations, jointly computed using a multi-layer neural network with a SOUL architecture."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8608051,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f08df805f14baa826dbddcb002277b15d3f1556",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach to perform the estimation of the translation model probabilities of a phrase-based statistical machine translation system. We use neural networks to directly learn the translation probability of phrase pairs using continuous representations. The system can be easily trained on the same data used to build standard phrase-based systems. We provide experimental evidence that the approach seems to be able to infer meaningful translation probabilities for phrase pairs not seen in the training data, or even predict a list of the most likely translations given a source phrase. The approach can be used to rescore n-best lists, but we also discuss an integration into the Moses decoder. A preliminary evaluation on the English/French IWSLT task achieved improvements in the BLEU score and a human analysis showed that the new model often chooses semantically better translations. Several extensions of this work are discussed."
            },
            "slug": "Continuous-Space-Translation-Models-for-Statistical-Schwenk",
            "title": {
                "fragments": [],
                "text": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Experimental evidence is provided that the approach seems to be able to infer meaningful translation probabilities for phrase pairs not seen in the training data, or even predict a list of the most likely translations given a source phrase."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39839719"
                        ],
                        "name": "Libin Shen",
                        "slug": "Libin-Shen",
                        "structuredName": {
                            "firstName": "Libin",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Libin Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2362587"
                        ],
                        "name": "Jinxi Xu",
                        "slug": "Jinxi-Xu",
                        "structuredName": {
                            "firstName": "Jinxi",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinxi Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 81
                            }
                        ],
                        "text": "\u2022 Forward and backward rule probabilities \u2022 4-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 Length distribution (Shen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 144
                            }
                        ],
                        "text": "Our baseline decoder contains a large and powerful set of features, which include:\n\u2022 Forward and backward rule probabilities \u2022 4-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 7 sparse feature types, totaling 50k features\n(Chiang et al., 2009) \u2022 LM adaptation (Snover et al., 2008)\nWe also perform 1000-best rescoring with the following features:\n\u2022 5-gram Kneser-Ney LM \u2022 Recurrent neural network language model\n(RNNLM) (Mikolov et al., 2010)\nAlthough we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "\u2026backward rule probabilities \u2022 4-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 7 sparse feature types,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 69
                            }
                        ],
                        "text": "We use a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": ", 2010) \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "\u2026decoder contains a large and powerful set of features, which include:\n\u2022 Forward and backward rule probabilities \u2022 4-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 171695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b1a0b92b34a707ff84b5c41121713624a4ba140",
            "isKey": true,
            "numCitedBy": 59,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel string-to-dependency algorithm for statistical machine translation. This algorithm employs a target dependency language model during decoding to exploit long distance word relations, which cannot be modeled with a traditional n-gram language model. Experiments show that the algorithm achieves significant improvement in MT performance over a state-of-the-art hierarchical string-to-string system on NIST MT06 and MT08 newswire evaluation sets."
            },
            "slug": "String-to-Dependency-Statistical-Machine-Shen-Xu",
            "title": {
                "fragments": [],
                "text": "String-to-Dependency Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A novel string-to-dependency algorithm for statistical machine translation that employs a target dependency language model during decoding to exploit long distance word relations, which cannot be modeled with a traditional n-gram language model."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 147
                            }
                        ],
                        "text": "Initially, these models were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (Bengio et al., 2003; Schwenk, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1274371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4a258df43cc14e46988de9a4a7b2f0ea817529b",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. \n \nIn this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. \n \nWe also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks."
            },
            "slug": "Continuous-Space-Language-Models-for-Statistical-Schwenk",
            "title": {
                "fragments": [],
                "text": "Continuous Space Language Models for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes to use a new statistical language model that is based on a continuous representation of the words in the vocabulary, which achieves consistent improvements in the BLEU score on the development and test data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2245567"
                        ],
                        "name": "M. Karafi\u00e1t",
                        "slug": "M.-Karafi\u00e1t",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Karafi\u00e1t",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Karafi\u00e1t"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816892"
                        ],
                        "name": "L. Burget",
                        "slug": "L.-Burget",
                        "structuredName": {
                            "firstName": "Luk\u00e1\u0161",
                            "lastName": "Burget",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Burget"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899242"
                        ],
                        "name": "J. Cernock\u00fd",
                        "slug": "J.-Cernock\u00fd",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Cernock\u00fd",
                            "middleNames": [
                                "Honza"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cernock\u00fd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803071"
                        ],
                        "name": "S. Khudanpur",
                        "slug": "S.-Khudanpur",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Khudanpur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Khudanpur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 586,
                                "start": 581
                            }
                        ],
                        "text": "Our baseline decoder contains a large and powerful set of features, which include:\n\u2022 Forward and backward rule probabilities \u2022 4-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 7 sparse feature types, totaling 50k features\n(Chiang et al., 2009) \u2022 LM adaptation (Snover et al., 2008)\nWe also perform 1000-best rescoring with the following features:\n\u2022 5-gram Kneser-Ney LM \u2022 Recurrent neural network language model\n(RNNLM) (Mikolov et al., 2010)\nAlthough we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 73
                            }
                        ],
                        "text": "\u2022 5-gram Kneser-Ney LM \u2022 Recurrent neural network language model (RNNLM) (Mikolov et al., 2010)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 39
                            }
                        ],
                        "text": "It is also interesting to see that the RNNLM is no longer beneficial when the NNJM is used."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 101
                            }
                        ],
                        "text": "We can see that the rescoring-only NNJM performs very well when used on top of a baseline without an RNNLM (+1.5 BLEU), but the gain on top of the RNNLM is very small (+0.3 BLEU)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 114
                            }
                        ],
                        "text": "Considering that our baseline is already +0.3 BLEU better than the 1st place result of MT12 and contains a strong RNNLM, we consider this to be quite an extraordinary improvement.12\nFor the Chinese-English condition, there is an improvement of +0.8 BLEU from the primary NNJM and +1.3 BLEU overall."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 140
                            }
                        ],
                        "text": "\u2026(Snover et al., 2008)\nWe also perform 1000-best rescoring with the following features:\n\u2022 5-gram Kneser-Ney LM \u2022 Recurrent neural network language model\n(RNNLM) (Mikolov et al., 2010)\nAlthough we consider the RNNLM to be part of our baseline, we give it special treatment in the results\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 70
                            }
                        ],
                        "text": "The gain from the decoding NNJM is large in both cases (+2.6 BLEU w/o RNNLM, +1.6 BLEU w/ RNNLM)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17048224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "isKey": true,
            "numCitedBy": 4900,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition"
            },
            "slug": "Recurrent-neural-network-based-language-model-Mikolov-Karafi\u00e1t",
            "title": {
                "fragments": [],
                "text": "Recurrent neural network based language model"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209023"
                        ],
                        "name": "J. Crego",
                        "slug": "J.-Crego",
                        "structuredName": {
                            "firstName": "Josep",
                            "lastName": "Crego",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crego"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846431"
                        ],
                        "name": "Fran\u00e7ois Yvon",
                        "slug": "Fran\u00e7ois-Yvon",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Yvon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7ois Yvon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37876303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c36d355e01e1ed90e57bffbbfc274d4d98952b96",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we present an extension of n-gram-based translation models based on factored language models (FLMs). Translation units employed in the n-gram-based approach to statistical machine translation (SMT) are based on mappings of sequences of raw words, while translation model probabilities are estimated through standard language modeling of such bilingual units. Therefore, similar to other translation model approaches (phrase-based or hierarchical), the sparseness problem of the units being modeled leads to unreliable probability estimates, even under conditions where large bilingual corpora are available. In order to tackle this problem, we extend the n-gram-based approach to SMT by tightly integrating more general word representations, such as lemmas and morphological classes, and we use the flexible framework of FLMs to apply a number of different back-off techniques. In this work, we show that FLMs can also be successfully applied to translation modeling, yielding more robust probability estimates that integrate larger bilingual contexts during the translation process."
            },
            "slug": "Factored-bilingual-n-gram-language-models-for-Crego-Yvon",
            "title": {
                "fragments": [],
                "text": "Factored bilingual n-gram language models for statistical machine translation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work extends the n-gram-based approach to SMT by tightly integrating more general word representations, such as lemmas and morphological classes, and uses the flexible framework of FLMs to apply a number of different back-off techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Translation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583391"
                        ],
                        "name": "Nal Kalchbrenner",
                        "slug": "Nal-Kalchbrenner",
                        "structuredName": {
                            "firstName": "Nal",
                            "lastName": "Kalchbrenner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nal Kalchbrenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 0
                            }
                        ],
                        "text": "Kalchbrenner and Blunsom (2013) implement a convolutional recurrent NNJM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12639289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944a1cfd79dbfb6fef460360a0765ba790f4027a",
            "isKey": false,
            "numCitedBy": 1235,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations."
            },
            "slug": "Recurrent-Continuous-Translation-Models-Kalchbrenner-Blunsom",
            "title": {
                "fragments": [],
                "text": "Recurrent Continuous Translation Models"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 128
                            }
                        ],
                        "text": "Therefore, we also present results using a simpler version of our decoder which emulates Chiang\u2019s original Hiero implementation (Chiang, 2007)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 3505719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0db6eb46ca9941660acc775e3ca39bf4434c18be",
            "isKey": false,
            "numCitedBy": 1299,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a statistical machine translation model that uses hierarchical phrasesphrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system's training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrase-based system."
            },
            "slug": "Hierarchical-Phrase-Based-Translation-Chiang",
            "title": {
                "fragments": [],
                "text": "Hierarchical Phrase-Based Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A statistical machine translation model that uses hierarchical phrasesphrases that contain subphrasing that is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145250904"
                        ],
                        "name": "J. Mari\u00f1o",
                        "slug": "J.-Mari\u00f1o",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Mari\u00f1o",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mari\u00f1o"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694652"
                        ],
                        "name": "Rafael E. Banchs",
                        "slug": "Rafael-E.-Banchs",
                        "structuredName": {
                            "firstName": "Rafael",
                            "lastName": "Banchs",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rafael E. Banchs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209023"
                        ],
                        "name": "J. Crego",
                        "slug": "J.-Crego",
                        "structuredName": {
                            "firstName": "Josep",
                            "lastName": "Crego",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crego"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786911"
                        ],
                        "name": "A. Gispert",
                        "slug": "A.-Gispert",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gispert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gispert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40010027"
                        ],
                        "name": "Patrik Lambert",
                        "slug": "Patrik-Lambert",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Lambert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrik Lambert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779548"
                        ],
                        "name": "Jos\u00e9 A. R. Fonollosa",
                        "slug": "Jos\u00e9-A.-R.-Fonollosa",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Fonollosa",
                            "middleNames": [
                                "A.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jos\u00e9 A. R. Fonollosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398996347"
                        ],
                        "name": "M. Costa-juss\u00e0",
                        "slug": "M.-Costa-juss\u00e0",
                        "structuredName": {
                            "firstName": "Marta",
                            "lastName": "Costa-juss\u00e0",
                            "middleNames": [
                                "Ruiz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Costa-juss\u00e0"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Although there has been a substantial amount of past work in lexicalized joint models (Marino et al., 2006; Crego and Yvon, 2010), nearly all of these papers have used older statistical techniques such as Kneser-Ney or Maximum Entropy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1821900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b7ec490154397c2691d3404eccd412665fa5e6a",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes in detail an n-gram approach to statistical machine translation. This approach consists of a log-linear combination of a translation model based on n-grams of bilingual units, which are referred to as tuples, along with four specific feature functions. Translation performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS)."
            },
            "slug": "N-gram-based-Machine-Translation-Mari\u00f1o-Banchs",
            "title": {
                "fragments": [],
                "text": "N-gram-based Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This article describes in detail an n-gram approach to statistical machine translation that consists of a log-linear combination of a translation model based on n- grams of bilingual units, which are referred to as tuples, along with four specific feature functions."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909504"
                        ],
                        "name": "Jason Riesa",
                        "slug": "Jason-Riesa",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Riesa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Riesa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144542160"
                        ],
                        "name": "A. Irvine",
                        "slug": "A.-Irvine",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Irvine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Irvine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 45404405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf716f02c8f8f7ed87bfd8aaae7f7a963585e695",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an accurate word alignment algorithm that heavily exploits source and target-language syntax. Using a discriminative framework and an efficient bottom-up search algorithm, we train a model of hundreds of thousands of syntactic features. Our new model (1) helps us to very accurately model syntactic transformations between languages; (2) is language-independent; and (3) with automatic feature extraction, assists system developers in obtaining good word-alignment performance off-the-shelf when tackling new language pairs. We analyze the impact of our features, describe inference under the model, and demonstrate significant alignment and translation quality improvements over already-powerful baselines trained on very large corpora. We observe translation quality improvements corresponding to 1.0 and 1.3 BLEU for Arabic-English and Chinese-English, respectively."
            },
            "slug": "Feature-Rich-Language-Independent-Syntax-Based-for-Riesa-Irvine",
            "title": {
                "fragments": [],
                "text": "Feature-Rich Language-Independent Syntax-Based Alignment for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An accurate word alignment algorithm that heavily exploits source and target-language syntax and is language-independent, and with automatic feature extraction, assists system developers in obtaining good word-alignment performance off-the-shelf when tackling new language pairs is presented."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2860351"
                        ],
                        "name": "Will Y. Zou",
                        "slug": "Will-Y.-Zou",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Zou",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Y. Zou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46724030"
                        ],
                        "name": "Daniel Matthew Cer",
                        "slug": "Daniel-Matthew-Cer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cer",
                            "middleNames": [
                                "Matthew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Matthew Cer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 931054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d3233d858660aff451a6c2561a05378ed09725a",
            "isKey": false,
            "numCitedBy": 535,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task."
            },
            "slug": "Bilingual-Word-Embeddings-for-Phrase-Based-Machine-Zou-Socher",
            "title": {
                "fragments": [],
                "text": "Bilingual Word Embeddings for Phrase-Based Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence is proposed, which significantly out-perform baselines in word semantic similarity."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 75
                            }
                        ],
                        "text": "far too sparse for standard probability models such as Kneser-Ney back-off (Kneser and Ney, 1995) or Maximum Entropy (Rosenfeld, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9685476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9548ac30c113562a51e603dbbc8e9fa651cfd3ab",
            "isKey": false,
            "numCitedBy": 1792,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10% in terms of perplexity and 5% in terms of word error rate."
            },
            "slug": "Improved-backing-off-for-M-gram-language-modeling-Kneser-Ney",
            "title": {
                "fragments": [],
                "text": "Improved backing-off for M-gram language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes to use distributions which are especially optimized for the task of back-off, which are quite different from the probability distributions that are usually used for backing-off."
            },
            "venue": {
                "fragments": [],
                "text": "1995 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32278254"
                        ],
                        "name": "Matthew G. Snover",
                        "slug": "Matthew-G.-Snover",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Snover",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew G. Snover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752326"
                        ],
                        "name": "B. Dorr",
                        "slug": "B.-Dorr",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Dorr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dorr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "\u2026(Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 7 sparse feature types, totaling 50k features\n(Chiang et al., 2009) \u2022 LM adaptation (Snover et al., 2008)\nWe also perform 1000-best rescoring with the following features:\n\u2022 5-gram Kneser-Ney LM \u2022 Recurrent neural\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2009) \u2022 LM adaptation (Snover et al., 2008)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 9393879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b281a9d0c728979f7ffccba1a61d0fc1d29530c1",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model. While bi-lingual parallel data are expensive to generate, monolingual data are relatively common. Yet monolingual data have been under-utilized, having been used primarily for training a language model in the target language. This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. The method exploits the existence of comparable text---multiple texts in the target language that discuss the same or similar stories as found in the source language document. For every source document that is to be translated, a large monolingual data set in the target language is searched for documents that might be comparable to the source documents. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system."
            },
            "slug": "Language-and-Translation-Model-Adaptation-using-Snover-Dorr",
            "title": {
                "fragments": [],
                "text": "Language and Translation Model Adaptation using Comparable Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories by exploiting the existence of comparable text---multiple texts in the target language that discuss the same or similar stories as found in the source language document."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909943744"
                        ],
                        "name": "Christian Janvin",
                        "slug": "Christian-Janvin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Janvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Janvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 228
                            }
                        ],
                        "text": "In recent years, neural network models have become increasingly popular in NLP. Initially, these models were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (Bengio et al., 2003; Schwenk, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 112
                            }
                        ],
                        "text": "Our neural network architecture is almost identical to the original feed-forward NNLM architecture described in Bengio et al. (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 147
                            }
                        ],
                        "text": "Initially, these models were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (Bengio et al., 2003; Schwenk, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221275765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "isKey": false,
            "numCitedBy": 6009,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            },
            "slug": "A-Neural-Probabilistic-Language-Model-Bengio-Ducharme",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120618881"
                        ],
                        "name": "S. Matsoukas",
                        "slug": "S.-Matsoukas",
                        "structuredName": {
                            "firstName": "Spyridon",
                            "lastName": "Matsoukas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Matsoukas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Training is performed on a single Tesla K10 GPU, with each epoch (128*20k = 2.6M samples) taking roughly 1100 seconds to run, resulting in a total training time of \u223c12 hours."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "\u2026Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 7 sparse feature types, totaling 50k features\n(Chiang et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 262
                            }
                        ],
                        "text": "Our baseline decoder contains a large and powerful set of features, which include:\n\u2022 Forward and backward rule probabilities \u2022 4-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 7 sparse feature types, totaling 50k features\n(Chiang et al., 2009) \u2022 LM adaptation (Snover et al., 2008)\nWe also perform 1000-best rescoring with the following features:\n\u2022 5-gram Kneser-Ney LM \u2022 Recurrent neural network language model\n(RNNLM) (Mikolov et al., 2010)\nAlthough we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17192521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b30a56a1540f86f40946caa41ce137c61aca76f4",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In the area of machine translation (MT) system combination, previous work on generating input hypotheses has focused on varying a core aspect of the MT system, such as the decoding algorithm or alignment algorithm. In this paper, we propose a new method for generating diverse hypotheses from a single MT system using traits. These traits are simple properties of the MT output such as \"average output length\" and \"average rule length.\" Our method is designed to select hypotheses which vary in trait value but do not significantly degrade in BLEU score. These hypotheses can be combined using standard system combination techniques to produce a 1.2-1.5 BLEU gain on the Arabic-English NIST MT06/MT08 translation task."
            },
            "slug": "Trait-Based-Hypothesis-Selection-For-Machine-Devlin-Matsoukas",
            "title": {
                "fragments": [],
                "text": "Trait-Based Hypothesis Selection For Machine Translation"
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39509434"
                        ],
                        "name": "Antti-Veikko I. Rosti",
                        "slug": "Antti-Veikko-I.-Rosti",
                        "structuredName": {
                            "firstName": "Antti-Veikko",
                            "lastName": "Rosti",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antti-Veikko I. Rosti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40379722"
                        ],
                        "name": "Bing Zhang",
                        "slug": "Bing-Zhang",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120618881"
                        ],
                        "name": "S. Matsoukas",
                        "slug": "S.-Matsoukas",
                        "structuredName": {
                            "firstName": "Spyridon",
                            "lastName": "Matsoukas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Matsoukas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 114
                            }
                        ],
                        "text": "For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16909338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b025b96ee557d10cd43415740058ac751cbddbf",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "BBN submitted system combination outputs for Czech-English, German-English, Spanish-English, French-English, and All-English language pairs. All combinations were based on confusion network decoding. An incremental hypothesis alignment algorithm with flexible matching was used to build the networks. The bi-gram decoding weights for the single source language translations were tuned directly to maximize the BLEU score of the decoding output. Approximate expected BLEU was used as the objective function in gradient based optimization of the combination weights for a 44 system multi-source language combination (All-English). The system combination gained around 0.4--2.0 BLEU points over the best individual systems on the single source conditions. On the multi-source condition, the system combination gained 6.6 BLEU points."
            },
            "slug": "BBN-System-Description-for-WMT10-System-Combination-Rosti-Zhang",
            "title": {
                "fragments": [],
                "text": "BBN System Description for WMT10 System Combination Task"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Approximate expected BLEU was used as the objective function in gradient based optimization of the combination weights for a 44 system multi-source language combination (All-English)."
            },
            "venue": {
                "fragments": [],
                "text": "WMT@ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 164
                            }
                        ],
                        "text": "However, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network\u2019s ability to semantically generalize (Mikolov et al., 2013) and learn nonlinear relationships."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7478738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
            "isKey": false,
            "numCitedBy": 3051,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \u201cKing Man + Woman\u201d results in a vector very close to \u201cQueen.\u201d We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems."
            },
            "slug": "Linguistic-Regularities-in-Continuous-Space-Word-Mikolov-Yih",
            "title": {
                "fragments": [],
                "text": "Linguistic Regularities in Continuous Space Word Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The vector-space word representations that are implicitly learned by the input-layer weights are found to be surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 67
                            }
                        ],
                        "text": "1], to prevent the training from entering degenerate search spaces (Pascanu et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 157
                            }
                        ],
                        "text": "However, we have found it beneficial to clip each weight update to the range of [-0.1, 0.1], to prevent the training from entering degenerate search spaces (Pascanu et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14650762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84069287da0a6b488b8c933f3cb5be759cb6237e",
            "isKey": true,
            "numCitedBy": 3801,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section."
            },
            "slug": "On-the-difficulty-of-training-recurrent-neural-Pascanu-Mikolov",
            "title": {
                "fragments": [],
                "text": "On the difficulty of training recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem and validates empirically the hypothesis and proposed solutions."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34227637"
                        ],
                        "name": "Zhongqiang Huang",
                        "slug": "Zhongqiang-Huang",
                        "structuredName": {
                            "firstName": "Zhongqiang",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhongqiang Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3327459"
                        ],
                        "name": "Rabih Zbib",
                        "slug": "Rabih-Zbib",
                        "structuredName": {
                            "firstName": "Rabih",
                            "lastName": "Zbib",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rabih Zbib"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 147
                            }
                        ],
                        "text": "\u2026\u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 7 sparse feature types, totaling 50k features\n(Chiang et al., 2009) \u2022 LM adaptation (Snover et al., 2008)\nWe also\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": ", 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 7 sparse feature types, totaling 50k features (Chiang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15208097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ef2650f81fc16db9e0477343781acd51357ca22",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets."
            },
            "slug": "Factored-Soft-Source-Syntactic-Constraints-for-Huang-Devlin",
            "title": {
                "fragments": [],
                "text": "Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets and keeps translation rules intact and factorizes the use of syntactic constraints through two separate models."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5219389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de2df29b0a0312de7270c3f5a0af6af5645cf91a",
            "isKey": false,
            "numCitedBy": 4470,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."
            },
            "slug": "A-Systematic-Comparison-of-Various-Statistical-Och-Ney",
            "title": {
                "fragments": [],
                "text": "A Systematic Comparison of Various Statistical Alignment Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "far too sparse for standard probability models such as Kneser-Ney back-off (Kneser and Ney, 1995) or Maximum Entropy (Rosenfeld, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 219
                            }
                        ],
                        "text": "Although there has been a substantial amount of past work in lexicalized joint models (Marino et al., 2006; Crego and Yvon, 2010), nearly all of these papers have used older statistical techniques such as Kneser-Ney or Maximum Entropy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 231
                            }
                        ],
                        "text": "2We have found that the affiliation heuristic is robust to\nsmall differences, such as left vs. right preference.\nfar too sparse for standard probability models such as Kneser-Ney back-off (Kneser and Ney, 1995) or Maximum Entropy (Rosenfeld, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13110923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "076fa8d095c37c657f2aff39cf90bc2ea883b7cb",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the document's history, we propose and usetrigger pairsas the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, we apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the ME solution. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. The ME framework is extremely general: any phenomenon that can be described in terms of statistics of the text can be readily incorporated. An adaptive language model based on the ME approach was trained on theWall Street Journalcorpus, and showed a 32\u201339% perplexity reduction over the baseline. When interfaced to SPHINX-II, Carnegie Mellon's speech recognizer, it reduced its error rate by 10\u201314%. This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "slug": "A-maximum-entropy-approach-to-adaptive-statistical-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources, and shows the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 90
                            }
                        ],
                        "text": "Auli et al. (2013) use a fixed continuous-space source representation, obtained from LDA (Blei et al., 2003) or a source-only NNLM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 77
                            }
                        ],
                        "text": "(2013) use a fixed continuous-space source representation, obtained from LDA (Blei et al., 2003) or a source-only NNLM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3177797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f198043a866e9187925a8d8db9a55e3bfdd47f2c",
            "isKey": false,
            "numCitedBy": 30944,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Dirichlet-Allocation-Blei-Ng",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet Allocation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058713"
                        ],
                        "name": "M. Yasuhara",
                        "slug": "M.-Yasuhara",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Yasuhara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yasuhara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116800787"
                        ],
                        "name": "Toru Tanaka",
                        "slug": "Toru-Tanaka",
                        "structuredName": {
                            "firstName": "Toru",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toru Tanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3352120"
                        ],
                        "name": "Jun-ya Norimatsu",
                        "slug": "Jun-ya-Norimatsu",
                        "structuredName": {
                            "firstName": "Jun-ya",
                            "lastName": "Norimatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun-ya Norimatsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37491468"
                        ],
                        "name": "Mikio Yamamoto",
                        "slug": "Mikio-Yamamoto",
                        "structuredName": {
                            "firstName": "Mikio",
                            "lastName": "Yamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikio Yamamoto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "4M lookups/second, which is on par with fast backoff LM implementations (Tanaka et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16079625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6737bc030457fa69f307d312dd33176b4eadc02",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Ngram language models tend to increase in size with inflating the corpus size, and consume considerable resources. In this paper, we propose an efficient method for implementing ngram models based on doublearray structures. First, we propose a method for representing backwards suffix trees using double-array structures and demonstrate its efficiency. Next, we propose two optimization methods for improving the efficiency of data representation in the double-array structures. Embedding probabilities into unused spaces in double-array structures reduces the model size. Moreover, tuning the word IDs in the language model makes the model smaller and faster. We also show that our method can be used for building large language models using the division method. Lastly, we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used."
            },
            "slug": "An-Efficient-Language-Model-Using-Double-Array-Yasuhara-Tanaka",
            "title": {
                "fragments": [],
                "text": "An Efficient Language Model Using Double-Array Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An efficient method for implementing ngram models based on doublearray structures is proposed and it is shown that it outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696645"
                        ],
                        "name": "Nizar Habash",
                        "slug": "Nizar-Habash",
                        "structuredName": {
                            "firstName": "Nizar",
                            "lastName": "Habash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nizar Habash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066239830"
                        ],
                        "name": "Ryan Roth",
                        "slug": "Ryan-Roth",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702447"
                        ],
                        "name": "Owen Rambow",
                        "slug": "Owen-Rambow",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Rambow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Owen Rambow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37558091"
                        ],
                        "name": "R. Eskander",
                        "slug": "R.-Eskander",
                        "structuredName": {
                            "firstName": "Ramy",
                            "lastName": "Eskander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Eskander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2891042"
                        ],
                        "name": "Nadi Tomeh",
                        "slug": "Nadi-Tomeh",
                        "structuredName": {
                            "firstName": "Nadi",
                            "lastName": "Tomeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nadi Tomeh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9902281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53ac234b5c0950bc5240a48fa7077e9f728f9547",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "The many differences between Dialectal Arabic and Modern Standard Arabic (MSA) pose a challenge to the majority of Arabic natural language processing tools, which are designed for MSA. In this paper, we retarget an existing state-of-the-art MSA morphological tagger to Egyptian Arabic (ARZ). Our evaluation demonstrates that our ARZ morphology tagger outperforms its MSA variant on ARZ input in terms of accuracy in part-of-speech tagging, diacritization, lemmatization and tokenization; and in terms of utility for ARZ-toEnglish statistical machine translation."
            },
            "slug": "Morphological-Analysis-and-Disambiguation-for-Habash-Roth",
            "title": {
                "fragments": [],
                "text": "Morphological Analysis and Disambiguation for Dialectal Arabic"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper retargets an existing state-of-the-art MSA morphological tagger to Egyptian Arabic (ARZ), and demonstrates that the ARZ morphology tagger outperforms its MSA variant on ARZ input in terms of accuracy in part- of-speech tagging, diacritization, lemmatization and tokenization; and interms of utility for ARZ-toEnglish statistical machine translation."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 50
                            }
                        ],
                        "text": "The primary purpose of this is as a comparison to Le et al. (2012), whose model can only be used in k-best rescoring."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 36
                            }
                        ],
                        "text": "Even class-based approaches such as Le et al. (2012) require a 2-20k shortlist vocabulary, and are therefore still quite costly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 46
                            }
                        ],
                        "text": "Unlike previous approaches to joint modeling (Le et al., 2012), our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger improvements than k-best rescoring only."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 41
                            }
                        ],
                        "text": "The most similar work that we know of is Le et al. (2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Continuous space translation models with"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 141
                            }
                        ],
                        "text": "\u2026which include:\n\u2022 Forward and backward rule probabilities \u2022 4-gram Kneser-Ney LM \u2022 Dependency LM (Shen et al., 2010) \u2022 Contextual lexical smoothing (Devlin, 2009) \u2022 Length distribution (Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60649279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ca5a796e5fd5e2b65b3789928f9c98311653acc",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Lexical-Features-for-Statistical-Machine-Devlin",
            "title": {
                "fragments": [],
                "text": "Lexical Features for Statistical Machine Translation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34782608"
                        ],
                        "name": "G. Orr",
                        "slug": "G.-Orr",
                        "structuredName": {
                            "firstName": "Genevieve",
                            "lastName": "Orr",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 91
                            }
                        ],
                        "text": "Optimization is performed using standard back propagation with stochastic gradient ascent (LeCun et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20158889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b87274e6d9aa4e6ba5148898aa92941617d2b6ed",
            "isKey": false,
            "numCitedBy": 2630,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-BackProp-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Efficient BackProp"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks: Tricks of the Trade"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 117
                            }
                        ],
                        "text": "far too sparse for standard probability models such as Kneser-Ney back-off (Kneser and Ney, 1995) or Maximum Entropy (Rosenfeld, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 219
                            }
                        ],
                        "text": "Although there has been a substantial amount of past work in lexicalized joint models (Marino et al., 2006; Crego and Yvon, 2010), nearly all of these papers have used older statistical techniques such as Kneser-Ney or Maximum Entropy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 214
                            }
                        ],
                        "text": "2We have found that the affiliation heuristic is robust to\nsmall differences, such as left vs. right preference.\nfar too sparse for standard probability models such as Kneser-Ney back-off (Kneser and Ney, 1995) or Maximum Entropy (Rosenfeld, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Computer, Speech and Language, 10:187\u2013228."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 146
                            }
                        ],
                        "text": "\u2026(Shen et al., 2010) \u2022 Trait features (Devlin and Matsoukas, 2012) \u2022 Factored source syntax (Huang et al., 2013) \u2022 7 sparse feature types, totaling 50k features\n(Chiang et al., 2009) \u2022 LM adaptation (Snover et al., 2008)\nWe also perform 1000-best rescoring with the following features:\n\u2022 5-gram\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "001 new features for statistical machine translation"
            },
            "venue": {
                "fragments": [],
                "text": "HLT-NAACL"
            },
            "year": 2009
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 18,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 32,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Fast-and-Robust-Neural-Network-Joint-Models-for-Devlin-Zbib/0894b06cff1cd0903574acaa7fcf071b144ae775?sort=total-citations"
}