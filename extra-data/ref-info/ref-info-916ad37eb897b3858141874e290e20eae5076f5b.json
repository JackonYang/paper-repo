{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388416"
                        ],
                        "name": "S. Nowozin",
                        "slug": "S.-Nowozin",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Nowozin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowozin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2871555"
                        ],
                        "name": "P. Gehler",
                        "slug": "P.-Gehler",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Gehler",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 2
                            }
                        ],
                        "text": ", [113] for a qualitative as well as quantitative study)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 147
                            }
                        ],
                        "text": "However, practical experiments suggest that, in computer vision tasks, two-stage training does not incur a significant loss in prediction accuracy [113]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1762509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eb464791ac91e8db4e284f48efef5b9e320701c",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent progress in per-pixel object class labeling of natural images can be attributed to the use of multiple types of image features and sound statistical learning approaches. Within the latter, Conditional Random Fields (CRF) are prominently used for their ability to represent interactions between random variables. Despite their popularity in computer vision, parameter learning for CRFs has remained difficult, popular approaches being cross-validation and piecewise training. \n \nIn this work, we propose a simple yet expressive tree-structured CRF based on a recent hierarchical image segmentation method. Our model combines and weights multiple image features within a hierarchical representation and allows simple and efficient globally-optimal learning of \u2245 105 parameters. The tractability of our model allows us to pose and answer some of the open questions regarding parameter learning applying to CRF-based approaches. The key findings for learning CRF models are, from the obvious to the surprising, i) multiple image features always help, ii) the limiting dimension with respect to current models is the amount of training data, iii) piecewise training is competitive, iv) current methods for max-margin training fail for models with many parameters."
            },
            "slug": "On-Parameter-Learning-in-CRF-Based-Approaches-to-Nowozin-Gehler",
            "title": {
                "fragments": [],
                "text": "On Parameter Learning in CRF-Based Approaches to Object Class Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a simple yet expressive tree-structured CRF based on a recent hierarchical image segmentation method that combines and weights multiple image features within a hierarchical representation and allows simple and efficient globally-optimal learning of \u2245 105 parameters."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72492981"
                        ],
                        "name": "Alahari Karteek",
                        "slug": "Alahari-Karteek",
                        "structuredName": {
                            "firstName": "Alahari",
                            "lastName": "Karteek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alahari Karteek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145485799"
                        ],
                        "name": "Chris Russell",
                        "slug": "Chris-Russell",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Russell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Alternatively, training modification to the S-SVM training have been proposed that approximate the optimization problem in a way similar to pseudo-likelihood training [22], or to piece-wise training of CRFs [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1936180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e8b6e5f8ae803fef4919d56d16424625f16d8eb",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional Random Field models have proved effective for several low-level computer vision problems. Inference in these models involves solving a combinatorial optimization problem, with methods such as graph cuts, belief propagation. Although several methods have been proposed to learn the model parameters from training data, they suffer from various drawbacks. Learning these parameters involves computing the partition function, which is intractable. To overcome this, state-of-the-art structured learning methods frame the problem as one of large margin estimation. Iterative solutions have been proposed to solve the resulting convex optimization problem. Each iteration involves solving an inference problem over all the labels, which limits the efficiency of these structured methods. In this paper we present an efficient large margin piece-wise learning method which is widely applicable. We show how the resulting optimization problem can be reduced to an equivalent convex problem with a small number of constraints, and solve it using an efficient scheme. Our method is both memory and computationally efficient. We show results on publicly available standard datasets."
            },
            "slug": "Efficient-piecewise-learning-for-conditional-random-Karteek-Russell",
            "title": {
                "fragments": [],
                "text": "Efficient piecewise learning for conditional random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how the resulting optimization problem can be reduced to an equivalent convex problem with a small number of constraints, and solve it using an efficient scheme."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152732433"
                        ],
                        "name": "Uwe Schmidt",
                        "slug": "Uwe-Schmidt",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Schmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uwe Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1510745655"
                        ],
                        "name": "Q. Gao",
                        "slug": "Q.-Gao",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 46
                            }
                        ],
                        "text": "[177] and continuous MRF natural image models [134]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3112504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff81ff15be185a2d59a8ce599d34c97d4a62e1b8",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov random fields (MRFs) are popular and generic probabilistic models of prior knowledge in low-level vision. Yet their generative properties are rarely examined, while application-specific models and non-probabilistic learning are gaining increased attention. In this paper we revisit the generative aspects of MRFs, and analyze the quality of common image priors in a fully application-neutral setting. Enabled by a general class of MRFs with flexible potentials and an efficient Gibbs sampler, we find that common models do not capture the statistics of natural images well. We show how to remedy this by exploiting the efficient sampler for learning better generative MRFs based on flexible potentials. We perform image restoration with these models by computing the Bayesian minimum mean squared error estimate (MMSE) using sampling. This addresses a number of shortcomings that have limited generative MRFs so far, and leads to substantially improved performance over maximum a-posteriori (MAP) estimation. We demonstrate that combining our learned generative models with sampling-based MMSE estimation yields excellent application results that can compete with recent discriminative methods."
            },
            "slug": "A-generative-perspective-on-MRFs-in-low-level-Schmidt-Gao",
            "title": {
                "fragments": [],
                "text": "A generative perspective on MRFs in low-level vision"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper revisits the generative aspects of MRFs, and addresses a number of shortcomings that have limited generative MRFs so far, and demonstrates that combining learned generative models with sampling-based MMSE estimation yields excellent application results that can compete with recent discriminative methods."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388416"
                        ],
                        "name": "S. Nowozin",
                        "slug": "S.-Nowozin",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Nowozin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowozin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1914930,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6222c1aa95b28865fdc20fc4ead15ac81dc75842",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov random field (MRF, CRF) models are popular in computer vision. However, in order to be computationally tractable they are limited to incorporate only local interactions and cannot model global properties, such as connectedness, which is a potentially useful high-level prior for object segmentation. In this work, we overcome this limitation by deriving a potential function that enforces the output labeling to be connected and that can naturally be used in the framework of recent MAP-MRF LP relaxations. Using techniques from polyhedral combinatorics, we show that a provably tight approximation to the MAP solution of the resulting MRF can still be found efficiently by solving a sequence of max-flow problems. The efficiency of the inference procedure also allows us to learn the parameters of a MRF with global connectivity potentials by means of a cutting plane algorithm. We experimentally evaluate our algorithm on both synthetic data and on the challenging segmentation task of the PASCAL VOC 2008 data set. We show that in both cases the addition of a connectedness prior significantly reduces the segmentation error."
            },
            "slug": "Global-connectivity-potentials-for-random-field-Nowozin-Lampert",
            "title": {
                "fragments": [],
                "text": "Global connectivity potentials for random field models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work derives a potential function that enforces the output labeling to be connected and that can naturally be used in the framework of recent MAP-MRF LP relaxations, and shows that a provably tight approximation to the MAP solution of the resulting MRF can still be found efficiently by solving a sequence of max-flow problems."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891751"
                        ],
                        "name": "E. Pasztor",
                        "slug": "E.-Pasztor",
                        "structuredName": {
                            "firstName": "Egon",
                            "lastName": "Pasztor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Pasztor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2575670"
                        ],
                        "name": "Owen Carmichael",
                        "slug": "Owen-Carmichael",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Carmichael",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Owen Carmichael"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "An early work using belief propagation in computer vision for superresolution is [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1414109,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "861897df39716877fb1e03a7d09a234faca076e9",
            "isKey": false,
            "numCitedBy": 1220,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a learning-based method for low-level vision problems\u2014estimating scenes from images. We generate a synthetic world of scenes and their corresponding rendered images, modeling their relationships with a Markov network. Bayesian belief propagation allows us to efficiently find a local maximum of the posterior probability for the scene, given an image. We call this approach VISTA\u2014Vision by Image/Scene TrAining.We apply VISTA to the \u201csuper-resolution\u201d problem (estimating high frequency details from a low-resolution image), showing good results. To illustrate the potential breadth of the technique, we also apply it in two other problem domains, both simplified. We learn to distinguish shading from reflectance variations in a single image under particular lighting conditions. For the motion estimation problem in a \u201cblobs world\u201d, we show figure/ground discrimination, solution of the aperture problem, and filling-in arising from application of the same probabilistic machinery."
            },
            "slug": "Learning-Low-Level-Vision-Freeman-Pasztor",
            "title": {
                "fragments": [],
                "text": "Learning Low-Level Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A learning-based method for low-level vision problems\u2014estimating scenes from images with Bayesian belief propagation, applied to the \u201csuper-resolution\u201d problem (estimating high frequency details from a low-resolution image), showing good results."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[146] use a structured support vector machine to predict a label for every pixel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13252941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb86d3cd5d18477be5164b5c9335bfb79e7124a7",
            "isKey": false,
            "numCitedBy": 309,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computer vision problems are naturally formulated as random fields, specifically MRFs or CRFs. The introduction of graph cuts has enabled efficient and optimal inference in associative random fields, greatly advancing applications such as segmentation, stereo reconstruction and many others. However, while fast inference is now widespread, parameter learning in random fields has remained an intractable problem. This paper shows how to apply fast inference algorithms, in particular graph cuts, to learn parameters of random fields with similar efficiency. We find optimal parameter values under standard regularized objective functions that ensure good generalization. Our algorithm enables learning of many parameters in reasonable time, and we explore further speedup techniques. We also discuss extensions to non-associative and multi-class problems. We evaluate the method on image segmentation and geometry recognition."
            },
            "slug": "Learning-CRFs-Using-Graph-Cuts-Szummer-Kohli",
            "title": {
                "fragments": [],
                "text": "Learning CRFs Using Graph Cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper shows how to apply fast inference algorithms, in particular graph cuts, to learn parameters of random fields with similar efficiency, and finds optimal parameter values under standard regularized objective functions that ensure good generalization."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862313"
                        ],
                        "name": "J. Weinman",
                        "slug": "J.-Weinman",
                        "structuredName": {
                            "firstName": "Jerod",
                            "lastName": "Weinman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weinman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859684"
                        ],
                        "name": "Lam C. Tran",
                        "slug": "Lam-C.-Tran",
                        "structuredName": {
                            "firstName": "Lam",
                            "lastName": "Tran",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lam C. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1972076"
                        ],
                        "name": "C. Pal",
                        "slug": "C.-Pal",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Pal",
                            "middleNames": [
                                "Joseph"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12000409,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70b73f8a0a46bf540ee15d0f179201ea1ff7b07d",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "As richer models for stereo vision are constructed, there is a growing interest in learning model parameters. To estimate parameters in Markov Random Field (MRF) based stereo formulations, one usually needs to perform approximate probabilistic inference. Message passing algorithms based on variational methods and belief propagation are widely used for approximate inference in MRFs. Conditional Random Fields (CRFs) are discriminative versions of traditional MRFs and have recently been applied to the problem of stereo vision. However, CRF parameter training typically requires expensive inference steps for each iteration of optimization. Inference is particularly slow when there are many discrete disparity levels, due to high state space cardinality. We present a novel CRF for stereo matching with an explicit occlusion model and propose sparse message passing to dramatically accelerate the approximate inference needed for parameter optimization. We show that sparse variational message passing iteratively minimizes the KL divergence between the approximation and model distributions by optimizing a lower bound on the partition function. Our experimental results show reductions in inference time of one order of magnitude with no loss in approximation quality. Learning using sparse variational message passing improves results over prior work using graph cuts."
            },
            "slug": "Efficiently-Learning-Random-Fields-for-Stereo-with-Weinman-Tran",
            "title": {
                "fragments": [],
                "text": "Efficiently Learning Random Fields for Stereo Vision with Sparse Message Passing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that sparse variational message passing iteratively minimizes the KL divergence between the approximation and model distributions by optimizing a lower bound on the partition function."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "More information about directed models can be found in [9, 77]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14995779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0a9b181fc252108de45720d4645ac245e1ba463",
            "isKey": false,
            "numCitedBy": 6021,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Most tasks require a person or an automated system to reason -- to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs."
            },
            "slug": "Probabilistic-Graphical-Models-Principles-and-Koller-Friedman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Graphical Models - Principles and Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The framework of probabilistic graphical models, presented in this book, provides a general approach for causal reasoning and decision making under uncertainty, allowing interpretable models to be constructed and then manipulated by reasoning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145349582"
                        ],
                        "name": "Sanjiv Kumar",
                        "slug": "Sanjiv-Kumar",
                        "structuredName": {
                            "firstName": "Sanjiv",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjiv Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "the task of recognizing man-made objects in images, originally proposed in [87]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1282113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015293bf7c4cf7ce50a01ce1ceb11f584d123d25",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classification of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classification problems using the graph min-cut algorithms. The performance of the model was verified on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments."
            },
            "slug": "Discriminative-Fields-for-Modeling-Spatial-in-Kumar-Hebert",
            "title": {
                "fragments": [],
                "text": "Discriminative Fields for Modeling Spatial Dependencies in Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed DRF model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758219"
                        ],
                        "name": "Matthew B. Blaschko",
                        "slug": "Matthew-B.-Blaschko",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Blaschko",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Blaschko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It has also been suggested to train hybrid generative/ discriminative models that avoid the need to perform prediction tasks at training time altogether [88]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 733873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "296b567818d3fb2704c71360d9edd8e0527b6202",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminative techniques, such as conditional random fields (CRFs) or structure aware maximum-margin techniques (maximum margin Markov networks (M3N), structured output support vector machines (S-SVM)), are state-of-the-art in the prediction of structured data. However, to achieve good results these techniques require complete and reliable ground truth, which is not always available in realistic problems. Furthermore, training either CRFs or margin-based techniques is computationally costly, because the runtime of current training methods depends not only on the size of the training set but also on properties of the output space to which the training samples are assigned.We propose an alternative model for structured output prediction, Joint Kernel Support Estimation (JKSE), which is rather generative in nature as it relies on estimating the joint probability density of samples and labels in the training set. This makes it tolerant against incomplete or incorrect labels and also opens the possibility of learning in situations where more than one output label can be considered correct.At the same time, we avoid typical problems of generative models as we do not attempt to learn the full joint probability distribution, but we model only its support in a joint reproducing kernel Hilbert space. As a consequence, JKSE training is possible by an adaption of the classical one-class SVM procedure. The resulting optimization problem is convex and efficiently solvable even with tens of thousands of training examples. A particular advantage of JKSE is that the training speed depends only on the size of the training set, and not on the total size of the label space. No inference step during training is required (as M3N and S-SVM would) nor do we have calculate a partition function (as CRFs do).Experiments on realistic data show that, for suitable kernel functions, our method works efficiently and robustly in situations that discriminative techniques have problems with or that are computationally infeasible for them."
            },
            "slug": "Structured-prediction-by-joint-kernel-support-Lampert-Blaschko",
            "title": {
                "fragments": [],
                "text": "Structured prediction by joint kernel support estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An alternative model for structured output prediction, Joint Kernel Support Estimation (JKSE), which is rather generative in nature as it relies on estimating the joint probability density of samples and labels in the training set and works efficiently and robustly in situations that discriminative techniques have problems with or that are computationally infeasible for them."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10705448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0467915e32f83b14c98b0da8b155620919cedd4f",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "A drawback of structured prediction methods is that parameter estimation requires repeated inference, which is intractable for general structures. In this paper, we present an approximate training algorithm called piecewise training (PW) that divides the factors into tractable subgraphs, which we call pieces, that are trained independently. Piecewise training can be interpreted as approximating the exact likelihood using belief propagation, and different ways of making this interpretation yield different insights into the method. We also present an extension to piecewise training, called piecewise pseudolikelihood (PWPL), designed for when variables have large cardinality. On several real-world natural language processing tasks, piecewise training performs superior to Besag\u2019s pseudolikelihood and sometimes comparably to exact maximum likelihood. In addition, PWPL performs similarly to PW and superior to standard pseudolikelihood, but is five to ten times more computationally efficient than batch maximum likelihood training."
            },
            "slug": "Piecewise-training-for-structured-prediction-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Piecewise training for structured prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper presents an approximate training algorithm called piecewise training (PW) that divides the factors into tractable subgraphs, which the authors call pieces, that are trained independently and performs superior to Besag's pseudolikelihood and sometimes comparably to exact maximum likelihood."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758219"
                        ],
                        "name": "Matthew B. Blaschko",
                        "slug": "Matthew-B.-Blaschko",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Blaschko",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Blaschko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Blaschko and Lampert [21] construct a kernelized S-SVM for this task that learns a prediction function into the set of 4-tuples of bounding box coordinates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10402359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8483da1bc3302c34b437baf8e329390da8f2c9bf",
            "isKey": false,
            "numCitedBy": 366,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Sliding window classifiers are among the most successful and widely applied techniques for object localization. However, training is typically done in a way that is not specific to the localization task. First a binary classifier is trained using a sample of positive and negative examples, and this classifier is subsequently applied to multiple regions within test images. We propose instead to treat object localization in a principled way by posing it as a problem of predicting structured data: we model the problem not as binary classification, but as the prediction of the bounding box of objects located in images. The use of a joint-kernelframework allows us to formulate the training procedure as a generalization of an SVM, which can be solved efficiently. We further improve computational efficiency by using a branch-and-bound strategy for localization during both training and testing. Experimental evaluation on the PASCAL VOC and TU Darmstadt datasets show that the structured training procedure improves performance over binary training as well as the best previously published scores."
            },
            "slug": "Learning-to-Localize-Objects-with-Structured-Output-Blaschko-Lampert",
            "title": {
                "fragments": [],
                "text": "Learning to Localize Objects with Structured Output Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes to treat object localization in a principled way by posing it as a problem of predicting structured data: it model the problem not as binary classification, but as the prediction of the bounding box of objects located in images."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3203698"
                        ],
                        "name": "B. Potetz",
                        "slug": "B.-Potetz",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Potetz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Potetz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14587123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a6cc31c0ffb912e126819b7c3a6a645888b8cf9",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief propagation over pairwise connected Markov random fields has become a widely used approach, and has been successfully applied to several important computer vision problems. However, pairwise interactions are often insufficient to capture the full statistics of the problem. Higher-order interactions are sometimes required. Unfortunately, the complexity of belief propagation is exponential in the size of the largest clique. In this paper, we introduce a new technique to compute belief propagation messages in time linear with respect to clique size for a large class of potential functions over real-valued variables. We demonstrate this technique in two applications. First, we perform efficient inference in graphical models where the spatial prior of natural images is captured by 2 times 2 cliques. This approach shows significant improvement over the commonly used pairwise-connected models, and may benefit a variety of applications using belief propagation to infer images or range images. Finally, we apply these techniques to shape-from-shading and demonstrate significant improvement over previous methods, both in quality and in flexibility."
            },
            "slug": "Efficient-Belief-Propagation-for-Vision-Using-Nodes-Potetz",
            "title": {
                "fragments": [],
                "text": "Efficient Belief Propagation for Vision Using Linear Constraint Nodes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new technique to compute belief propagation messages in time linear with respect to clique size for a large class of potential functions over real-valued variables and may benefit a variety of applications using belief propagation to infer images or range images."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 190
                            }
                        ],
                        "text": "Mean field methods perform approximate probabilistic inference by searching within a tractable subset of distributions for the distribution which best approximates the original distribution [66, 160]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2073260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6120cc252bc74239012f11b8b075cb7cb16bee26",
            "isKey": false,
            "numCitedBy": 2947,
            "numCiting": 127,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case."
            },
            "slug": "An-Introduction-to-Variational-Methods-for-Models-Jordan-Ghahramani",
            "title": {
                "fragments": [],
                "text": "An Introduction to Variational Methods for Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields), and describes a general framework for generating variational transformations based on convex duality."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2193311"
                        ],
                        "name": "V. Jojic",
                        "slug": "V.-Jojic",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Jojic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Jojic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145273587"
                        ],
                        "name": "Stephen Gould",
                        "slug": "Stephen-Gould",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gould",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[65] and also Savchynskyy et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6247355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92151e744f4eea51ff1deb4c9c296549d2b07a0a",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Approximate MAP inference in graphical models is an important and challenging problem for many domains including computer vision, computational biology and natural language understanding. Current state-of-the-art approaches employ convex relaxations of these problems as surrogate objectives, but only provide weak running time guarantees. In this paper, we develop an approximate inference algorithm that is both efficient and has strong theoretical guarantees. Specifically, our algorithm is guaranteed to converge to an e-accurate solution of the convex relaxation in O (1/e) time. We demonstrate our approach on synthetic and real-world problems and show that it outperforms current state-of-the-art techniques."
            },
            "slug": "Accelerated-dual-decomposition-for-MAP-inference-Jojic-Gould",
            "title": {
                "fragments": [],
                "text": "Accelerated dual decomposition for MAP inference"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper develops an approximate inference algorithm that is both efficient and has strong theoretical guarantees, and is guaranteed to converge to an e-accurate solution of the convex relaxation in O (1/e) time."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3) generalizes the Hinge loss to multiple outputs labels [148]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": false,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123333909"
                        ],
                        "name": "M.I. Jordan",
                        "slug": "M.I.-Jordan",
                        "structuredName": {
                            "firstName": "M.I.",
                            "lastName": "Jordan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M.I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207178945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d98d0d1900b13b87aa4ffd6b69c046beb63f0434",
            "isKey": false,
            "numCitedBy": 3901,
            "numCiting": 303,
            "paperAbstract": {
                "fragments": [],
                "text": "The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances \u2014 including the key problems of computing marginals and modes of probability distributions \u2014 are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms \u2014 among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations \u2014 can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models."
            },
            "slug": "Graphical-Models,-Exponential-Families,-and-Wainwright-Jordan",
            "title": {
                "fragments": [],
                "text": "Graphical Models, Exponential Families, and Variational Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1752880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47f3de18f63136823c523f74460ddb7a014bda7a",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a Bayesian framework for parsing images into their constituent visual patterns. The parsing algorithm optimizes the posterior probability and outputs a scene representation as a \u201cparsing graph\u201d, in a spirit similar to parsing sentences in speech and natural language. The algorithm constructs the parsing graph and re-configures it dynamically using a set of moves, which are mostly reversible Markov chain jumps. This computational framework integrates two popular inference approaches\u2014generative (top-down) methods and discriminative (bottom-up) methods. The former formulates the posterior probability in terms of generative models for images defined by likelihood functions and priors. The latter computes discriminative probabilities based on a sequence (cascade) of bottom-up tests/filters. In our Markov chain algorithm design, the posterior probability, defined by the generative models, is the invariant (target) probability for the Markov chain, and the discriminative probabilities are used to construct proposal probabilities to drive the Markov chain. Intuitively, the bottom-up discriminative probabilities activate top-down generative models. In this paper, we focus on two types of visual patterns\u2014generic visual patterns, such as texture and shading, and object patterns including human faces and text. These types of patterns compete and cooperate to explain the image and so image parsing unifies image segmentation, object detection, and recognition (if we use generic visual patterns only then image parsing will correspond to image segmentation (Tu and Zhu, 2002. IEEE Trans. PAMI, 24(5):657\u2013673). We illustrate our algorithm on natural images of complex city scenes and show examples where image segmentation can be improved by allowing object specific knowledge to disambiguate low-level segmentation cues, and conversely where object detection can be improved by using generic visual patterns to explain away shadows and occlusions."
            },
            "slug": "Image-Parsing:-Unifying-Segmentation,-Detection,-Tu-Chen",
            "title": {
                "fragments": [],
                "text": "Image Parsing: Unifying Segmentation, Detection, and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A Bayesian framework for parsing images into their constituent visual patterns that optimizes the posterior probability and outputs a scene representation as a \u201cparsing graph\u201d, in a spirit similar to parsing sentences in speech and natural language is presented."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Felzenszwalb and Huttenlocher [38] showed that the efficiency of belief propagation can be increased when using special types of factors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8702465,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3197ff5aa8f9cd36f98bcc8762b96250bdb4168",
            "isKey": false,
            "numCitedBy": 1174,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov random field models provide a robust and unified framework for early vision problems such as stereo and image restoration. Inference algorithms based on graph cuts and belief propagation have been found to yield accurate results, but despite recent advances are often too slow for practical use. In this paper we present some algorithmic techniques that substantially improve the running time of the loopy belief propagation approach. One of the techniques reduces the complexity of the inference algorithm to be linear rather than quadratic in the number of possible labels for each pixel, which is important for problems such as image restoration that have a large label set. Another technique speeds up and reduces the memory requirements of belief propagation on grid graphs. A third technique is a multi-grid method that makes it possible to obtain good results with a small fixed number of message passing iterations, independent of the size of the input images. Taken together these techniques speed up the standard algorithm by several orders of magnitude. In practice we obtain results that are as accurate as those of other global methods (e.g., using the Middlebury stereo benchmark) while being nearly as fast as purely local methods."
            },
            "slug": "Efficient-Belief-Propagation-for-Early-Vision-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Efficient Belief Propagation for Early Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Algorithmic techniques are presented that substantially improve the running time of the loopy belief propagation approach and reduce the complexity of the inference algorithm to be linear rather than quadratic in the number of possible labels for each pixel, which is important for problems such as image restoration that have a large label set."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500336"
                        ],
                        "name": "Alex Kulesza",
                        "slug": "Alex-Kulesza",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Kulesza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Kulesza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, recent results suggest that the use of suboptimal predictions during training can lead to prediction functions with bad performance [39, 86, 99], whereas alternative approaches based on relaxing the prediction problem work well in training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This stability is particularly important when the relaxation is used for solving prediction problems during parameter learning [39, 86, 99] and other approximate solution approaches do not come with this guarantee."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "theoretical results by Kulesza and Pereira [86] showed the severity of the problem: when training a structured perceptron already small prediction errors can lead to large deviations of the learned weight vector."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 10876177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "325ea1f2022ee3886a5810df76dcfbe4010ad439",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In many structured prediction problems, the highest-scoring labeling is hard to compute exactly, leading to the use of approximate inference methods. However, when inference is used in a learning algorithm, a good approximation of the score may not be sufficient. We show in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees. There are two reasons for this. First, approximate methods can effectively reduce the expressivity of an underlying model by making it impossible to choose parameters that reliably give good predictions. Second, approximations can respond to parameter changes in such a way that standard learning algorithms are misled. In contrast, we give two positive results in the form of learning bounds for the use of LP-relaxed inference in structured perceptron and empirical risk minimization settings. We argue that without understanding combinations of inference and learning, such as these, that are appropriately compatible, learning performance under approximate inference cannot be guaranteed."
            },
            "slug": "Structured-Learning-with-Approximate-Inference-Kulesza-Pereira",
            "title": {
                "fragments": [],
                "text": "Structured Learning with Approximate Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees, and argued that without understanding combinations of inference and learning, such as these that are appropriately compatible, learning performance under approximate inference cannot be guaranteed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14489533"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "It has been used parameter estimation in MRF in the early 1990s, [48], and more recently to learn the parameters of CRF in image segmentation, [159], and stereo depth estimation, [161]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20990858,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "d1a636654dea9109946c06da03735d87c2c561f9",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We attempt to unify several approaches to image segmentation in early vision under a common framework. The Bayesian approach is very attractive since: (i) it enables the assumptions used to be explicitly stated in the probability distributions, and (ii) it can be extended to deal with most other problems in early vision. Here, we consider the Markov random field formalism, a special case of the Bayesian approach, in which the probability distributions are specified by an energy function.We show that: (i) our discrete formulations for the energy function is closely related to the continuous formulation; (ii) by using the mean field (MF) theory approach, introduced by Geiger and Girosi [1991], several previous attempts to solve these energy functions are effectively equivalent; (iii) by varying the parameters of the energy functions we can obtain connections to nonlinear diffusion and minimal description length approaches to image segmentation; and (iv) simple modifications to the energy can give a direct relation to robust statistics or can encourage hysteresis and nonmaximum suppression."
            },
            "slug": "A-common-framework-for-image-segmentation-Geiger-Yuille",
            "title": {
                "fragments": [],
                "text": "A common framework for image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The Markov random field formalism is considered, a special case of the Bayesian approach, in which the probability distributions are specified by an energy function, and simple modifications to the energy can give a direct relation to robust statistics or can encourage hysteresis and nonmaximum suppression."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49933077"
                        ],
                        "name": "Louis-Philippe Morency",
                        "slug": "Louis-Philippe-Morency",
                        "structuredName": {
                            "firstName": "Louis-Philippe",
                            "lastName": "Morency",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Louis-Philippe Morency"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3171632"
                        ],
                        "name": "A. Quattoni",
                        "slug": "A.-Quattoni",
                        "structuredName": {
                            "firstName": "Ariadna",
                            "lastName": "Quattoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Quattoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "2 Gesture recognition with a chain-structured latent variable conditional random field [106]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[106] perform gesture recognition using a conditional random with chain-structured latent variables, see Figure 5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7117722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4d628ea1c03b9f70b1a23a61129aeee91e35062",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Many problems in vision involve the prediction of a class label for each frame in an unsegmented sequence. In this paper, we develop a discriminative framework for simultaneous sequence segmentation and labeling which can capture both intrinsic and extrinsic class dynamics. Our approach incorporates hidden state variables which model the sub-structure of a class sequence and learn dynamics between class labels. Each class label has a disjoint set of associated hidden states, which enables efficient training and inference in our model. We evaluated our method on the task of recognizing human gestures from unsegmented video streams and performed experiments on three different datasets of head and eye gestures. Our results demonstrate that our model compares favorably to Support Vector Machines, Hidden Markov Models, and Conditional Random Fields on visual gesture recognition tasks."
            },
            "slug": "Latent-Dynamic-Discriminative-Models-for-Continuous-Morency-Quattoni",
            "title": {
                "fragments": [],
                "text": "Latent-Dynamic Discriminative Models for Continuous Gesture Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A discriminative framework for simultaneous sequence segmentation and labeling which can capture both intrinsic and extrinsic class dynamics and incorporates hidden state variables which model the sub-structure of a class sequence and learn dynamics between class labels."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2731900"
                        ],
                        "name": "Oliver J. Woodford",
                        "slug": "Oliver-J.-Woodford",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Woodford",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oliver J. Woodford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The above ideas have been generalized to the so called marginal probability field model in [169]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14372063,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39895a507c887280a694f2a137bb0cac58dab6d8",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years the Markov Random Field (MRF) has become the de facto probabilistic model for low-level vision applications. However, in a maximum a posteriori (MAP) framework, MRFs inherently encourage delta function marginal statistics. By contrast, many low-level vision problems have heavy tailed marginal statistics, making the MRF model unsuitable. In this paper we introduce a more general Marginal Probability Field (MPF), of which the MRF is a special, linear case, and show that convex energy MPFs can be used to encourage arbitrary marginal statistics. We introduce a flexible, extensible framework for effectively optimizing the resulting NP-hard MAP problem, based around dual-decomposition and a modified min-cost flow algorithm, and which achieves global optimality in some instances. We use a range of applications, including image denoising and texture synthesis, to demonstrate the benefits of this class of MPF over MRFs."
            },
            "slug": "A-global-perspective-on-MAP-inference-for-low-level-Woodford-Rother",
            "title": {
                "fragments": [],
                "text": "A global perspective on MAP inference for low-level vision"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A flexible, extensible framework for effectively optimizing the resulting NP-hard MAP problem is introduced, based around dual-decomposition and a modified min-cost flow algorithm, and which achieves global optimality in some instances."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 52
                            }
                        ],
                        "text": "The name structured support vector machine learning [151] stems from this observation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 159
                            }
                        ],
                        "text": "then possible to prove convergence after O( 1 \u03b52 ) steps with the guarantee that the objective value at the solution differs at most \u03b5 from the global mimimum [151]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 77
                            }
                        ],
                        "text": "13) numerically by keeping a working set over the non-zero coefficients, see [151]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[151], but this has found less use in computer vision as it leads to a more complicated optimization problem, see [128]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17671150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc97e7dbb821a4edfb5151bff4352655eedca9ee",
            "isKey": true,
            "numCitedBy": 2247,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach."
            },
            "slug": "Large-Margin-Methods-for-Structured-and-Output-Tsochantaridis-Joachims",
            "title": {
                "fragments": [],
                "text": "Large Margin Methods for Structured and Interdependent Output Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation and presents a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33913193"
                        ],
                        "name": "Xuming He",
                        "slug": "Xuming-He",
                        "structuredName": {
                            "firstName": "Xuming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400347470"
                        ],
                        "name": "M. A. Carreira-Perpi\u00f1\u00e1n",
                        "slug": "M.-A.-Carreira-Perpi\u00f1\u00e1n",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Carreira-Perpi\u00f1\u00e1n",
                            "middleNames": [
                                "\u00c1."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Carreira-Perpi\u00f1\u00e1n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Contrastive divergence training has proved a competitive training technique for large scale CRFs [58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11859305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "363b56f85e12389017ba8894056a1b309e46a5f7",
            "isKey": false,
            "numCitedBy": 933,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to include contextual features for labeling images, in which each pixel is assigned to one of a finite set of labels. The features are incorporated into a probabilistic framework, which combines the outputs of several components. Components differ in the information they encode. Some focus on the image-label mapping, while others focus solely on patterns within the label field. Components also differ in their scale, as some focus on fine-resolution patterns while others on coarser, more global structure. A supervised version of the contrastive divergence algorithm is applied to learn these features from labeled image data. We demonstrate performance on two real-world image databases and compare it to a classifier and a Markov random field."
            },
            "slug": "Multiscale-conditional-random-fields-for-image-He-Zemel",
            "title": {
                "fragments": [],
                "text": "Multiscale conditional random fields for image labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "An approach to include contextual features for labeling images, in which each pixel is assigned to one of a finite set of labels, are incorporated into a probabilistic framework, which combines the outputs of several components."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35681810"
                        ],
                        "name": "Jo\u00e3o Carreira",
                        "slug": "Jo\u00e3o-Carreira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Carreira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Carreira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Carreira and Sminchisescu [30] recently proposed the constrained parametric min cuts (CPMC) method for generating class-independent object hypotheses based on a collection of figure-ground segmentation tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6041168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4131a2862d9f926c6727da6dc75c8fda25f4a9e5",
            "isKey": false,
            "numCitedBy": 476,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel framework for generating and ranking plausible objects hypotheses in an image using bottom-up processes and mid-level cues. The object hypotheses are represented as figure-ground segmentations, and are extracted automatically, without prior knowledge about properties of individual object classes, by solving a sequence of constrained parametric min-cut problems (CPMC) on a regular image grid. We then learn to rank the object hypotheses by training a continuous model to predict how plausible the segments are, given their mid-level region properties. We show that this algorithm significantly outperforms the state of the art for low-level segmentation in the VOC09 segmentation dataset. It achieves the same average best segmentation covering as the best performing technique to date [2], 0.61 when using just the top 7 ranked segments, instead of the full hierarchy in [2]. Our method achieves 0.78 average best covering using 154 segments. In a companion paper [18], we also show that the algorithm achieves state-of-the art results when used in a segmentation-based recognition pipeline."
            },
            "slug": "Constrained-parametric-min-cuts-for-automatic-Carreira-Sminchisescu",
            "title": {
                "fragments": [],
                "text": "Constrained parametric min-cuts for automatic object segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that this algorithm significantly outperforms the state of the art for low-level segmentation in the VOC09 segmentation dataset and achieves the same average best segmentation covering as the best performing technique to date."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2767462"
                        ],
                        "name": "A. Asuncion",
                        "slug": "A.-Asuncion",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Asuncion",
                            "middleNames": [
                                "U."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Asuncion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47362268"
                        ],
                        "name": "Qiang Liu",
                        "slug": "Qiang-Liu",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740137"
                        ],
                        "name": "A. Ihler",
                        "slug": "A.-Ihler",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Ihler",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ihler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Despite the many simplifications compared to a complete sampling approach, there are theoretic results that guarantee convergence to the global minimum under certain conditions [173], and that characterize the role of the sampling scheme used in the contrastive divergence procedure [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10724350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f788d291a100398d1d1cc59417d8fa5e357fbe22",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Composite likelihood methods provide a wide spectrum of computationally efficient techniques for statistical tasks such as parameter estimation and model selection. In this paper, we present a formal connection between the optimization of composite likelihoods and the well-known contrastive divergence algorithm. In particular, we show that composite likelihoods can be stochastically optimized by performing a variant of contrastive divergence with random-scan blocked Gibbs sampling. By using higher-order composite likelihoods, our proposed learning framework makes it possible to trade off computation time for increased accuracy. Furthermore, one can choose composite likelihood blocks that match the model\u2019s dependence structure, making the optimization of higher-order composite likelihoods computationally efficient. We empirically analyze the performance of blocked contrastive divergence on various models, including visible Boltzmann machines, conditional random fields, and exponential random graph models, and we demonstrate that using higher-order blocks improves both the accuracy of parameter estimates and the rate of convergence."
            },
            "slug": "Learning-with-Blocks:-Composite-Likelihood-and-Asuncion-Liu",
            "title": {
                "fragments": [],
                "text": "Learning with Blocks: Composite Likelihood and Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows that composite likelihoods can be stochastically optimized by performing a variant of contrastive divergence with random-scan blocked Gibbs sampling, and demonstrates that using higher-order blocks improves both the accuracy of parameter estimates and the rate of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50256971"
                        ],
                        "name": "Thomas Finley",
                        "slug": "Thomas-Finley",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Finley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Finley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, recent results suggest that the use of suboptimal predictions during training can lead to prediction functions with bad performance [39, 86, 99], whereas alternative approaches based on relaxing the prediction problem work well in training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This stability is particularly important when the relaxation is used for solving prediction problems during parameter learning [39, 86, 99] and other approximate solution approaches do not come with this guarantee."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Finley and Joachims [39] analyzed the situation of S-SVMs in more detail, when the loss-augment prediction problem can be performed only approximately."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 505690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc243b4920cdc9f45ef18ef0bad5b6576c444a34",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "While discriminative training (e.g., CRF, structural SVM) holds much promise for machine translation, image segmentation, and clustering, the complex inference these applications require make exact training intractable. This leads to a need for approximate training methods. Unfortunately, knowledge about how to perform efficient and effective approximate training is limited. Focusing on structural SVMs, we provide and explore algorithms for two different classes of approximate training algorithms, which we call undergenerating (e.g., greedy) and overgenerating (e.g., relaxations) algorithms. We provide a theoretical and empirical analysis of both types of approximate trained structural SVMs, focusing on fully connected pairwise Markov random fields. We find that models trained with overgenerating methods have theoretic advantages over undergenerating methods, are empirically robust relative to their undergenerating brethren, and relaxed trained models favor non-fractional predictions from relaxed predictors."
            },
            "slug": "Training-structural-SVMs-when-exact-inference-is-Finley-Joachims",
            "title": {
                "fragments": [],
                "text": "Training structural SVMs when exact inference is intractable"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work provides a theoretical and empirical analysis of both types of approximate trained structural SVMs, focusing on fully connected pairwise Markov random fields, and finds that models trained with overgenerating methods have theoretic advantages over undergeneration methods, are empirically robust relative to their undergenerating brethren, and relaxed trained models favor non-fractional predictions from relaxed predictors."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145644643"
                        ],
                        "name": "Andr\u00e9 F. T. Martins",
                        "slug": "Andr\u00e9-F.-T.-Martins",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Martins",
                            "middleNames": [
                                "F.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andr\u00e9 F. T. Martins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16010178,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4dfc0611c68a2d8f097fd4f4f0166860fdd9da1",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent approaches to learning structured predictors often require approximate inference for tractability; yet its effects on the learned model are unclear. Meanwhile, most learning algorithms act as if computational cost was constant within the model class. This paper sheds some light on the first issue by establishing risk bounds for max-margin learning with LP relaxed inference and addresses the second issue by proposing a new paradigm that attempts to penalize \"time-consuming\" hypotheses. Our analysis relies on a geometric characterization of the outer polyhedra associated with the LP relaxation. We then apply these techniques to the problem of dependency parsing, for which a concise LP formulation is provided that handles non-local output features. A significant improvement is shown over arc-factored models."
            },
            "slug": "Polyhedral-outer-approximations-with-application-to-Martins-Smith",
            "title": {
                "fragments": [],
                "text": "Polyhedral outer approximations with application to natural language parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper establishes risk bounds for max-margin learning with LP relaxed inference and addresses the second issue by proposing a new paradigm that attempts to penalize \"time-consuming\" hypotheses."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792728"
                        ],
                        "name": "Bogdan Savchynskyy",
                        "slug": "Bogdan-Savchynskyy",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Savchynskyy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bogdan Savchynskyy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40569085"
                        ],
                        "name": "J\u00f6rg H. Kappes",
                        "slug": "J\u00f6rg-H.-Kappes",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Kappes",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J\u00f6rg H. Kappes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067705802"
                        ],
                        "name": "S. Schmidt",
                        "slug": "S.-Schmidt",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679944"
                        ],
                        "name": "C. Schn\u00f6rr",
                        "slug": "C.-Schn\u00f6rr",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Schn\u00f6rr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schn\u00f6rr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[130] applied Nesterov\u2019s smoothing technique [110] to (4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 635273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a38f1d0ac45a44fcb7f58f3568a2f6bdfc8c1d1",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the MAP-labeling problem for graphical models by optimizing a dual problem obtained by Lagrangian decomposition. In this paper, we focus specifically on Nes-terov's optimal first-order optimization scheme for non-smooth convex programs, that has been studied for a range of other problems in computer vision and machine learning in recent years. We show that in order to obtain an efficiently convergent iteration, this approach should be augmented with a dynamic estimation of a corresponding Lip-schitz constant, leading to a runtime complexity of O(1/\u220a) in terms of the desired precision \u220a. Additionally, we devise a stopping criterion based on a duality gap as a sound basis for competitive comparison and show how to compute it efficiently. We evaluate our results using the publicly available Middlebury database and a set of computer generated graphical models that highlight specific aspects, along with other state-of-the-art methods for MAP-inference."
            },
            "slug": "A-study-of-Nesterov's-scheme-for-Lagrangian-and-MAP-Savchynskyy-Kappes",
            "title": {
                "fragments": [],
                "text": "A study of Nesterov's scheme for Lagrangian decomposition and MAP labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper focuses specifically on Nes-terov's optimal first-order optimization scheme for non-smooth convex programs, and shows that in order to obtain an efficiently convergent iteration, this approach should be augmented with a dynamic estimation of a corresponding Lip-schitz constant."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is possible because of a crucial insight by Hinton [61]: instead of waiting for the Markov chain to converge against its equilibrium distribution before computing a gradient approximation, we can use samples from the very beginning of the Markov chain."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4571,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728641"
                        ],
                        "name": "Lubor Ladicky",
                        "slug": "Lubor-Ladicky",
                        "structuredName": {
                            "firstName": "Lubor",
                            "lastName": "Ladicky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubor Ladicky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[74, 75] gave an example of an energy term with simple structure, called the Pn generalized Potts potential which can be optimized using graph cuts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 690715,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87073fd45685b78cb5a68e5eae331d88f2a2be63",
            "isKey": false,
            "numCitedBy": 1008,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel framework for labelling problems which is able to combine multiple segmentations in a principled manner. Our method is based on higher order conditional random fields and uses potentials defined on sets of pixels (image segments) generated using unsupervised segmentation algorithms. These potentials enforce label consistency in image regions and can be seen as a generalization of the commonly used pairwise contrast sensitive smoothness potentials. The higher order potential functions used in our framework take the form of the Robust Pn model and are more general than the Pn Potts model recently proposed by Kohli et al. We prove that the optimal swap and expansion moves for energy functions composed of these potentials can be computed by solving a st-mincut problem. This enables the use of powerful graph cut based move making algorithms for performing inference in the framework. We test our method on the problem of multi-class object segmentation by augmenting the conventional crf used for object segmentation with higher order potentials defined on image regions. Experiments on challenging data sets show that integration of higher order potentials quantitatively and qualitatively improves results leading to much better definition of object boundaries. We believe that this method can be used to yield similar improvements for many other labelling problems."
            },
            "slug": "Robust-Higher-Order-Potentials-for-Enforcing-Label-Kohli-Ladicky",
            "title": {
                "fragments": [],
                "text": "Robust Higher Order Potentials for Enforcing Label Consistency"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This paper proposes a novel framework for labelling problems which is able to combine multiple segmentations in a principled manner based on higher order conditional random fields and uses potentials defined on sets of pixels generated using unsupervised segmentation algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732879"
                        ],
                        "name": "L. Torresani",
                        "slug": "L.-Torresani",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Torresani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Torresani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 75
                            }
                        ],
                        "text": "Feature matching between images has been addressed using graph matching in [150], where dual decomposition provided strong relaxations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2545075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "695465a300d7d6387ad5c59054873bf40d5841f4",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a new approach for establishing correspondences between sparse image features related by an unknown non-rigid mapping and corrupted by clutter and occlusion, such as points extracted from a pair of images containing a human figure in distinct poses. We formulate this matching task as an energy minimization problem by defining a complex objective function of the appearance and the spatial arrangement of the features. Optimization of this energy is an instance of graph matching, which is in general a NP-hard problem. We describe a novel graph matching optimization technique, which we refer to as dual decomposition (DD), and demonstrate on a variety of examples that this method outperforms existing graph matching algorithms. In the majority of our examples DD is able to find the global minimum within a minute. The ability to globally optimize the objective allows us to accurately learn the parameters of our matching model from training examples. We show on several matching tasks that our learned model yields results superior to those of state-of-the-art methods."
            },
            "slug": "Feature-Correspondence-Via-Graph-Matching:-Models-Torresani-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "Feature Correspondence Via Graph Matching: Models and Global Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A novel graph matching optimization technique, which is referred to as dual decomposition (DD), is described, and it is demonstrated on a variety of examples that this method outperforms existing graph matching algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15116562,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9a54374aec5c92296c7b24436f08934643829ae",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a refined mean field approximation for inference and learning in probabilistic neural networks. Our mean field theory, unlike most, does not assume that the units behave as independent degrees of freedom; instead, it exploits in a principled way the existence of large substructures that are computationally tractable. To illustrate the advantages of this framework, we show how to incorporate weak higher order interactions into a first-order hidden Markov model, treating the corrections (but not the first order structure) within mean field theory."
            },
            "slug": "Exploiting-Tractable-Substructures-in-Intractable-Saul-Jordan",
            "title": {
                "fragments": [],
                "text": "Exploiting Tractable Substructures in Intractable Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A refined mean field approximation for inference and learning in probabilistic neural networks is developed, and it is shown how to incorporate weak higher order interactions into a first-order hidden Markov model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144719476"
                        ],
                        "name": "Adrian Barbu",
                        "slug": "Adrian-Barbu",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Barbu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrian Barbu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, in [10] image segmentation is performed by simulated annealing, but the simulation is carried out using an efficient sampler for partitionings, giving rise to a hundredfold speedup over naive single-variable Gibbs sampling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 410716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "681c3cfa3775e5631bc7286e43ff90691d3dcdda",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Many vision tasks can be formulated as graph partition problems that minimize energy functions. For such problems, the Gibbs sampler provides a general solution but is very slow, while other methods, such as Ncut and graph cuts are computationally effective but only work for specific energy forms and are not generally applicable. In this paper, we present a new inference algorithm that generalizes the Swendsen-Wang method to arbitrary probabilities defined on graph partitions. We begin by computing graph edge weights, based on local image features. Then, the algorithm iterates two steps: (1) graph clustering - it forms connected components by cutting the edges probabilistically based on their weights; (2) graph relabeling - it selects one connected component and flips probabilistically, the coloring of all vertices in the component simultaneously. Thus, it realizes the split, merge, and regrouping of a \"chunk\" of the graph, in contrast to Gibbs sampler that flips a single vertex. We prove that this algorithm simulates ergodic and reversible Markov chain jumps in the space of graph partitions and is applicable to arbitrary posterior probabilities or energy functions defined on graphs. We demonstrate the algorithm on two typical problems in computer vision-image segmentation and stereo vision. Experimentally, we show that it is 100-400 times faster in CPU time than the classical Gibbs sampler and 20-40 times faster then the DDMCMC segmentation algorithm. For stereo, we compare performance with graph cuts and belief propagation. We also show that our algorithm can automatically infer generative models and obtain satisfactory results (better than the graphic cuts or belief propagation) in the same amount of time."
            },
            "slug": "Generalizing-Swendsen-Wang-to-sampling-arbitrary-Barbu-Zhu",
            "title": {
                "fragments": [],
                "text": "Generalizing Swendsen-Wang to sampling arbitrary posterior probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new inference algorithm that generalizes the Swendsen-Wang method to arbitrary probabilities defined on graph partitions and is applicable to arbitrary posterior probabilities or energy functions defined on graphs, and proves that it simulates ergodic and reversible Markov chain jumps in the space of graph partitions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13571735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a9049a50dfe94fa4473880a9b60c99333ade685",
            "isKey": false,
            "numCitedBy": 1644,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images."
            },
            "slug": "Learning-a-classification-model-for-segmentation-Ren-Malik",
            "title": {
                "fragments": [],
                "text": "Learning a classification model for segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A two-class classification model for grouping is proposed that defines a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation, and trains a linear classifier to combine these features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Superpixels introduced in [124, 107] are a common example of this reduction for image labeling tasks: by representing large sets of pixels as a single superpixel, a set of decisions \u2014 assigning one label to each pixel \u2014 is reduced to the single decision of assigning a label to the superpixel, as shown in Figures 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2983119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32510e7f88bc0767fbbc811397ba068dbc4cf549",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we show how segmentation as preprocessing paradigm can be used to improve the efficiency and accuracy of model search in an image. We operationalize this idea using an over-segmentation of an image into superpixels. The problem domain we explore is human body pose estimation from still images. The superpixels prove useful in two ways. First, we restrict the joint positions in our human body model to lie at centers of superpixels, which reduces the size of the model search space. In addition, accurate support masks for computing features on half-limbs of the body model are obtained by using agglomerations of superpixels as half limb segments. We present results on a challenging dataset of people in sports news images"
            },
            "slug": "Guiding-model-search-using-segmentation-Mori",
            "title": {
                "fragments": [],
                "text": "Guiding model search using segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "It is shown how segmentation as preprocessing paradigm can be used to improve the efficiency and accuracy of model search in an image by using an over-segmentation of an image into superpixels."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922280"
                        ],
                        "name": "O. Veksler",
                        "slug": "O.-Veksler",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Veksler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Veksler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[27] use an MRF to encourage a piecewise smooth estimate of disparity values."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6436838,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b543d70d3e4fe6673a2e39832f005fa7ebc79cec",
            "isKey": false,
            "numCitedBy": 528,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov Random Fields (MRFs) can be used for a wide variety of vision problems. In this paper we focus on MRFs with two-valued clique potentials, which form a generalized Potts model. We show that the maximum a posteriori estimate of such an MRF can be obtained by solving a multiway minimum cut problem on a graph. We develop efficient algorithms for computing good approximations to the minimum multiway, cut. The visual correspondence problem can be formulated as an MRF in our framework; this yields quite promising results on real data with ground truth. We also apply our techniques to MRFs with linear clique potentials."
            },
            "slug": "Markov-random-fields-with-efficient-approximations-Boykov-Veksler",
            "title": {
                "fragments": [],
                "text": "Markov random fields with efficient approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper shows that the maximum a posteriori estimate of such an MRF can be obtained by solving a multiway minimum cut problem on a graph, and develops efficient algorithms for computing good approximations to the minimum multiway, cut."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144773105"
                        ],
                        "name": "S. Vicente",
                        "slug": "S.-Vicente",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Vicente",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vicente"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[158] propose a more efficient procedure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "For the simplest model in [158] with binary label set L = {0,1}, the function g(x,y) is given as follows."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "In [158] the authors derive an approach to binary image segmentation incorporating a global appearance model of the object to be segmented."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8317980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64d1f354287192d031d7f4f84bab3c60d2425e46",
            "isKey": true,
            "numCitedBy": 114,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Many interactive image segmentation approaches use an objective function which includes appearance models as an unknown variable. Since the resulting optimization problem is NP-hard the segmentation and appearance are typically optimized separately, in an EM-style fashion. One contribution of this paper is to express the objective function purely in terms of the unknown segmentation, using higher-order cliques. This formulation reveals an interesting bias of the model towards balanced segmentations. Furthermore, it enables us to develop a new dual decomposition optimization procedure, which provides additionally a lower bound. Hence, we are able to improve on existing optimizers, and verify that for a considerable number of real world examples we even achieve global optimality. This is important since we are able, for the first time, to analyze the deficiencies of the model. Another contribution is to establish a property of a particular dual decomposition approach which involves convex functions depending on foreground area. As a consequence, we show that the optimal decomposition for our problem can be computed efficiently via a parametric maxflow algorithm."
            },
            "slug": "Joint-optimization-of-segmentation-and-appearance-Vicente-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "Joint optimization of segmentation and appearance models"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper expresses the objective function purely in terms of the unknown segmentation, using higher-order cliques, and shows that the optimal decomposition for the problem can be computed efficiently via a parametric maxflow algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50256971"
                        ],
                        "name": "Thomas Finley",
                        "slug": "Thomas-Finley",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Finley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Finley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3166569"
                        ],
                        "name": "C. Yu",
                        "slug": "C.-Yu",
                        "structuredName": {
                            "firstName": "Chun-Nam",
                            "lastName": "Yu",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The opposite direction requires a more careful analysis of independences between the constraints, see [63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14211670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f30aba767d71c1db5ea70b041d9fcc2b9b1ddad4",
            "isKey": false,
            "numCitedBy": 1083,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminative training approaches like structural SVMs have shown much promise for building highly complex and accurate models in areas like natural language processing, protein structure prediction, and information retrieval. However, current training algorithms are computationally expensive or intractable on large datasets. To overcome this bottleneck, this paper explores how cutting-plane methods can provide fast training not only for classification SVMs, but also for structural SVMs. We show that for an equivalent \u201c1-slack\u201d reformulation of the linear SVM training problem, our cutting-plane method has time complexity linear in the number of training examples. In particular, the number of iterations does not depend on the number of training examples, and it is linear in the desired precision and the regularization parameter. Furthermore, we present an extensive empirical evaluation of the method applied to binary classification, multi-class classification, HMM sequence tagging, and CFG parsing. The experiments show that the cutting-plane algorithm is broadly applicable and fast in practice. On large datasets, it is typically several orders of magnitude faster than conventional training methods derived from decomposition methods like SVM-light, or conventional cutting-plane methods. Implementations of our methods are available at www.joachims.org."
            },
            "slug": "Cutting-plane-training-of-structural-SVMs-Joachims-Finley",
            "title": {
                "fragments": [],
                "text": "Cutting-plane training of structural SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper explores how cutting-plane methods can provide fast training not only for classification SVMs, but also for structural SVMs and presents an extensive empirical evaluation of the method applied to binary classification, multi-class classification, HMM sequence tagging, and CFG parsing."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740145"
                        ],
                        "name": "V. Lempitsky",
                        "slug": "V.-Lempitsky",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lempitsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lempitsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14664687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3f0567853773023196890cb0a52e2292daca56f",
            "isKey": false,
            "numCitedBy": 497,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computer vision applications rely on the efficient optimization of challenging, so-called non-submodular, binary pairwise MRFs. A promising graph cut based approach for optimizing such MRFs known as \"roof duality\" was recently introduced into computer vision. We study two methods which extend this approach. First, we discuss an efficient implementation of the \"probing\" technique introduced recently by Bows et al. (2006). It simplifies the MRF while preserving the global optimum. Our code is 400-700 faster on some graphs than the implementation of the work of Bows et al. (2006). Second, we present a new technique which takes an arbitrary input labeling and tries to improve its energy. We give theoretical characterizations of local minima of this procedure. We applied both techniques to many applications, including image segmentation, new view synthesis, super-resolution, diagram recognition, parameter learning, texture restoration, and image deconvolution. For several applications we see that we are able to find the global minimum very efficiently, and considerably outperform the original roof duality approach. In comparison to existing techniques, such as graph cut, TRW, BP, ICM, and simulated annealing, we nearly always find a lower energy."
            },
            "slug": "Optimizing-Binary-MRFs-via-Extended-Roof-Duality-Rother-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "Optimizing Binary MRFs via Extended Roof Duality"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An efficient implementation of the \"probing\" technique is discussed, which simplifies the MRF while preserving the global optimum, and a new technique which takes an arbitrary input labeling and tries to improve its energy is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Further generalizations of the mean field approach are made in [170] and [167]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7950005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "742432ccfa877d29ec30b19f0b7a3f5d4422681c",
            "isKey": false,
            "numCitedBy": 648,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational message passing has been implemented in the form of a general purpose inference engine called VIBES ('Variational Inference for BayEsian networkS') which allows models to be specified graphically and then solved variationally without recourse to coding."
            },
            "slug": "Variational-Message-Passing-Winn-Bishop",
            "title": {
                "fragments": [],
                "text": "Variational Message Passing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Variational Message Passing is introduced, a general purpose algorithm for applying variational inference to Bayesian Networks and can be applied to very general class of conjugate-exponential models because it uses a factorised variational approximation."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719613"
                        ],
                        "name": "S. E. Shimony",
                        "slug": "S.-E.-Shimony",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Shimony",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. E. Shimony"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Both inference problems are known to be NP-hard for general graphs and factors [138] but can be tractable if suitably restricted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13405361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a5867d0b9b997108633ff3da314edf69b0122c",
            "isKey": false,
            "numCitedBy": 394,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Finding-MAPs-for-Belief-Networks-is-NP-Hard-Shimony",
            "title": {
                "fragments": [],
                "text": "Finding MAPs for Belief Networks is NP-Hard"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13693897"
                        ],
                        "name": "Nathan D. Ratliff",
                        "slug": "Nathan-D.-Ratliff",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Ratliff",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan D. Ratliff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8195063"
                        ],
                        "name": "Martin A. Zinkevich",
                        "slug": "Martin-A.-Zinkevich",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Zinkevich",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Zinkevich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[123] propose maximum margin planning for this task: they use a structured support vector machine to learn a prediction function for planning: training data consists of images in a per-pixel color representation, and example paths with the intended property, e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1044953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "117a50fbdfd473e43e550c6103733e6cb4aecb4c",
            "isKey": false,
            "numCitedBy": 628,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Imitation learning of sequential, goal-directed behavior by standard supervised techniques is often difficult. We frame learning such behaviors as a maximum margin structured prediction problem over a space of policies. In this approach, we learn mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior. Further, we demonstrate a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference. Although the technique is general, it is particularly relevant in problems where A* and dynamic programming approaches make learning policies tractable in problems beyond the limitations of a QP formulation. We demonstrate our approach applied to route planning for outdoor mobile robots, where the behavior a designer wishes a planner to execute is often clear, while specifying cost functions that engender this behavior is a much more difficult task."
            },
            "slug": "Maximum-margin-planning-Ratliff-Bagnell",
            "title": {
                "fragments": [],
                "text": "Maximum margin planning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work learns mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior, and demonstrates a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9140880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "434bc6e9b1aed420f16e279467a551b0df70f20f",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The mean field methods, which entail approximating intractable probability distributions variationally with distributions from a tractable family, enjoy high efficiency, guaranteed convergence, and provide lower bounds on the true likelihood. But due to requirement for model-specific derivation of the optimization equations and unclear inference quality in various models, it is not widely used as a generic approximate inference algorithm. In this paper, we discuss a generalized mean field theory on variational approximation to a broad class of intractable distributions using a rich set of tractable distributions via constrained optimization over distribution spaces. We present a class of generalized mean field (GMF) algorithms for approximate inference in complex exponential family models, which entails limiting the optimization over the class of cluster-factorizable distributions. GMF is a generic method requiring no model-specific derivations. It factors a complex model into a set of disjoint variable clusters, and uses a set of canonical fix-point equations to iteratively update the cluster distributions, and converge to locally optimal cluster marginals that preserve the original dependency structure within each cluster, hence, fully decomposed the overall inference problem. We empirically analyzed the effect of different tractable family (clusters of different granularity) on inference quality, and compared GMF with BP on several canonical models. Possible extension to higher-order MF approximation is also discussed."
            },
            "slug": "A-generalized-mean-field-algorithm-for-variational-Xing-Jordan",
            "title": {
                "fragments": [],
                "text": "A generalized mean field algorithm for variational inference in exponential families"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A class of generalized mean field algorithms for approximate inference in complex exponential family models, which entails limiting the optimization over the class of cluster-factorizable distributions via constrained optimization over distribution spaces is presented."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7344844"
                        ],
                        "name": "Patrick A. Pletscher",
                        "slug": "Patrick-A.-Pletscher",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pletscher",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick A. Pletscher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706780"
                        ],
                        "name": "Cheng Soon Ong",
                        "slug": "Cheng-Soon-Ong",
                        "structuredName": {
                            "firstName": "Cheng Soon",
                            "lastName": "Ong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng Soon Ong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2254743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "942ec6bdd961facadcf75d94d94e3e67c3199494",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of training discriminative structured output predictors, such as conditional random fields (CRFs) and structured support vector machines (SSVMs). A generalized loss function is introduced, which jointly maximizes the entropy and the margin of the solution. The CRF and SSVM emerge as special cases of our framework. The probabilistic interpretation of large margin methods reveals insights about margin and slack rescaling. Furthermore, we derive the corresponding extensions for latent variable models, in which training operates on partially observed outputs. Experimental results for multiclass, linear-chain models and multiple instance learning demonstrate that the generalized loss can improve accuracy of the resulting classifiers."
            },
            "slug": "Entropy-and-Margin-Maximization-for-Structured-Pletscher-Ong",
            "title": {
                "fragments": [],
                "text": "Entropy and Margin Maximization for Structured Output Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A generalized loss function is introduced, which jointly maximizes the entropy and the margin of the solution, and the corresponding extensions for latent variable models, in which training operates on partially observed outputs are derived."
            },
            "venue": {
                "fragments": [],
                "text": "ECML/PKDD"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3717791"
                        ],
                        "name": "M. P. Kumar",
                        "slug": "M.-P.-Kumar",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Pawan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. P. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[74, 75] gave an example of an energy term with simple structure, called the Pn generalized Potts potential which can be optimized using graph cuts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3084505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37f70fdac335148c52b6e29087dcfa838253af70",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we extend the class of energy functions for which the optimal alpha-expansion and alphabeta-swap moves can be computed in polynomial time. Specifically, we introduce a class of higher order clique potentials and show that the expansion and swap moves for any energy function composed of these potentials can be found by minimizing a submodular function. We also show that for a subset of these potentials, the optimal move can be found by solving an st-mincut problem. We refer to this subset as the P3 Potts model. Our results enable the use of powerful move making algorithms i.e. alpha-expansion and alphabeta-swap for minimization of energy functions involving higher order cliques. Such functions have the capability of modelling the rich statistics of natural scenes and can be used for many applications in computer vision. We demonstrate their use on one such application i.e. the texture based video segmentation problem."
            },
            "slug": "P3-&-Beyond:-Solving-Energies-with-Higher-Order-Kohli-Kumar",
            "title": {
                "fragments": [],
                "text": "P3 & Beyond: Solving Energies with Higher Order Cliques"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A class of higher order clique potentials is introduced and it is shown that the expansion and swap moves for any energy function composed of these potentials can be found by minimizing a submodular function."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 215
                            }
                        ],
                        "text": "This holds in weaker form also for variants such as the averaged structured perceptron which introduces regularization by returning the average of all weight vector obtained during training instead of the final one [Collins, 2002]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 31
                            }
                        ],
                        "text": "Structured perceptron learning [Collins, 2002] generalizes the multiclass perceptron [Crammer and Singer, 2003] to structured output spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10888973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a7958b418bceb48a315384568091ab1898b1640",
            "isKey": false,
            "numCitedBy": 2272,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "slug": "Discriminative-Training-Methods-for-Hidden-Markov-Collins",
            "title": {
                "fragments": [],
                "text": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results on part-of-speech tagging and base noun phrase chunking are given, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740145"
                        ],
                        "name": "V. Lempitsky",
                        "slug": "V.-Lempitsky",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lempitsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lempitsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 514676,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae89e41f77a7c3ac84be98d83fd925b539639b57",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient global optimization techniques such as graph cut exist for energies corresponding to binary image segmentation from low-level cues. However, introducing a high-level prior such as a shape prior or a color-distribution prior into the segmentation process typically results in an energy that is much harder to optimize. The main contribution of the paper is a new global optimization framework for a wide class of such energies. The framework is built upon two powerful techniques: graph cut and branch-and-bound. These techniques are unified through the derivation of lower bounds on the energies. Being computable via graph cut, these bounds are used to prune branches within a branch-and-bound search. \n \nWe demonstrate that the new framework can compute globally optimal segmentations for a variety of segmentation scenarios in a reasonable time on a modern CPU. These scenarios include unsupervised segmentation of an object undergoing 3D pose change, category-specific shape segmentation, and the segmentation under intensity/color priors defined by Chan-Vese and GrabCut functionals."
            },
            "slug": "Image-Segmentation-by-Branch-and-Mincut-Lempitsky-Blake",
            "title": {
                "fragments": [],
                "text": "Image Segmentation by Branch-and-Mincut"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that the new framework can compute globally optimal segmentations for a variety of segmentation scenarios in a reasonable time on a modern CPU."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39869361"
                        ],
                        "name": "J. Johnson",
                        "slug": "J.-Johnson",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Johnson",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688721"
                        ],
                        "name": "D. Malioutov",
                        "slug": "D.-Malioutov",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Malioutov",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Malioutov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 95
                            }
                        ],
                        "text": "This derivation is similar to recently proposed efficient MAP inference algorithms, such as in [64, 82, 160, 162]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14433916,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "9bdb624f6f52609c231d0cee8abd8068b4199ec8",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a general framework for MAP es- timation in discrete and Gaussian graphical models using Lagrangian relaxation techniques. The key idea is to refor- mulate an intractable estimation problem as one defined on a more tractable graph, but subject to additional constraints. Relaxing these constraints gives a tractable dual problem, one defined by a thin graph, which is then optimized by an iterative procedure. When this iterative optimization leads to a consistent estimate, one which also satisfies the constraints, then it corresponds to an optimal MAP estimate of the original model. Otherwise there is a \"duality gap\", and we obtain a bound on the optimal solution. Thus, our approach combines convex optimization with dynamic programming techniques applicable for thin graphs. The popular tree-reweighted max- product (TRMP) method may be seen as solving a particular class of such relaxations, where the intractable graph is relaxed to a set of spanning trees. We also consider relaxations to a set of small induced subgraphs, thin subgraphs (e.g. loops), and a connected tree obtained by \"unwinding\" cycles. In addition, we propose a new class of multiscale relaxations that introduce \"summary\" variables. The potential benefits of such generalizations include: reducing or eliminating the \"duality gap\" in hard problems, reducing the number of Lagrange multipliers in the dual problem, and accelerating convergence of the iterative optimization procedure."
            },
            "slug": "Lagrangian-Relaxation-for-MAP-Estimation-in-Models-Johnson-Malioutov",
            "title": {
                "fragments": [],
                "text": "Lagrangian Relaxation for MAP Estimation in Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A general framework for MAP es- timation in discrete and Gaussian graphical models using Lagrangian relaxation techniques is developed, and a new class of multiscale relaxations that introduce \"summary\" variables are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709053"
                        ],
                        "name": "Daniel Scharstein",
                        "slug": "Daniel-Scharstein",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Scharstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Scharstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922280"
                        ],
                        "name": "O. Veksler",
                        "slug": "O.-Veksler",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Veksler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Veksler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696487"
                        ],
                        "name": "A. Agarwala",
                        "slug": "A.-Agarwala",
                        "structuredName": {
                            "firstName": "Aseem",
                            "lastName": "Agarwala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agarwala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802944"
                        ],
                        "name": "M. Tappen",
                        "slug": "M.-Tappen",
                        "structuredName": {
                            "firstName": "Marshall",
                            "lastName": "Tappen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tappen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1210309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0f02f771339d3e5524238a9d960b2ef505e2f47",
            "isKey": false,
            "numCitedBy": 1029,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Among the most exciting advances in early vision has been the development of efficient energy minimization algorithms for pixel-labeling tasks such as depth or texture computation. It has been known for decades that such problems can be elegantly expressed as Markov random fields, yet the resulting energy minimization problems have been widely viewed as intractable. Algorithms such as graph cuts and loopy belief propagation (LBP) have proven to be very powerful: For example, such methods form the basis for almost all the top-performing stereo methods. However, the trade-offs among different energy minimization algorithms are still not well understood. In this paper, we describe a set of energy minimization benchmarks and use them to compare the solution quality and runtime of several common energy minimization algorithms. We investigate three promising methods-graph cuts, LBP, and tree-reweighted message passing-in addition to the well-known older iterated conditional mode (ICM) algorithm. Our benchmark problems are drawn from published energy functions used for stereo, image stitching, interactive segmentation, and denoising. We also provide a general-purpose software interface that allows vision researchers to easily switch between optimization methods. The benchmarks, code, images, and results are available at http://vision.middlebury.edu/MRF/."
            },
            "slug": "A-Comparative-Study-of-Energy-Minimization-Methods-Szeliski-Zabih",
            "title": {
                "fragments": [],
                "text": "A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A set of energy minimization benchmarks are described and used to compare the solution quality and runtime of several common energy minimizations algorithms and a general-purpose software interface is provided that allows vision researchers to easily switch between optimization methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50126864"
                        ],
                        "name": "Joshua Goodman",
                        "slug": "Joshua-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joshua Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 2
                            }
                        ],
                        "text": ", [Goodman, 2004] for an overview of exponential family priors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1710724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "326e0f4eb9f56005774c64d1fce5ea77d0f15d4f",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum entropy models are a common modeling technique, but prone to overfitting. We show that using an exponential distribution as a prior leads to bounded absolute discounting by a constant. We show that this prior is better motivated by the data than previous techniques such as a Gaussian prior, and often produces lower error rates. Exponential priors also lead to a simpler learning algorithm and to easier to understand behavior. Furthermore, exponential priors help explain the success of some previous smoothing techniques, and suggest simple variations that work better."
            },
            "slug": "Exponential-Priors-for-Maximum-Entropy-Models-Goodman",
            "title": {
                "fragments": [],
                "text": "Exponential Priors for Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is shown that using an exponential distribution as a prior leads to bounded absolute discounting by a constant, and is better motivated by the data than previous techniques such as a Gaussian prior, and often produces lower error rates."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3166569"
                        ],
                        "name": "C. Yu",
                        "slug": "C.-Yu",
                        "structuredName": {
                            "firstName": "Chun-Nam",
                            "lastName": "Yu",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10240161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ae20e0bdfddc1888148e0fcde88d937e96318d2",
            "isKey": false,
            "numCitedBy": 714,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a large-margin formulation and algorithm for structured output prediction that allows the use of latent variables. Our proposal covers a large range of application problems, with an optimization problem that can be solved efficiently using Concave-Convex Programming. The generality and performance of the approach is demonstrated through three applications including motiffinding, noun-phrase coreference resolution, and optimizing precision at k in information retrieval."
            },
            "slug": "Learning-structural-SVMs-with-latent-variables-Yu-Joachims",
            "title": {
                "fragments": [],
                "text": "Learning structural SVMs with latent variables"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A large-margin formulation and algorithm for structured output prediction that allows the use of latent variables and the generality and performance of the approach is demonstrated through three applications including motiffinding, noun-phrase coreference resolution, and optimizing precision at k in information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37077406"
                        ],
                        "name": "C. Teo",
                        "slug": "C.-Teo",
                        "structuredName": {
                            "firstName": "Choon",
                            "lastName": "Teo",
                            "middleNames": [
                                "Hui"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Teo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713876"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15174027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d7f4d7d9acfe8b89a88ab45ee8480adb7fd0489",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Gaussian Processes, Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as L1 and L2 penalties. In addition to the unified framework we present tight convergence bounds, which show that our algorithm converges in O(1/e) steps to e precision for general convex problems and in O(log (1/e)) steps for continuously differentiable problems. We demonstrate the performance of our general purpose solver on a variety of publicly available data sets."
            },
            "slug": "Bundle-Methods-for-Regularized-Risk-Minimization-Teo-Vishwanathan",
            "title": {
                "fragments": [],
                "text": "Bundle Methods for Regularized Risk Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The theory and implementation of a scalable and modular convex solver which solves all these estimation problems, which can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as L1 and L2 penalties is described."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802944"
                        ],
                        "name": "M. Tappen",
                        "slug": "M.-Tappen",
                        "structuredName": {
                            "firstName": "Marshall",
                            "lastName": "Tappen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tappen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10387823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5648b62c444556ea86d615037167e648bb771451",
            "isKey": false,
            "numCitedBy": 521,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent stereo algorithms have achieved impressive results by modelling the disparity image as a Markov Random Field (MRF). An important component of an MRF-based approach is the inference algorithm used to find the most likely setting of each node in the MRF. Algorithms have been proposed which use graph cuts or belief propagation for inference. These stereo algorithms differ in both the inference algorithm used and the formulation of the MRF. It is unknown whether to attribute the responsibility for differences in performance to the MRF or the inference algorithm. We address this through controlled experiments by comparing the belief propagation algorithm and the graph cuts algorithm on the same MRF's, which have been created for calculating stereo disparities. We find that the labellings produced by the two algorithms are comparable. The solutions produced by graph cuts have a lower energy than those produced with belief propagation, but this does not necessarily lead to increased performance relative to the ground truth."
            },
            "slug": "Comparison-of-graph-cuts-with-belief-propagation-Tappen-Freeman",
            "title": {
                "fragments": [],
                "text": "Comparison of graph cuts with belief propagation for stereo, using identical MRF parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work compares the belief propagation algorithm and the graph cuts algorithm on the same MRF's, which have been created for calculating stereo disparities, and finds that the labellings produced by the two algorithms are comparable."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758219"
                        ],
                        "name": "Matthew B. Blaschko",
                        "slug": "Matthew-B.-Blaschko",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Blaschko",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Blaschko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[89, 90] is a branch-and-bound method to locate within a given image x \u2208 X a rectangle y\u2217 that achieves a maximum classification score as measured"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "and loss-augmented prediction are performed efficiently by a branchand-bound search [90]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6148423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ad32b70ee21b6fc16ff4caf7b4ada2aaf13cabc",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To estimate the object's location, one can take a sliding window approach, but this strongly increases the computational cost because the classifier or similarity function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch and bound scheme that allows efficient maximization of a large class of quality functions over all possible subimages. It converges to a globally optimal solution typically in linear or even sublinear time, in contrast to the quadratic scaling of exhaustive or sliding window search. We show how our method is applicable to different object detection and image retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest-neighbor classifiers based on the lambda2 distance. We demonstrate state-of-the-art localization performance of the resulting systems on the UIUC Cars data set, the PASCAL VOC 2006 data set, and in the PASCAL VOC 2007 competition."
            },
            "slug": "Efficient-Subwindow-Search:-A-Branch-and-Bound-for-Lampert-Blaschko",
            "title": {
                "fragments": [],
                "text": "Efficient Subwindow Search: A Branch and Bound Framework for Object Localization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A simple yet powerful branch and bound scheme that allows efficient maximization of a large class of quality functions over all possible subimages and converges to a globally optimal solution typically in linear or even sublinear time, in contrast to the quadratic scaling of exhaustive or sliding window search."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "proposed for images by Besag [17]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15128952,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "47865b56fee61d9c9ff477f7c79f090cc6663d3a",
            "isKey": false,
            "numCitedBy": 4634,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "may 7th, 1986, Professor A. F. M. Smith in the Chair] SUMMARY A continuous two-dimensional region is partitioned into a fine rectangular array of sites or \"pixels\", each pixel having a particular \"colour\" belonging to a prescribed finite set. The true colouring of the region is unknown but, associated with each pixel, there is a possibly multivariate record which conveys imperfect information about its colour according to a known statistical model. The aim is to reconstruct the true scene, with the additional knowledge that pixels close together tend to have the same or similar colours. In this paper, it is assumed that the local characteristics of the true scene can be represented by a nondegenerate Markov random field. Such information can be combined with the records by Bayes' theorem and the true scene can be estimated according to standard criteria. However, the computational burden is enormous and the reconstruction may reflect undesirable largescale properties of the random field. Thus, a simple, iterative method of reconstruction is proposed, which does not depend on these large-scale characteristics. The method is illustrated by computer simulations in which the original scene is not directly related to the assumed random field. Some complications, including parameter estimation, are discussed. Potential applications are mentioned briefly."
            },
            "slug": "On-the-Statistical-Analysis-of-Dirty-Pictures-Besag",
            "title": {
                "fragments": [],
                "text": "On the Statistical Analysis of Dirty Pictures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145617808"
                        ],
                        "name": "D. Barber",
                        "slug": "D.-Barber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Barber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Barber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "More information about directed models can be found in [9, 77]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For a more detailed review of the belief propagation algorithms and proofs of its correctness for tree-structured factor graphs, see [9, 85, 98, 171]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6664936,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92ace17730c2173e642934d64f96d359697b7a93",
            "isKey": false,
            "numCitedBy": 1341,
            "numCiting": 324,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online."
            },
            "slug": "Bayesian-reasoning-and-machine-learning-Barber",
            "title": {
                "fragments": [],
                "text": "Bayesian reasoning and machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Comprehensive and coherent, this hands-on text develops everything from basic reasoning to advanced techniques within the framework of graphical models, and develops analytical and problem-solving skills that equip them for the real world."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50716462"
                        ],
                        "name": "Andrew C. Gallagher",
                        "slug": "Andrew-C.-Gallagher",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gallagher",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew C. Gallagher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40894914"
                        ],
                        "name": "Tsuhan Chen",
                        "slug": "Tsuhan-Chen",
                        "structuredName": {
                            "firstName": "Tsuhan",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsuhan Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "higher-order decompositions yielding stronger bounds are proposed in [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[11] to approximately solve hard random field instances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1901177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51c184d445523bf637174d6edb8b267087b3f11b",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum a posteriori (MAP) inference in Markov Random Fields (MRFs) is an NP-hard problem, and thus research has focussed on either finding efficiently solvable subclasses (e.g. trees), or approximate algorithms (e.g. Loopy Belief Propagation (BP) and Tree-reweighted (TRW) methods). This paper presents a unifying perspective of these approximate techniques called \"Decomposition Methods\". These are methods that decompose the given problem over a graph into tractable subproblems over subgraphs and then employ message passing over these subgraphs to merge the solutions of the subproblems into a global solution. This provides a new way of thinking about BP and TRW as successive steps in a hierarchy of decomposition methods. Using this framework, we take a principled first step towards extending this hierarchy beyond trees. We leverage a new class of graphs amenable to exact inference, called outer-planar graphs, and propose an approximate inference algorithm called Outer-Planar Decomposition (OPD). OPD is a strict generalization of BP and TRW, and contains both of them as special cases. Our experiments show that this extension beyond trees is indeed very powerful -OPD outperforms current state-of-art inference methods on hard non-submodular synthetic problems and is competitive on real computer vision applications."
            },
            "slug": "Beyond-trees:-MRF-inference-via-outer-planar-Batra-Gallagher",
            "title": {
                "fragments": [],
                "text": "Beyond trees: MRF inference via outer-planar decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Outer-Planar Decomposition (OPD) is a strict generalization of BP and TRW, and contains both of them as special cases, and outperforms current state-of-art inference methods on hard non-submodular synthetic problems and is competitive on real computer vision applications."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758219"
                        ],
                        "name": "Matthew B. Blaschko",
                        "slug": "Matthew-B.-Blaschko",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Blaschko",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Blaschko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[89, 90] is a branch-and-bound method to locate within a given image x \u2208 X a rectangle y\u2217 that achieves a maximum classification score as measured"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6131848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54b224478a63e33441c651175c522f3702062fc4",
            "isKey": false,
            "numCitedBy": 800,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To perform localization, one can take a sliding window approach, but this strongly increases the computational cost, because the classifier function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch-and-bound scheme that allows efficient maximization of a large class of classifier functions over all possible subimages. It converges to a globally optimal solution typically in sublinear time. We show how our method is applicable to different object detection and retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest neighbor classifiers based on the chi2-distance. We demonstrate state-of-the-art performance of the resulting systems on the UIUC Cars dataset, the PASCAL VOC 2006 dataset and in the PASCAL VOC 2007 competition."
            },
            "slug": "Beyond-sliding-windows:-Object-localization-by-Lampert-Blaschko",
            "title": {
                "fragments": [],
                "text": "Beyond sliding windows: Object localization by efficient subwindow search"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A simple yet powerful branch-and-bound scheme that allows efficient maximization of a large class of classifier functions over all possible subimages and converges to a globally optimal solution typically in sublinear time is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746662"
                        ],
                        "name": "D. Sontag",
                        "slug": "D.-Sontag",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sontag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[141] provides a detailed introduction to dual decomposition and dual ascent methods for inference."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 198640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "172cb5614f337048d608a7d7f7c7f91513042f77",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Many inference problems with discrete variables result in a difficult com-binatorial optimization problem. In recent years, the technique of dual decomposition , also called Lagrangian relaxation, has proven to be a powerful means of solving these inference problems by decomposing them into simpler components that are repeatedly solved independently and combined into a global solution. In this chapter, we introduce the general technique of dual decomposition through its application to the problem of finding the most likely (MAP) assignment in Markov random fields. We discuss both subgradient and block coordinate descent approaches to solving the dual problem. The resulting message-passing algorithms are similar to max-product, but can be shown to solve a linear programming relaxation of the MAP problem. We show how many of the MAP algorithms are related to each other, and also quantify when the MAP solution can and cannot be decoded directly from the dual solution. 1.1 Introduction Many problems in engineering and the sciences require solutions to challenging combinatorial optimization problems. These include traditional problems such as scheduling, planning, fault diagnosis, or searching for molecular conformations. In addition, a wealth of combinatorial problems arise directly from probabilistic modeling (graphical models). Graphical models (see Koller and Friedman, 2009, for a textbook introduction) have been widely adopted in areas such as computational biology, machine vision, and natural language processing, and are increasingly being used as a framework for expressing combinatorial problems. Consider, for example, a protein side-chain placement problem where the goal is to find the minimum energy conformation of amino acid side-chains along a fixed carbon backbone. The orientations of the side-chains are represented by discretized angles called rotamers. The combinatorial difficulty arises here from the fact that rotamer choices for nearby amino acids are energetically coupled. For globular proteins, for example, such couplings may be present for most pairs of side-chain orientations. This problem is couched in probabilistic modeling terms by associating molecular conformations with the setting of discrete random variables corresponding to the rotamer angles. The interactions between such random variables come from the energetic couplings between nearby amino acids. Finding the minimum energy conformation is then equivalently solved by finding the most probable assignment of states to the variables. We will consider here combinatorial problems that are expressed in terms of structured probability models (graphical models). A graphical model is defined over a set of discrete variables x = {x j } j\u2208V. Local \u2026"
            },
            "slug": "1-Introduction-to-Dual-Decomposition-for-Inference-Sontag",
            "title": {
                "fragments": [],
                "text": "1 Introduction to Dual Decomposition for Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This chapter introduces the general technique of dual decomposition through its application to the problem of finding the most likely (MAP) assignment in Markov random fields, and discusses both subgradient and block coordinate descent approaches to solving the dual problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Structured perceptron learning [33] generalizes the multi-class perceptron [35] to structured output spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8729730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28b9bacde6499f8cc6f7e70feee4232107211e39",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study online classification algorithms for multiclass problems in the mistake bound model. The hypotheses we use maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarity-score between each prototype and the input instance and then sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultracon-servativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems."
            },
            "slug": "Ultraconservative-Online-Algorithms-for-Multiclass-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "Ultraconservative Online Algorithms for Multiclass Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper studies online classification algorithms for multiclass problems in the mistake bound model and introduces the notion of ultracon-servativeness, a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388416"
                        ],
                        "name": "S. Nowozin",
                        "slug": "S.-Nowozin",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Nowozin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowozin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[12]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 33754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "304014ae51faa657d7594fbc13068e7d11895845",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an efficient and adaptive method for MAP-MRF inference that provides increasingly tighter upper and lower bounds on the optimal objective. Similar to Sontag et al. (2008b), our method starts by solving the first-order LOCAL(G) linear programming relaxation. This is followed by an adaptive tightening of the relaxation where we incrementally add higher-order interactions to enforce proper marginalization over groups of variables. Computing the best interaction to add is an NP-hard problem. We show good solutions to this problem can be readily obtained from \u201clocal primal-dual gaps\u201d given the current primal solution and a dual reparameterization vector. This is not only extremely efficient, but in contrast to previous approaches, also allows us to search over prohibitively large sets of candidate interactions to add. We demonstrate the superiority of our approach on MAP-MRF inference problems encountered in computer vision."
            },
            "slug": "Tighter-Relaxations-for-MAP-MRF-Inference:-A-Local-Batra-Nowozin",
            "title": {
                "fragments": [],
                "text": "Tighter Relaxations for MAP-MRF Inference: A Local Primal-Dual Gap based Separation Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "An efficient and adaptive method for MAP-MRF inference that provides increasingly tighter upper and lower bounds on the optimal objective and allows us to search over prohibitively large sets of candidate interactions to add."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198578"
                        ],
                        "name": "J. Yedidia",
                        "slug": "J.-Yedidia",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yedidia",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yedidia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52835993,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8921b3462a3575b0b5de602a975bd608f6f6652",
            "isKey": false,
            "numCitedBy": 1611,
            "numCiting": 115,
            "paperAbstract": {
                "fragments": [],
                "text": "Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a \"valid\" or \"maxent-normal\" approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the \"Bethe method\", the \"junction graph method\", the \"cluster variation method\", and the \"region graph method\". Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP."
            },
            "slug": "Constructing-free-energy-approximations-and-belief-Yedidia-Freeman",
            "title": {
                "fragments": [],
                "text": "Constructing free-energy approximations and generalized belief propagation algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work explains how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms, and describes empirical results showing that GBP can significantly outperform BP."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The influential paper of Geman and Geman [49] proposed the Gibbs sampler for obtaining approximate samples from otherwise intractable distributions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(1) Ising model with external field [49]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Gibbs sampler, first proposed by Geman and Geman [49] is a special case of the Metropolis\u2013Hastings chain in which each proposal is always accepted."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In case the temperature decreases slowly enough, the following theoretical result is known due to Geman and Geman [49]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It has been used to model image denoising tasks [49] and is the simplest case of a Markov random field."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For the original proof, see [49]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": true,
            "numCitedBy": 18707,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3307885"
                        ],
                        "name": "Taesup Kim",
                        "slug": "Taesup-Kim",
                        "structuredName": {
                            "firstName": "Taesup",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taesup Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388416"
                        ],
                        "name": "S. Nowozin",
                        "slug": "S.-Nowozin",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Nowozin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowozin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145954697"
                        ],
                        "name": "C. Yoo",
                        "slug": "C.-Yoo",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Yoo",
                            "middleNames": [
                                "Dong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yoo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The idea of grouping variables to obtain a smaller inference problem has been explored further by [70]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1035401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7bb78e8bdf4f311ef86ef560615daeaf3eb9a13f",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of efficiently solving large-scale energy minimization problems encountered in computer vision. We propose an energy-aware method for merging random variables to reduce the size of the energy to be minimized. The method examines the energy function to find groups of variables which are likely to take the same label in the minimum energy state and thus can be represented by a single random variable. We propose and evaluate a number of extremely efficient variable grouping strategies. Experimental results show that our methods result in a dramatic reduction in the computational cost and memory requirements (in some cases by a factor of one hundred) with almost no drop in the accuracy of the final result. Comparative evaluation with efficient super-pixel generation methods, which are commonly used in variable grouping, reveals that our methods are far superior both in terms of accuracy and running time."
            },
            "slug": "Variable-grouping-for-energy-minimization-Kim-Nowozin",
            "title": {
                "fragments": [],
                "text": "Variable grouping for energy minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An energy-aware method for merging random variables to reduce the size of the energy to be minimized and a number of extremely efficient variable grouping strategies are proposed and evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39092098"
                        ],
                        "name": "Y. Wu",
                        "slug": "Y.-Wu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Wu",
                            "middleNames": [
                                "Nian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[177] and continuous MRF natural image models [134]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2171181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a5f775f7490f410692bb224ff021da97d5e0ddc",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a statistical theory for texture modeling. This theory combines filtering theory and Markov random field modeling through the maximum entropy principle, and interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view. Our theory characterizes the ensemble of images I with the same texture appearance by a probability distribution f(I) on a random field, and the objective of texture modeling is to make inference about f(I), given a set of observed texture examples.In our theory, texture modeling consists of two steps. (1) A set of filters is selected from a general filter bank to capture features of the texture, these filters are applied to observed texture images, and the histograms of the filtered images are extracted. These histograms are estimates of the marginal distributions of f( I). This step is called feature extraction. (2) The maximum entropy principle is employed to derive a distribution p(I), which is restricted to have the same marginal distributions as those in (1). This p(I) is considered as an estimate of f( I). This step is called feature fusion. A stepwise algorithm is proposed to choose filters from a general filter bank. The resulting model, called FRAME (Filters, Random fields And Maximum Entropy), is a Markov random field (MRF) model, but with a much enriched vocabulary and hence much stronger descriptive ability than the previous MRF models used for texture modeling. Gibbs sampler is adopted to synthesize texture images by drawing typical samples from p(I), thus the model is verified by seeing whether the synthesized texture images have similar visual appearances to the texture images being modeled. Experiments on a variety of 1D and 2D textures are described to illustrate our theory and to show the performance of our algorithms. These experiments demonstrate that many textures which are previously considered as from different categories can be modeled and synthesized in a common framework."
            },
            "slug": "Filters,-Random-Fields-and-Maximum-Entropy-(FRAME):-Zhu-Wu",
            "title": {
                "fragments": [],
                "text": "Filters, Random Fields and Maximum Entropy (FRAME): Towards a Unified Theory for Texture Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The resulting model, called FRAME (Filters, Random fields And Maximum Entropy), is a Markov random field (MRF) model, but with a much enriched vocabulary and hence much stronger descriptive ability than the previous MRF models used for texture modeling."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746662"
                        ],
                        "name": "D. Sontag",
                        "slug": "D.-Sontag",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sontag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sontag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786843"
                        ],
                        "name": "A. Globerson",
                        "slug": "A.-Globerson",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Globerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Globerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6051475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79f1905baaaa30e4a6b256a51bdd7200eccac782",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new class of consistency constraints for Linear Programming (LP) relaxations for finding the most probable (MAP) configuration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and significantly faster than a method that does not use partitioning."
            },
            "slug": "Clusters-and-Coarse-Partitions-in-LP-Relaxations-Sontag-Globerson",
            "title": {
                "fragments": [],
                "text": "Clusters and Coarse Partitions in LP Relaxations"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A dual message passing algorithm is obtained and applied to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly, and solves many of these problems exactly, and significantly faster than a method that does not use partitioning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2977931"
                        ],
                        "name": "D. Tuia",
                        "slug": "D.-Tuia",
                        "structuredName": {
                            "firstName": "Devis",
                            "lastName": "Tuia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tuia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397420485"
                        ],
                        "name": "J. Mu\u00f1oz-Mar\u00ed",
                        "slug": "J.-Mu\u00f1oz-Mar\u00ed",
                        "structuredName": {
                            "firstName": "Jordi",
                            "lastName": "Mu\u00f1oz-Mar\u00ed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mu\u00f1oz-Mar\u00ed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143940227"
                        ],
                        "name": "M. Kanevski",
                        "slug": "M.-Kanevski",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Kanevski",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kanevski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397959153"
                        ],
                        "name": "G. Camps-Valls",
                        "slug": "G.-Camps-Valls",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Camps-Valls",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Camps-Valls"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14537177,
            "fieldsOfStudy": [
                "Environmental Science",
                "Computer Science",
                "Mathematics"
            ],
            "id": "378ee2be23ed38453ae8e00854b0a11e2ed4bb63",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional kernel classifiers assume independence among the classification outputs. As a consequence, each misclassification receives the same weight in the loss function. Moreover, the kernel function only takes into account the similarity between input values and ignores possible relationships between the classes to be predicted. These assumptions are not consistent for most of real-life problems. In the particular case of remote sensing data, this is not a good assumption either. Segmentation of images acquired by airborne or satellite sensors is a very active field of research in which one tries to classify a pixel into a predefined set of classes of interest (e.g. water, grass, trees, etc.). In this situation, the classes share strong relationships, e.g. a tree is naturally (and spectrally) more similar to grass than to water. In this paper, we propose a first approach to remote sensing image classification using structured output learning. In our approach, the output space structure is encoded using a hierarchical tree, and these relations are added to the model in both the kernel and the loss function. The methodology gives rise to a set of new tools for structured classification, and generalizes the traditional non-structured classification methods. Comparison to standard SVM is done numerically, statistically and by visual inspection of the obtained classification maps. Good results are obtained in the challenging case of a multispectral image of very high spatial resolution acquired with QuickBird over a urban area."
            },
            "slug": "Structured-Output-SVM-for-Remote-Sensing-Image-Tuia-Mu\u00f1oz-Mar\u00ed",
            "title": {
                "fragments": [],
                "text": "Structured Output SVM for Remote Sensing Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "In this paper, a first approach to remote sensing image classification using structured output learning is proposed, which gives rise to a set of new tools for structured classification, and generalizes the traditional non-structured classification methods."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Workshop on Machine Learning for Signal Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144735785"
                        ],
                        "name": "Matthew A. Brown",
                        "slug": "Matthew-A.-Brown",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brown",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew A. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144565371"
                        ],
                        "name": "P. P\u00e9rez",
                        "slug": "P.-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. P\u00e9rez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[20]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 632396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddf8e27c6686a8ee276a709f0bf98f32f7d41252",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of interactive foreground/background segmentation in still images is of great practical importance in image editing. The state of the art in interactive segmentation is probably represented by the graph cut algorithm of Boykov and Jolly (ICCV 2001). Its underlying model uses both colour and contrast information, together with a strong prior for region coherence. Estimation is performed by solving a graph cut problem for which very efficient algorithms have recently been developed. However the model depends on parameters which must be set by hand and the aim of this work is for those constants to be learned from image data."
            },
            "slug": "Interactive-Image-Segmentation-Using-an-Adaptive-Blake-Rother",
            "title": {
                "fragments": [],
                "text": "Interactive Image Segmentation Using an Adaptive GMMRF Model"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Estimation is performed by solving a graph cut problem for which very efficient algorithms have recently been developed, however the model depends on parameters which must be set by hand and the aim of this work is for those constants to be learned from image data."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145686644"
                        ],
                        "name": "S. Ramalingam",
                        "slug": "S.-Ramalingam",
                        "structuredName": {
                            "firstName": "Srikumar",
                            "lastName": "Ramalingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ramalingam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72492981"
                        ],
                        "name": "Alahari Karteek",
                        "slug": "Alahari-Karteek",
                        "structuredName": {
                            "firstName": "Alahari",
                            "lastName": "Karteek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alahari Karteek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21982,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6013133ea948799b82813ffed7d5a570fcf53a7e",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of exactly inferring the maximum a posteriori solutions of discrete multi-label MRFs or CRFs with higher order cliques. We present a framework to transform special classes of multi-label higher order functions to submodular second order Boolean functions (referred to as Fs 2), which can be minimized exactly using graph cuts and we characterize those classes. The basic idea is to use two or more Boolean variables to encode the states of a single multi-label variable. There are many ways in which this can be done and much interesting research lies in finding ways which are optimal or minimal in some sense. We study the space of possible encodings and find the ones that can transform the most general class of functions to Fs 2. Our main contributions are two-fold. First, we extend the subclass of submodular energy functions that can be minimized exactly using graph cuts. Second, we show how higher order potentials can be used to improve single view 3D reconstruction results. We believe that our work on exact minimization of higher order energy functions will lead to similar improvements in solutions of other labelling problems."
            },
            "slug": "Exact-inference-in-multi-label-CRFs-with-higher-Ramalingam-Kohli",
            "title": {
                "fragments": [],
                "text": "Exact inference in multi-label CRFs with higher order cliques"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a framework to transform special classes of multi-label higher order functions to submodular second order Boolean functions (referred to as Fs 2), which can be minimized exactly using graph cuts and characterize those classes."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2487006"
                        ],
                        "name": "B. Fulkerson",
                        "slug": "B.-Fulkerson",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Fulkerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Fulkerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715959"
                        ],
                        "name": "Stefano Soatto",
                        "slug": "Stefano-Soatto",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Soatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Soatto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[47] introduce a conditional random field on the superpixels level for the classical problem of figure-ground segmentation, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2117454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52727dcc2d948093b8be500b5e8c66ed9b2ae729",
            "isKey": false,
            "numCitedBy": 688,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method to identify and localize object classes in images. Instead of operating at the pixel level, we advocate the use of superpixels as the basic unit of a class segmentation or pixel localization scheme. To this end, we construct a classifier on the histogram of local features found in each superpixel. We regularize this classifier by aggregating histograms in the neighborhood of each superpixel and then refine our results further by using the classifier in a conditional random field operating on the superpixel graph. Our proposed method exceeds the previously published state-of-the-art on two challenging datasets: Graz-02 and the PASCAL VOC 2007 Segmentation Challenge."
            },
            "slug": "Class-segmentation-and-object-localization-with-Fulkerson-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Class segmentation and object localization with superpixel neighborhoods"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A method to identify and localize object classes in images by constructing a classifier on the histogram of local features found in each superpixel using superpixels as the basic unit of a class segmentation or pixel localization scheme."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15514452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7678da8b2eb70a5383f203d948564d8f48c0c62a",
            "isKey": false,
            "numCitedBy": 761,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We study how closely the optimal Bayes error rate can be approximately reached using a classification algorithm that computes a classifier by minimizing a convex upper bound of the classification error function. The measurement of closeness is characterized by the loss function used in the estimation. We show that such a classification scheme can be generally regarded as a (nonmaximum-likelihood) conditional in-class probability estimate, and we use this analysis to compare various convex loss functions that have appeared in the literature. Furthermore, the theoretical insight allows us to design good loss functions with desirable properties. Another aspect of our analysis is to demonstrate the consistency of certain classification methods using convex risk minimization. This study sheds light on the good performance of some recently proposed linear classification methods including boosting and support vector machines. It also shows their limitations and suggests possible improvements."
            },
            "slug": "Statistical-behavior-and-consistency-of-methods-on-Zhang",
            "title": {
                "fragments": [],
                "text": "Statistical behavior and consistency of classification methods based on convex risk minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This study sheds light on the good performance of some recently proposed linear classification methods including boosting and support vector machines and shows their limitations and suggests possible improvements."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145811593"
                        ],
                        "name": "B. Kelm",
                        "slug": "B.-Kelm",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Kelm",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kelm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055149852"
                        ],
                        "name": "Natalie Mueller",
                        "slug": "Natalie-Mueller",
                        "structuredName": {
                            "firstName": "Natalie",
                            "lastName": "Mueller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Natalie Mueller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143893221"
                        ],
                        "name": "B. Menze",
                        "slug": "B.-Menze",
                        "structuredName": {
                            "firstName": "Bjoern",
                            "lastName": "Menze",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Menze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685187"
                        ],
                        "name": "F. Hamprecht",
                        "slug": "F.-Hamprecht",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Hamprecht",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hamprecht"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 104
                            }
                        ],
                        "text": "In the block ICM method the neighborhood is enlarged by allowing a larger subset of variables to change [Kelm et al., 2006; Kittler and F\u00f6glein, 1984]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14765920,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8767ed938dba7938656b36c3a88b800e3723056a",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In cancer, pathological tissue often exhibits abnormal perfusion and vascular permeability. These can be estimated by monitoring the abundance of an injected contrast medium over time, using Dynamic Contrast-Enhanced (DCE) MR Imaging. The resulting spatially resolved time curves are usually interpreted in terms of a pharmacokinetic model which is fitted by maximum likelihood. However, the resulting nonlinear least squares (NLLS) problem may exhibit spurious local optima leading to false parameter estimates at individual voxels in the generated parameter map. We propose the application of a spatial prior model in form of a generalized Gaussian Markov random field. By using information from parameter estimates at neighboring voxels and computing a maximum a posteriori solution for the whole parameter map at once, false local optima at individual voxels can be avoided. Since the number of variables gets very big for common image resolutions, standard NLLS solvers cannot be employed anymore. We therefore propose a generalized iterated conditional modes (ICM) approach operating on blocks instead of sites. Results on DCE-MR images of the prostate show less speckle noise in the resulting parameter maps. Furthermore, the mean square error (MSE) in the affected voxels is significantly smaller, thus reflecting a better fit."
            },
            "slug": "Bayesian-Estimation-of-Smooth-Parameter-Maps-for-MR-Kelm-Mueller",
            "title": {
                "fragments": [],
                "text": "Bayesian Estimation of Smooth Parameter Maps for Dynamic Contrast-Enhanced MR Images with Block-ICM"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A generalized iterated conditional modes (ICM) approach operating on blocks instead of sites is proposed, showing less speckle noise in the resulting parameter maps and the mean square error in the affected voxels is significantly smaller, thus reflecting a better fit."
            },
            "venue": {
                "fragments": [],
                "text": "2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For energies which do not satisfy regularity conditions, Kolmogorov and Rother [79] give a graphcut-based iterative algorithm, QPBO, that uses probing techniques from combinatorial optimization, producing an approximate minimizer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15319364,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41dd3da0216e5f47511c2e7413089043c11295f6",
            "isKey": false,
            "numCitedBy": 415,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimization techniques based on graph cuts have become a standard tool for many vision applications. These techniques allow to minimize efficiently certain energy functions corresponding to pairwise Markov random fields (MRFs). Currently, there is an accepted view within the computer vision community that graph cuts can only be used for optimizing a limited class of MRF energies (e.g., submodular functions). In this survey, we review some results that show that graph cuts can be applied to a much larger class of energy functions (in particular, nonsubmodular functions). While these results are well-known in the optimization community, to our knowledge they were not used in the context of computer vision and MRF optimization. We demonstrate the relevance of these results to vision on the problem of binary texture restoration."
            },
            "slug": "Minimizing-Nonsubmodular-Functions-with-Graph-Kolmogorov-Rother",
            "title": {
                "fragments": [],
                "text": "Minimizing Nonsubmodular Functions with Graph Cuts-A Review"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This survey reviews some results that show that graph cuts can be applied to a much larger class of energy functions (in particular, nonsubmodular functions) and demonstrates the relevance of these results to vision on the problem of binary texture restoration."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2338183"
                        ],
                        "name": "M. M\u00e9zard",
                        "slug": "M.-M\u00e9zard",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "M\u00e9zard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00e9zard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145071265"
                        ],
                        "name": "A. Montanari",
                        "slug": "A.-Montanari",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Montanari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Montanari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[171] showed a close connection between the so called Bethe free energy approximation in physics and the loopy belief propagation algorithm [104]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For a fascinating account of the connections between physics, computer science and information theory, see [104]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 209859640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5afd63715fc06e6285da1f0450de5e6d286689a6",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book presents a unified approach to a rich and rapidly evolving research domain at the interface between statistical physics, theoretical computer science/discrete mathematics, and coding/information theory. It is accessible to graduate students and researchers without a specific training in any of these fields. The selected topics include spin glasses, error correcting codes, satisfiability, and are central to each field. The approach focuses on large random instances, adopting a common probabilistic formulation in terms of graphical models. It presents message passing algorithms like belief propagation and survey propagation, and their use in decoding and constraint satisfaction solving. It also explains analysis techniques like density evolution and the cavity method, and uses them to study phase transitions."
            },
            "slug": "Information,-Physics,-and-Computation-M\u00e9zard-Montanari",
            "title": {
                "fragments": [],
                "text": "Information, Physics, and Computation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The approach focuses on large random instances, adopting a common probabilistic formulation in terms of graphical models, and presents message passing algorithms like belief propagation and survey propagation, and their use in decoding and constraint satisfaction solving."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72492981"
                        ],
                        "name": "Alahari Karteek",
                        "slug": "Alahari-Karteek",
                        "structuredName": {
                            "firstName": "Alahari",
                            "lastName": "Karteek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alahari Karteek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[3] and Komodakis et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1635248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29a95562bb9c25677bd9b9a2d895f1d05e597ef1",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present novel techniques that improve the computational and memory efficiency of algorithms for solving multi-label energy functions arising from discrete MRFs or CRFs. These methods are motivated by the observations that the performance of minimization algorithms depends on: (a) the initialization used for the primal and dual variables; and (b) the number of primal variables involved in the energy function. Our first method (dynamic alpha-expansion) works by dasiarecyclingpsila results from previous problem instances. The second method simplifies the energy function by dasiareducingpsila the number of unknown variables, and can also be used to generate a good initialization for the dynamic alpha-expansion algorithm by dasiareusingpsila dual variables. We test the performance of our methods on energy functions encountered in the problems of stereo matching, and colour and object based segmentation. Experimental results show that our methods achieve a substantial improvement in the performance of alpha-expansion, as well as other popular algorithms such as sequential tree-reweighted message passing, and max-product belief propagation. In most cases we achieve a 10-15 times speed-up in the computation time. Our modified alpha-expansion algorithm provides similar performance to Fast-PD. However, it is much simpler and can be made orders of magnitude faster by using the initialization schemes proposed in the paper."
            },
            "slug": "Reduce,-reuse-&-recycle:-Efficiently-solving-MRFs-Karteek-Kohli",
            "title": {
                "fragments": [],
                "text": "Reduce, reuse & recycle: Efficiently solving multi-label MRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Novel techniques that improve the computational and memory efficiency of algorithms for solving multi-label energy functions arising from discrete MRFs or CRFs and can be made orders of magnitude faster by using the initialization schemes proposed in the paper."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144773105"
                        ],
                        "name": "S. Vicente",
                        "slug": "S.-Vicente",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Vicente",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vicente"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1282023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d3657222978b48c1523e4772c962b5942f0a69f",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Graph cut is a popular technique for interactive image segmentation. However, it has certain shortcomings. In particular, graph cut has problems with segmenting thin elongated objects due to the ldquoshrinking biasrdquo. To overcome this problem, we propose to impose an additional connectivity prior, which is a very natural assumption about objects. We formulate several versions of the connectivity constraint and show that the corresponding optimization problems are all NP-hard. For some of these versions we propose two optimization algorithms: (i) a practical heuristic technique which we call DijkstraGC, and (ii) a slow method based on problem decomposition which provides a lower bound on the problem. We use the second technique to verify that for some practical examples DijkstraGC is able to find the global minimum."
            },
            "slug": "Graph-cut-based-image-segmentation-with-priors-Vicente-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "Graph cut based image segmentation with connectivity priors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work forms several versions of the connectivity constraint and shows that the corresponding optimization problems are all NP-hard."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7585417,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a912be1031c08cb21d5bf65a7240d3ca2aee51eb",
            "isKey": false,
            "numCitedBy": 522,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a unifying view of messagepassing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (\u2018exclusive\u2019 versus \u2018inclusive\u2019 Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals."
            },
            "slug": "Divergence-measures-and-message-passing-Minka",
            "title": {
                "fragments": [],
                "text": "Divergence measures and message passing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a unifying view of messagepassing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66193516"
                        ],
                        "name": "H. Ishikawa",
                        "slug": "H.-Ishikawa",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Ishikawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ishikawa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Ishikawa [62] gives a characterization of energies representable as graph cut problems for the case of multilabel states, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16802586,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "7e96ad71dffb3886054527dd37a7e0b505c38063",
            "isKey": false,
            "numCitedBy": 602,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a method to solve exactly a first order Markov random field optimization problem in more generality than was previously possible. The MRF has a prior term that is convex in terms of a linearly ordered label set. The method maps the problem into a minimum-cut problem for a directed graph, for which a globally optimal solution can be found in polynomial time. The convexity of the prior function in the energy is shown to be necessary and sufficient for the applicability of the method."
            },
            "slug": "Exact-Optimization-for-Markov-Random-Fields-with-Ishikawa",
            "title": {
                "fragments": [],
                "text": "Exact Optimization for Markov Random Fields with Convex Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method to solve exactly a first order Markov random field optimization problem in more generality than was previously possible is introduced, which maps the problem into a minimum-cut problem for a directed graph, for which a globally optimal solution can be found in polynomial time."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145155783"
                        ],
                        "name": "C. Robert",
                        "slug": "C.-Robert",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Robert",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Robert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 50937448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a1227f2fc93d22e13c649e97cfffafc97f3127e",
            "isKey": false,
            "numCitedBy": 1163,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Winner of the 2004 DeGroot Prize This paperback edition, a reprint of the 2001 edition, is a graduate-level textbook that introduces Bayesian statistics and decision theory. It covers both the basic ideas of statistical theory, and also some of the more modern and advanced topics of Bayesian statistics such as complete class theorems, the Stein effect, Bayesian model choice, hierarchical and empirical Bayes modeling, Monte Carlo integration including Gibbs sampling, and other MCMC techniques. It was awarded the 2004 DeGroot Prize by the International Society for Bayesian Analysis (ISBA) for setting \"a new standard for modern textbooks dealing with Bayesian methods, especially those using MCMC techniques, and that it is a worthy successor to DeGroot's and Berger's earlier texts\"."
            },
            "slug": "The-Bayesian-choice-:-from-decision-theoretic-to-Robert",
            "title": {
                "fragments": [],
                "text": "The Bayesian choice : from decision-theoretic foundations to computational implementation"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paperback edition, a reprint of the 2001 edition, is a graduate-level textbook that introduces Bayesian statistics and decision theory and is a worthy successor to DeGroot's and Berger's earlier texts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740145"
                        ],
                        "name": "V. Lempitsky",
                        "slug": "V.-Lempitsky",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lempitsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lempitsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34824003"
                        ],
                        "name": "T. Sharp",
                        "slug": "T.-Sharp",
                        "structuredName": {
                            "firstName": "Toby",
                            "lastName": "Sharp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sharp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2170636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39a5d6a92fe290cc5e584240aed8819642ed71b7",
            "isKey": false,
            "numCitedBy": 380,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "User-provided object bounding box is a simple and popular interaction paradigm considered by many existing interactive image segmentation frameworks. However, these frameworks tend to exploit the provided bounding box merely to exclude its exterior from consideration and sometimes to initialize the energy minimization. In this paper, we discuss how the bounding box can be further used to impose a powerful topological prior, which prevents the solution from excessive shrinking and ensures that the user-provided box bounds the segmentation in a sufficiently tight way. The prior is expressed using hard constraints incorporated into the global energy minimization framework leading to an NP-hard integer program. We then investigate the possible optimization strategies including linear relaxation as well as a new graph cut algorithm called pinpointing. The latter can be used either as a rounding method for the fractional LP solution, which is provably better than thresholding-based rounding, or as a fast standalone heuristic. We evaluate the proposed algorithms on a publicly available dataset, and demonstrate the practical benefits of the new prior both qualitatively and quantitatively."
            },
            "slug": "Image-segmentation-with-a-bounding-box-prior-Lempitsky-Kohli",
            "title": {
                "fragments": [],
                "text": "Image segmentation with a bounding box prior"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper discusses how the bounding box can be further used to impose a powerful topological prior, which prevents the solution from excessive shrinking and ensures that the user-provided box bounds the segmentation in a sufficiently tight way."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48491978"
                        ],
                        "name": "W. K. Hastings",
                        "slug": "W.-K.-Hastings",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Hastings",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. K. Hastings"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21204149,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "143d2e02ab91ae6259576ac50b664b8647af8988",
            "isKey": false,
            "numCitedBy": 13619,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed. For numerical problems in a large number of dimensions, Monte Carlo methods are often more efficient than conventional numerical methods. However, implementation of the Monte Carlo methods requires sampling from high dimensional probability distributions and this may be very difficult and expensive in analysis and computer time. General methods for sampling from, or estimating expectations with respect to, such distributions are as follows. (i) If possible, factorize the distribution into the product of one-dimensional conditional distributions from which samples may be obtained. (ii) Use importance sampling, which may also be used for variance reduction. That is, in order to evaluate the integral J = X) p(x)dx = Ev(f), where p(x) is a probability density function, instead of obtaining independent samples XI, ..., Xv from p(x) and using the estimate J, = Zf(xi)/N, we instead obtain the sample from a distribution with density q(x) and use the estimate J2 = Y{f(xj)p(x1)}/{q(xj)N}. This may be advantageous if it is easier to sample from q(x) thanp(x), but it is a difficult method to use in a large number of dimensions, since the values of the weights w(xi) = p(x1)/q(xj) for reasonable values of N may all be extremely small, or a few may be extremely large. In estimating the probability of an event A, however, these difficulties may not be as serious since the only values of w(x) which are important are those for which x -A. Since the methods proposed by Trotter & Tukey (1956) for the estimation of conditional expectations require the use of importance sampling, the same difficulties may be encountered in their use. (iii) Use a simulation technique; that is, if it is difficult to sample directly from p(x) or if p(x) is unknown, sample from some distribution q(y) and obtain the sample x values as some function of the corresponding y values. If we want samples from the conditional dis"
            },
            "slug": "Monte-Carlo-Sampling-Methods-Using-Markov-Chains-Hastings",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Sampling Methods Using Markov Chains and Their Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "have been proposed [78, 174], an overview of different variants and recent results is available in [102]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For this reason alternative algorithms such as tree-reweighted message passing [78], generalized max-product [142], and max-sum diffusion [162] have been proposed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8616813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0bcc580a1e9e32b3329363bab43331ed9c5a7d4",
            "isKey": false,
            "numCitedBy": 1302,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for discrete energy minimization are of fundamental importance in computer vision. In this paper, we focus on the recent technique proposed by Wainwright et al. (Nov. 2005)- tree-reweighted max-product message passing (TRW). It was inspired by the problem of maximizing a lower bound on the energy. However, the algorithm is not guaranteed to increase this bound - it may actually go down. In addition, TRW does not always converge. We develop a modification of this algorithm which we call sequential tree-reweighted message passing. Its main property is that the bound is guaranteed not to decrease. We also give a weak tree agreement condition which characterizes local maxima of the bound with respect to TRW algorithms. We prove that our algorithm has a limit point that achieves weak tree agreement. Finally, we show that, our algorithm requires half as much memory as traditional message passing approaches. Experimental results demonstrate that on certain synthetic and real problems, our algorithm outperforms both the ordinary belief propagation and tree-reweighted algorithm in (M. J. Wainwright, et al., Nov. 2005). In addition, on stereo problems with Potts interactions, we obtain a lower energy than graph cuts"
            },
            "slug": "Convergent-Tree-Reweighted-Message-Passing-for-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "Convergent Tree-Reweighted Message Passing for Energy Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper develops a modification of the recent technique proposed by Wainwright et al. (Nov. 2005), called sequential tree-reweighted message passing, which outperforms both the ordinary belief propagation and tree- reweighted algorithm in both synthetic and real problems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1889982"
                        ],
                        "name": "F. Kschischang",
                        "slug": "F.-Kschischang",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Kschischang",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kschischang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143681410"
                        ],
                        "name": "H. Loeliger",
                        "slug": "H.-Loeliger",
                        "structuredName": {
                            "firstName": "Hans-Andrea",
                            "lastName": "Loeliger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Loeliger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Factor graphs are undirected graphical models that make explicit the factorization of the probability function [85]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For a more detailed review of the belief propagation algorithms and proofs of its correctness for tree-structured factor graphs, see [9, 85, 98, 171]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The sum\u2013product algorithm [85, 98] is a dynamic programming algorithm that given a discrete distribution in the form (2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14394619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08c370eb9ba13bfb836349e7f3ea428be4697818",
            "isKey": false,
            "numCitedBy": 4131,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms that must deal with complicated global functions of many variables often exploit the manner in which the given functions factor as a product of \"local\" functions, each of which depends on a subset of the variables. Such a factorization can be visualized with a bipartite graph that we call a factor graph, In this tutorial paper, we present a generic message-passing algorithm, the sum-product algorithm, that operates in a factor graph. Following a single, simple computational rule, the sum-product algorithm computes-either exactly or approximately-various marginal functions derived from the global function. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative \"turbo\" decoding algorithm, Pearl's (1988) belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform (FFT) algorithms."
            },
            "slug": "Factor-graphs-and-the-sum-product-algorithm-Kschischang-Frey",
            "title": {
                "fragments": [],
                "text": "Factor graphs and the sum-product algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A generic message-passing algorithm, the sum-product algorithm, that operates in a factor graph, that computes-either exactly or approximately-various marginal functions derived from the global function."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For graphs such as the one shown in G\u2032 where all nodes are connected to the source- and sinknode, specialized max-flow algorithms with superior empirical performance have been developed, see Boykov and Kolmogorov [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "They have been reviewed in Boykov and Kolmogorov [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 36955056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6116661b3ed8126bd983024c4158b70ae410f88b",
            "isKey": false,
            "numCitedBy": 2189,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "After [15], [31], [19], [8], [25], [5], minimum cut/maximum flow algorithms on graphs emerged as an increasingly useful tool for exact or approximate energy minimization in low-level vision. The combinatorial optimization literature provides many min-cut/max-flow algorithms with different polynomial time complexity. Their practical efficiency, however, has to date been studied mainly outside the scope of computer vision. The goal of this paper is to provide an experimental comparison of the efficiency of min-cut/max flow algorithms for applications in vision. We compare the running times of several standard algorithms, as well as a new algorithm that we have recently developed. The algorithms we study include both Goldberg-Tarjan style \"push-relabel\" methods and algorithms based on Ford-Fulkerson style \"augmenting paths.\" We benchmark these algorithms on a number of typical graphs in the contexts of image restoration, stereo, and segmentation. In many cases, our new algorithm works several times faster than any of the other methods, making near real-time performance possible. An implementation of our max-flow/min-cut algorithm is available upon request for research purposes."
            },
            "slug": "An-experimental-comparison-of-min-cut/max-flow-for-Boykov-Kolmogorov",
            "title": {
                "fragments": [],
                "text": "An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper compares the running times of several standard algorithms, as well as a new algorithm that is recently developed that works several times faster than any of the other methods, making near real-time performance possible."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE transactions on pattern analysis and machine intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143826285"
                        ],
                        "name": "Frank R. Schmidt",
                        "slug": "Frank-R.-Schmidt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Schmidt",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank R. Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076723"
                        ],
                        "name": "Eno T\u00f6ppe",
                        "slug": "Eno-T\u00f6ppe",
                        "structuredName": {
                            "firstName": "Eno",
                            "lastName": "T\u00f6ppe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eno T\u00f6ppe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695302"
                        ],
                        "name": "D. Cremers",
                        "slug": "D.-Cremers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cremers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cremers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1048525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f47f386aefbed950d0766a7356cf9bf8ea999611",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a fast graph cut algorithm for planar graphs. It is based on the graph theoretical work and leads to an efficient method that we apply on shape matching and image segmentation. In contrast to currently used methods in computer vision, the presented approach provides an upper bound for its runtime behavior that is almost linear. In particular, we are able to match two different planar shapes of N points in O(N2 log N) and segment a given image of N pixels in O(N log N). We present two experimental benchmark studies which demonstrate that the presented method is also in practice faster than previously proposed graph cut methods: On planar shape matching and image segmentation we observe a speed-up of an order of magnitude, depending on resolution."
            },
            "slug": "Efficient-planar-graph-cuts-with-applications-in-Schmidt-T\u00f6ppe",
            "title": {
                "fragments": [],
                "text": "Efficient planar graph cuts with applications in Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A fast graph cut algorithm for planar graphs based on the graph theoretical work is presented and leads to an efficient method that is applied on shape matching and image segmentation that is also in practice faster than previously proposed graph cut methods."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": false,
            "numCitedBy": 5910,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3063671"
                        ],
                        "name": "D. Kamenetsky",
                        "slug": "D.-Kamenetsky",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Kamenetsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kamenetsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 39177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f98587c9857c28326e718c8cec70963c2f2d64cb",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We give polynomial-time algorithms for the exact computation of lowest-energy states, worst margin violators, partition functions, and marginals in certain binary undirected graphical models. Our approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints; instead we require planarity to establish a correspondence with perfect matchings in an expanded dual graph. Maximum-margin parameter estimation for a boundary detection task shows our approach to be efficient and effective. A C++ implementation is available from http://nic.schraudolph.org/isinf/."
            },
            "slug": "Efficient-Exact-Inference-in-Planar-Ising-Models-Schraudolph-Kamenetsky",
            "title": {
                "fragments": [],
                "text": "Efficient Exact Inference in Planar Ising Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints; instead it requires planarity to establish a correspondence with perfect matchings in an expanded dual graph."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116757950,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1406b6d771c270aff4dcb1c96e4f5c62c02c00a5",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In rather formal terms, the situation with which this paper is concerned may be described as follows. We are given a fixed system of n sites, labelled by the first n positive integers, and an associated vector x of observations, Xi, . . ., Xn, which, in turn, is presumed to be a realization of a vector X of (dependent) random variables, Xi, . . ., X.. In practice, the sites may represent points or regions in space and the random variables may be either continuous or discrete. The main statistical objectives are the following: firstly, to provide a means of using the available concomitant information, particularly the configuration of the sites, to attach a plausible probability distribution to the random vector X; secondly, to estimate any unknown parameters in the distribution from the realization x; thirdly, where possible, to quantify the extent of disagreement between hypothesis and observation."
            },
            "slug": "Statistical-Analysis-of-Non-Lattice-Data-Besag",
            "title": {
                "fragments": [],
                "text": "Statistical Analysis of Non-Lattice Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684677"
                        ],
                        "name": "G. Elidan",
                        "slug": "G.-Elidan",
                        "structuredName": {
                            "firstName": "Gal",
                            "lastName": "Elidan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Elidan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143685627"
                        ],
                        "name": "Ian McGraw",
                        "slug": "Ian-McGraw",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "McGraw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian McGraw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Alternative message schedules are discussed in [36]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9008993,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8850119d05f852e67a71151b6da2f88e9bcac9be",
            "isKey": false,
            "numCitedBy": 289,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Inference for probabilistic graphical models is still very much a practical challenge in large domains. The commonly used and effective belief propagation (BP) algorithm and its generalizations often do not converge when applied to hard, real-life inference tasks. While it is widely recognized that the scheduling of messages in these algorithms may have significant consequences, this issue remains largely unexplored. In this work, we address the question of how to schedule messages for asynchronous propagation so that a fixed point is reached faster and more often. We first show that any reasonable asynchronous BP converges to a unique fixed point under conditions similar to those that guarantee convergence of synchronous BP. In addition, we show that the convergence rate of a simple round-robin schedule is at least as good as that of synchronous propagation. We then propose residual belief propagation (RBP), a novel, easy-to-implement, asynchronous propagation algorithm that schedules messages in an informed way, that pushes down a bound on the distance from the fixed point. Finally, we demonstrate the superiority of RBP over state-of-the-art methods for a variety of challenging synthetic and real-life problems: RBP converges significantly more often than other methods; and it significantly reduces running time until convergence, even when other methods converge."
            },
            "slug": "Residual-Belief-Propagation:-Informed-Scheduling-Elidan-McGraw",
            "title": {
                "fragments": [],
                "text": "Residual Belief Propagation: Informed Scheduling for Asynchronous Message Passing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "RBP is proposed, a novel, easy-to-implement, asynchronous propagation algorithm that schedules messages in an informed way that pushes down a bound on the distance from the fixed point and demonstrates the superiority of RBP over state-of-the-art methods for a variety of challenging synthetic and real-life problems."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770124"
                        ],
                        "name": "Sunita Sarawagi",
                        "slug": "Sunita-Sarawagi",
                        "structuredName": {
                            "firstName": "Sunita",
                            "lastName": "Sarawagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunita Sarawagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110342266"
                        ],
                        "name": "Rahul Gupta",
                        "slug": "Rahul-Gupta",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Gupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6133717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc2af689f4324a20e218a87fb4097dce6fc87d32",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Tsochantaridis et al. (2005) proposed two formulations for maximum margin training of structured spaces: margin scaling and slack scaling. While margin scaling has been extensively used since it requires the same kind of MAP inference as normal structured prediction, slack scaling is believed to be more accurate and better-behaved. We present an efficient variational approximation to the slack scaling method that solves its inference bottleneck while retaining its accuracy advantage over margin scaling.\n We further argue that existing scaling approaches do not separate the true labeling comprehensively while generating violating constraints. We propose a new max-margin trainer PosLearn that generates violators to ensure separation at each position of a decomposable loss function. Empirical results on real datasets illustrate that PosLearn can reduce test error by up to 25% over margin scaling and 10% over slack scaling. Further, PosLearn violators can be generated more efficiently than slack violators; for many structured tasks the time required is just twice that of MAP inference."
            },
            "slug": "Accurate-max-margin-training-for-structured-output-Sarawagi-Gupta",
            "title": {
                "fragments": [],
                "text": "Accurate max-margin training for structured output spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new max-margin trainer Pos learn that generates violators to ensure separation at each position of a decomposable loss function to present an efficient variational approximation to the slack scaling method that solves its inference bottleneck while retaining its accuracy advantage over margin scaling."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145644643"
                        ],
                        "name": "Andr\u00e9 F. T. Martins",
                        "slug": "Andr\u00e9-F.-T.-Martins",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Martins",
                            "middleNames": [
                                "F.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andr\u00e9 F. T. Martins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35537344"
                        ],
                        "name": "P. Aguiar",
                        "slug": "P.-Aguiar",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Aguiar",
                            "middleNames": [
                                "M.",
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Aguiar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34659351"
                        ],
                        "name": "M\u00e1rio A. T. Figueiredo",
                        "slug": "M\u00e1rio-A.-T.-Figueiredo",
                        "structuredName": {
                            "firstName": "M\u00e1rio",
                            "lastName": "Figueiredo",
                            "middleNames": [
                                "A.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M\u00e1rio A. T. Figueiredo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[100] augment the subproblems by a strictly convex proximal term stabilizing the iterations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16907821,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "310581f5adcd2cdbe58f66880f784e63c78d69d2",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose combining augmented Lagrangian optimization with the dual decomposition method to obtain a fast algorithm for approximate MAP (maximum a posteriori) inference on factor graphs. We also show how the proposed algorithm can efficiently handle problems with (possibly global) structural constraints. The experimental results reported testify for the state-of-the-art performance of the proposed approach."
            },
            "slug": "Augmenting-Dual-Decomposition-for-MAP-Inference-Martins-Smith",
            "title": {
                "fragments": [],
                "text": "Augmenting Dual Decomposition for MAP Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper combines augmented Lagrangian optimization with the dual decomposition method to obtain a fast algorithm for approximate MAP (maximum a posteriori) inference on factor graphs and shows how the proposed algorithm can efficiently handle problems with (possibly global) structural constraints."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Despite the many simplifications compared to a complete sampling approach, there are theoretic results that guarantee convergence to the global minimum under certain conditions [173], and that characterize the role of the sampling scheme used in the contrastive divergence procedure [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15466658,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "883b8a189fe1bb97b9ad2a382e057ba7e2a2e56f",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The Convergence of Contrastive Divergences Alan Yuille Department of Statistics University of California at Los Angeles Los Angeles, CA 90095 yuille@stat.ucla.edu Abstract This paper analyses the Contrastive Divergence algorithm for learning statistical parameters. We relate the algorithm to the stochastic approxi- mation literature. This enables us to specify conditions under which the algorithm is guaranteed to converge to the optimal solution (with proba- bility 1). This includes necessary and suf\ufb01cient conditions for the solu- tion to be unbiased. 1 Introduction Many learning problems can be reduced to statistical inference of parameters. But inference algorithms for this task tend to be very slow. Recently Hinton proposed a new algorithm called contrastive divergences (CD) [1]. Computer simulations show that this algorithm tends to converge, and to converge rapidly, although not always to the correct solution [2]. Theoretical analysis shows that CD can fail but does not give conditions which guarantee convergence [3,4]. This paper relates CD to the stochastic approximation literature [5,6] and hence derives elementary conditions which ensure convergence (with probability 1). We conjecture that far stronger results can be obtained by applying more advanced techniques such as those described by Younes [7]. We also give necessary and suf\ufb01cient conditions for the solution of CD to be unbiased. Section (2) describes CD and shows that it is closely related to a class of stochastic ap- proximation algorithms for which convergence results exist. In section (3) we state and give a proof of a simple convergence theorem for stochastic approximation algorithms. Section (4) applies the theorem to give suf\ufb01cient conditions for convergence of CD. 2 Contrastive Divergence and its Relations The task of statistical inference is to estimate the model parameters \u03c9 \u2217 which minimize the Kullback-Leibler divergence D(P 0 (x)||P (x|\u03c9)) between the empirical distribution func-"
            },
            "slug": "The-Convergence-of-Contrastive-Divergences-Yuille",
            "title": {
                "fragments": [],
                "text": "The Convergence of Contrastive Divergences"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper relates the Contrastive Divergence algorithm to the stochastic approximation literature and derives elementary conditions which ensure convergence, and conjecture that far stronger results can be obtained by applying more advanced techniques such as those described by Younes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713874"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145610994"
                        ],
                        "name": "Mark W. Schmidt",
                        "slug": "Mark-W.-Schmidt",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Schmidt",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark W. Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1978101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "950e8d556e30d2a873172bfa90f4f36da5286c07",
            "isKey": false,
            "numCitedBy": 358,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, to the training of Conditional Random Fields (CRFs). On several large data sets, the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS, the leading method reported to date. We report results for both exact and inexact inference techniques."
            },
            "slug": "Accelerated-training-of-conditional-random-fields-Vishwanathan-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Accelerated training of conditional random fields with stochastic gradient methods"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, is applied to the training of Conditional Random Fields (CRFs) and the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46737210"
                        ],
                        "name": "Talya Meltzer",
                        "slug": "Talya-Meltzer",
                        "structuredName": {
                            "firstName": "Talya",
                            "lastName": "Meltzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Talya Meltzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786843"
                        ],
                        "name": "A. Globerson",
                        "slug": "A.-Globerson",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Globerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Globerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2055055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5875dcb76e3bd4250f2e09ab54b5d426903bbf09",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Message-passing algorithms have emerged as powerful techniques for approximate inference in graphical models. When these algorithms converge, they can be shown to find local (or sometimes even global) optima of variational formulations to the inference problem. But many of the most popular algorithms are not guaranteed to converge. This has lead to recent interest in convergent message-passing algorithms. \n \nIn this paper, we present a unified view of convergent message-passing algorithms. We present a simple derivation of an abstract algorithm, tree-consistency bound optimization (TCBO) that is provably convergent in both its sum and max product forms. We then show that many of the existing convergent algorithms are instances of our TCBO algorithm, and obtain novel convergent algorithms \"for free\" by exchanging maximizations and summations in existing algorithms. In particular, we show that Wainwright's non-convergent sum-product algorithm for tree based variational bounds, is actually convergent with the right update order for the case where trees are monotonic chains."
            },
            "slug": "Convergent-message-passing-algorithms-a-unifying-Meltzer-Globerson",
            "title": {
                "fragments": [],
                "text": "Convergent message passing algorithms - a unifying view"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a simple derivation of an abstract algorithm, tree-consistency bound optimization (TCBO) that is provably convergent in both its sum and max product forms, and shows that Wainwright's non-convergent sum-product algorithm for tree based variational bounds, is actually convergent with the right update order."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144559083"
                        ],
                        "name": "B. Korte",
                        "slug": "B.-Korte",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Korte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Korte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713195"
                        ],
                        "name": "J. Vygen",
                        "slug": "J.-Vygen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Vygen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vygen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 83
                            }
                        ],
                        "text": "Branch and bound is a divide and conquer algorithm and can be described as follows [Bertsekas, 1995; Korte and Vygen, 2008; Clausen, 1999]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 51010742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd8563e97ca1278db2bda9cce53abadc912f7a18",
            "isKey": false,
            "numCitedBy": 1504,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This comprehensive textbook on combinatorial optimization places special emphasis on theoretical results and algorithms with provably good performance, in contrast to heuristics. It has arisen as the basis of several courses on combinatorial optimization and more special topics at graduate level. It contains complete but concise proofs, also for many deep results, some of which did not appear in a textbook before. Many very recent topics are covered as well, and many references are provided. Thus this book represents the state of the art of combinatorial optimization. This fourth edition is again significantly extended, most notably with new material on linear programming, the network simplex algorithm, and the max-cut problem. Many further additions and updates are included as well. From the reviews of the previous editions: \"This book on combinatorial optimization is a beautiful example of the ideal textbook.\" Operations Research Letters 33 (2005), p.216-217 \"The second edition (with corrections and many updates) of this very recommendable book documents the relevant knowledge on combinatorial optimization and records those problems and algorithms that define this discipline today. To read this is very stimulating for all the researchers, practitioners, and students interested in combinatorial optimization.\" OR News 19 (2003), p.42 \"... has become a standard textbook in the field.\" Zentralblatt MATH 1099.90054"
            },
            "slug": "Combinatorial-Optimization:-Theory-and-Algorithms-Korte-Vygen",
            "title": {
                "fragments": [],
                "text": "Combinatorial Optimization: Theory and Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This fourth edition of this comprehensive textbook on combinatorial optimization is again significantly extended, most notably with new material on linear programming, the network simplex algorithm, and the max-cut problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3264000"
                        ],
                        "name": "G. Roberts",
                        "slug": "G.-Roberts",
                        "structuredName": {
                            "firstName": "Gareth",
                            "lastName": "Roberts",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Roberts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31987667"
                        ],
                        "name": "J. Rosenthal",
                        "slug": "J.-Rosenthal",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Rosenthal",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rosenthal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "An introduction into MCMC methods for the use in machine learning is given in [Murray, 2009]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2319890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6307e78896e0174ce70d0834f8fc6e457638953d",
            "isKey": false,
            "numCitedBy": 309,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the simplest and most powerful practical uses of the ergodic theory of Markov chains is in Markov chain Monte Carlo (MCMC). Suppose we wish to simulate from a probability density \u03c0 (which will be called the target density) but that direct simulation is either impossible or practically infeasible (possibly due to the high dimensionality of \u03c0). This generic problem occurs in diverse scientific applications, for instance Statistics, Computer Science, and Statistical Physics. Markov chain Monte Carlo offers an indirect solution based on the observation that it is much easier to construct an ergodic Markov chain with \u03c0 as a stationary probability measure, than to simulate directly from \u03c0. This is because of the ingenious MetropolisHastings algorithm which takes an arbitrary Markov chain and adjusts it using a simple accept-reject mechanism to ensure the stationarity of \u03c0 for the resulting process. The algorithms was introduced by Metropolis et al. (1953) in a Statistical Physics context, and was generalised by Hastings (1970). It was considered in the context of image analysis (Geman and Geman, 1984) data augmentation (Tanner and Wong, 1987). However, its routine use in Statistics (especially for Bayesian inference) did not take place until its popularisation by Gelfand and Smith (1990). For modern discussions of MCMC, see e.g. Tierney (1994), Smith and Roberts (1993), Gilks et al. (1996), and Roberts and Rosenthal (1998b). The number of financial applications of MCMC is rapidly growing (see for example the reviews of Kim et al., 1996 and Johannes and Polson, 2003). In this area, important problems revolve around the need to impute latent (or imperfectly observed) time-series such as stochastic volatility processes. Modern developments have often combined the use of MCMC methods with filtering or particle filtering methodology. In Actuarial Sciences, MCMC appears to have huge potential in hitherto intractabile inference problems, much of this untapped as yet (though see Scollnik, 2001, Ntzoufras and Dellaportas, 2002, and Bladt et al., 2003)."
            },
            "slug": "Markov-chain-Monte-Carlo-Roberts-Rosenthal",
            "title": {
                "fragments": [],
                "text": "Markov chain Monte Carlo"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Markov chain Monte Carlo appears to have huge potential in hitherto intractabile inference problems, much of this untapped as yet (though see Scollnik, 2001, Ntzoufras and Dellaportas, 2002, and Bladt et al., 2003)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "8 Pictorial structure model of Felzenszwalb and Huttenlocher [37] for person recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "8 show a pictorial structures model for a person taken from [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In [37] the pose space is a four tuple (x,y,s,\u03b8), where (x,y) are the absolute image coordinates, s is a scale, and \u03b8 is the rotation of the part."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Felzenszwalb and Huttenlocher [37] make this computation tractable by restricting the pairwise energy functions to be of the form:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9018871,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6deeed19c56ec6e737a909de9ee172b93f0d0a89",
            "isKey": true,
            "numCitedBy": 449,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A pictorial structure is a collection of parts arranged in a deformable configuration. Each part is represented using a simple appearance model and the deformable configuration is represented by spring-like connections between pairs of parts. While pictorial structures were introduced a number of years ago, they have not been broadly applied to matching and recognition problems. This has been due in part to the computational difficulty of matching pictorial structures to images. In this paper we present an efficient algorithm for finding the best global match of a pictorial stucture to an image. With this improved algorithm, pictorial structures provide a practical and powerful framework for quantitative descriptions of objects and scenes, and are suitable for many generic image recognition problems. We illustrate the approach using simple models of a person and a car."
            },
            "slug": "Efficient-matching-of-pictorial-structures-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Efficient matching of pictorial structures"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An efficient algorithm for finding the best global match of a pictorial stucture to an image is presented and it is shown that this approach is suitable for many generic image recognition problems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9194763"
                        ],
                        "name": "T. Hesterberg",
                        "slug": "T.-Hesterberg",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Hesterberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hesterberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 113
                            }
                        ],
                        "text": "The Metropolis-Hastings method is so general that many variations have been proposed; for an in-depth review see [Liu, 2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 161
                            }
                        ],
                        "text": "A general introduction into Monte Carlo techniques and their wide applicability in science as well as guidelines on designing efficient samplers can be found in [Liu, 2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6552928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47ba51a5913a6f2be8ab874a5f2d246735c89614",
            "isKey": false,
            "numCitedBy": 1675,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The strength of this book is in bringing together advanced Monte Carlo (MC) methods developed in many disciplines. This intent is clear from the outset: \u201cMany researchers in different scienti\u008e c areas have contributed to its development: : : communications among researchers in these \u008e elds are very limited. It is therefore desirable to develop a relatively general framework in which scientists in every \u008e eld: : : can compare their Monte Carlo techniques and learn from one another.\u201d Throughout the book are examples of techniques invented, or reinvented, in different \u008e elds that may be applied elsewhere. This is occasionally embarrassing to those of us who are statisticians. Consider this statement: \u201cUsing the HMC to solve statistical inference problems was \u008e rst introduced by Neil (1996). This effort was only 10 years behind that in physics and theoretical chemistry. In contrast, statisticians were 40 years late in using the Metropolis algorithm.\u201d The book serves \u201cthree audiences: researchers specializing in the study of Monte Carlo algorithms; scientists who are interested in using advanced Monte Carlo techniques; and graduate students...second-year graduate-level course on Monte Carlo methods.\u201d Chapter 1 gives an overview and a variety of applications. These include the Ising model, molecular structure simulation, bioinformatics, target tracking, hypothesis testing for astronomical observations, Bayesian inference of multilevel models, missing-data problems. Chapter 2 covers basic MC methods and begins sequential methods, including exact sampling for chain-structured models, and sequential importance sampling and rejection control, with applications in solving a linear system, missing data, and populations genetics. Chapter 3 expands on sequential methods. The common thread is that each observation from a multivariate distribution is generated sequentially from approximate conditional distributions. The ratio between the joint density (of dimensions generated so far) and the approximation is an importance sampling weight and is a martingale; for high-dimensional problems, this tends to diverge, with most observations having weights near 0 and a few having high weight. Remedies include a variety of pruning and enrichment (also known as Russian roulette and splitting) and resampling techniques. Applications include growing a polymer, missing data, nonlinear \u008e ltering, and (in Chap. 4) molecular simulation, population genetics, motif patterns in DNA sequences, counting 0\u20131 tables with \u008e xed margins, parametric Bayes analysis, approximating permanents, target tracking, and digital communications. Chapter 5 introduces Markov chain Monte Carlo (MCMC) methods, with Metropolis\u2013Hastings and a number of generalizations, including multipoint, reversible jumping, and dynamic weighting rules. Chapters 6\u20138 treat MCMC methods based on the Gibbs sampler, including data augmentation, cluster algorithms, partial resampling, slice sampler, metropolized Gibbs, hit-and-run, random-ray, collapsing and grouping, the Swendsen\u2013Wang algorithm as data augmentation, transformation groups, and generalized Gibbs. Applications include Gaussian random \u008e elds, texture synthesis Bayesian probit regression, stochastic differential equations, hierarchical Bayes, \u008e nding motifs in protein or DNA sequences, Ising and Potts models, inference with multivariate t distributions, and parameter expansion for data augmentation. Chapter 9 considers hybrid MC and a connection to molecular dynamics algorithms used in structural biology and theoretical chemistry. Also covered are some strategies for improving ef\u008e ciency, including surrogate transition, window, and multipoint methods, and applications in Bayesian analysis and stochastic volatility. Chapters 10 and 11 discuss recent methods for ef\u008e cient MC sampling, including temperature-based methods (simulated tempering, parallel tempering, and simulated annealing), reweighting methods (umbrella sampling and multicanonical sampling) and evolution-based methods (adaptive direction sampling and conjugate gradient MC). Chapters 12 and 13 cover theory for Markov chains and their convergence rates. The book focuses on relatively more dif\u008e cult MC applications where \u201cdirectly generating independent samples from the target distribution \u008f is not feasible.\u201d It omits discussion of some relatively simple MC techniques that are valuable in applications where direct generation is feasible and which could be adapted for other applications; e.g. strati\u008e ed sampling (the \u201cstrati\u008e ed sampling\u201d technique discussed here is unusual and of limited value) post-strati\u008e cation, and defensive mixture designs in importance sampling (Hesterberg 1995). The treatment of importance sampling (IS) could be improved. The book describes the original motivation for IS\u2014focusing attention on \u201cimportant\u201d regions\u2014then indicates:"
            },
            "slug": "Monte-Carlo-Strategies-in-Scientific-Computing-Hesterberg",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Strategies in Scientific Computing"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The strength of this book is in bringing together advanced Monte Carlo methods developed in many disciplines, including the Ising model, molecular structure simulation, bioinformatics, target tracking, hypothesis testing for astronomical observations, Bayesian inference of multilevel models, missing-data problems."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145763419"
                        ],
                        "name": "Chao Chen",
                        "slug": "Chao-Chen",
                        "structuredName": {
                            "firstName": "Chao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145091933"
                        ],
                        "name": "D. Freedman",
                        "slug": "D.-Freedman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Freedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Freedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5574752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc8ede322c8371a20cf606aeb41b94c376af1b6a",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce TopoCut: a new way to integrate knowledge about topological properties (TPs) into random field image segmentation model. Instead of including TPs as additional constraints during minimization of the energy function, we devise an efficient algorithm for modifying the unary potentials such that the resulting segmentation is guaranteed with the desired properties. Our method is more flexible in the sense that it handles more topology constraints than previous methods, which were only able to enforce pairwise or global connectivity. In particular, our method is very fast, making it for the first time possible to enforce global topological properties in practical image segmentation tasks."
            },
            "slug": "Enforcing-topological-constraints-in-random-field-Chen-Freedman",
            "title": {
                "fragments": [],
                "text": "Enforcing topological constraints in random field image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work introduces TopoCut: a new way to integrate knowledge about topological properties (TPs) into random field image segmentation model, and devise an efficient algorithm for modifying the unary potentials such that the resulting segmentation is guaranteed with the desired properties."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388416"
                        ],
                        "name": "S. Nowozin",
                        "slug": "S.-Nowozin",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Nowozin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowozin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2594093"
                        ],
                        "name": "S. Jegelka",
                        "slug": "S.-Jegelka",
                        "structuredName": {
                            "firstName": "Stefanie",
                            "lastName": "Jegelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Jegelka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2888526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96696008c0d64287f24236475aa8ed910ec06b0e",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new method to quantify the solution stability of a large class of combinatorial optimization problems arising in machine learning. As practical example we apply the method to correlation clustering, clustering aggregation, modularity clustering, and relative performance significance clustering. Our method is extensively motivated by the idea of linear programming relaxations. We prove that when a relaxation is used to solve the original clustering problem, then the solution stability calculated by our method is conservative, that is, it never overestimates the solution stability of the true, unrelaxed problem. We also demonstrate how our method can be used to compute the entire path of optimal solutions as the optimization problem is increasingly perturbed. Experimentally, our method is shown to perform well on a number of benchmark problems."
            },
            "slug": "Solution-stability-in-linear-programming-graph-and-Nowozin-Jegelka",
            "title": {
                "fragments": [],
                "text": "Solution stability in linear programming relaxations: graph partitioning and unsupervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proved that when a relaxation is used to solve the original clustering problem, then the solution stability calculated by the method is conservative, that is, it never overestimates the solution Stability of the true, unrelaxed problem."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60688891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "932a106c21a1db1e1876459c1521d27fd152caac",
            "isKey": false,
            "numCitedBy": 8461,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Looking for competent reading resources? We have pattern recognition and machine learning information science and statistics to read, not only read, but also download them or even check out online. Locate this fantastic book writtern by by now, simply here, yeah just here. Obtain the reports in the kinds of txt, zip, kindle, word, ppt, pdf, as well as rar. Once again, never ever miss to review online and download this book in our site right here. Click the link."
            },
            "slug": "Pattern-Recognition-and-Machine-Learning-Science-Bishop",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Machine Learning (Information Science and Statistics)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103911692"
                        ],
                        "name": "M. Kutner",
                        "slug": "M.-Kutner",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kutner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kutner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59099297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d0a9360bbc8484bdc635738a4a2f059ed89745e",
            "isKey": false,
            "numCitedBy": 10739,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Applied Linear Statistical Models 5e is the long established leading authoritative text and reference on statistical modeling. The text includes brief introductory and review material, and then proceeds through regression and modeling for the first half, and through ANOVA and Experimental Design in the second half. All topics are presented in a precise and clear style supported with solved examples, numbered formulae, graphic illustrations, and \"Notes\" to provide depth and statistical accuracy and precision. The Fifth edition provides an increased use of computing and graphical analysis throughout, without sacrificing concepts or rigor. In general, the 5e uses larger data sets in examples and exercises, and where methods can be automated within software without loss of understanding, it is so done."
            },
            "slug": "Applied-Linear-Statistical-Models-Kutner",
            "title": {
                "fragments": [],
                "text": "Applied Linear Statistical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "Applied Linear Statistical Models 5e uses larger data sets in examples and exercises, and where methods can be automated within software without loss of understanding, it is so done."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197071"
                        ],
                        "name": "M. Jolly",
                        "slug": "M.-Jolly",
                        "structuredName": {
                            "firstName": "Marie-Pierre",
                            "lastName": "Jolly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jolly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "1 Undirected graph cut construction of [29]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2245438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d3b177e8d027d44c191e739a3a70ccacc2eac82",
            "isKey": false,
            "numCitedBy": 4174,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as \"object\" or \"background\" to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both \"object\" and \"background\" segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm."
            },
            "slug": "Interactive-graph-cuts-for-optimal-boundary-&-of-in-Boykov-Jolly",
            "title": {
                "fragments": [],
                "text": "Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A new technique for general purpose interactive segmentation of N-dimensional images where the user marks certain pixels as \"object\" or \"background\" to provide hard constraints for segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[171] and Heskes [59]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of generalizations, including the expectation propagation algorithm have been proposed, for a discussion, see [59, 105]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It is now known that if the algorithm converges then it converged to a fix-point of the Bethe free energy and this connection has been fruitfully used to derive a family of similar algorithms [59, 160, 165], some of which can ensure convergence to a fix point."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16348864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d00b06ce1bf91bb5bf27b8287f015623c997de8a",
            "isKey": true,
            "numCitedBy": 102,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Loopy and generalized belief propagation are popular algorithms for approximate inference in Markov random fields and Bayesian networks. Fixed points of these algorithms have been shown to correspond to extrema of the Bethe and Kikuchi free energy, both of which are approximations of the exact Helmholtz free energy. However, belief propagation does not always converge, which motivates approaches that explicitly minimize the Kikuchi/Bethe free energy, such as CCCP and UPS. \n \nHere we describe a class of algorithms that solves this typically non-convex constrained minimization problem through a sequence of convex constrained minimizations of upper bounds on the Kikuchi free energy. Intuitively one would expect tighter bounds to lead to faster algorithms, which is indeed convincingly demonstrated in our simulations. Several ideas are applied to obtain tight convex bounds that yield dramatic speed-ups over CCCP."
            },
            "slug": "Convexity-Arguments-for-Efficient-Minimization-of-Heskes",
            "title": {
                "fragments": [],
                "text": "Convexity Arguments for Efficient Minimization of the Bethe and Kikuchi Free Energies"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A class of algorithms is described that solves this typically non-convex constrained constrained minimization problem through a sequence of convex constrained minimizations of upper bounds on the Kikuchi free energy."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746662"
                        ],
                        "name": "D. Sontag",
                        "slug": "D.-Sontag",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sontag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sontag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46737210"
                        ],
                        "name": "Talya Meltzer",
                        "slug": "Talya-Meltzer",
                        "structuredName": {
                            "firstName": "Talya",
                            "lastName": "Meltzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Talya Meltzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786843"
                        ],
                        "name": "A. Globerson",
                        "slug": "A.-Globerson",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Globerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Globerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14429740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54d1776c32b5a8e5e9cc7a6fcbe9b44ad84be19e",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear Programming (LP) relaxations have become powerful tools for finding the most probable (MAP) configuration in graphical models. These relaxations can be solved efficiently using message-passing algorithms such as belief propagation and, when the relaxation is tight, provably find the MAP configuration. The standard LP relaxation is not tight enough in many real-world problems, however, and this has lead to the use of higher order cluster-based LP relaxations. The computational cost increases exponentially with the size of the clusters and limits the number and type of clusters we can use. We propose to solve the cluster selection problem monotonically in the dual LP, iteratively selecting clusters with guaranteed improvement, and quickly re-solving with the added clusters by reusing the existing solution. Our dual message-passing algorithm finds the MAP configuration in protein side-chain placement, protein design, and stereo problems, in cases where the standard LP relaxation fails."
            },
            "slug": "Tightening-LP-Relaxations-for-MAP-using-Message-Sontag-Meltzer",
            "title": {
                "fragments": [],
                "text": "Tightening LP Relaxations for MAP using Message Passing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to solve the cluster selection problem monotonically in the dual LP, iteratively selecting clusters with guaranteed improvement, and quickly re-solving with the added clusters by reusing the existing solution."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145878635"
                        ],
                        "name": "Tom\u00e1\u0161 Werner",
                        "slug": "Tom\u00e1\u0161-Werner",
                        "structuredName": {
                            "firstName": "Tom\u00e1\u0161",
                            "lastName": "Werner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom\u00e1\u0161 Werner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This derivation is similar to recently proposed efficient MAP inference algorithms, such as in [64, 82, 160, 162]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For this reason alternative algorithms such as tree-reweighted message passing [78], generalized max-product [142], and max-sum diffusion [162] have been proposed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The following linear programming relaxation has first been proposed by Schlesinger in the 1970s [131] and has been extensively analyzed by Wainwright and Jordan [160] and Werner [162]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "in structure to the max-sum diffusion method reviewed in [162] and to max-product belief propagation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11901033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f156f0c7ef3ac5274d92234cd3a3a40f9ca1fb94",
            "isKey": true,
            "numCitedBy": 403,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": "The max-sum labeling problem, defined as maximizing a sum of binary (i.e., pairwise) functions of discrete variables, is a general NP-hard optimization problem with many applications, such as computing the MAP configuration of a Markov random field. We review a not widely known approach to the problem, developed by Ukrainian researchers Schlesinger et al. in 1976, and show how it contributes to recent results, most importantly, those on the convex combination of trees and tree-reweighted max-product. In particular, we review Schlesinger et al.'s upper bound on the max-sum criterion, its minimization by equivalent transformations, its relation to the constraint satisfaction problem, the fact that this minimization is dual to a linear programming relaxation of the original problem, and the three kinds of consistency necessary for optimality of the upper bound. We revisit problems with Boolean variables and supermodular problems. We describe two algorithms for decreasing the upper bound. We present an example application for structural image analysis."
            },
            "slug": "A-Linear-Programming-Approach-to-Max-Sum-Problem:-A-Werner",
            "title": {
                "fragments": [],
                "text": "A Linear Programming Approach to Max-Sum Problem: A Review"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work reviews a not widely known approach to the max-sum labeling problem, developed by Ukrainian researchers Schlesinger et al. in 1976, and shows how it contributes to recent results, most importantly, those on the convex combination of trees and tree-reweighted max-product."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8147588"
                        ],
                        "name": "N. Nasrabadi",
                        "slug": "N.-Nasrabadi",
                        "structuredName": {
                            "firstName": "Nasser",
                            "lastName": "Nasrabadi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nasrabadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 63317738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "668b1277fbece28c4841eeab1c97e4ebd0079700",
            "isKey": false,
            "numCitedBy": 10185,
            "numCiting": 389,
            "paperAbstract": {
                "fragments": [],
                "text": "Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models."
            },
            "slug": "Pattern-Recognition-and-Machine-Learning-Bishop-Nasrabadi",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Probability Distributions, linear models for Regression, Linear Models for Classification, Neural Networks, Graphical Models, Mixture Models and EM, Sampling Methods, Continuous Latent Variables, Sequential Data are studied."
            },
            "venue": {
                "fragments": [],
                "text": "J. Electronic Imaging"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089883"
                        ],
                        "name": "W. Gilks",
                        "slug": "W.-Gilks",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Gilks",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gilks"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118130285,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "be29aa63b8fe754b23f9143cc42c79d2fc5fa601",
            "isKey": false,
            "numCitedBy": 695,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov chain Monte Carlo (MCMC) is a technique for estimating by simulation the expectation of a statistic in a complex model. Successive random selections form a Markov chain, the stationary distribution of which is the target distribution. It is particularly useful for the evaluation of posterior distributions in complex Bayesian models. In the Metropolis\u2013Hastings algorithm, items are selected from an arbitrary \u201cproposal\u201d distribution and are retained or not according to an acceptance rule. The Gibbs sampler is a special case in which the proposal distributions are conditional distributions of single components of a vector parameter. Various special cases and applications are considered. \n \n \nKeywords: \n \nMetropolis\u2013Hastings; \nGibbs sampler; \nstationary distribution; \nproposal distribution; \nhybrid chain; \nreversible jump; \nhierarchical; \nmissing data"
            },
            "slug": "Markov-Chain-Monte-Carlo-Gilks",
            "title": {
                "fragments": [],
                "text": "Markov Chain Monte Carlo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2505902"
                        ],
                        "name": "N. Komodakis",
                        "slug": "N.-Komodakis",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Komodakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Komodakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2441655"
                        ],
                        "name": "G. Tziritas",
                        "slug": "G.-Tziritas",
                        "structuredName": {
                            "firstName": "Georgios",
                            "lastName": "Tziritas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tziritas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680727"
                        ],
                        "name": "N. Paragios",
                        "slug": "N.-Paragios",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Paragios",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Paragios"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[83]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 744028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c827d162ab1b22ecc836e01f963b2c969e04cd99",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A new efficient MRF optimization algorithm, called Fast-PD, is proposed, which generalizes a-expansion. One of its main advantages is that it offers a substantial speedup over that method, e.g. it can be at least 3-9 times faster than a-expansion. Its efficiency is a result of the fact that Fast-PD exploits information coming not only from the original MRF problem, but also from a dual problem. Furthermore, besides static MRFs, it can also be used for boosting the performance of dynamic MRFs, i.e. MRFs varying over time. On top of that, Fast-PD makes no compromise about the optimality of its solutions: it can compute exactly the same answer as a-expansion, but, unlike that method, it can also guarantee an almost optimal solution for a much wider class of NP-hard MRF problems. Results on static and dynamic MRFs demonstrate the algorithm's efficiency and power. E.g., Fast-PD has been able to compute disparity for stereoscopic sequences in real time, with the resulting disparity coinciding with that of a-expansion."
            },
            "slug": "Fast,-Approximately-Optimal-Solutions-for-Single-Komodakis-Tziritas",
            "title": {
                "fragments": [],
                "text": "Fast, Approximately Optimal Solutions for Single and Dynamic MRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new efficient MRF optimization algorithm, called Fast-PD, is proposed, which generalizes a-expansion and can also guarantee an almost optimal solution for a much wider class of NP-hard MRF problems."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800855"
                        ],
                        "name": "A. Nedi\u0107",
                        "slug": "A.-Nedi\u0107",
                        "structuredName": {
                            "firstName": "Angelia",
                            "lastName": "Nedi\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nedi\u0107"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13930560,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "1bcddc47cda03a28133d6fdbbd9f386de7d4b6c3",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a class of subgradient methods for minimizing a convex function that consists of the sum of a large number of component functions. This type of minimization arises in a dual context from Lagrangian relaxation of the coupling constraints of large scale separable problems. The idea is to perform the subgradient iteration incrementally, by sequentially taking steps along the subgradients of the component functions, with intermediate adjustment of the variables after processing each component function. This incremental approach has been very successful in solving large differentiable least squares problems, such as those arising in the training of neural networks, and it has resulted in a much better practical rate of convergence than the steepest descent method."
            },
            "slug": "Convergence-Rate-of-Incremental-Subgradient-Nedi\u0107-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Convergence Rate of Incremental Subgradient Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An incremental approach to minimizing a convex function that consists of the sum of a large number of component functions is considered, which has been very successful in solving large differentiable least squares problems, such as those arising in the training of neural networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897348"
                        ],
                        "name": "Petter Strandmark",
                        "slug": "Petter-Strandmark",
                        "structuredName": {
                            "firstName": "Petter",
                            "lastName": "Strandmark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petter Strandmark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713563"
                        ],
                        "name": "F. Kahl",
                        "slug": "F.-Kahl",
                        "structuredName": {
                            "firstName": "Fredrik",
                            "lastName": "Kahl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kahl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3104163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "999bf723befd72b833f7c89d7fb341e76b85d126",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Graph cuts methods are at the core of many state-of-the-art algorithms in computer vision due to their efficiency in computing globally optimal solutions. In this paper, we solve the maximum flow/minimum cut problem in parallel by splitting the graph into multiple parts and hence, further increase the computational efficacy of graph cuts. Optimality of the solution is guaranteed by dual decomposition, or more specifically, the solutions to the subproblems are constrained to be equal on the overlap with dual variables. We demonstrate that our approach both allows (i) faster processing on multi-core computers and (ii) the capability to handle larger problems by splitting the graph across multiple computers on a distributed network. Even though our approach does not give a theoretical guarantee of speedup, an extensive empirical evaluation on several applications with many different data sets consistently shows good performance. An open source implementation of the dual decomposition method is also made publicly available."
            },
            "slug": "Parallel-and-distributed-graph-cuts-by-dual-Strandmark-Kahl",
            "title": {
                "fragments": [],
                "text": "Parallel and distributed graph cuts by dual decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper solves the maximum flow/minimum cut problem in parallel by splitting the graph into multiple parts and hence, further increase the computational efficacy of graph cuts."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778663"
                        ],
                        "name": "Vojtech Franc",
                        "slug": "Vojtech-Franc",
                        "structuredName": {
                            "firstName": "Vojtech",
                            "lastName": "Franc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vojtech Franc"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 233207635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b826049deed05378e7c1682de57597ab5df4682",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Cutting-plane methods are optimization techniques that incrementally construct an approximation of a feasible set or an objective function by linear inequalities called cutting planes. Numerous variants of this basic idea are among standard tools used in convex nonsmooth optimization and integer linear programing. Recently, cutting-plane methods have seen growing interest in the field of machine learning. In this chapter, we describe the basic theory behind these methods and show three of their successful applications to solving machine learning problems: regularized risk minimization, multiple kernel learning, and MAP inference in graphical models."
            },
            "slug": "Cutting-Plane-Methods-in-Machine-Learning-Franc",
            "title": {
                "fragments": [],
                "text": "Cutting-Plane Methods in Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The basic theory behind cutting-plane methods is described and three of their successful applications to solving machine learning problems are shown: regularized risk minimization, multiple kernel learning, and MAP inference in graphical models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145091933"
                        ],
                        "name": "D. Freedman",
                        "slug": "D.-Freedman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Freedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Freedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738441"
                        ],
                        "name": "P. Drineas",
                        "slug": "P.-Drineas",
                        "structuredName": {
                            "firstName": "Petros",
                            "lastName": "Drineas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Drineas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similar transformations can be made for the pairwise energies, and a complete characterization of graph cut solvable binary energy functions has been given by Kolmogorov and Zabin [80] and Freedman and Drineas [44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7556238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5284a9631062378ebdf52b24bb26933cfa65e47",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The recent explosion of interest in graph cut methods in computer vision naturally spawns the question: what energy functions can be minimized via graph cuts? This question was first attacked by two papers of Kolmogorov and Zabih, in which they dealt with functions with pair-wise and triplewise pixel interactions. In this work, we extend their results in two directions. First, we examine the case of k-wise pixel interactions; the results are derived from a purely algebraic approach. Second, we discuss the applicability of provably approximate algorithms. Both of these developments should help researchers best understand what can and cannot be achieved when designing graph cut based algorithms."
            },
            "slug": "Energy-minimization-via-graph-cuts:-settling-what-Freedman-Drineas",
            "title": {
                "fragments": [],
                "text": "Energy minimization via graph cuts: settling what is possible"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "First, the case of k-wise pixel interactions is examined; the results are derived from a purely algebraic approach; second, the applicability of provably approximate algorithms are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143676697"
                        ],
                        "name": "Y. Nesterov",
                        "slug": "Y.-Nesterov",
                        "structuredName": {
                            "firstName": "Yurii",
                            "lastName": "Nesterov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nesterov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[130] applied Nesterov\u2019s smoothing technique [110] to (4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2391217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e6c6086ea725737aa6081a57ea68d43a24ca3b9",
            "isKey": false,
            "numCitedBy": 2435,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.In this paper we propose a new approach for constructing efficient schemes for non-smooth convex optimization. It is based on a special smoothing technique, which can be applied to functions with explicit max-structure. Our approach can be considered as an alternative to black-box minimization. From the viewpoint of efficiency estimates, we manage to improve the traditional bounds on the number of iterations of the gradient schemes from keeping basically the complexity of each iteration unchanged."
            },
            "slug": "Smooth-minimization-of-non-smooth-functions-Nesterov",
            "title": {
                "fragments": [],
                "text": "Smooth minimization of non-smooth functions"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A new approach for constructing efficient schemes for non-smooth convex optimization is proposed, based on a special smoothing technique, which can be applied to functions with explicit max-structure, and can be considered as an alternative to black-box minimization."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 19
                            }
                        ],
                        "text": "have been proposed [78, 174], an overview of different variants and recent results is available in [102]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 782136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "974fee9c3836d91aee9492c07379898ccc2c3a85",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This article introduces a class of discrete iterative algorithms that are provably convergent alternatives to belief propagation (BP) and generalized belief propagation (GBP). Our work builds on recent results by Yedidia, Freeman, and Weiss (2000), who showed that the fixed points of BP and GBP algorithms correspond to extrema of the Bethe and Kikuchi free energies, respectively. We obtain two algorithms by applying CCCP to the Bethe and Kikuchi free energies, respectively (CCCP is a procedure, introduced here, for obtaining discrete iterative algorithms by decomposing a cost function into a concave and a convex part). We implement our CCCP algorithms on two- and three-dimensional spin glasses and compare their results to BP and GBP. Our simulations show that the CCCP algorithms are stable and converge very quickly (the speed of CCCP is similar to that of BP and GBP). Unlike CCCP, BP will often not converge for these problems (GBP usually, but not always, converges). The results found by CCCP applied to the Bethe or Kikuchi free energies are equivalent, or slightly better than, those found by BP or GBP, respectively (when BP and GBP converge). Note that for these, and other problems, BP and GBP give very accurate results (see Yedidia et al., 2000), and failure to converge is their major error mode. Finally, we point out that our algorithms have a large range of inference and learning applications."
            },
            "slug": "CCCP-Algorithms-to-Minimize-the-Bethe-and-Kikuchi-Yuille",
            "title": {
                "fragments": [],
                "text": "CCCP Algorithms to Minimize the Bethe and Kikuchi Free Energies: Convergent Alternatives to Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A class of discrete iterative algorithms that are provably convergent alternatives to believe propagation (BP) and generalized belief propagation (GBP) and are pointed out that have a large range of inference and learning applications."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145257017"
                        ],
                        "name": "Anand Rangarajan",
                        "slug": "Anand-Rangarajan",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rangarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Rangarajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, good results have been reported using locally optimal optimization techniques, in particular the concave\u2013convex procedure (CCCP) [175] for which pseudo-code is given in Algorithm 16."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1668136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffb607e61e10a3bb54463b334aaf5ea9c7c04be6",
            "isKey": false,
            "numCitedBy": 1065,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The concave-convex procedure (CCCP) is a way to construct discrete-time iterative dynamical systems that are guaranteed to decrease global optimization and energy functions monotonically. This procedure can be applied to almost any optimization problem, and many existing algorithms can be interpreted in terms of it. In particular, we prove that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP. We show that many existing neural network and mean-field theory algorithms are also examples of CCCP. The generalized iterative scaling algorithm and Sinkhorn's algorithm can also be expressed as CCCP by changing variables. CCCP can be used both as a new way to understand, and prove the convergence of, existing optimization algorithms and as a procedure for generating new algorithms."
            },
            "slug": "The-Concave-Convex-Procedure-Yuille-Rangarajan",
            "title": {
                "fragments": [],
                "text": "The Concave-Convex Procedure"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "It is proved that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2518560"
                        ],
                        "name": "F. Liang",
                        "slug": "F.-Liang",
                        "structuredName": {
                            "firstName": "Faming",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30257261"
                        ],
                        "name": "Chuanhai Liu",
                        "slug": "Chuanhai-Liu",
                        "structuredName": {
                            "firstName": "Chuanhai",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuanhai Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34101041"
                        ],
                        "name": "R. Carroll",
                        "slug": "R.-Carroll",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Carroll",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Carroll"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118728102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9099ec19e1f7058109db6b119d2362cd3ca44c5",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. Acknowledgments. Publisher's Acknowledgments. 1 Bayesian Inference and Markov Chain Monte Carlo. 1.1 Bayes. 1.1.1 Specification of Bayesian Models. 1.1.2 The Jeffreys Priors and Beyond. 1.2 Bayes Output. 1.2.1 Credible Intervals and Regions. 1.2.2 Hypothesis Testing: Bayes Factors. 1.3 Monte Carlo Integration. 1.3.1 The Problem. 1.3.2 Monte Carlo Approximation. 1.3.3 Monte Carlo via Importance Sampling. 1.4 Random Variable Generation. 1.4.1 Direct or TransformationMethods. 1.4.2 Acceptance-Rejection Methods. 1.4.3 The Ratio-of-UniformsMethod and Beyond. 1.4.4 Adaptive Rejection Sampling. 1.4.5 Perfect Sampling. 1.5 Markov ChainMonte Carlo. 1.5.1 Markov Chains. 1.5.2 Convergence Results. 1.5.3 Convergence Diagnostics. Exercises. 2 The Gibbs Sampler. 2.1 The Gibbs Sampler. 2.2 Data Augmentation. 2.3 Implementation Strategies and Acceleration Methods. 2.3.1 Blocking and Collapsing. 2.3.2 Hierarchical Centering and Reparameterization. 2.3.3 Parameter Expansion for Data Augmentation. 2.3.4 Alternating Subspace-Spanning Resampling. 2.4 Applications. 2.4.1 The Student-tModel. 2.4.2 Robit Regression or Binary Regression with the Student-t Link. 2.4.3 Linear Regression with Interval-Censored Responses. Exercises. Appendix 2A: The EMand PX-EMAlgorithms. 3 The Metropolis-Hastings Algorithm. 3.1 TheMetropolis-Hastings Algorithm. 3.1.1 Independence Sampler. 3.1.2 RandomWalk Chains. 3.1.3 Problems withMetropolis-Hastings Simulations. 3.2 Variants of theMetropolis-Hastings Algorithm. 3.2.1 The Hit-and-Run Algorithm. 3.2.2 The Langevin Algorithm. 3.2.3 TheMultiple-TryMH Algorithm. 3.3 Reversible Jump MCMC Algorithm for Bayesian Model Selection Problems. 3.3.1 Reversible JumpMCMC Algorithm. 3.3.2 Change-Point Identification. 3.4 Metropolis-Within-Gibbs Sampler for ChIP-chip Data Analysis. 3.4.1 Metropolis-Within-Gibbs Sampler. 3.4.2 Bayesian Analysis for ChIP-chip Data. Exercises. 4 Auxiliary Variable MCMC Methods. 4.1 Simulated Annealing. 4.2 Simulated Tempering. 4.3 The Slice Sampler. 4.4 The Swendsen-Wang Algorithm. 4.5 TheWolff Algorithm. 4.6 The Mo/ller Algorithm. 4.7 The Exchange Algorithm. 4.8 The DoubleMH Sampler. 4.8.1 Spatial AutologisticModels. 4.9 Monte CarloMH Sampler. 4.9.1 Monte CarloMH Algorithm. 4.9.2 Convergence. 4.9.3 Spatial AutologisticModels (Revisited). 4.9.4 Marginal Inference. 4.10 Applications. 4.10.1 AutonormalModels. 4.10.2 Social Networks. Exercises. 5 Population-Based MCMC Methods. 5.1 Adaptive Direction Sampling. 5.2 Conjugate GradientMonte Carlo. 5.3 SampleMetropolis-Hastings Algorithm. 5.4 Parallel Tempering. 5.5 EvolutionaryMonte Carlo. 5.5.1 Evolutionary Monte Carlo in Binary-Coded Space. 5.5.2 EvolutionaryMonte Carlo in Continuous Space. 5.5.3 Implementation Issues. 5.5.4 Two Illustrative Examples. 5.5.5 Discussion. 5.6 Sequential Parallel Tempering for Simulation of High Dimensional Systems. 5.6.1 Build-up Ladder Construction. 5.6.2 Sequential Parallel Tempering. 5.6.3 An Illustrative Example: the Witch s Hat Distribution. 5.6.4 Discussion. 5.7 Equi-Energy Sampler. 5.8 Applications. 5.8.1 Bayesian Curve Fitting. 5.8.2 Protein Folding Simulations: 2D HPModel. 5.8.3 Bayesian Neural Networks for Nonlinear Time Series Forecasting. Exercises. Appendix 5A: Protein Sequences for 2D HPModels. 6 Dynamic Weighting. 6.1 DynamicWeighting. 6.1.1 The IWIWPrinciple. 6.1.2 Tempering DynamicWeighting Algorithm. 6.1.3 DynamicWeighting in Optimization. 6.2 DynamicallyWeighted Importance Sampling. 6.2.1 The Basic Idea. 6.2.2 A Theory of DWIS. 6.2.3 Some IWIWp Transition Rules. 6.2.4 Two DWIS Schemes. 6.2.5 Weight Behavior Analysis. 6.2.6 A Numerical Example. 6.3 Monte Carlo Dynamically Weighted Importance Sampling. 6.3.1 Sampling from Distributions with Intractable Normalizing Constants. 6.3.2 Monte Carlo Dynamically Weighted Importance Sampling. 6.3.3 Bayesian Analysis for Spatial Autologistic Models. 6.4 Sequentially Dynamically Weighted Importance Sampling. Exercises. 7 Stochastic Approximation Monte Carlo. 7.1 MulticanonicalMonte Carlo. 7.2 1/k-Ensemble Sampling. 7.3 TheWang-Landau Algorithm. 7.4 Stochastic ApproximationMonte Carlo. 7.5 Applications of Stochastic ApproximationMonte Carlo. 7.5.1 Efficient p-Value Evaluation for Resampling-Based Tests. 7.5.2 Bayesian Phylogeny Inference. 7.5.3 Bayesian Network Learning. 7.6 Variants of Stochastic ApproximationMonte Carlo. 7.6.1 Smoothing SAMC forModel Selection Problems. 7.6.2 Continuous SAMC for Marginal Density Estimation. 7.6.3 Annealing SAMC for Global Optimization. 7.7 Theory of Stochastic ApproximationMonte Carlo. 7.7.1 Convergence. 7.7.2 Convergence Rate. 7.7.3 Ergodicity and its IWIWProperty. 7.8 Trajectory Averaging: Toward the Optimal Convergence Rate. 7.8.1 Trajectory Averaging for a SAMCMC Algorithm. 7.8.2 Trajectory Averaging for SAMC. 7.8.3 Proof of Theorems 7.8.2 and 7.8.3. Exercises. Appendix 7A: Test Functions for Global Optimization. 8 Markov Chain Monte Carlo with Adaptive Proposals. 8.1 Stochastic Approximation-Based Adaptive Algorithms. 8.1.1 Ergodicity andWeak Law of Large Numbers. 8.1.2 AdaptiveMetropolis Algorithms. 8.2 Adaptive IndependentMetropolis-Hastings Algorithms. 8.3 Regeneration-Based Adaptive Algorithms. 8.3.1 Identification of Regeneration Times. 8.3.2 Proposal Adaptation at Regeneration Times. 8.4 Population-Based Adaptive Algorithms. 8.4.1 ADS, EMC, NKC andMore. 8.4.2 Adaptive EMC. 8.4.3 Application to Sensor Placement Problems. Exercises. References. Index."
            },
            "slug": "Advanced-Markov-Chain-Monte-Carlo-Methods:-Learning-Liang-Liu",
            "title": {
                "fragments": [],
                "text": "Advanced Markov Chain Monte Carlo Methods: Learning from Past Samples"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This chapter discusses Bayesian Inference and Markov Chain Monte Carlo, which automates the very labor-intensive and therefore time-heavy and expensive process of Bayesian Model Selection."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 188
                            }
                        ],
                        "text": "Another reason why we might be satisfied with a good suboptimal solution is model uncertainty resulting from both using a simplified or wrong model and from the parameter estimation error [Bottou and Bousquet, 2007]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7431525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5936754b5762260bf102ac95d7b26cfc9d31956a",
            "isKey": false,
            "numCitedBy": 1485,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation-estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways."
            },
            "slug": "The-Tradeoffs-of-Large-Scale-Learning-Bottou-Bousquet",
            "title": {
                "fragments": [],
                "text": "The Tradeoffs of Large Scale Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms and shows distinct tradeoffs for the case of small-scale and large-scale learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409964566"
                        ],
                        "name": "Dong C. Liu",
                        "slug": "Dong-C.-Liu",
                        "structuredName": {
                            "firstName": "Dong C.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong C. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", the L-BFGS method (limited memory BFGS) [96] that stores a sequence of vector operations instead of the matrix Bk."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5681609,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1267fe36b5ece49a9d8f913eb67716a040bbcced",
            "isKey": false,
            "numCitedBy": 5862,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems."
            },
            "slug": "On-the-limited-memory-BFGS-method-for-large-scale-Liu-Nocedal",
            "title": {
                "fragments": [],
                "text": "On the limited memory BFGS method for large scale optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence, and the convergence properties are studied to prove global convergence on uniformly convex problems."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143826285"
                        ],
                        "name": "Frank R. Schmidt",
                        "slug": "Frank-R.-Schmidt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Schmidt",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank R. Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735363"
                        ],
                        "name": "D. Farin",
                        "slug": "D.-Farin",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Farin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Farin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695302"
                        ],
                        "name": "D. Cremers",
                        "slug": "D.-Cremers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cremers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cremers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14568371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8cb553de7095c37c52c8e5735172f73b39b7aafb",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The matching of planar shapes can be cast as a problem of finding the shortest path through a graph spanned by the two shapes, where the nodes of the graph encode the local similarity of respective points on each contour. While this problem can be solved using dynamic time warping, the complete search over the initial correspondence leads to cubic runtime in the number of sample points. In this paper, we cast the shape matching problem as one of finding the shortest circular path on a torus. We propose an algorithm to determine this shortest cycle which has provably sub-cubic runtime. Numerical experiments demonstrate that the proposed algorithm provides faster shape matching than previous methods. As an application, we show that it allows to efficiently compute a clustering of a shape data base."
            },
            "slug": "Fast-Matching-of-Planar-Shapes-in-Sub-cubic-Runtime-Schmidt-Farin",
            "title": {
                "fragments": [],
                "text": "Fast Matching of Planar Shapes in Sub-cubic Runtime"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An algorithm to determine this shortest cycle which has provably sub-cubic runtime is proposed and it is shown that it allows to efficiently compute a clustering of a shape data base."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2465976"
                        ],
                        "name": "M. Fischler",
                        "slug": "M.-Fischler",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Fischler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fischler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3394928"
                        ],
                        "name": "R. Elschlager",
                        "slug": "R.-Elschlager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Elschlager",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Elschlager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The pictorial structures model first proposed by Fischler and Elschlager [40] models objects as individual parts with pairwise relations between parts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14554383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "719da2a0ddd38e78151e1cb2db31703ea8b2e490",
            "isKey": false,
            "numCitedBy": 1527,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of \"goodness\" of matching or detection."
            },
            "slug": "The-Representation-and-Matching-of-Pictorial-Fischler-Elschlager",
            "title": {
                "fragments": [],
                "text": "The Representation and Matching of Pictorial Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The primary problem dealt with in this paper is the specification of a descriptive scheme, and a metric on which to base the decision of \"goodness\" of matching or detection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4628585"
                        ],
                        "name": "Yoonkyung Lee",
                        "slug": "Yoonkyung-Lee",
                        "structuredName": {
                            "firstName": "Yoonkyung",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoonkyung Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108066200"
                        ],
                        "name": "Yi Lin",
                        "slug": "Yi-Lin",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "1Unfortunately, the most satisfactory consistency property does not hold for the general multi-class or structured prediction situations [Lee et al., 2004]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7066611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94b295e711ec744583597432c19749a5cd61038e",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 154,
            "paperAbstract": {
                "fragments": [],
                "text": "Two-category support vector machines (SVM) have been very popular in the machine learning community for classification problems. Solving multicategory problems by a series of binary classifiers is quite common in the SVM paradigm; however, this approach may fail under various circumstances. We propose the multicategory support vector machine (MSVM), which extends the binary SVM to the multicategory case and has good theoretical properties. The proposed method provides a unifying framework when there are either equal or unequal misclassification costs. As a tuning criterion for the MSVM, an approximate leave-one-out cross-validation function, called Generalized Approximate Cross Validation, is derived, analogous to the binary case. The effectiveness of the MSVM is demonstrated through the applications to cancer classification using microarray data and cloud classification with satellite radiance profiles."
            },
            "slug": "Multicategory-Support-Vector-Machines-Lee-Lin",
            "title": {
                "fragments": [],
                "text": "Multicategory Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The MSVM is proposed, which extends the binary SVM to the multicategory case and has good theoretical properties, and an approximate leave-one-out cross-validation function is derived, analogous to the binary case."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876131"
                        ],
                        "name": "C. Geyer",
                        "slug": "C.-Geyer",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Geyer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Geyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "An introduction to general Markov chains and their application to statistical inference is considered in [51, 126]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121738787,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0c6b0171fe26bb84e89c244e1f859ebdf86e6e45",
            "isKey": false,
            "numCitedBy": 1758,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov chain Monte Carlo using the Metropolis-Hastings algorithm is a general method for the simulation of stochastic processes having probability densities known up to a constant of proportionality. Despite recent advances in its theory, the practice has remained controversial. This article makes the case for basing all inference on one long run of the Markov chain and estimating the Monte Carlo error by standard nonparametric methods well-known in the time-series and operations research literature. In passing it touches on the Kipnis-Varadhan central limit theorem for reversible Markov chains, on some new variance estimators, on judging the relative efficiency of competing Monte Carlo schemes, on methods for constructing more rapidly mixing Markov chains and on diagnostics for Markov chain Monte Carlo."
            },
            "slug": "Practical-Markov-Chain-Monte-Carlo-Geyer",
            "title": {
                "fragments": [],
                "text": "Practical Markov Chain Monte Carlo"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The case is made for basing all inference on one long run of the Markov chain and estimating the Monte Carlo error by standard nonparametric methods well-known in the time-series and operations research literature."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2505902"
                        ],
                        "name": "N. Komodakis",
                        "slug": "N.-Komodakis",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Komodakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Komodakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680727"
                        ],
                        "name": "N. Paragios",
                        "slug": "N.-Paragios",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Paragios",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Paragios"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2441655"
                        ],
                        "name": "G. Tziritas",
                        "slug": "G.-Tziritas",
                        "structuredName": {
                            "firstName": "Georgios",
                            "lastName": "Tziritas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tziritas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This derivation is similar to recently proposed efficient MAP inference algorithms, such as in [64, 82, 160, 162]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[82] to approximately solve the linear programming relaxation by iterating MAP inference on tree-structured subgraphs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6596960,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38649fa7ab5b237dc65290f20e16adc648363bc2",
            "isKey": false,
            "numCitedBy": 336,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A new message-passing scheme for MRF optimization is proposed in this paper. This scheme inherits better theoretical properties than all other state-of-the-art message passing methods and in practice performs equally well/outperforms them. It is based on the very powerful technique of Dual Decomposition [1] and leads to an elegant and general framework for understanding/designing message-passing algorithms that can provide new insights into existing techniques. Promising experimental results and comparisons with the state of the art demonstrate the extreme theoretical and practical potentials of our approach."
            },
            "slug": "MRF-Optimization-via-Dual-Decomposition:-Revisited-Komodakis-Paragios",
            "title": {
                "fragments": [],
                "text": "MRF Optimization via Dual Decomposition: Message-Passing Revisited"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new message-passing scheme for MRF optimization that inherits better theoretical properties than all other state-of-the-art message passing methods and in practice performs equally well/outperforms them."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3280984"
                        ],
                        "name": "M. Maher",
                        "slug": "M.-Maher",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Maher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Because integer linear programs [168] are a very general model class, formulating a problem as integer program is often possible even for nonlinear objective functions and complicated finite feasible sets by introducing additional \u201cauxiliary\u201d variables [166]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62183279,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "04d770d6d354cdb6e23ac276ed5f226dac5431c9",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The 5th edition of Model Building in Mathematical Programming discusses the general principles of model building in mathematical programming and demonstrates how they can be applied by using several simplified but practical problems from widely different contexts. Suggested formulations and solutions are given together with some computational experience to give the reader a feel for the computational difficulty of solving that particular type of model. Furthermore, this book illustrates the scope and limitations of mathematical programming, and shows how it can be applied to real situations. By emphasizing the importance of the building and interpreting of models rather than the solution process, the author attempts to fill a gap left by the many works which concentrate on the algorithmic side of the subject."
            },
            "slug": "Model-Building-in-Mathematical-Programming-Maher",
            "title": {
                "fragments": [],
                "text": "Model Building in Mathematical Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The 5th edition of Model Building in Mathematical Programming discusses the general principles of model building in mathematical programming and demonstrates how they can be applied by using several simplified but practical problems from widely different contexts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39493519"
                        ],
                        "name": "M. Guignard",
                        "slug": "M.-Guignard",
                        "structuredName": {
                            "firstName": "Monique",
                            "lastName": "Guignard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Guignard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 197
                            }
                        ],
                        "text": "Instead of dropping a constraint completely in order to construct a relaxation \u2013 as done when relaxing an integer linear program to a linear program, an alternative method is Lagrangian relaxation [Guignard, 2003; Lemar\u00e9chal, 2001]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53479681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46823f9323d7ecaf53cb0d5ad6b9190e50e40ad4",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reviews some of the most intriguing results and questions related to Lagrangean relaxation. It recalls essential properties of the Lagrangean relaxation and of the Lagrangean function, describes several algorithms to solve the Lagrangean dual problem, and considers Lagrangean heuristics, ad-hoc or generic, because these are an integral part of any Lagrangean approximation scheme. It discusses schemes that can potentially improve the Lagrangean relaxation bound, and describes several applications of Lagrangean relaxation, which demonstrate the flexibility of the approach, and permit either the computation of strong bounds on the optimal value of the MIP problem, or the use of a Lagrangean heuristic, possibly followed by an iterative improvement heuristic. The paper also analyzes several interesting questions, such as why it is sometimes possible to get a strong bound by solving simple problems, and why an a-priori weaker relaxation can sometimes be \u201cjust as good\u201d as an a-priori stronger one."
            },
            "slug": "Lagrangean-relaxation-Guignard",
            "title": {
                "fragments": [],
                "text": "Lagrangean relaxation"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This paper reviews some of the most intriguing results and questions related to Lagrangean relaxation, and describes several applications of Lagrangeans relaxation, which demonstrate the flexibility of the approach, and permit either the computation of strong bounds on the optimal value of the MIP problem, or the use of alagrangean heuristic, possibly followed by an iterative improvement heuristic."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732982"
                        ],
                        "name": "R. Ahuja",
                        "slug": "R.-Ahuja",
                        "structuredName": {
                            "firstName": "Ravindra",
                            "lastName": "Ahuja",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ahuja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772886"
                        ],
                        "name": "\u00d6zlem Ergun",
                        "slug": "\u00d6zlem-Ergun",
                        "structuredName": {
                            "firstName": "\u00d6zlem",
                            "lastName": "Ergun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d6zlem Ergun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795576"
                        ],
                        "name": "J. Orlin",
                        "slug": "J.-Orlin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Orlin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Orlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2823036"
                        ],
                        "name": "Abraham P. Punnen",
                        "slug": "Abraham-P.-Punnen",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Punnen",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abraham P. Punnen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "There are at least two other common names for local search methods, move-making methods [73], and very large-scale neighborhood search [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1946785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e126763532b9cf1ce42544318446386db7b333a",
            "isKey": false,
            "numCitedBy": 644,
            "numCiting": 142,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-survey-of-very-large-scale-neighborhood-search-Ahuja-Ergun",
            "title": {
                "fragments": [],
                "text": "A survey of very large-scale neighborhood search techniques"
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Appl. Math."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720384"
                        ],
                        "name": "V. Vazirani",
                        "slug": "V.-Vazirani",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Vazirani",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vazirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 834161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "112e14d8f5d9e9b1cd99ae862cefc085566fb580",
            "isKey": false,
            "numCitedBy": 3976,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "Covering the basic techniques used in the latest research work, the author consolidates progress made so far, including some very recent and promising results, and conveys the beauty and excitement of work in the field. He gives clear, lucid explanations of key results and ideas, with intuitive proofs, and provides critical examples and numerous illustrations to help elucidate the algorithms. Many of the results presented have been simplified and new insights provided. Of interest to theoretical computer scientists, operations researchers, and discrete mathematicians."
            },
            "slug": "Approximation-Algorithms-Vazirani",
            "title": {
                "fragments": [],
                "text": "Approximation Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Covering the basic techniques used in the latest research work, the author consolidates progress made so far, including some very recent and promising results, and conveys the beauty and excitement of work in the field."
            },
            "venue": {
                "fragments": [],
                "text": "Springer Berlin Heidelberg"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746201"
                        ],
                        "name": "K. Anstreicher",
                        "slug": "K.-Anstreicher",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Anstreicher",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Anstreicher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736128"
                        ],
                        "name": "L. Wolsey",
                        "slug": "L.-Wolsey",
                        "structuredName": {
                            "firstName": "Laurence",
                            "lastName": "Wolsey",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wolsey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "70) from the iterates of the subgradient method due to a result of Shor [6], that guarantees"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34751624,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "81e9c9fa66d414ec6b538b0cfab72ad3507bbe36",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The subgradient method is both a heavily employed and widely studied algorithm for non-differentiable optimization. Nevertheless, there are some basic properties of subgradient optimization that, while \u201cwell known\u201d to specialists, seem to be rather poorly known in the larger optimization community. This note concerns two such properties, both applicable to subgradient optimization using the divergent series steplength rule. The first involves convergence of the iterative process, and the second deals with the construction of primal estimates when subgradient optimization is applied to maximize the Lagrangian dual of a linear program. The two topics are related in that convergence of the iterates is required to prove correctness of the primal construction scheme."
            },
            "slug": "Two-\u201cwell-known\u201d-properties-of-subgradient-Anstreicher-Wolsey",
            "title": {
                "fragments": [],
                "text": "Two \u201cwell-known\u201d properties of subgradient optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two topics are related in that convergence of the iterates is required to prove correctness of the primal construction scheme, and the construction of primal estimates when subgradient optimization is applied to maximize the Lagrangian dual of a linear program."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143768294"
                        ],
                        "name": "J. Clausen",
                        "slug": "J.-Clausen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Clausen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Clausen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 72
                            }
                        ],
                        "text": "A more extensive discussion of branch-and-bound methods can be found in [32, 168]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 83
                            }
                        ],
                        "text": "Branch and bound is a divide and conquer algorithm and can be described as follows [13, 32, 84]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16580792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fffdebefecd6a60a841acc8aafb7f0e89a76f996",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A large number of real-world planning problems called combinatorial optimization problems share the following properties: They are optimization problems, are easy to state, and have a finite but usually very large number of feasible solutions. While some of these as e.g. the Shortest Path problem and the Minimum Spanning Tree problem have polynomial algoritms, the majority of the problems in addition share the property that no polynomial method for their solution is known. Examples here are vehicle \u2217Department of Computer Science, University of Copenhagen, Universitetsparken 1, DK2100 Copenhagen, Denmark."
            },
            "slug": "Branch-and-Bound-Algorithms-Principles-and-Examples-Clausen",
            "title": {
                "fragments": [],
                "text": "Branch and Bound Algorithms-Principles and Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A large number of real-world planning problems called combinatorial optimization problems share the following properties: They are optimization problems, are easy to state, and have a finite but usually very large numberof feasible solutions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143871349"
                        ],
                        "name": "F. Barahona",
                        "slug": "F.-Barahona",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Barahona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Barahona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2712392"
                        ],
                        "name": "R. Anbil",
                        "slug": "R.-Anbil",
                        "structuredName": {
                            "firstName": "Ranga",
                            "lastName": "Anbil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Anbil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In practise another popular method originally proposed by Barahona and Anbil [8] to obtain an approximate primal solution is to take the average of all primal iterates as a geometric series and thus obtaining a moving average \u0233t as approximate primal solution, \u0233 = \u03b3y(\u03bb,\u03bc) + (1 \u2212 \u03b3)\u0233t\u22121, (4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16837035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5d8bc6143c3d64069335724458f7eb7ef96b8a6",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.We present an extension to the subgradient algorithm to produce primal as well as dual solutions. It can be seen as a fast way to carry out an approximation of Dantzig-Wolfe decomposition. This gives a fast method for producing approximations for large scale linear programs. It is based on a new theorem in linear programming duality. We present successful experience with linear programs coming from set partitioning, set covering, max-cut and plant location."
            },
            "slug": "The-volume-algorithm:-producing-primal-solutions-a-Barahona-Anbil",
            "title": {
                "fragments": [],
                "text": "The volume algorithm: producing primal solutions with a subgradient method"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An extension to the subgradient algorithm to produce primal as well as dual solutions based on a new theorem in linear programming duality is presented, which gives a fast method for producing approximations for large scale linear programs."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145878635"
                        ],
                        "name": "Tom\u00e1\u0161 Werner",
                        "slug": "Tom\u00e1\u0161-Werner",
                        "structuredName": {
                            "firstName": "Tom\u00e1\u0161",
                            "lastName": "Werner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom\u00e1\u0161 Werner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 186
                            }
                        ],
                        "text": "In the example below we only consider unary and pairwise factors, but there also exist straightforward extensions of the linear program to higher order factors, such as the one given in [Werner, 2008]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 166
                            }
                        ],
                        "text": "For a given factor graph (V,F , E), each relaxation instance is defined by means of a set of triplets, J = {(A,B, yB)|A,B \u2286 V, \u2205 6 = B \u2282 A}, the so called pencil set [Werner, 2008]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7682719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "448a9af2fe594494f9e2f8fb12088a1a69ecb36a",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "LP relaxation approach to soft constraint optimisation (i.e. MAP-MRF) has been mostly considered only for binary problems. We present its generalisation to n-ary problems, including a simple algorithm to optimise the LP bound, n-ary max-sum diffusion. As applications, we show that a hierarchy of gradually tighter polyhedral relaxations of MAP-MRF is obtained by adding zero interactions. We propose a cutting plane algorithm, where cuts correspond to adding zero interactions and the separation problem to finding an unsatisfiable constraint satisfaction subproblem. Next, we show that certain high-arity interactions, e.g. certain global constraints, can be included into the framework in a principled way. Finally, we prove that n-ary max-sum diffusion finds global optimum for n-ary supermodular problems."
            },
            "slug": "High-arity-interactions,-polyhedral-relaxations,-Werner",
            "title": {
                "fragments": [],
                "text": "High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (MAP-MRF)"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that certain high-arity interactions, e.g. certain global constraints, can be included into the framework in a principled way and it is proved that n-ary max-sum diffusion finds global optimum for n-ARY supermodular problems."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 198120256,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43c3bfffdcd313c549b2045980855ea001d6f13b",
            "isKey": false,
            "numCitedBy": 10652,
            "numCiting": 270,
            "paperAbstract": {
                "fragments": [],
                "text": "Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side."
            },
            "slug": "Numerical-Optimization-Nocedal-Wright",
            "title": {
                "fragments": [],
                "text": "Numerical Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization, responding to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145847131"
                        ],
                        "name": "S. Kirkpatrick",
                        "slug": "S.-Kirkpatrick",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Kirkpatrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kirkpatrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5882723"
                        ],
                        "name": "C. D. Gelatt",
                        "slug": "C.-D.-Gelatt",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gelatt",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Gelatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88645967"
                        ],
                        "name": "M. Vecchi",
                        "slug": "M.-Vecchi",
                        "structuredName": {
                            "firstName": "Michelle",
                            "lastName": "Vecchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vecchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Simulated annealing [1, 71] is a well-known method to approximately solve problems of the form (4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205939,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "dd5061631a4d11fa394f4421700ebf7e78dcbc59",
            "isKey": false,
            "numCitedBy": 39637,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods."
            },
            "slug": "Optimization-by-Simulated-Annealing-Kirkpatrick-Gelatt",
            "title": {
                "fragments": [],
                "text": "Optimization by Simulated Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The loopy belief propagation algorithm made approximate inference possible in previously intractable models [46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6518359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8871254256b95f52fe6a2c0edeee0fa706c1117",
            "isKey": false,
            "numCitedBy": 370,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Until recently, artificial intelligence researchers have frowned upon the application of probability propagation in Bayesian belief networks that have cycles. The probability propagation algorithm is only exact in networks that are cycle-free. However, it has recently been discovered that the two best error-correcting decoding algorithms are actually performing probability propagation in belief networks with cycles."
            },
            "slug": "A-Revolution:-Belief-Propagation-in-Graphs-with-Frey-Mackay",
            "title": {
                "fragments": [],
                "text": "A Revolution: Belief Propagation in Graphs with Cycles"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Until recently, artificial intelligence researchers have frowned upon the application of probability propagation in Bayesian belief networks that have cycles, but it has recently been discovered that the two best error-correcting decoding algorithms are actually performing probability propagation with cycles."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145155783"
                        ],
                        "name": "C. Robert",
                        "slug": "C.-Robert",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Robert",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Robert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144898907"
                        ],
                        "name": "G. Casella",
                        "slug": "G.-Casella",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Casella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Casella"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59843537,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "64b483e8d68b3f7518c2078d1b707d4543b6f9dc",
            "isKey": false,
            "numCitedBy": 1223,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Are you looking to uncover monte carlo statistical methods springer texts in statistics Digitalbook. Correct here it is possible to locate as well as download monte carlo statistical methods springer texts in statistics Book. We've got ebooks for every single topic monte carlo statistical methods springer texts in statistics accessible for download cost-free. Search the site also as find Jean Campbell eBook in layout. We also have a fantastic collection of information connected to this Digitalbook for you. As well because the best part is you could assessment as well as download for monte carlo statistical methods springer texts in statistics eBook"
            },
            "slug": "Monte-Carlo-Statistical-Methods-(Springer-Texts-in-Robert-Casella",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Statistical Methods (Springer Texts in Statistics)"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "It is possible to locate as well as download monte carlo statistical methods springer texts in statistics Book."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39493519"
                        ],
                        "name": "M. Guignard",
                        "slug": "M.-Guignard",
                        "structuredName": {
                            "firstName": "Monique",
                            "lastName": "Guignard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Guignard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29204527"
                        ],
                        "name": "Siwhan Kim",
                        "slug": "Siwhan-Kim",
                        "structuredName": {
                            "firstName": "Siwhan",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siwhan Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "34 Schematic interpretation of the geometry of Lagrangian decomposition, following [54]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For the special case where Yk are finite sets \u2014 as is the case for discrete models \u2014 and we have a linear cost function g(x,y) = c(x) y, the following theorem providing a simple interpretation of Lagrangian decomposition was proven by [54]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u201d For the original paper, see [54], for an up to date textbook covering other problem decomposition techniques, see [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20513738,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6e211923d17d70a60575967df3ab8de951c905d0",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a mixed-integer programming problem with two matrix constraints, it is possible to define a Lagrangean relaxation such that the relaxed problem decomposes additively into two subproblems, each having one of the two matrices of the original problem as its constraints. There is one Lagrangean multiplier per variable. We prove that the optimal value of this new Lagrangean dual dominates the optimal value of the Lagrangean dual obtained by relaxing one set of constraints and give a necessary condition for a strict improvement. We show on an example that the resulting bound improvement can be substantial. We show on a complex practical problem how Lagrangean decomposition may help uncover hidden special structures and thus yield better solution methodology."
            },
            "slug": "Lagrangean-decomposition:-A-model-yielding-stronger-Guignard-Kim",
            "title": {
                "fragments": [],
                "text": "Lagrangean decomposition: A model yielding stronger lagrangean bounds"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown on a complex practical problem how Lagrangean decomposition may help uncover hidden special structures and thus yield better solution methodology."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144102674"
                        ],
                        "name": "C. Papadimitriou",
                        "slug": "C.-Papadimitriou",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Papadimitriou",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papadimitriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2996331"
                        ],
                        "name": "K. Steiglitz",
                        "slug": "K.-Steiglitz",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Steiglitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Steiglitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Most implementations rely on the fact that instead of the min cut one can solve an equivalent max flow problem, which follows from linear programming duality [116]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29222216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1565c128b727550a73bc4e105a3353420112485d",
            "isKey": false,
            "numCitedBy": 7242,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "This clearly written , mathematically rigorous text includes a novel algorithmic exposition of the simplex method and also discusses the Soviet ellipsoid algorithm for linear programming; efficient algorithms for network flow, matching, spanning trees, and matroids; the theory of NP-complete problems; approximation algorithms, local search heuristics for NPcomplete problems, more. All chapters are supplemented by thoughtprovoking problems. A useful work for graduate-level students with backgrounds in computer science, operations research, and electrical engineering. Mathematicians wishing a self-contained introduction need look no further.\u2014American Mathematical Monthly. 1982 ed."
            },
            "slug": "Combinatorial-Optimization:-Algorithms-and-Papadimitriou-Steiglitz",
            "title": {
                "fragments": [],
                "text": "Combinatorial Optimization: Algorithms and Complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This clearly written, mathematically rigorous text includes a novel algorithmic exposition of the simplex method and also discusses the Soviet ellipsoid algorithm for linear programming; efficient algorithms for network flow, matching, spanning trees, and matroids; the theory of NP-complete problems; approximation algorithms, local search heuristics for NPcomplete problems, more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238841"
                        ],
                        "name": "Stan Birchfield",
                        "slug": "Stan-Birchfield",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Birchfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stan Birchfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086151"
                        ],
                        "name": "Carlo Tomasi",
                        "slug": "Carlo-Tomasi",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Tomasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlo Tomasi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Birchfield and Tomasi [18] proposed to perform a simple per-pixel disparity estimation by locally matching a block of pixels between the two images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17646603,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "3ebbd75ab128c863e3b655efe104e1c47af13096",
            "isKey": false,
            "numCitedBy": 646,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of image sampling, traditional measures of pixel dissimilarity can assign a large value to two corresponding pixels in a stereo pair, even in the absence of noise and other degrading effects. We propose a measure of dissimilarity that is provably insensitive to sampling because it uses the linearly interpolated intensity functions surrounding the pixels. Experiments on real images show that our measure alleviates the problem of sampling with little additional computational overhead."
            },
            "slug": "A-Pixel-Dissimilarity-Measure-That-Is-Insensitive-Birchfield-Tomasi",
            "title": {
                "fragments": [],
                "text": "A Pixel Dissimilarity Measure That Is Insensitive to Image Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Experiments show that the proposed measure of dissimilarity uses the linearly interpolated intensity functions surrounding the pixels alleviates the problem of sampling with little additional computational overhead."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699418"
                        ],
                        "name": "A. Frangioni",
                        "slug": "A.-Frangioni",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Frangioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Frangioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For the special case of integer linear programs we have considered earlier, the above result can be strengthened [43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17804014,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "94ecc1387420a9202e02ac58670600a1315e0b18",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well-known that the Lagrangian dual of an Integer Linear Program (ILP) provides the same bound as a continuous relaxation involving the convex hull of all the optimal solutions of the Lagrangian relaxation. It is less often realized that this equivalence is effective, in that basically all known algorithms for solving the Lagrangian dual either naturally compute an (approximate) optimal solution of the \u201cconvexified relaxation\u201d, or can be modified to do so. After recalling these results we elaborate on the importance of the availability of primal information produced by the Lagrangian dual within both exact and approximate approaches to the original (ILP), using three optimization problems with different structure to illustrate some of the main points."
            },
            "slug": "About-Lagrangian-Methods-in-Integer-Optimization-Frangioni",
            "title": {
                "fragments": [],
                "text": "About Lagrangian Methods in Integer Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The importance of the availability of primal information produced by the Lagrangian dual within both exact and approximate approaches to the original (ILP) is elaborate, using three optimization problems with different structure to illustrate some of the main points."
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Oper. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48306096"
                        ],
                        "name": "M. Hestenes",
                        "slug": "M.-Hestenes",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Hestenes",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hestenes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50079822"
                        ],
                        "name": "E. Stiefel",
                        "slug": "E.-Stiefel",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Stiefel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Stiefel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Conjugate gradient [60] optimization has also successfully been applied to conditional log-likelihood minimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2207234,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "466daddfb6340c28cb8da548007028c8cc5df687",
            "isKey": false,
            "numCitedBy": 7182,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "An iterative algorithm is given for solving a system Ax=k of n linear equations in n unknowns. The solution is given in n steps. It is shown that this method is a special case of a very general method which also includes Gaussian elimination. These general algorithms are essentially algorithms for finding an n dimensional ellipsoid. Connections are made with the theory of orthogonal polynomials and continued fractions."
            },
            "slug": "Methods-of-conjugate-gradients-for-solving-linear-Hestenes-Stiefel",
            "title": {
                "fragments": [],
                "text": "Methods of conjugate gradients for solving linear systems"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "An iterative algorithm is given for solving a system Ax=k of n linear equations in n unknowns and it is shown that this method is a special case of a very general method which also includes Gaussian elimination."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741426"
                        ],
                        "name": "P. Gallinari",
                        "slug": "P.-Gallinari",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Gallinari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gallinari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", to avoid the need for an a priori choice of the parameters \u03b7 by automatic gain adaption [159], and the integration of second-order information [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 347551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b44ff78214ccd975ce16fbbc333423ca78d99141",
            "isKey": false,
            "numCitedBy": 358,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The SGD-QN algorithm is a stochastic gradient descent algorithm that makes careful use of second-order information and splits the parameter update into independently scheduled components. Thanks to this design, SGD-QN iterates nearly as fast as a first-order stochastic gradient descent but requires less iterations to achieve the same accuracy. This algorithm won the \"Wild Track\" of the first PASCAL Large Scale Learning Challenge (Sonnenburg et al., 2008)."
            },
            "slug": "SGD-QN:-Careful-Quasi-Newton-Stochastic-Gradient-Bordes-Bottou",
            "title": {
                "fragments": [],
                "text": "SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The SGD-QN algorithm is a stochastic gradient descent algorithm that makes careful use of second-order information and splits the parameter update into independently scheduled components."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103572659"
                        ],
                        "name": "R. Kikuchi",
                        "slug": "R.-Kikuchi",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Kikuchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kikuchi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "consistency over larger groups of variables can be dated back to Kikuchi [69]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119505101,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "aa59898d5d643aec5cc523523f07b42452cd514e",
            "isKey": false,
            "numCitedBy": 1426,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method of approximation for order-disorder phenomena is developed. In Sec. A, the method is explained for the one-dimensional Ising lattice. Sections B and C cover the approximations already known, such as those of Bethe (Sec. B) and of Kramers-Wannier (Sec. C), which are shown to be derived as special cases of the method with suitable choices of variables. In Sec. D, an improved treatment is explained for the three-dimensional simple cubic Ising lattice. This approximation is found to agree with the rigorous expansion of the partition function up to the fourth moment by Kirkwood's moment method, so far as the disordered state is concerned. In Sec. E the general formula for the entropy is given. In Sec. H an improved treatment of the face-centered lattice (Ising model) is given."
            },
            "slug": "A-Theory-of-Cooperative-Phenomena-Kikuchi",
            "title": {
                "fragments": [],
                "text": "A Theory of Cooperative Phenomena"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2505902"
                        ],
                        "name": "N. Komodakis",
                        "slug": "N.-Komodakis",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Komodakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Komodakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680727"
                        ],
                        "name": "N. Paragios",
                        "slug": "N.-Paragios",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Paragios",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Paragios"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[140, 142], Werner [163], and Komodakis and Paragios [81] are all based on the dual optimization problem (4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13393051,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eeaf5b76f8d779f60fc778e50f5fa8c21ac36ba",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new MRF optimization algorithm, which is derived from Linear Programming and manages to go beyond current state-of-the-art techniques (such as those based on graph-cuts or belief propagation). It does so by relying on a much tighter class of LP-relaxations, called cycle-relaxations. With the help of this class of relaxations, our algorithm tries to deal with a difficulty lying at the heart of MRF optimization: the existence of inconsistent cycles. To this end, it uses an operation called cycle-repairing. The goal of that operation is to fix any inconsistent cycles that may appear during optimization, instead of simply ignoring them as usually done up to now. The more the repaired cycles, the tighter the underlying LP relaxation becomes. As a result of this procedure, our algorithm is capable of providing almost optimal solutions even for very general MRFs with arbitrary potentials. Experimental results verify its effectiveness on difficult MRF problems, as well as its better performance compared to the state of the art."
            },
            "slug": "Beyond-Loose-LP-Relaxations:-Optimizing-MRFs-by-Komodakis-Paragios",
            "title": {
                "fragments": [],
                "text": "Beyond Loose LP-Relaxations: Optimizing MRFs by Repairing Cycles"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new MRF optimization algorithm is presented, which is derived from Linear Programming and manages to go beyond current state-of-the-art techniques (such as those based on graph-cuts or belief propagation), by relying on a much tighter class of LP- Relaxations, called cycle-relaxations."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2604191"
                        ],
                        "name": "J. F\u00f6glein",
                        "slug": "J.-F\u00f6glein",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "F\u00f6glein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. F\u00f6glein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the block ICM method the neighborhood is enlarged by allowing a larger subset of variables to change [68, 72]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38290659,
            "fieldsOfStudy": [
                "Environmental Science",
                "Computer Science"
            ],
            "id": "3edb929a6bf2b26f5faaeb7c755aa28b9ca7ac73",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Contextual-classification-of-multispectral-pixel-Kittler-F\u00f6glein",
            "title": {
                "fragments": [],
                "text": "Contextual classification of multispectral pixel data"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713217"
                        ],
                        "name": "D. Bertsimas",
                        "slug": "D.-Bertsimas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Bertsimas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsimas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Linear programming problems can be solved in polynomial time [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60706246,
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "id": "c8557a70ecdeec83f70954c5f169393c7f04fc9e",
            "isKey": false,
            "numCitedBy": 2692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "p. 27, l. \u221211, replace \u201cSchwartz\u201d by \u201cSchwarz\u201d p. 69, l. \u221213: \u201cai\u2217x = bi\u201d should be \u201cai\u2217x = bi\u2217\u201d p. 126, l. 16, replace \u201cinequality constraints\u201d by \u201clinear inequality constraints\u201d p. 153, l. \u22128, replace aix 6= bi by aix 6= bi p. 163, Example 4.9, first line: replace \u201cfrom\u201d with \u201cform\u201d p. 165, l. 11, replace p\u2032Ax \u2265 0 by p\u2032Ax \u2265 0 p. 175, l. 1, replace \u201cTo this see\u201d by \u201cTo see this\u201d p. 203, l. 12: replace x \u2265 0 by x \u2265 0, xn+1 \u2265 0 p. 216, l. \u22126: replace \u201c\u2264 c}\u201d by \u201c\u2264 c\u2032}\u201d p. 216, l. \u22123: replace c\u2032 by (c1)\u2032 p. 216, l. \u22122: replace c\u2032 by (c2)\u2032 p. 216, l. \u22121: right-hand side should be \u03bb(c1)\u2032 + (1\u2212 \u03bb)(c2)\u2032 p. 220, l. \u221212: replace \u201cadded to the pivot row\u201d by \u201cadded to the zeroth row\u201d"
            },
            "slug": "Introduction-to-linear-optimization-Bertsimas-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "Introduction to linear optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796408"
                        ],
                        "name": "Olle H\u00e4ggstr\u00f6m",
                        "slug": "Olle-H\u00e4ggstr\u00f6m",
                        "structuredName": {
                            "firstName": "Olle",
                            "lastName": "H\u00e4ggstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olle H\u00e4ggstr\u00f6m"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Further details on Markov chains can be found in [55]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Markov chains and MCMC methods for discrete models such as the discrete factor graphs are discussed in the monograph [55]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117850846,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "eabc032e5d664decad2f3c5153ee965c5c6eadcd",
            "isKey": false,
            "numCitedBy": 472,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Basics of probability theory 2. Markov chains 3. Computer simulation of Markov chains 4. Irreducible and aperiodic Markov chains 5. Stationary distributions 6. Reversible Markov chains 7. Markov chain Monte Carlo 8. Fast convergence of MCMC algorithms 9. Approximate counting 10. Propp-Wilson algorithm 11. Sandwiching 12. Propp-Wilson with read once randomness 13. Simulated annealing 14. Further reading."
            },
            "slug": "Finite-Markov-Chains-and-Algorithmic-Applications-H\u00e4ggstr\u00f6m",
            "title": {
                "fragments": [],
                "text": "Finite Markov Chains and Algorithmic Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28791020"
                        ],
                        "name": "B. Raphael",
                        "slug": "B.-Raphael",
                        "structuredName": {
                            "firstName": "Bertram",
                            "lastName": "Raphael",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Raphael"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206799161,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "221aa3be55a4ead8fc2aa83b12aac370bfba72f5",
            "isKey": false,
            "numCitedBy": 9145,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies."
            },
            "slug": "A-Formal-Basis-for-the-Heuristic-Determination-of-Hart-Nilsson",
            "title": {
                "fragments": [],
                "text": "A Formal Basis for the Heuristic Determination of Minimum Cost Paths"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "How heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching is described and an optimality property of a class of search strategies is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Sci. Cybern."
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073146021"
                        ],
                        "name": "Peter Rossmanith",
                        "slug": "Peter-Rossmanith",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rossmanith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Rossmanith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 20
                            }
                        ],
                        "text": "Simulated annealing [Kirkpatrick et al., 1983; Aarts et al., 1997] is a well-known method to approximately solve problems of the form (4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 51
                            }
                        ],
                        "text": "A good general reference on simulated annealing is [Aarts et al., 1997]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 832999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25c57899968dc79a3245234746e28967693f9afa",
            "isKey": false,
            "numCitedBy": 1658,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The planning of FWA-networks is a time consuming process. The aim with this paper is to find a model that can locate base stations in designspace and connect end-user to the base stations and solve the model within one hour. This paper describes the exact mathematical model for the base station location problem in FWA-networks and why it is not possible to solve the problem using this exact method within the given timeframe. Instead the base station location problem is solved using the Metaheuristic Simulated Annealing while minimizing the number of not connected end-users. c948718 Niels M. J\u00f8rgensen Simulated annealing in FWA-network planning Page 1 of 95 Preface During the time I was writing this master thesis I was pleased with the assistance form not only my supervisors. I was also supported by the people at both O/ZA group at L.M. Ericsson Denmark A/S and Operational Research (OR) -group at Informatics, Mathematics and Modeling (IMM) at the Technical University of Denmark (DTU). I am very grateful for this support and I would like to thank all the people who have contributed to my paper and have been very helpful when things have appeared impossible. At IMM I would like to thank my co-students Michael L\u00f8ve, Kim Riis S\u00f8rensen, Mette Krogh-Jespersen and the Ph.D.-students Thomas K. Stidsen and Jesper A. Hansen for their assistance in all aspects of the project. At L.M. Ericsson I would like to thank Ulrik Heins and Nils Rinaldi for explaining the mysteries of radio transmission and Mazeyar Firouzi for helping in mastering C-programming. Especially I would like to thank my two supervisors. For the academic and theoretical part, Professor Jens Clausen, IMM at DTU and for the commercial and practical part, Senior Specialist Ph.D. Christian Kloch, O/ZAF at L.M. Ericsson Denmark A/S. _______________________________ Niels M. J\u00f8rgensen, c948718 IMM, DTU Lyngby, 31. July 2001 c948718 Niels M. J\u00f8rgensen Simulated annealing in FWA-network planning"
            },
            "slug": "Simulated-Annealing-Rossmanith",
            "title": {
                "fragments": [],
                "text": "Simulated Annealing"
            },
            "venue": {
                "fragments": [],
                "text": "Algorithms Unplugged"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32922277"
                        ],
                        "name": "N. Metropolis",
                        "slug": "N.-Metropolis",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Metropolis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Metropolis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91743329"
                        ],
                        "name": "A. W. Rosenbluth",
                        "slug": "A.-W.-Rosenbluth",
                        "structuredName": {
                            "firstName": "Arianna",
                            "lastName": "Rosenbluth",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. W. Rosenbluth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2991661"
                        ],
                        "name": "M. Rosenbluth",
                        "slug": "M.-Rosenbluth",
                        "structuredName": {
                            "firstName": "Marshall",
                            "lastName": "Rosenbluth",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rosenbluth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46516796"
                        ],
                        "name": "A. H. Teller",
                        "slug": "A.-H.-Teller",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Teller",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. H. Teller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3840350"
                        ],
                        "name": "E. Teller",
                        "slug": "E.-Teller",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Teller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Teller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1046577,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f6a13f116e270dde9d67848495f801cdb8efa25d",
            "isKey": false,
            "numCitedBy": 32413,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two\u2010dimensional rigid\u2010sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four\u2010term virial coefficient expansion."
            },
            "slug": "Equation-of-state-calculations-by-fast-computing-Metropolis-Rosenbluth",
            "title": {
                "fragments": [],
                "text": "Equation of state calculations by fast computing machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For a more detailed review of the belief propagation algorithms and proofs of its correctness for tree-structured factor graphs, see [9, 85, 98, 171]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The sum\u2013product algorithm [85, 98] is a dynamic programming algorithm that given a discrete distribution in the form (2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5436619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7f15848cd0fbb3d08f351595da833b1627de9c3",
            "isKey": false,
            "numCitedBy": 8764,
            "numCiting": 249,
            "paperAbstract": {
                "fragments": [],
                "text": "Fun and exciting textbook on the mathematics underpinning the most dynamic areas of modern science and engineering."
            },
            "slug": "Information-Theory,-Inference,-and-Learning-Mackay",
            "title": {
                "fragments": [],
                "text": "Information Theory, Inference, and Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A fun and exciting textbook on the mathematics underpinning the most dynamic areas of modern science and engineering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Algorithm 11 contains pseudo-code for the frequently used Broyden\u2013Fletcher\u2013Goldfarb\u2013 Shanno (BFGS) [41] procedure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123487779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b84b383ad59f79e607ad0f08a8a10876631a0cd",
            "isKey": false,
            "numCitedBy": 9912,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Table of Notation Part 1: Unconstrained Optimization Introduction Structure of Methods Newton-like Methods Conjugate Direction Methods Restricted Step Methods Sums of Squares and Nonlinear Equations Part 2: Constrained Optimization Introduction Linear Programming The Theory of Constrained Optimization Quadratic Programming General Linearly Constrained Optimization Nonlinear Programming Other Optimization Problems Non-Smooth Optimization References Subject Index."
            },
            "slug": "Practical-Methods-of-Optimization-Fletcher",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The aim of this book is to provide a Discussion of Constrained Optimization and its Applications to Linear Programming and Other Optimization Problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120492501"
                        ],
                        "name": "J. Picard",
                        "slug": "J.-Picard",
                        "structuredName": {
                            "firstName": "Jean-Claude",
                            "lastName": "Picard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Picard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744003"
                        ],
                        "name": "M. Queyranne",
                        "slug": "M.-Queyranne",
                        "structuredName": {
                            "firstName": "Maurice",
                            "lastName": "Queyranne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Queyranne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206798248,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "27756a9b38a42c1189afd3c5ca6636006585091b",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a characterization of all minimum cuts, separating a source from a sink in a network. A binary relation is associated with any maximum flow in this network, and minimum cuts are identified with closures for this relation. As a consequence, finding all minimum cuts reduces to a straightforward enumeration. Applications of this results arise in sensitivity and parametric analyses of networks, the vertex packing and maximum closure problems, in unconstrained pseudo-boolean optimization and project selection, as well as in other areas of application of minimum cuts."
            },
            "slug": "On-the-structure-of-all-minimum-cuts-in-a-network-Picard-Queyranne",
            "title": {
                "fragments": [],
                "text": "On the structure of all minimum cuts in a network and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A characterization of all minimum cuts, separating a source from a sink in a network, and applications arise in sensitivity and parametric analyses of networks, the vertex packing and maximum closure problems, in unconstrained pseudo-boolean optimization and project selection, as well as in other areas of application of minimum cuts."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8773884"
                        ],
                        "name": "C. Lemar\u00e9chal",
                        "slug": "C.-Lemar\u00e9chal",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Lemar\u00e9chal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lemar\u00e9chal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Instead of dropping a constraint completely in order to construct a relaxation \u2014 as done when relaxing an integer linear program to a linear program, an alternative method is Lagrangian relaxation [53, 92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9048698,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67b657e7e6b0e8f363cb176847f7970e9136908e",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Lagrangian relaxation is a tool to find upper bounds on a given (arbitrary) maximization problem. Sometimes, the bound is exact and an optimal solution is found. Our aim in this paper is to review this technique, the theory behind it, its numerical aspects, its relation with other techniques such as column generation."
            },
            "slug": "Lagrangian-Relaxation-Lemar\u00e9chal",
            "title": {
                "fragments": [],
                "text": "Lagrangian Relaxation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The aim in this paper is to review this technique, the theory behind it, its numerical aspects, its relation with other techniques such as column generation."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Combinatorial Optimization"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 69
                            }
                        ],
                        "text": "The SVM outputs are normalized into the range [0,1] by Platt-scaling [119] and become one-dimensional features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64295966,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b7caf811d6980627caad1a8b3053f40348693508",
            "isKey": false,
            "numCitedBy": 436,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Fitting a Sigmoid After the SVM, Empirical Tests, Conclusions, Appendix: Pseudo-code for the Sigmoid Training"
            },
            "slug": "Probabilities-for-SV-Machines-Smola-Bartlett",
            "title": {
                "fragments": [],
                "text": "Probabilities for SV Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction, Fitting a Sigmoid After the SVM, Empirical Tests, Conclusions, Appendix: Pseudo-code for the Sigmoids Training."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2567671"
                        ],
                        "name": "A. M. Geoffrion",
                        "slug": "A.-M.-Geoffrion",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Geoffrion",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. M. Geoffrion"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "The definition is due to Geoffrion [50]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 11
                            }
                        ],
                        "text": "[50] A. M. Geoffrion, \u201cLagrangian relaxation for integer programming,\u201d Mathematical Programming Study, vol. 2, pp. 82\u2013114, 1974."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18343550,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "f4744062a1594844662fc7d0aa2139ddeab2a213",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "It is a pleasure to write this commentary because it offers an opportunity to express my gratitude to several people who helped me in ways that turned out to be essential to the birth of [8]. They also had a good deal to do with shaping my early career and, consequently, much of what followed."
            },
            "slug": "Lagrangian-Relaxation-for-Integer-Programming-Geoffrion",
            "title": {
                "fragments": [],
                "text": "Lagrangian Relaxation for Integer Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "It is a pleasure to write this commentary because it offers an opportunity to express my gratitude to several people who helped me in ways that turned out to be essential to the birth of [8]."
            },
            "venue": {
                "fragments": [],
                "text": "50 Years of Integer Programming"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145155783"
                        ],
                        "name": "C. Robert",
                        "slug": "C.-Robert",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Robert",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Robert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144898907"
                        ],
                        "name": "G. Casella",
                        "slug": "G.-Casella",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Casella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Casella"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61502973,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "7e7240aff31e77cd0d7cc5ad7dbe933aaf473028",
            "isKey": false,
            "numCitedBy": 5001,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We have sold 4300 copies worldwide of the first edition (1999). This new edition contains five completely new chapters covering new developments."
            },
            "slug": "Monte-Carlo-Statistical-Methods-Robert-Casella",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Statistical Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This new edition contains five completely new chapters covering new developments and has sold 4300 copies worldwide of the first edition (1999)."
            },
            "venue": {
                "fragments": [],
                "text": "Springer Texts in Statistics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238873"
                        ],
                        "name": "Tomas Fencl",
                        "slug": "Tomas-Fencl",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Fencl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Fencl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1939596"
                        ],
                        "name": "J. Bilek",
                        "slug": "J.-Bilek",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Bilek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bilek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15984100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf6eec9ec3b431bf74aaea0ec2b6740dad718c77",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This document deals with a design of a physical and a logical topology of communication networks that are applied in the control engineering. The design of the physical topology works with aspects of demands for redundant links between nodes. Thanks to knowledge of the physical topology, we can design the logical topology according to permitted delays of delivered communication frames. The whole procedure of a network design proceeds like an iterative task on the basis of an evolution algorithm."
            },
            "slug": "Network-Optimization-Fencl-Bilek",
            "title": {
                "fragments": [],
                "text": "Network Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This document deals with a design of a physical and a logical topology of communication networks that are applied in the control engineering and works with aspects of demands for redundant links between nodes."
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157892296"
                        ],
                        "name": "D. K. Smith",
                        "slug": "D.-K.-Smith",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Smith",
                            "middleNames": [
                                "K.",
                                "Skip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. K. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 189864167,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "bf86896c23300a46b7fc76298e365984c0b05105",
            "isKey": false,
            "numCitedBy": 10989,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "no exception. MRP II and JIT=TQC in purchasing and supplier education are covered in Chapter 15. Without proper education MRP II and JIT=TQC will not be successful and will not generate their true bene\u00aets. Suppliers are key to the success of MRP II and JIT=TQC. They therefore need to understand these disciplines. Purchasing in the 21st century is going to be marked by continuous changes, by who can gain the competitive edge \u00aerst, who will be the most \u0304exible and who will build the best supplier relationships. This will only be achieved by following the process as described in Schorr in a step by step fashion. An organization must however be willing to, as Schorr states in Chapter 16, `create the spark, ignite change'! Only then can it happen! If you really want to know something about purchasing then this is the book to read. It is most de\u00aenitely relevant and more importantly up to date. It will certainly be a handy reference book for a course on purchasing."
            },
            "slug": "Numerical-Optimization-Smith",
            "title": {
                "fragments": [],
                "text": "Numerical Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "J. Oper. Res. Soc."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705564"
                        ],
                        "name": "G. Nemhauser",
                        "slug": "G.-Nemhauser",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nemhauser",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nemhauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736128"
                        ],
                        "name": "L. Wolsey",
                        "slug": "L.-Wolsey",
                        "structuredName": {
                            "firstName": "Laurence",
                            "lastName": "Wolsey",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wolsey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17594683,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "c2f462695a0e61d113634e6fb4ec72dad643c543",
            "isKey": false,
            "numCitedBy": 4161,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The principles of integer programming are directed toward finding solutions to problems from the fields of economic planning, engineering design, and combinatorial optimization. This highly respected and much-cited text, a standard of graduate-level courses since 1972, presents a comprehensive treatment of the first two decades of research on integer programming."
            },
            "slug": "Integer-Programming-Nemhauser-Wolsey",
            "title": {
                "fragments": [],
                "text": "Integer Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This holds in general and [28] showed that the optimal labeling can be constructed from the \u03b1\u2013\u03b2-mincut C as"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The most popular local search method in computer vision is the \u03b1-expansion graphcut method for multi-label discrete MAP inference problems [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The analysis and proof can be found in [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[28] proposed two neighborhoods, the \u201c\u03b1-expansion\u201d neighborhood N\u03b1: Y \u00d7 N\u2192 2Y and the \u201c\u03b1\u2013\u03b2-swap\u201d neighborhood N\u03b1,\u03b2 : Y \u00d7 N \u00d7 N\u2192 2Y ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 45232003,
            "fieldsOfStudy": [],
            "id": "a57520b68e73b5e1fc3668b443daf74ebe957cc7",
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast Approximate Energy Minimization via Graph Cuts"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 330,
                                "start": 321
                            }
                        ],
                        "text": "Berkeley) Tomas Pajdla (Czech Tech U) Pietro Perona (Caltech) Marc Pollefeys (U. North Carolina) Jean Ponce (UIUC) Long Quan (HKUST) Cordelia Schmid (INRIA) Steve Seitz (U. Washington) Amnon Shashua (Hebrew Univ) Peter Shirley (U. of Utah) Stefano Soatto (UCLA) Joachim Weickert (U. Saarland) Song Chun Zhu (UCLA) Andrew Zisserman (Oxford Univ)\nEditorial Scope"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 118
                            }
                        ],
                        "text": "8 Illustration of Object Category Localization with latent occlusion variables (simplified from Vedaldi and Zisserman [156])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 24
                            }
                        ],
                        "text": "[156] A. Vedaldi and A. Zisserman, \u201cStructured output regression for detection with\npartial occulsion,\u201d in Conference on Neural Information Processing Systems (NIPS), 2009."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "Vedaldi and Zisserman [156] extend S-SVM based object category localization (Example 6."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structured output regression for detection with partial occulsion"
            },
            "venue": {
                "fragments": [],
                "text": "Conference on Neural Information Processing Systems (NIPS), 2009."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759608"
                        ],
                        "name": "P. Pardalos",
                        "slug": "P.-Pardalos",
                        "structuredName": {
                            "firstName": "Panos",
                            "lastName": "Pardalos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pardalos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 215996958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f455031fe62b63fcc1b4dd0c9312a2e504d52229",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Network-Optimization-Pardalos",
            "title": {
                "fragments": [],
                "text": "Network Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815154"
                        ],
                        "name": "G. Bakir",
                        "slug": "G.-Bakir",
                        "structuredName": {
                            "firstName": "G\u00f6khan",
                            "lastName": "Bakir",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Bakir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153379696"
                        ],
                        "name": "T. Hofmann",
                        "slug": "T.-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713876"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 62
                            }
                        ],
                        "text": "Optimizing a convex upper bound nevertheless makes sense, see [McAllester, 2007] for an in-depth discussion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125921315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28254f227335479cdf2ffdc71fe7179b47048064",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-Bounds-and-Consistency-for-Labeling-Bakir-Hofmann",
            "title": {
                "fragments": [],
                "text": "Generalization Bounds and Consistency for Structured Labeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746662"
                        ],
                        "name": "D. Sontag",
                        "slug": "D.-Sontag",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sontag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sontag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786843"
                        ],
                        "name": "A. Globerson",
                        "slug": "A.-Globerson",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Globerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Globerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[141] provides a detailed introduction to dual decomposition and dual ascent methods for inference."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 126381971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad9322f24db6d987b6f6111db12155cce8b919a8",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-dual-composition-for-inference-Sontag-Globerson",
            "title": {
                "fragments": [],
                "text": "Introduction to dual composition for inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32916871"
                        ],
                        "name": "J. E. Kelley",
                        "slug": "J.-E.-Kelley",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Kelley",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. E. Kelley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123053096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "689436a165ecc98c60ddd2bbcfedfd71fd4c947a",
            "isKey": false,
            "numCitedBy": 1501,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Cutting-Plane-Method-for-Solving-Convex-Kelley",
            "title": {
                "fragments": [],
                "text": "The Cutting-Plane Method for Solving Convex Programs"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "151155530"
                        ],
                        "name": "N. Shor",
                        "slug": "N.-Shor",
                        "structuredName": {
                            "firstName": "Naum",
                            "lastName": "Shor",
                            "middleNames": [
                                "Zuselevich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Shor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, subgradient methods [139] that we introduced in Section 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One simple and popular method is a generalization of gradient descent known as subgradient method, shown in Algorithm 8 and originally proposed by Shor [139]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121446366,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f02a1b3de0bead764e6d214e8e1a3078dda47fa7",
            "isKey": false,
            "numCitedBy": 1017,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Minimization-Methods-for-Non-Differentiable-Shor",
            "title": {
                "fragments": [],
                "text": "Minimization Methods for Non-Differentiable Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Springer Series in Computational Mathematics"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102882986"
                        ],
                        "name": "\u7766\u61b2 \u67f3\u6d66",
                        "slug": "\u7766\u61b2-\u67f3\u6d66",
                        "structuredName": {
                            "firstName": "\u7766\u61b2",
                            "lastName": "\u67f3\u6d66",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u7766\u61b2 \u67f3\u6d66"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118115221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6a6caac9bba81bf195c0ab02b568d3f9e921a43",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combinatorial-Optimization-:-Theory-and-Algorithms-\u7766\u61b2",
            "title": {
                "fragments": [],
                "text": "Combinatorial Optimization : Theory and Algorithms (3rd Edition), B. Korte and J. Vygen \u8457, \u51fa\u7248\u793e Springer, \u767a\u884c 2006\u5e74, \u5168\u30da\u30fc\u30b8 597\u9801, \u4fa1\u683c 53.45\u30e6\u30fc\u30ed, ISBN 3-540-25684-9"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64649729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11106aadd1c133477b163f08de6c9436cd5468fe",
            "isKey": false,
            "numCitedBy": 9680,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonlinear-Programming-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Nonlinear Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145089978"
                        ],
                        "name": "D. Damen",
                        "slug": "D.-Damen",
                        "structuredName": {
                            "firstName": "Dima",
                            "lastName": "Damen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Damen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967104"
                        ],
                        "name": "David C. Hogg",
                        "slug": "David-C.-Hogg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hogg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David C. Hogg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64711781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd116435b6f93e803e8db708ad4d0bce71499982",
            "isKey": false,
            "numCitedBy": 1554,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Vision-and-Pattern-Recognition-(CVPR)-Damen-Hogg",
            "title": {
                "fragments": [],
                "text": "Computer Vision and Pattern Recognition (CVPR)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742208"
                        ],
                        "name": "M. Pollefeys",
                        "slug": "M.-Pollefeys",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Pollefeys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pollefeys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63374487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19d6e925ed9643c28981edd233d074b6e3a793c6",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "European-conference-on-computer-vision-(ECCV)-Fitzgibbon-Pollefeys",
            "title": {
                "fragments": [],
                "text": "European conference on computer vision (ECCV)"
            },
            "venue": {
                "fragments": [],
                "text": "eccv 2006"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222262424,
            "fieldsOfStudy": [],
            "id": "870e6ff38b78b3f9e664771bb350534f6b1b8245",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applied Linear Statistical Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145878635"
                        ],
                        "name": "Tom\u00e1\u0161 Werner",
                        "slug": "Tom\u00e1\u0161-Werner",
                        "structuredName": {
                            "firstName": "Tom\u00e1\u0161",
                            "lastName": "Werner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom\u00e1\u0161 Werner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62812754,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "11bdc041f063143d539474500a9c86b79f2dc400",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Revisiting-the-Decomposition-Approach-to-Inference-Werner",
            "title": {
                "fragments": [],
                "text": "Revisiting the Decomposition Approach to Inference in Exponential Families and Graphical Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104192849"
                        ],
                        "name": "R. Zabin",
                        "slug": "R.-Zabin",
                        "structuredName": {
                            "firstName": "Rawaa",
                            "lastName": "Zabin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Details are given by Kolmogorov and Zabin [80]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Similar transformations can be made for the pairwise energies, and a complete characterization of graph cut solvable binary energy functions has been given by Kolmogorov and Zabin [80] and Freedman and Drineas [44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, for energy functions with only unary and pairwise terms, Kolmogorov and Zabin [80] show the following theorem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53303132,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6a1876f699838d79c184b7a2349f927c6f5ec99e",
            "isKey": false,
            "numCitedBy": 2088,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last few years, several new algorithms based on graph cuts have been developed to solve energy minimization problems in computer vision. Each of these techniques constructs a graph such that the minimum cut on the graph also minimizes the energy. Yet, because these graph constructions are complex and highly specific to a particular energy function, graph cuts have seen limited application to date. In this paper, we give a characterization of the energy functions that can be minimized by graph cuts. Our results are restricted to functions of binary variables. However, our work generalizes many previous constructions and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, image restoration, and scene reconstruction. We give a precise characterization of what energy functions can be minimized using graph cuts, among the energy functions that can be written as a sum of terms containing three or fewer binary variables. We also provide a general-purpose construction to minimize such an energy function. Finally, we give a necessary condition for any energy function of binary variables to be minimized by graph cuts. Researchers who are considering the use of graph cuts to optimize a particular energy function can use our results to determine if this is possible and then follow our construction to create the appropriate graph. A software implementation is freely available."
            },
            "slug": "What-energy-functions-can-be-minimized-via-graph-Kolmogorov-Zabin",
            "title": {
                "fragments": [],
                "text": "What energy functions can be minimized via graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work gives a precise characterization of what energy functions can be minimized using graph cuts, among the energy functions that can be written as a sum of terms containing three or fewer binary variables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "There are at least two other common names for local search methods, move-making methods [73], and very large-scale neighborhood search [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27508986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6f2916c5d133c7cff88e4762b62f25a3ca207fc",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "MAP-inference-in-Discrete-Models-Kohli",
            "title": {
                "fragments": [],
                "text": "MAP inference in Discrete Models"
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803380"
                        ],
                        "name": "K. Dowsland",
                        "slug": "K.-Dowsland",
                        "structuredName": {
                            "firstName": "Kathryn",
                            "lastName": "Dowsland",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Dowsland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116793374"
                        ],
                        "name": "Jonathan M. Thompson",
                        "slug": "Jonathan-M.-Thompson",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Thompson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan M. Thompson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30845250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d05cecdc505e2bbe2e64842bba51c7a631fcf7ab",
            "isKey": false,
            "numCitedBy": 1352,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Simulated-Annealing-Dowsland-Thompson",
            "title": {
                "fragments": [],
                "text": "Simulated Annealing"
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of Natural Computing"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Using the kernel trick [135] we can replace inner products between feature functions by evaluations of the kernel function k((x,y),(x\u2032,y\u2032)) = \u3008\u03c6(x,y),\u03c6(x\u2032,y\u2032)\u3009."
                    },
                    "intents": []
                }
            ],
            "corpusId": 221303277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96cff156cdc8a479aa994eae875a0860d663d317",
            "isKey": false,
            "numCitedBy": 2181,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-with-kernels-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 192
                            }
                        ],
                        "text": "It is now known that if the algorithm converges then it converged to a fix-point of the Bethe free energy and this connection has been fruitfully used to derive a family of similar algorithms [Heskes, 2006; Wainwright and Jordan, 2008; Werner, 2010], some of which can ensure convergence to a fix point."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Belief Propagation Fixed Points as Zero Gradients of a Function of Reparameterizations"
            },
            "venue": {
                "fragments": [],
                "text": "Center for Machine Perception,"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tractable Substructures in Intractable Networks'. In: Conference on Neural Information Notations and Acronyms 171 Processing Systems (NIPS)"
            },
            "venue": {
                "fragments": [],
                "text": "Tractable Substructures in Intractable Networks'. In: Conference on Neural Information Notations and Acronyms 171 Processing Systems (NIPS)"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A formal basis for the Notations and Acronyms 165"
            },
            "venue": {
                "fragments": [],
                "text": "A formal basis for the Notations and Acronyms 165"
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lagrangian Relaxation Computational Combinatorial Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Lagrangian Relaxation Computational Combinatorial Optimization"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 113
                            }
                        ],
                        "text": "Because d(x, y) is unknown, minimizing the Bayes risk directly is not possible, but structural risk minimization [Vapnik and Chervonenkis, 1974] offers an indirect way to identify a function with good predictive qualities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of pattern recognition (in Russian)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Enforcing topolog- Notations and Acronyms"
            },
            "venue": {
                "fragments": [],
                "text": "Enforcing topolog- Notations and Acronyms"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "\u201d For the original paper, see [54], for an up to date textbook covering other problem decomposition techniques, see [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gar\u0107\u0131a-Bertrand, Decomposition Techniques in Mathematical Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Max-margin Markov networks'. In: Conference on Neural Information Processing Systems (NIPS)"
            },
            "venue": {
                "fragments": [],
                "text": "Max-margin Markov networks'. In: Conference on Neural Information Processing Systems (NIPS)"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "We formalize this in the framework of statistical decision theory [19]: let d(X,Y ) denote the probability distribution of the data for the problem we try to solve, which we factor into the conditional probability distribution of the label d(y|x), and a data prior d(x)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern recognition and machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Markov chain Monte Carlo'. Tutorial at Machine Learning Summer School"
            },
            "venue": {
                "fragments": [],
                "text": "Markov chain Monte Carlo'. Tutorial at Machine Learning Summer School"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simulated Annealing Local Search in Combinatorial Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Simulated Annealing Local Search in Combinatorial Optimization"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 127
                            }
                        ],
                        "text": "This stability is particularly important when the relaxation is used for solving prediction problems during parameter learning [Kulesza and Pereira, 2007; Finley and Joachims, 2008; Martins et al., 2009] and other approximate solution approaches do not come with this guarantee."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 141
                            }
                        ],
                        "text": "In fact, recent results suggest that the use of suboptimal predictions during training can lead to prediction functions with bad performance [Kulesza and Pereira, 2007; Finley and Joachims, 2008; Martins et al., 2009], whereas alternative approaches based on relaxing the prediction problem work well in training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training structural SVMs"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[93] consider binary image segmentation tasks where an energy function depends on the value of a nonlocal parameter y \u2208 Y, where Y is a large finite set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image segmentation by branchand-mincut"
            },
            "venue": {
                "fragments": [],
                "text": "European Conference on Computer Vision (ECCV), 2008.  References 361"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Belief Propagation Fixed Points as Zero Gradients of a Function of Reparameterizations'. Center for Machine Perception, Czech Technical Uni- versity Prague"
            },
            "venue": {
                "fragments": [],
                "text": "Belief Propagation Fixed Points as Zero Gradients of a Function of Reparameterizations'. Center for Machine Perception, Czech Technical Uni- versity Prague"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "The Metropolis\u2013Hastings method is so general that many variations have been proposed; for an in-depth review see [97]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "Monte Carlo techniques and their wide applicability in science as well as guidelines on designing efficient samplers can be found in [97]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Strategies in Scientific Computing, Springer Series in Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "New York: Springer,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum margin planning'. In: International Conference on Machine Learing (ICML)"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum margin planning'. In: International Conference on Machine Learing (ICML)"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 96
                            }
                        ],
                        "text": "The following linear programming relaxation has first been proposed by Schlesinger in the 1970s [131] and has been extensively analyzed by Wainwright and Jordan [160] and Werner [162]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 85
                            }
                        ],
                        "text": "48), also known as the LOCAL relaxation [160], was initially proposed in Schlesinger [131] but has since been extended to models with factors of higher-order."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 12
                            }
                        ],
                        "text": "[131] M. I. Schlesinger, \u201cSyntactic analysis of two-dimensional visual signals in noisy conditions in Russian,\u201d Kibernetika, vol. 4, pp. 113\u2013130, 1976."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic analysis of two-dimensional visual signals in noisy conditions in Russian"
            },
            "venue": {
                "fragments": [],
                "text": "Kibernetika, vol. 4, pp. 113\u2013130, 1976."
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "Conditional random fields can be seen as a form a logistic regression classifiers (see [Neter et al., 1996]), as both share the objective of maximizing the conditional log-likelihood of the observed training pairs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applied linear statistical models. McGraw-Hill, 4 edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to dual decomposition for inference, \" in Optimization for Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Introduction to dual decomposition for inference, \" in Optimization for Machine Learning"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Decomposition Techniques in Mathematical Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Decomposition Techniques in Mathematical Programming"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Dual Decomposition for Inference Optimization for Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Introduction to Dual Decomposition for Inference Optimization for Machine Learning"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structured Learning with Approximate Inference'. In: Conference on Neural Information Processing Systems (NIPS)"
            },
            "venue": {
                "fragments": [],
                "text": "Structured Learning with Approximate Inference'. In: Conference on Neural Information Processing Systems (NIPS)"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 47
                            }
                        ],
                        "text": "Alternative message schedules are discussed in [Elidan et al., 2006]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Residual Belief Propa"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Belief propagation fixed points as zero gradients of a function of reparameterizations Center for Machine Perception"
            },
            "venue": {
                "fragments": [],
                "text": "Belief propagation fixed points as zero gradients of a function of reparameterizations Center for Machine Perception"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponential Families, and Variational Inference"
            },
            "venue": {
                "fragments": [],
                "text": "Graphical Models Foundations and Trends in Machine Learning"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilities for SV Machines Advances in large margin classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "Probabilities for SV Machines Advances in large margin classifiers"
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 69,
            "methodology": 59,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 205,
        "totalPages": 21
    },
    "page_url": "https://www.semanticscholar.org/paper/Structured-Learning-and-Prediction-in-Computer-Nowozin-Lampert/916ad37eb897b3858141874e290e20eae5076f5b?sort=total-citations"
}