{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 140
                            }
                        ],
                        "text": "The first application of a neural network language model to large vocabulary continuous speech recognition was performed by Schwenk (2001), Schwenk and Gauvain (2002), showing word error reductions on the DARPA HUB5 conversational telephone speech recognition task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 272
                            }
                        ],
                        "text": "\u2026reasonable for a back-off language model, for which a language model request corresponds basically to a table look-up using hashing techniques, but this led to long decoding times in our first experiments when the neural language model was used directly during decoding (Schwenk and Gauvain, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14249141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e41498c05d4c68e4750fb84a380317a112d97b01",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes ongoing work on a new approach for language modeling for large vocabulary continuous speech recognition. Almost all state.. o. f-the-art systems use statistical n-gram language models estimated on text corpora. One principle problem with such language models is the fact that many of the n-grams are never observed even in very large training corpora, and therefore it is common to back-off to a lower-order model. In this paper we propose to address this problem by carrying out the estimation task in a continuous space, enabling a smooth interpolation of the probabilities. A neural network is used to learn the projection of the words onto a continuous space and to estimate the n-gram probabilities. The connectionist language model is being evaluated on the DARPA HUB5 conversational telephone speech recognition task and preliminary results show consistent improvements in both perplexity and word error rate."
            },
            "slug": "Connectionist-language-modeling-for-large-speech-Schwenk-Gauvain",
            "title": {
                "fragments": [],
                "text": "Connectionist language modeling for large vocabulary continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The connectionist language model is being evaluated on the DARPA HUB5 conversational telephone speech recognition task and preliminary results show consistent improvements in both perplexity and word error rate."
            },
            "venue": {
                "fragments": [],
                "text": "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51012238"
                        ],
                        "name": "H. Schwenk",
                        "slug": "H.-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schwenk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 149
                            }
                        ],
                        "text": "\u2026and extends our continued research on neural network language models for large vocabulary continuous speech recognition (Schwenk and Gauvain, 2003; Schwenk, 2004; Schwenk and Gauvain, 2004a; Schwenk and Gauvain, 2004b; Schwenk and Gauvain, 2005a; Schwenk and Gauvain, 2005b) and provides extensive\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 100
                            }
                        ],
                        "text": "In the following sections several improvements are presented to drastically reduce this complexity (Schwenk, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16930073,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6fb7546a29320eadad868af66835059db93d99f",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently there has been increasing interest in using neural networks for language modeling. In contrast to the well-known backoff n-gram language models, the neural network approach tries to limit the data sparseness problem by performing the estimation in a continuous space, allowing by this means smooth interpolations. The complexity to train such a model and to calculate one n-gram probability is however several orders of magnitude higher than for the backoff models, making the new approach difficult to use in real applications. In this paper several techniques are presented that allow the use of a neural network language model in a large vocabulary speech recognition system, in particular very, fast lattice rescoring and efficient training of large neural networks on training corpora of over 10 million words. The described approach achieves significant word error reductions with respect to a carefully tuned 4-gram backoff language model in a state of the art conversational speech recognizer for the DARPA rich transcriptions evaluations."
            },
            "slug": "Efficient-training-of-large-neural-networks-for-Schwenk",
            "title": {
                "fragments": [],
                "text": "Efficient training of large neural networks for language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The described approach achieves significant word error reductions with respect to a carefully tuned 4-gram backoff language model in a state of the art conversational speech recognizer for the DARPA rich transcriptions evaluations."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12469208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b395470a57c48d174c4216ea21a7a58bc046917",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-off n-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available.In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-of-the-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time."
            },
            "slug": "Training-Neural-Network-Language-Models-on-Very-Schwenk-Gauvain",
            "title": {
                "fragments": [],
                "text": "Training Neural Network Language Models on Very Large Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "New algorithms to train a neural network language model on very large text corpora are presented, making possible the use of the approach in domains where several hundreds of millions words of texts are available."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15139377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed22171ce376d213bf64d998d594f876f6912cb7",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently there is growing interest in using neural networks for language modeling. In contrast to the well known backoff ngram language models (LM), the neural network approach tries to limit problems from the data sparseness by performing the estimation in a continuous space, allowing by these means smooth interpolations. Therefore this type of LM is interesting for tasks for which only a very limited amount of in-domain training data is available, such as the modeling of conversational speech. In this paper we analyze the generalization behavior of the neural network LM for in-domain training corpora varying from 7M to over 21M words. In all cases, significant word error reductions were observed compared to a carefully tuned 4-gram backoff language model in a state of the art conversational speech recognizer for the NIST rich transcription evaluations. We also apply ensemble learning methods and discuss their connections with LM interpolation."
            },
            "slug": "Neural-network-language-models-for-conversational-Schwenk-Gauvain",
            "title": {
                "fragments": [],
                "text": "Neural network language models for conversational speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The generalization behavior of the neural network LM for in-domain training corpora varying from 7M to over 21M words is analyzed and significant word error reductions were observed compared to a carefully tuned 4-gram backoff language model in a state of the art conversational speech recognizer for the NIST rich transcription evaluations."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14538334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592a744942cabc7596c90dd6a4e13bdd233b677",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Large vocabulary continuous speech recognizers for English Broadcast News achieve today word error rates below 10%. An important factor for this succes is the availability of large amounts of acoustic and language modeling training data. In this paper the recognition of French Broadcast News and English and Spanish parliament speeches is addressed, tasks for which less resources are available. A neural network language model is applied that takes better advantage of the limited amount of training data. This approach performs the estimation of the probabilities in a continuous space, allowing by this means smooth interpolations. Word error reduction of up to 0.9% absolute are reported with respect to a carefully tuned backoff language model trained on the same data."
            },
            "slug": "Building-continuous-space-language-models-for-Schwenk-Gauvain",
            "title": {
                "fragments": [],
                "text": "Building continuous space language models for transcribing european languages"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The recognition of French Broadcast News and English and Spanish parliament speeches is addressed, tasks for which less resources are available, and a neural network language model is applied that takes better advantage of the limited amount of training data."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057272295"
                        ],
                        "name": "Frederic Morin",
                        "slug": "Frederic-Morin",
                        "structuredName": {
                            "firstName": "Frederic",
                            "lastName": "Morin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frederic Morin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 89
                            }
                        ],
                        "text": ", 2003), resampling techniques (Bengio and S\u00e9n\u00e9cal, 2003) and hierarchical decomposition (Morin and Bengio, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 219
                            }
                        ],
                        "text": "Therefore several speed-up techniques have been proposed in the literature, in particular parallel implementations (Bengio et al., 2003), resampling techniques (Bengio and Se\u0301ne\u0301cal, 2003) and hierarchical decomposition (Morin and Bengio, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1326925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c19fbefdeead6a4154a22a9c8551a18b1530033a",
            "isKey": false,
            "numCitedBy": 942,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy."
            },
            "slug": "Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio",
            "title": {
                "fragments": [],
                "text": "Hierarchical Probabilistic Neural Network Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition, constrained by the prior knowledge extracted from the WordNet semantic hierarchy is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 143
                            }
                        ],
                        "text": "This paper summarizes and extends our continued research on neural network language models for large vocabulary continuous speech recognition (Schwenk and Gauvain, 2003; Schwenk, 2004; Schwenk and Gauvain, 2004a; Schwenk and Gauvain, 2004b; Schwenk and Gauvain, 2005a; Schwenk and Gauvain, 2005b)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13515285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c06b7af4128a2c7c1b9977585bc026c6916322e9",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Language modeling for conversational speech suffers from the limited amount of available adequate training data. This paper describes a new approach that performs the estimation of the language model probabilities in a continuous space, allowing by these means smooth interpolation of unobserved n-grams. This continuous space language model is used during the last decoding pass of a state-of-the-art conversational telephone speech recognizer to rescore word lattices. For this type of speech data, it achieves consistent word error reductions of more than 0.4% compared to a carefully tuned backoff n-gram language model."
            },
            "slug": "USING-CONTINUOUS-SPACE-LANGUAGE-MODELS-FOR-SPEECH-Schwenk-Gauvain",
            "title": {
                "fragments": [],
                "text": "USING CONTINUOUS SPACE LANGUAGE MODELS FOR CONVERSATIONAL SPEECH RECOGNITION"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes a new approach that performs the estimation of the language model probabilities in a continuous space, allowing by these means smooth interpolation of unobserved n-grams."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937779"
                        ],
                        "name": "R. Kuhn",
                        "slug": "R.-Kuhn",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Kuhn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kuhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 31924166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be1fed9544830df1137e72b1d2396c40d3e18365",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech-recognition systems must often decide between competing ways of breaking up the acoustic input into strings of words. Since the possible strings may be acoustically similar, a language model is required; given a word string, the model returns its linguistic probability. Several Markov language models are discussed. A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented. The model also contains a 3g-gram component of the traditional type. The combined model and a pure 3g-gram model were tested on samples drawn from the Lancaster-Oslo/Bergen (LOB) corpus of English text. The relative performance of the two models is examined, and suggestions for the future improvements are made. >"
            },
            "slug": "A-Cache-Based-Natural-Language-Model-for-Speech-Kuhn-Mori",
            "title": {
                "fragments": [],
                "text": "A Cache-Based Natural Language Model for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented and contains a 3g-gram component of the traditional type."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204681"
                        ],
                        "name": "L. Lamel",
                        "slug": "L.-Lamel",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Lamel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707447"
                        ],
                        "name": "G. Adda",
                        "slug": "G.-Adda",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Adda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Adda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108579657"
                        ],
                        "name": "Langzhou Chen",
                        "slug": "Langzhou-Chen",
                        "structuredName": {
                            "firstName": "Langzhou",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Langzhou Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50653913"
                        ],
                        "name": "F. Lef\u00e8vre",
                        "slug": "F.-Lef\u00e8vre",
                        "structuredName": {
                            "firstName": "Fabrice",
                            "lastName": "Lef\u00e8vre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lef\u00e8vre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 38
                            }
                        ],
                        "text": "This system is described in detail in Gauvain et al. (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9776092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbc659924151c767f907de9b412d9b7619d26690",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the development of a speech recognition system for the processing of telephone conversations, starting with a state-of-the-art broadcast news transcription system. We identify major changes and improvements in acoustic and language modeling, as well as decoding, which are required to achieve state-of-the-art performance on conversational speech. Some major changes on the acoustic side include the use of speaker normalization (VTLN), the need to cope with channel variability, and the need for efficient speaker adaptation and better pronunciation modeling. On the linguistic side the primary challenge is to cope with the limited amount of language model training data. To address this issue we make use of a data selection technique, and a smoothing technique based on a neural network language model. At the decoding level lattice rescoring and minimum word error decoding are applied. On the development data, the improvements yield an overall word error rate of 24.9% whereas the original BN transcription system had a word error rate of about 50% on the same data."
            },
            "slug": "Conversational-telephone-speech-recognition-Gauvain-Lamel",
            "title": {
                "fragments": [],
                "text": "Conversational telephone speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper describes the development of a speech recognition system for the processing of telephone conversations, starting with a state-of-the-art broadcast news transcription system, and identifies major changes and improvements in acoustic and language modeling, as well as decoding, which are required to achieve state of theart performance on conversational speech."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47825073"
                        ],
                        "name": "Weiqi Wang",
                        "slug": "Weiqi-Wang",
                        "structuredName": {
                            "firstName": "Weiqi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiqi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1999488"
                        ],
                        "name": "M. Harper",
                        "slug": "M.-Harper",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Harper",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Harper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 169
                            }
                        ],
                        "text": "Only very recently have new language model approaches been successfully used in large vocabulary continuous speech recognizers, for example the SuperARV language model (Wang et al., 2004) and Random Forest language models (Xu and Mangu, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "All rights reserved."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13908253,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "1ac8987534ad3be87d4b70195c1beb039b102409",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Structured language models have recently been shown to give significant improvements in large-vocabulary recognition relative to traditional word N-gram models, but typically imply a heavy computational burden and have not been applied to large training sets or complex recognition systems. Previously, we developed a linguistically motivated and computationally efficient almost-parsing language model, using a data structure derived from constraint dependency grammar parsing, that tightly integrates knowledge of words, lexical features, and syntactic constraints. We show that such a model can be used effectively and efficiently in all stages of a complex, multi-pass conversational telephone speech recognition system. Compared to a state-of-the-art 4-gram interpolated word- and class-based language model, we obtained a 6.2% relative word error reduction (a 1.6% absolute reduction) on a recent NIST evaluation set."
            },
            "slug": "The-use-of-a-linguistically-motivated-language-in-Wang-Stolcke",
            "title": {
                "fragments": [],
                "text": "The use of a linguistically motivated language model in conversational speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A linguistically motivated and computationally efficient almost-parsing language model is developed, using a data structure derived from constraint dependency grammar parsing, that tightly integrates knowledge of words, lexical features, and syntactic constraints in all stages of a complex, multi-pass conversational telephone speech recognition system."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 175
                            }
                        ],
                        "text": "Statistical machine translation is not considered in this work, but initial studies have shown that the proposed neural network language model can also be used for this task (Schwenk et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1274371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4a258df43cc14e46988de9a4a7b2f0ea817529b",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. \n \nIn this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. \n \nWe also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks."
            },
            "slug": "Continuous-Space-Language-Models-for-Statistical-Schwenk",
            "title": {
                "fragments": [],
                "text": "Continuous Space Language Models for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes to use a new statistical language model that is based on a continuous representation of the words in the vocabulary, which achieves consistent improvements in the BLEU score on the development and test data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102811815"
                        ],
                        "name": "Marcello Federico",
                        "slug": "Marcello-Federico",
                        "structuredName": {
                            "firstName": "Marcello",
                            "lastName": "Federico",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcello Federico"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5161261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e44a6eb19639aa00d46c97548ac68fcd446b0161",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic n-gram language models have been successfully applied in continuous speech recognition for several years. Such language models provide many computational advantages but also require huge text corpora for parameter estimation. Moreover, the texts must exactly reflect, in a statistical sense, the user's language. Estimating a language model on a sample that is not representative severely affects speech recognition performance. A solution to this problem is provided by the Bayesian learning framework. Beyond the classical estimates, a Bayes derived interpolation model is proposed. Empirical comparisons have been carried out on a 10,000-word radiological reporting domain. Results are provided in terms of perplexity and recognition accuracy."
            },
            "slug": "Bayesian-estimation-methods-for-n-gram-language-Federico",
            "title": {
                "fragments": [],
                "text": "Bayesian estimation methods for n-gram language model adaptation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Empirical comparisons have been carried out on a 10,000-word radiological reporting domain and a Bayes derived interpolation model is proposed, providing results in terms of perplexity and recognition accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909943744"
                        ],
                        "name": "Christian Janvin",
                        "slug": "Christian-Janvin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Janvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Janvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 116
                            }
                        ],
                        "text": "Recently, a new approach was proposed based on a continuous representation of the words (Bengio et al., 2000, 2001; Bengio et al., 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 116
                            }
                        ],
                        "text": "Therefore several speed-up techniques have been proposed in the literature, in particular parallel implementations (Bengio et al., 2003), resampling techniques (Bengio and Se\u0301ne\u0301cal, 2003) and hierarchical decomposition (Morin and Bengio, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "All rights reserved."
                    },
                    "intents": []
                }
            ],
            "corpusId": 221275765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "isKey": false,
            "numCitedBy": 6011,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            },
            "slug": "A-Neural-Probabilistic-Language-Model-Bengio-Ducharme",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2092025743"
                        ],
                        "name": "P. Xu",
                        "slug": "P.-Xu",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718611"
                        ],
                        "name": "L. Mangu",
                        "slug": "L.-Mangu",
                        "structuredName": {
                            "firstName": "Lidia",
                            "lastName": "Mangu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mangu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 230
                            }
                        ],
                        "text": "Only very recently have new language model approaches been successfully used in large vocabulary continuous speech recognizers, for example the SuperARV language model (Wang et al., 2004) and Random Forest language models (Xu and Mangu, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14580691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ccfd99d94047c590899e19d4886d716fd1172c0",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the challenges in large vocabulary speech recognition is the availability of large amounts of data for training language models. In most state-of-the-art speech recognition systems, (cid:0) -gram models with Kneser-Ney smoothing still prevail due to their simplicity and effectiveness. In this paper, we study the performance of a new language model, the random forest language model, in the IBM conversational telephony speech recognition system. We show that although the random forest language models are designed to deal with the data sparseness problem, they also achieve statistically signi\ufb01cant improvements over (cid:0) -gram models when the training data has over 500 million words."
            },
            "slug": "Using-random-forest-language-models-in-the-IBM-CTS-Xu-Mangu",
            "title": {
                "fragments": [],
                "text": "Using random forest language models in the IBM RT-04 CTS system"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that although the random forest language models are designed to deal with the data sparseness problem, they also achieve statistically signi\ufb01cant improvements over (cid:0) -gram models when the training data has over 500 million words."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145395455"
                        ],
                        "name": "H. Kuo",
                        "slug": "H.-Kuo",
                        "structuredName": {
                            "firstName": "Hong-Kwang",
                            "lastName": "Kuo",
                            "middleNames": [
                                "Jeff"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kuo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398481836"
                        ],
                        "name": "E. Fosler-Lussier",
                        "slug": "E.-Fosler-Lussier",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Fosler-Lussier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fosler-Lussier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36357862"
                        ],
                        "name": "Hui Jiang",
                        "slug": "Hui-Jiang",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9391905"
                        ],
                        "name": "Chin-Hui Lee",
                        "slug": "Chin-Hui-Lee",
                        "structuredName": {
                            "firstName": "Chin-Hui",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Hui Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 182
                            }
                        ],
                        "text": "Unfortunately, it is quite difficult to use other criteria than perplexity with back-off language models, although several attempts have been reported in the literature, for example (Ito et al., 1999; Chen et al., 2000; Kuoi et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10927688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d0d679a558a80929172d2a114723af3e7f6c0d8",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe how discriminative training can be applied to language models for speech recognition. Language models are important to guide the speech recognition search, particularly in compensating for mistakes in acoustic decoding. A frequently used measure of the quality of language models is the perplexity; however, what is more important for accurate decoding is not necessarily having the maximum likelihood hypothesis, but rather the best separation of the correct string from the competing, acoustically confusible hypotheses. Discriminative training can help to improve language models for the purpose of speech recognition by improving the separation of the correct hypothesis from the competing hypotheses. We describe the algorithm and demonstrate modest improvements in word and sentence error rates on the DARPA Communicator task without any increase in language model complexity."
            },
            "slug": "Discriminative-training-of-language-models-for-Kuo-Fosler-Lussier",
            "title": {
                "fragments": [],
                "text": "Discriminative training of language models for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper describes the algorithm and demonstrates modest improvements in word and sentence error rates on the DARPA Communicator task without any increase in language model complexity."
            },
            "venue": {
                "fragments": [],
                "text": "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117203394"
                        ],
                        "name": "Zheng Chen",
                        "slug": "Zheng-Chen",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102429215"
                        ],
                        "name": "Kai-Fu Lee",
                        "slug": "Kai-Fu-Lee",
                        "structuredName": {
                            "firstName": "Kai-Fu",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Fu Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8392859"
                        ],
                        "name": "Mingjing Li",
                        "slug": "Mingjing-Li",
                        "structuredName": {
                            "firstName": "Mingjing",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingjing Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 182
                            }
                        ],
                        "text": "Unfortunately, it is quite difficult to use other criteria than perplexity with back-off language models, although several attempts have been reported in the literature, for example (Ito et al., 1999; Chen et al., 2000; Kuoi et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15299144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4df160605e810fb4b7189e9967cf184ff5fd1aa4",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical language models have been successfully applied to a lot of problems, including speech recognition, handwriting, Chinese pinyin-input etc. In recognition, statistical language model, such as trigram, is used to provide adequate information to predict the probabilities of hypothesized word sequences. The traditional method relying on distribution estimation are sub-optimal when the assumed distribution form is not the true one, and that \u201coptimality\u201d in distribution estimation does not automatically translate into \u201coptimality\u201d in classifier design. This paper proposed a discriminative training method to minimize the error rate of recognizer rather than estimate the distribution of training data. Furthermore, lexicon is also optimized to minimize the error rate of the decoder through discriminative training. Compared to the traditional LM building method, our systems gets approximately 5%-25% recognition error reduction with discriminative training on language model building."
            },
            "slug": "Discriminative-training-on-language-model-Chen-Lee",
            "title": {
                "fragments": [],
                "text": "Discriminative training on language model"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposed a discriminative training method to minimize the error rate of recognizer rather than estimate the distribution of training data, which gets approximately 5%-25% recognition error reduction with discrim inative training on language model building."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 198954554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c49051aea576ea6746826dc8049bbe37f9da14dc",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe how to use a neural network language model for the BN and CTS task in the RT04 evaluation. The new approach performs the estimation of the language model probabilities in a continuous space, allowing by this means smooth interpolations. Details are given on training data selection, fast training and decoding algorithms and parameter estimation. The neural network language model achieved word error reductions of 0.5% for the CTS task and of 0.3% for the BN task with an additional decoding cost of 0.05xRT."
            },
            "slug": "USING-NEURAL-NETWORK-LANGUAGE-MODELS-FOR-LVCSR-Schwenk-Gauvain",
            "title": {
                "fragments": [],
                "text": "USING NEURAL NETWORK LANGUAGE MODELS FOR LVCSR"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The new approach performs the estimation of the language model probabilities in a continuous space, allowing by this means smooth interpolations to be performed in the RT04 evaluation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108943475"
                        ],
                        "name": "Masami Nakamura",
                        "slug": "Masami-Nakamura",
                        "structuredName": {
                            "firstName": "Masami",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masami Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 3
                            }
                        ],
                        "text": "In Nakamura and Shikano (1989), a neural network is used to predict word categories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122532740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52f164f715febf097b8756a7308b416539b7c7da",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes word category prediction based on neural network models for constructing an accurate word recognition system. It is difficult to represent hidden linguistic structure and make an N\u2010gram word prediction model using traditional stochastic approaches. In this paper, two neural network models that can learn hidden linguistic structure are proposed. These models can easily be expanded from Bigram to N\u2010gram networks. They were tested by training experiments with an open English text database. The Trigram word category prediction rates show that neural network models are comparable to stochastic models. Trigram neural network models compress information about 150 times, which is the ratio of the Trigram stochastic model free parameters (893 = 704 969) to the neural network model link weights (4649). In addition, this paper proposes a new method that dynamically controls the training parameters, updating step size and momentum. These techniques are effective for calculating the efficiency of this system."
            },
            "slug": "A-study-of-English-word-category-prediction-based-Nakamura-Shikano",
            "title": {
                "fragments": [],
                "text": "A study of English word category prediction based on neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Two neural network models that can learn hidden linguistic structure are proposed that can easily be expanded from Bigram to N\u2010gram networks and are effective for calculating the efficiency of this system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204681"
                        ],
                        "name": "L. Lamel",
                        "slug": "L.-Lamel",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Lamel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707447"
                        ],
                        "name": "G. Adda",
                        "slug": "G.-Adda",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Adda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Adda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 98
                            }
                        ],
                        "text": "A detailed description of the technology that is common to all speech recognizers can be found in Gauvain et al. (2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14311517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94f70b874913d65bdad1be4061e794b485457e77",
            "isKey": false,
            "numCitedBy": 490,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-LIMSI-Broadcast-News-transcription-system-Gauvain-Lamel",
            "title": {
                "fragments": [],
                "text": "The LIMSI Broadcast News transcription system"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802969"
                        ],
                        "name": "Ciprian Chelba",
                        "slug": "Ciprian-Chelba",
                        "structuredName": {
                            "firstName": "Ciprian",
                            "lastName": "Chelba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciprian Chelba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 210
                            }
                        ],
                        "text": "In the domain of language modeling for continuous speech recognition, a large body of interesting work has been carried out and many new approaches have been developed, for instance structured language models (Chelba and Jelinek, 2000) or maximum entropy language models (Rosenfeld, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14339957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1c3748820d6b5ab4e7334524815df9bb6d20aed",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition. The structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser. A maximum likelihood re-estimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model. Experiments on the Wall Street Journal and Switchboard corpora show improvement in both perplexity and word error rate?word lattice rescoring?over the standard 3-gram language model."
            },
            "slug": "Structured-language-modeling-Chelba-Jelinek",
            "title": {
                "fragments": [],
                "text": "Structured language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "An attempt at using the syntactic structure in natural language for improved language models for speech recognition using an original probabilistic parameterization of a shift-reduce parser."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108943475"
                        ],
                        "name": "Masami Nakamura",
                        "slug": "Masami-Nakamura",
                        "structuredName": {
                            "firstName": "Masami",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masami Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 3
                            }
                        ],
                        "text": "In Nakamura and Shikano (1989), a neural network is used to predict word categories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 51825049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a044d798082c6dda101932a13d6c324e2edd551b",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Using traditional statistical approaches, it is difficult to develop an N-gram word prediction model for constructing an accurate word recognition system because of the increased demand for sample data and parameters to memorize probabilities. To solve this problem, NETgrams, which are neural networks for N-gram word category prediction in text are proposed. NETgrams can easily be expanded from bigram to N-gram networks without exponentially increasing the number of free parameters. Training results show that the NETgrams are comparable to the statistical model and compress information. Results of analyzing the hidden layer (microfeatures) show that the word categories are classified into some linguistically significant groups. It is confirmed that NETgrams perform effectively for unknown data, i.e NETgrams interpolate sparse training data naturally just like the deleted interpolation. A method for speeding up the back-propagation algorithm, which can automatically determine better parameters and achieve a shorter training time is proposed.<<ETX>>"
            },
            "slug": "A-study-of-English-word-category-prediction-based-Nakamura-Shikano",
            "title": {
                "fragments": [],
                "text": "A study of English word category prediction based on neutral networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Training results show that the NETgrams are comparable to the statistical model and compress information, and a method for speeding up the back-propagation algorithm, which can automatically determine better parameters and achieve a shorter training time is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119081122"
                        ],
                        "name": "C. Lawrence",
                        "slug": "C.-Lawrence",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Lawrence",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145334214"
                        ],
                        "name": "M. Rahim",
                        "slug": "M.-Rahim",
                        "structuredName": {
                            "firstName": "Mazin",
                            "lastName": "Rahim",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rahim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33506011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8011745d9ca2c24ba0b06578bf25b7085dca0c2",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a family of maximum likelihood (ML) techniques that aim at reducing an acoustic mismatch between the training and testing conditions of hidden Markov model (HMM)-based automatic speech recognition (ASR) systems. Our study is conducted in two phases. In the first phase, we evaluate two classes of robustness techniques; those that represent the acoustic mismatch for the entire utterance as a single additive bias and those that represent the mismatch as a non-stationary bias. In the second phase, we propose a codebook-based stochastic matching (CBSM) approach for bias removal both at the feature level and at the model level. CBSM associates each bias with an ensemble of HMM mixture components that share similar acoustic characteristics. It is integrated with hierarchical signal bias removal and further extended to account for n -best candidates. Experimental results on connected digits, recorded over a cellular network, shows that incorporating bias removal reduces both the word and string error rates by about 12% and 16%, respectively, when using a global bias, and 36% and 31%, respectively, when using a non-stationary bias."
            },
            "slug": "Integrated-bias-removal-techniques-for-robust-Lawrence-Rahim",
            "title": {
                "fragments": [],
                "text": "Integrated bias removal techniques for robust speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A codebook-based stochastic matching (CBSM) approach for bias removal both at the feature level and at the model level is proposed, integrated with hierarchical signal bias removal and further extended to account for n -best candidates."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38232647"
                        ],
                        "name": "R. Iyer",
                        "slug": "R.-Iyer",
                        "structuredName": {
                            "firstName": "Rukmini",
                            "lastName": "Iyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Iyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 130
                            }
                        ],
                        "text": "One possibility is to increase the amount of training data by selecting conversational-like sentences in broadcast news material (Iyer and Ostendorf, 1999) and on the Internet (Bulyko et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 129
                            }
                        ],
                        "text": "One possibility is to increase the amount of training data by selecting conversational-like sentences in broadcast news material (Iyer and Ostendorf, 1999) and on the Internet (Bulyko et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206561038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f617510cfe7f8ba24b238f43b135aca60c1b7b1",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard statistical language modeling techniques suffer from sparse-data problems in tasks where large amounts of domain-specific text are not available. In this paper, we focus on improving the estimation of domain-dependent n -gram models by the selective use of out-of-domain text data. Previous approaches for estimating language models from multi-domain data have not accounted for the characteristic variations of style and content across domains. In contrast, this work aims at differentially weighting subsets of the out-of-domain data according to style and/or content similarity to the given task, where \u201cstyle\" is represented by part-of-speech statistics and \u201ccontent\" by the particular choice of vocabulary items. In addition to n -gram estimation, the differential weights can be used for lexicon design. Recognition experiments are based on the Switchboard corpus of spontaneous conversations, with out-of-domain text drawn from the Wall Street Journal and Broadcast News corpora. The similarity weighting approach gives a 3?5% reduction in word error rate over a domain-specific n -gram language model, providing some of the largest language modeling gains reported for the Switchboard task in recent years."
            },
            "slug": "Relevance-weighting-for-combining-multi-domain-data-Iyer-Ostendorf",
            "title": {
                "fragments": [],
                "text": "Relevance weighting for combining multi-domain data for n-gram language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The similarity weighting approach gives a 3?5% reduction in word error rate over a domain-specific n -gram language model, providing some of the largest language modeling gains reported for the Switchboard task in recent years."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204681"
                        ],
                        "name": "L. Lamel",
                        "slug": "L.-Lamel",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Lamel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707447"
                        ],
                        "name": "G. Adda",
                        "slug": "G.-Adda",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Adda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Adda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802247"
                        ],
                        "name": "C. Barras",
                        "slug": "C.-Barras",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Barras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Barras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998170"
                        ],
                        "name": "Eric Bilinski",
                        "slug": "Eric-Bilinski",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Bilinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Bilinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701385"
                        ],
                        "name": "Olivier Galibert",
                        "slug": "Olivier-Galibert",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Galibert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Galibert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32158903"
                        ],
                        "name": "A. Pujol",
                        "slug": "A.-Pujol",
                        "structuredName": {
                            "firstName": "Agusti",
                            "lastName": "Pujol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pujol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144809121"
                        ],
                        "name": "Xuan Zhu",
                        "slug": "Xuan-Zhu",
                        "structuredName": {
                            "firstName": "Xuan",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuan Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6853267,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "76f76dd09c76334b34b5eaf7c9645569c2dca008",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Published in the Tc-Star Speech to Speech Translation Workshop, Barcelona, pages 123n128, June 2006. This paper describes the speech recognizers evaluated in the TC-STAR Second Evaluation Campaign held in JanuaryFebruary 2006. Systems were developed to transcribe parliamentary speeches in English and Spanish, as well as Broadcast news in Mandarin Chinese. The speech recognizers are state-of-the-art systems using multiple decoding passes with models (lexicon, acoustic models, language models) trained for the different transcription tasks. Compared to the LIMSI TC-STAR 2005 European Parliament Plenary Sessions (EPPS) systems, relative word error rate reductions of about 30% have been achieved on the 2006 development data. The word error rates with the LIMSI systems on the 2006 EPPS evaluation data are 8.2% for English and 7.8% for Spanish. The character error rate for Mandarin for a joint system submission with the University of Karlsruhe was 9.8%. Experiments with cross-site adaptation and system combination are also described."
            },
            "slug": "THE-LIMSI-2006-TC-STAR-TRANSCRIPTION-SYSTEMS-\u2044-Lamel-Gauvain",
            "title": {
                "fragments": [],
                "text": "THE LIMSI 2006 TC-STAR TRANSCRIPTION SYSTEMS \u2044"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The speech recognizers evaluated in the TC-STAR Second Evaluation Campaign held in JanuaryFebruary 2006 were developed to transcribe parliamentary speeches in English and Spanish, as well as Broadcast news in Mandarin Chinese."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748407"
                        ],
                        "name": "J. Bellegarda",
                        "slug": "J.-Bellegarda",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Bellegarda",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bellegarda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8238767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a90c1ca6c335de94721d7445bb01b723c3d9a840",
            "isKey": false,
            "numCitedBy": 390,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical language models used in large-vocabulary speech recognition must properly encapsulate the various constraints, both local and global, present in the language. While local constraints are readily captured through n-gram modeling, global constraints, such as long-term semantic dependencies, have been more difficult to handle within a data-driven formalism. This paper focuses on the use of latent semantic analysis, a paradigm that automatically uncovers the salient semantic relationships between words and documents in a given corpus. In this approach, (discrete) words and documents are mapped onto a (continuous) semantic vector space, in which familiar clustering techniques can be applied. This leads to the specification of a powerful framework for automatic semantic classification, as well as the derivation of several language model families with various smoothing properties. Because of their large-span nature, these language models are well suited to complement conventional n-grams. An integrative formulation is proposed for harnessing this synergy, in which the latent semantic information is used to adjust the standard n-gram probability. Such hybrid language modeling compares favorably with the corresponding n-gram baseline: experiments conducted on the Wall Street Journal domain show a reduction in average word error rate of over 20%. This paper concludes with a discussion of intrinsic tradeoffs, such as the influence of training data selection on the resulting performance."
            },
            "slug": "Exploiting-latent-semantic-information-in-language-Bellegarda",
            "title": {
                "fragments": [],
                "text": "Exploiting latent semantic information in statistical language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper focuses on the use of latent semantic analysis, a paradigm that automatically uncovers the salient semantic relationships between words and documents in a given corpus, and proposes an integrative formulation for harnessing this synergy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35797272"
                        ],
                        "name": "Ahmad Emami",
                        "slug": "Ahmad-Emami",
                        "structuredName": {
                            "firstName": "Ahmad",
                            "lastName": "Emami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmad Emami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2092025743"
                        ],
                        "name": "P. Xu",
                        "slug": "P.-Xu",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Emami et al. (2003) used the neural network language model with a syntactical language model showing perplexity and word error improvements on the Wall Street Journal corpus."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 118
                            }
                        ],
                        "text": "The only other application of a neural network language model to speech recognition we are aware of was later done by Emami et al. (2003) and follow up work of the same authors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "All rights reserved."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6529491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d6036af971c1f11ab712cc41487376a94e63673",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the performance of the Structured Language Model when one of its components is modeled by a connectionist model. Using a connectionist model and a distributed representation of the items in the history makes the component able to use much longer contexts than possible with currently used interpolated or backoff models, both because of the inherent capability of the connectionist model to fight the data sparseness problem, and because of the only sub-linear growth in the model size when increasing the context length. Experiments show significant improvement in perplexity and moderate reduction in word error rate over the baseline SLM results on the UPENN treebank and Wall Street Journal (WSJ) corpora respectively. The results also show that the probability distribution obtained by our model is much less correlated to regular N-grams than the baseline SLM model."
            },
            "slug": "Using-a-connectionist-model-in-a-syntactical-based-Emami-Xu",
            "title": {
                "fragments": [],
                "text": "Using a connectionist model in a syntactical based language model"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work investigates the performance of the Structured Language Model when one of its components is modeled by a connectionist model, and shows that the probability distribution obtained by the model is much less correlated to regular N-grams than the baseline SLM model."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214660"
                        ],
                        "name": "Volker Steinbiss",
                        "slug": "Volker-Steinbiss",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Steinbiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Steinbiss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57374299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "606df60d518db088986e74fad1f357ea6e5312f2",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple and general scheme for the adaptation of stochastic language models to changing text styles is introduced. For each word in the running text, the adapted model is a linear combination of specific models, the interpolation parameters being estimated on the preceding text passage. Experiments on a 1.1-million English word corpus show the validity of the approach. The adaptation method improves a bigram language model by 10% in terms of test-set perplexity.<<ETX>>"
            },
            "slug": "On-the-dynamic-adaptation-of-stochastic-language-Kneser-Steinbiss",
            "title": {
                "fragments": [],
                "text": "On the dynamic adaptation of stochastic language models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The adaptation method improves a bigram language model by 10% in terms of test-set perplexity and shows the validity of the approach on a 1.1-million English word corpus."
            },
            "venue": {
                "fragments": [],
                "text": "1993 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102811815"
                        ],
                        "name": "Marcello Federico",
                        "slug": "Marcello-Federico",
                        "structuredName": {
                            "firstName": "Marcello",
                            "lastName": "Federico",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcello Federico"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 247
                            }
                        ],
                        "text": "Among others, the proposed methods include cache language models (Kuhn and de Mori, 1990), adaptive interpolation of several language models (Kneser and Steinbiss, 1993), maximum a posteriori (Federico, 1996) or minimum discrimination information (Federico, 1999), and adaptation methods based on information retrieval methods (Chen et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6178119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "add8a2aad71e86622ebaa6a514692408defc707c",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for n-gram language model adaptation based on the principle of minimum discrimination information. A background language model is adapted to fit constraints on its marginal distributions that are derived from new observed data. This work gives a different derivation of the model by Kneser et al. (1997) and extends its application to interpolated language models. The proposed method has been evaluated on an Italian 60K-word broadcast news task."
            },
            "slug": "Efficient-language-model-adaptation-through-MDI-Federico",
            "title": {
                "fragments": [],
                "text": "Efficient language model adaptation through MDI estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "The proposed method for n-gram language model adaptation based on the principle of minimum discrimination information has been evaluated on an Italian 60K-word broadcast news task and is extended to interpolated language models."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2420860"
                        ],
                        "name": "A. Ito",
                        "slug": "A.-Ito",
                        "structuredName": {
                            "firstName": "Akinori",
                            "lastName": "Ito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2929563"
                        ],
                        "name": "M. Kohda",
                        "slug": "M.-Kohda",
                        "structuredName": {
                            "firstName": "Masaki",
                            "lastName": "Kohda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kohda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 130
                            }
                        ],
                        "text": "One possibility is to increase the amount of training data by selecting conversational-like sentences in broadcast news material (Iyer and Ostendorf, 1999) and on the Internet (Bulyko et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14883251,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "e9569b88d918d61f7269484323f07c4c61ce95cd",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Though Perplexity shows good correlation with word error rate within simple n-gram framework like Wall Street Journal task, it has been reported that perplexity have poor correlation with WER when more complicated LM is used. In this paper, a global measure for language model evaluation is proposed which achieveshigher correlation between word accuracy. The metric is based on difference of LM score between a word in the evaluation text and the word that gives the maximum score at that context. Two experiments were carried out to investigate the correlation between word accuracy and the proposed measure. In the first experiment, LMs in this paper were created using n-gram adaptation by n-gram count mixture. 47 LMs were created for the experiments by changing mixture weight and vocabulary cut-off threshold. Correlation betwen perplexity and word accuracy was very poor (correlation coefficient -0.36). On the other hand, the proposed metric gave much higher correlation (correlation coefficient 0.82). In the second experiment, a simple mixture trigram model was employed to recognize Switchboard task data. The highest correlation between word accuracy and the proposed method was 0.81, which was much higher than the correlation between PP and accucary 0.59."
            },
            "slug": "A-new-metric-for-stochastic-language-model-Ito-Kohda",
            "title": {
                "fragments": [],
                "text": "A new metric for stochastic language model evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A global measure for language model evaluation is proposed which achieves higher correlation between word accuracy and the proposed measure."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108579657"
                        ],
                        "name": "Langzhou Chen",
                        "slug": "Langzhou-Chen",
                        "structuredName": {
                            "firstName": "Langzhou",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Langzhou Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204681"
                        ],
                        "name": "L. Lamel",
                        "slug": "L.-Lamel",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Lamel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707447"
                        ],
                        "name": "G. Adda",
                        "slug": "G.-Adda",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Adda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Adda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398188506"
                        ],
                        "name": "M. Adda-Decker",
                        "slug": "M.-Adda-Decker",
                        "structuredName": {
                            "firstName": "Martine",
                            "lastName": "Adda-Decker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Adda-Decker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12211579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b03366ec3ebc1a90ce57fe7a41050fe979bc7da3",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we report experiments on language model adaptation using information retrieval methods, drawing upon recent developments in information extraction and topic tracking. One of the problems is extracting reliable topic information with high confidence from the audio signal in the presence of recognition errors. The work in the information retrieval domain on information extraction and topic tracking suggested a new way to solve this problem. In this work, we make use of information retrieval methods to extract topic information in the word recognizer hypotheses, which are then used to automatically select adaptation data from a very large general text corpus. Two adaptive language models, a mixture based model and a MAP based model, have been investigated using the adaptation data. Experiments carried out with the LIMSI Mandarin broadcast news transcription system gives a relative character error rate reduction of 4.3% with this adaptation method."
            },
            "slug": "Using-information-retrieval-methods-for-language-Chen-Gauvain",
            "title": {
                "fragments": [],
                "text": "Using information retrieval methods for language model adaptation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work makes use of information retrieval methods to extract topic information in the word recognizer hypotheses, which are then used to automatically select adaptation data from a very large general text corpus."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35797272"
                        ],
                        "name": "Ahmad Emami",
                        "slug": "Ahmad-Emami",
                        "structuredName": {
                            "firstName": "Ahmad",
                            "lastName": "Emami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmad Emami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5779171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a493a23b86192aa74e6f394061288082e1e7cdb7",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an application of randomization techniques to class-based n-gram language models used in speech recognizers. The idea is to derive a language model from the combination of a set of random class-based models. Each of the constituent random class-based models is built using a separate clustering obtained via a different run of a randomized clustering algorithm. The random class-based model can compensate for some of the shortcomings of conventional class-based models by combining the different solutions obtained through random clusterings. Experimental results show that the combined random class-based model improves considerably in perplexity (PPL) and word error rate (WER) over both the n-gram and baseline class-based models."
            },
            "slug": "Random-clusterings-for-language-modeling-Emami-Jelinek",
            "title": {
                "fragments": [],
                "text": "Random clusterings for language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the combined random class-based model improves considerably in perplexity (PPL) and word error rate (WER) over both the n-gram and baseline class- based models."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145738410"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3156164"
                        ],
                        "name": "A. Rudnicky",
                        "slug": "A.-Rudnicky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Rudnicky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rudnicky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Xu and Rudnicky (2000) proposed language models using a simple neural network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 138
                            }
                        ],
                        "text": "Suppose we want to translate a French source sentence f to an English target sentence e:\ne \u00bc arg max e P \u00f0ejf \u00de \u00bc arg max P \u00f0f je\u00deP \u00f0e\u00de; \u00f02\u00de\n308/$ - see front matter 2006 Elsevier Ltd."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14974472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfab4ffa229c8af0174a683ff1eda524c4f59d00",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Currently, N-gram models are the most common and widely used models for statistical language modeling. In this paper, we investigated an alternative way to build language models, i.e., using artificial neural networks to learn the language model. Our experiment result shows that the neural network can learn a language model that has performance even better than standard statistical methods."
            },
            "slug": "Can-artificial-neural-networks-learn-language-Xu-Rudnicky",
            "title": {
                "fragments": [],
                "text": "Can artificial neural networks learn language models?"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This paper investigated an alternative way to build language models, i.e., using artificial neural networks to learn the language model, and shows that the neural network can learn a language model that has performance even better than standard statistical methods."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2806570"
                        ],
                        "name": "M. Dyer",
                        "slug": "M.-Dyer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Dyer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 219
                            }
                        ],
                        "text": "Therefore several speed-up techniques have been proposed in the literature, in particular parallel implementations (Bengio et al., 2003), resampling techniques (Bengio and Se\u0301ne\u0301cal, 2003) and hierarchical decomposition (Morin and Bengio, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5982270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33995b0f996229742b9ca28916c73d51f55766d2",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to connectionist natural language processing is proposed, which is based on hierarchically organized modular parallel distributed processing (PDP) networks and a central lexicon of distributed input/output representations. The modules communicate using these representations, which are global and publicly available in the system. The representations are developed automatically by all networks while they are learning their processing tasks. The resulting representations reflect the regularities in the subtasks, which facilitates robust processing in the face of noise and damage, supports improved generalization, and provides expectations about possible contexts. The lexicon can be extended by cloning new instances of the items, that is, by generating a number of items with known processing properties and distinct identities. This technique combinatorially increases the processing power of the system. The recurrent FGREP module, together with a central lexicon, is used as a basic building block in modeling higher level natural language tasks. A single module is used to form case-role representations of sentences from word-by-word sequential natural language input. A hierarchical organization of four recurrent FGREP modules (the DISPAR system) is trained to produce fully expanded paraphrases of script-based stories, where unmentioned events and role fillers are inferred."
            },
            "slug": "Natural-Language-Processing-With-Modular-PDP-and-Miikkulainen-Dyer",
            "title": {
                "fragments": [],
                "text": "Natural Language Processing With Modular PDP Networks and Distributed Lexicon"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An approach to connectionist natural language processing is proposed, which is based on hierarchically organized modular parallel distributed processing (PDP) networks and a central lexicon of distributed input/output representations."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3241934"
                        ],
                        "name": "J. Fiscus",
                        "slug": "J.-Fiscus",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Fiscus",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fiscus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "System combination is performed using cross-site adaptation and the ROVER algorithm (Fiscus, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18751160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0861015b3c89d68749548c19c6f056eee34eafc6",
            "isKey": false,
            "numCitedBy": 1153,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes a system developed at NIST to produce a composite automatic speech recognition (ASR) system output when the outputs of multiple ASR systems are available, and for which, in many cases, the composite ASR output has a lower error rate than any of the individual systems. The system implements a \"voting\" or rescoring process to reconcile differences in ASR system outputs. We refer to this system as the NIST Recognizer Output Voting Error Reduction (ROVER) system. As additional knowledge sources are added to an ASR system (e.g. acoustic and language models), error rates are typically decreased. This paper describes a post-recognition process which models the output generated by multiple ASR systems as independent knowledge sources that can be combined and used to generate an output with reduced error rate. To accomplish this, the outputs of multiple of ASR systems are combined into a single, minimal-cost word transition network (WTN) via iterative applications of dynamic programming (DP) alignments. The resulting network is searched by an automatic rescoring or \"voting\" process that selects the output sequence with the lowest score."
            },
            "slug": "A-post-processing-system-to-yield-reduced-word-Fiscus",
            "title": {
                "fragments": [],
                "text": "A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER)"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A post-recognition process which models the output generated by multiple ASR systems as independent knowledge sources that can be combined and used to generate an output with reduced error rate."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 272
                            }
                        ],
                        "text": "In the domain of language modeling for continuous speech recognition, a large body of interesting work has been carried out and many new approaches have been developed, for instance structured language models (Chelba and Jelinek, 2000) or maximum entropy language models (Rosenfeld, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13110923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "076fa8d095c37c657f2aff39cf90bc2ea883b7cb",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the document's history, we propose and usetrigger pairsas the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, we apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the ME solution. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. The ME framework is extremely general: any phenomenon that can be described in terms of statistics of the text can be readily incorporated. An adaptive language model based on the ME approach was trained on theWall Street Journalcorpus, and showed a 32\u201339% perplexity reduction over the baseline. When interfaced to SPHINX-II, Carnegie Mellon's speech recognizer, it reduced its error rate by 10\u201314%. This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "slug": "A-maximum-entropy-approach-to-adaptive-statistical-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources, and shows the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50126864"
                        ],
                        "name": "Joshua Goodman",
                        "slug": "Joshua-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joshua Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12982389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09c76da2361d46689825c4efc37ad862347ca577",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n -grams, skipping, interpolated Kneser?Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38 and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline."
            },
            "slug": "A-bit-of-progress-in-language-modeling-Goodman",
            "title": {
                "fragments": [],
                "text": "A bit of progress in language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A combination of all techniques together to a Katz smoothed trigram model with no count cutoffs achieves perplexity reductions between 38 and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2575459"
                        ],
                        "name": "I. Bulyko",
                        "slug": "I.-Bulyko",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Bulyko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Bulyko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 177
                            }
                        ],
                        "text": "One possibility is to increase the amount of training data by selecting conversational-like sentences in broadcast news material (Iyer and Ostendorf, 1999) and on the Internet (Bulyko et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 75
                            }
                        ],
                        "text": "\u2022 up to 525 M words of web data, collected by the University of Washington (Bulyko et al., 2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 362196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "489dc535b56c7b52890ec9bf2de100a446040be8",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Sources of training data suitable for language modeling of conversational speech are limited. In this paper, we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task, but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation of N-grams."
            },
            "slug": "Getting-More-Mileage-from-Web-Text-Sources-for-Bulyko-Ostendorf",
            "title": {
                "fragments": [],
                "text": "Getting More Mileage from Web Text Sources for Conversational Speech Language Modeling using Class-Dependent Mixtures"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper shows how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task, but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation of N-grams."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204681"
                        ],
                        "name": "L. Lamel",
                        "slug": "L.-Lamel",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Lamel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707447"
                        ],
                        "name": "G. Adda",
                        "slug": "G.-Adda",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Adda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Adda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998170"
                        ],
                        "name": "Eric Bilinski",
                        "slug": "Eric-Bilinski",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Bilinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Bilinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11716698,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0591b78373f96da6f5cc1bd9113e12f4c7b0af46",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes recent research carried out in the con-text of the FP6 Integrated Project C HIL in developing a system to automatically transcribe lectures and seminars. We made use of widely available corpora to train both the acoustic and language models, since only a small amount of C HIL data were available for system development. For acoustic model training made use of the transcribed portion of the TED corpus of Eurospeech recordings, as well as the ICSI, ISL, and NIST meeting corpora. For language model training, text materials were extracted from a variety of on-line conference proceedings. Word error rates of about 25% are obtained on test data extracted 12 seminars."
            },
            "slug": "Transcribing-lectures-and-seminars-Lamel-Adda",
            "title": {
                "fragments": [],
                "text": "Transcribing lectures and seminars"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper describes recent research carried out in the con-text of the FP6 Integrated Project C HIL in developing a system to automatically transcribe lectures and seminars, making use of widely available corpora to train both the acoustic and language models."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 80
                            }
                        ],
                        "text": "All back-off language models were trained using the SRI language model toolkit (Stolcke, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1988103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "399da68d3b97218b6c80262df7963baa89dcc71b",
            "isKey": false,
            "numCitedBy": 4997,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools."
            },
            "slug": "SRILM-an-extensible-language-modeling-toolkit-Stolcke",
            "title": {
                "fragments": [],
                "text": "SRILM - an extensible language modeling toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The functionality of the SRILM toolkit is summarized and its design and implementation is discussed, highlighting ease of rapid prototyping, reusability, and combinability of tools."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150603996"
                        ],
                        "name": "L. Nguyen",
                        "slug": "L.-Nguyen",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143993049"
                        ],
                        "name": "Sherif M. Abdou",
                        "slug": "Sherif-M.-Abdou",
                        "structuredName": {
                            "firstName": "Sherif",
                            "lastName": "Abdou",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherif M. Abdou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295739"
                        ],
                        "name": "M. Afify",
                        "slug": "M.-Afify",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Afify",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Afify"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10080270"
                        ],
                        "name": "J. Makhoul",
                        "slug": "J.-Makhoul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Makhoul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Makhoul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145819149"
                        ],
                        "name": "Spyros Matsoukas",
                        "slug": "Spyros-Matsoukas",
                        "structuredName": {
                            "firstName": "Spyros",
                            "lastName": "Matsoukas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Spyros Matsoukas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152901366"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144028698"
                        ],
                        "name": "Bing Xiang",
                        "slug": "Bing-Xiang",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Xiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204681"
                        ],
                        "name": "L. Lamel",
                        "slug": "L.-Lamel",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Lamel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707447"
                        ],
                        "name": "G. Adda",
                        "slug": "G.-Adda",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Adda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Adda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50653913"
                        ],
                        "name": "F. Lef\u00e8vre",
                        "slug": "F.-Lef\u00e8vre",
                        "structuredName": {
                            "firstName": "Fabrice",
                            "lastName": "Lef\u00e8vre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lef\u00e8vre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 55
                            }
                        ],
                        "text": "More details of this integrated system are provided in Nguyen et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 55453163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98a4985439a7e35d4c77a3346ca7f194a6710d1b",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the 2004 BBN/LIMSI 10xRT English Broadcast News (BN) transcription system which uses a tightly integrated combination of components from the BBN and LIMSI speech recognition systems. The integrated system uses both cross-site adaptation and system combination via ROVER, obtaining a word hypothesis that is better than is produced by either system alone, while remaining within the allotted time limit. The system configuration used for the evaluation has two components from each site and two ROVER combinations, and achieved a word error rate (WER) of 13.9% on the Dev04f set and 9.3% on the Dev04 set selected to match the progress set. Compared to last year\u2019s system, there is around 30% relative reduction on the WER."
            },
            "slug": "THE-2004-BBN/LIMSI-10xRT-ENGLISH-BROADCAST-NEWS-Nguyen-Abdou",
            "title": {
                "fragments": [],
                "text": "THE 2004 BBN/LIMSI 10xRT ENGLISH BROADCAST NEWS TRANSCRIPTION SYSTEM"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper describes the 2004 BBN/LIMSI 10xRT English Broadcast News (BN) transcription system which uses a tightly integrated combination of components from the BBN and LIMSI speech recognition systems, obtaining a word hypothesis that is better than is produced by either system alone, while remaining within the allotted time limit."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853032"
                        ],
                        "name": "J. Lai",
                        "slug": "J.-Lai",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Lai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 236
                            }
                        ],
                        "text": "For example in back-off n-gram word language models the context is limited to the n 1 previous words (Katz, 1987), or in class language models several words are grouped together into syntactical, semantical or statistical word classes (Brown et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10986188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3de5d40b60742e3dfa86b19e7f660962298492af",
            "isKey": false,
            "numCitedBy": 3318,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics."
            },
            "slug": "Class-Based-n-gram-Models-of-Natural-Language-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "Class-Based n-gram Models of Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work addresses the problem of predicting a word from previous words in a sample of text and discusses n-gram models based on classes of words, finding that these models are able to extract classes that have the flavor of either syntactically based groupings or semanticallybased groupings, depending on the nature of the underlying statistics."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9863,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 186
                            }
                        ],
                        "text": "(6)) can be seen as a smoothed version of the winner-takes-all activation model in which the unit with the largest input has output +1 while all other units have output 0 (Bridle, 1990; Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 133
                            }
                        ],
                        "text": "It can be shown that the outputs of a neural network trained in this manner converge to the posterior probabilities (see for example Bishop (1995))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 215
                            }
                        ],
                        "text": "\u2026classical learning theory we should minimize the sum of E over all examples, but this generally leads to very slow convergence with training corpora of several thousand or even a million examples (see for examples Bishop (1995) for a general description of neural networks and learning theory)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15338,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710580"
                        ],
                        "name": "A. Berger",
                        "slug": "A.-Berger",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Berger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 111
                            }
                        ],
                        "text": "The cost function of the neural network language model is also similar to that used in maximum entropy models (Berger et al., 1996), which corresponds to a neural network without a hidden layer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1085832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb486e03369a64de2d5b0df86ec0a7b55d3907db",
            "isKey": false,
            "numCitedBy": 3452,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Natural-Language-Berger-Pietra",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A maximum-likelihood approach for automatically constructing maximum entropy models is presented and how to implement this approach efficiently is described, using as examples several problems in natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This idea was also applied to statistical language modeling, for example in (Bellegarda, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 172
                            }
                        ],
                        "text": "(6)) can be seen as a smoothed version of the winner-takes-all activation model in which the unit with the largest input has output +1 while all other units have output 0 (Bridle, 1990; Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59636530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f462943c8d0af69c12a09058251848324135e5a",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct."
            },
            "slug": "Probabilistic-Interpretation-of-Feedforward-Network-Bridle",
            "title": {
                "fragments": [],
                "text": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two modifications are explained: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non- linearity of feed-forward non-linear networks with multiple outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NATO Neurocomputing"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38375852"
                        ],
                        "name": "S. Galliano",
                        "slug": "S.-Galliano",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Galliano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Galliano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2196683"
                        ],
                        "name": "E. Geoffrois",
                        "slug": "E.-Geoffrois",
                        "structuredName": {
                            "firstName": "Edouard",
                            "lastName": "Geoffrois",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Geoffrois"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746836"
                        ],
                        "name": "D. Mostefa",
                        "slug": "D.-Mostefa",
                        "structuredName": {
                            "firstName": "Djamel",
                            "lastName": "Mostefa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mostefa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678451"
                        ],
                        "name": "K. Choukri",
                        "slug": "K.-Choukri",
                        "structuredName": {
                            "firstName": "Khalid",
                            "lastName": "Choukri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Choukri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803495"
                        ],
                        "name": "J. Bonastre",
                        "slug": "J.-Bonastre",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Bonastre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bonastre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708671"
                        ],
                        "name": "G. Gravier",
                        "slug": "G.-Gravier",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Gravier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gravier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 183
                            }
                        ],
                        "text": "The 7xRT system was evaluated in the first French TECHNOLANGUE-ESTER automatic speech recognition benchmark test where it achieved the lowest word error rate by a significant margin (Galliano et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14563929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df32bad8a9a9def048cb084b99b7e5dd35f222c7",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives the final results of the ESTER evaluation campaign which started in 2003 and ended in January 2005. The aim of this campaign was to evaluate automatic broadcast news rich transcription systems for the French language. The evaluation tasks were divided into three main categories: orthographic transcription, event detection and tracking (e.g. speech vs. music, speaker tracking), and information extraction. The last one, limited to named entity detection in this evaluation, was a preliminary test. The paper reports on protocols and gives the results obtained in the campaign."
            },
            "slug": "The-ESTER-phase-II-evaluation-campaign-for-the-rich-Galliano-Geoffrois",
            "title": {
                "fragments": [],
                "text": "The ESTER phase II evaluation campaign for the rich transcription of French broadcast news"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper gives the final results of the ESTER evaluation campaign which started in 2003 and ended in January 2005, to evaluate automatic broadcast news rich transcription systems for the French language."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067193759"
                        ],
                        "name": "Stefan Heil",
                        "slug": "Stefan-Heil",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Heil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Heil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 87
                            }
                        ],
                        "text": "Similar ideas were also formulated in the context of character-based text compression (Schmidhuber, 1996)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13492169,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79521a6d8814f9162ed1f7028e9e007c4df7181a",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this paper is to show that neural networks may be promising tools for data compression without loss of information. We combine predictive neural nets and statistical coding techniques to compress text files. We apply our methods to certain short newspaper articles and obtain compression ratios exceeding those of the widely used Lempel-Ziv algorithms (which build the basis of the UNIX functions \"compress\" and \"gzip\"). The main disadvantage of our methods is that they are about three orders of magnitude slower than standard methods."
            },
            "slug": "Sequential-neural-text-compression-Schmidhuber-Heil",
            "title": {
                "fragments": [],
                "text": "Sequential neural text compression"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "It is shown that neural networks may be promising tools for data compression without loss of information and compression ratios exceeding those of the widely used Lempel-Ziv algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748118"
                        ],
                        "name": "J. Bilmes",
                        "slug": "J.-Bilmes",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Bilmes",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bilmes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760896"
                        ],
                        "name": "K. Asanovi\u0107",
                        "slug": "K.-Asanovi\u0107",
                        "structuredName": {
                            "firstName": "Krste",
                            "lastName": "Asanovi\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Asanovi\u0107"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295214"
                        ],
                        "name": "Chee-Whye Chin",
                        "slug": "Chee-Whye-Chin",
                        "structuredName": {
                            "firstName": "Chee-Whye",
                            "lastName": "Chin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chee-Whye Chin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700326"
                        ],
                        "name": "J. Demmel",
                        "slug": "J.-Demmel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Demmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Demmel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 143
                            }
                        ],
                        "text": "Bunch mode Further improvements can be obtained by propagating several examples at once through the network, which is also known as bunch mode (Bilmes et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 132
                            }
                        ],
                        "text": "After calculating the derivatives of the error function DK at the output layer, the following operations were performed (similar to Bilmes et al., 1997):\n(1) Update the bias at the output layer:\nk \u00bc k kDK i; \u00f017\u00de where i = (1,1, . . ., 1)T, with a dimension of the bunch size."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 133
                            }
                        ],
                        "text": "Further improvements can be obtained by propagating several examples at once through the network, which is also known as bunch mode (Bilmes et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Bilmes et al. (1997) reported that the number of epochs needed to achieve the same mean squared error increased with the bunch size."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13416650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d7867776be77bdb9f467b30c385cad8393f5240",
            "isKey": true,
            "numCitedBy": 33,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce PHiPAC, a coding methodology for developing portable high-performance numerical libraries in ANSI C. Using this methodology, we have developed code for optimized matrix multiply routines. These routines can achieve over 90% of peak performance on a variety of current workstations, and are often faster than vendor-supplied optimized libraries. We then describe the bunch-mode back-propagation algorithm and how it can use the PHiPAC derived matrix multiply routines. Using a set of plots, we investigate the tradeoffs between bunch size, convergence rate, and training speed using a standard speech recognition data set and show how use of the PHiPAC routines can lead to a significantly faster back-propagation learning algorithm."
            },
            "slug": "Using-PHiPAC-to-speed-error-back-propagation-Bilmes-Asanovi\u0107",
            "title": {
                "fragments": [],
                "text": "Using PHiPAC to speed error back-propagation learning"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "This work introduces PHiPAC, a coding methodology for developing portable high-performance numerical libraries in ANSI C, and develops code for optimized matrix multiply routines that can achieve over 90% of peak performance on a variety of current workstations and are often faster than vendor-supplied optimized libraries."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145816817"
                        ],
                        "name": "M. J. Bleda",
                        "slug": "M.-J.-Bleda",
                        "structuredName": {
                            "firstName": "Mar\u00eda",
                            "lastName": "Bleda",
                            "middleNames": [
                                "Jos\u00e9",
                                "Castro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Bleda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10213789"
                        ],
                        "name": "F. Prat",
                        "slug": "F.-Prat",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Prat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Prat"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35559071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8559665125d96cdffe38d5c0893cab92af1cd6a8",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with the introduction of long term memory in a Multilevel Darwinist Brain (MDB) structure based on Artificial Neural Networks and its implications on the capability of adapting to new environments and recognizing previously explored ones by autonomous robots. The introduction of long term memory greatly enhances the ability of the organisms that implement the MDB to deal with changing environments and at the same time recover from failures and changes in configurations. The paper describes the mechanism, introduces the long term mermoy within it and provides some examples of its operation both in theoretical problems and on a real robot whose perceptual and actuation mechanisms are changed periodically."
            },
            "slug": "New-Directions-in-Connectionist-Language-Modeling-Bleda-Prat",
            "title": {
                "fragments": [],
                "text": "New Directions in Connectionist Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The paper describes the mechanism, introduces the long term mermoy within it and provides some examples of its operation both in theoretical problems and on a real robot whose perceptual and actuation mechanisms are changed periodically."
            },
            "venue": {
                "fragments": [],
                "text": "IWANN"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643845523"
                        ],
                        "name": "F. ChenStanley",
                        "slug": "F.-ChenStanley",
                        "structuredName": {
                            "firstName": "F",
                            "lastName": "ChenStanley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. ChenStanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643789739"
                        ],
                        "name": "GoodmanJoshua",
                        "slug": "GoodmanJoshua",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "GoodmanJoshua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "GoodmanJoshua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215842252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4e8bed3b50a035e1eabad614fe4218a34b3b178",
            "isKey": false,
            "numCitedBy": 2861,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t..."
            },
            "slug": "An-empirical-study-of-smoothing-techniques-for-ChenStanley-GoodmanJoshua",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing techniques for language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A survey of the most widely-used algorithms for smoothing models for language n -gram modeling and an extensive empirical comparison of several of these smoothing techniques are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 131
                            }
                        ],
                        "text": "In latent semantic indexing feature vectors of documents and words are learned on the basis of their probability of co-occurences (Deerwester et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": false,
            "numCitedBy": 7019,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399006281"
                        ],
                        "name": "F. Fogelman-Souli\u00e9",
                        "slug": "F.-Fogelman-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Fogelman-Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fogelman-Souli\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195609364,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d703fbc34b55a829a3c703bfc268406dfce0cc0a",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This volume contains the collected papers of the NATO Advanced Research Workshop on Neurocomputing, held in February 1989. Various fields in neurocomputing were covered, including new or improved neural network algorithms, product units, recurrent multilayer networks, Boltzmann machines, Kohonen maps, growth algorithms, VLSI circuits, VLSI neurons, dedicated processors, coding, links with hidden Markov Models (HMMs), recognition of digits, wind maps, mammographic images, cortical columns, posture and movement co-ordination."
            },
            "slug": "Neurocomputing-:-algorithms,-architectures-and-Fogelman-Souli\u00e9-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Neurocomputing : algorithms, architectures and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This volume contains the collected papers of the NATO Advanced Research Workshop on Neurocomputing, held in February 1989, and covered new or improved neural network algorithms, product units, recurrent multilayer networks, Boltzmann machines, Kohonen maps, growth algorithms and dedicated processors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 72
                            }
                        ],
                        "text": "This is similar in spirit to the theory behind support vector machines (Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26321,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207738357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "046b7f6b48e4d9fcf173dea0a0802d7e87b383e1",
            "isKey": false,
            "numCitedBy": 5491,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy."
            },
            "slug": "Bagging-Predictors-Breiman",
            "title": {
                "fragments": [],
                "text": "Bagging Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 47328136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1ee87290fa827f1217b8fa2bccb3485da1a300e",
            "isKey": false,
            "numCitedBy": 15183,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy."
            },
            "slug": "Bagging-predictors-Breiman",
            "title": {
                "fragments": [],
                "text": "Bagging predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996444"
                        ],
                        "name": "A. Paccanaro",
                        "slug": "A.-Paccanaro",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Paccanaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Paccanaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5451965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9b8eb922d7530373f1e5fa6d6d2eb98cb60eda9",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear relational embedding (LRE) was introduced previously by the authors (1999) as a means of extracting a distributed representation of concepts from relational data. The original formulation cannot use negative information and cannot properly handle data in which there are multiple correct answers. In this paper we propose an extended formulation of LRE that solves both these problems. We present results in two simple domains, which show that learning leads to good generalization."
            },
            "slug": "Extracting-distributed-representations-of-concepts-Paccanaro-Hinton",
            "title": {
                "fragments": [],
                "text": "Extracting distributed representations of concepts and relations from positive and negative propositions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents results in two simple domains, which show that learning leads to good generalization in linear relational embedding and an extended formulation of LRE that solves both these problems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716964"
                        ],
                        "name": "J. Cocke",
                        "slug": "J.-Cocke",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cocke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cocke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069902"
                        ],
                        "name": "P. Roossin",
                        "slug": "P.-Roossin",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Roossin",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roossin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 266
                            }
                        ],
                        "text": "\u2026today common practice to introduce the acoustic model P(xjw) and the language model P(w) when searching for the best word sequence w*:\n0885-2 doi:10\nq Th E-m\nw \u00bc arg max w P \u00f0wjx\u00de \u00bc arg max w P\u00f0xjw\u00deP \u00f0w\u00de: \u00f01\u00de\nA similar decomposition is used in statistical machine translation (Brown et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 162
                            }
                        ],
                        "text": "It is also shown that this approach can be incorporated into a large vocabulary continuous speech recognizer using a lattice rescoring framework at a very low additional processing time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14386564,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1066659ec1afee9dce586f6f49b7d44527827e1",
            "isKey": false,
            "numCitedBy": 1940,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results."
            },
            "slug": "A-Statistical-Approach-to-Machine-Translation-Brown-Cocke",
            "title": {
                "fragments": [],
                "text": "A Statistical Approach to Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The application of the statistical approach to translation from French to English and preliminary results are described and the results are given."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 177
                            }
                        ],
                        "text": "There is an additional gain of about 0.3% for the second and third systems, although they are based on the hypothesis of the first system, including system combination with two BBN systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 72
                            }
                        ],
                        "text": "3 M words of Fisher data transcribed by WordWave and distributed by BBN (Kimball et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "BBN runs a two pass decoding (system B1, 2.6xRT), followed by a full LIMSI decode that adapts on this result (system L1, 2.7xRT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "BBN runs then another two pass decode (system B2, 2.1xRT) by adapting on the ROVER of L1 and B2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "LIMSI and BBN also developed a joint system for the English broadcast news task of the 2004 NIST evaluation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 218
                            }
                        ],
                        "text": "\u2022 CTS transcripts without breath noise (21.2 M words): 2.9 M words of Switchboard transcriptions provided by the Mississippi State University, 18.3 M words of Fisher data transcribed by WordWave and distributed by BBN (Kimball et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "LIMSI and BBN in Cambridge developed a tightly integrated combination of five BBN and three LIMSI speech recognition systems for the 2004 NIST English CTS evaluation (Prasad et al., 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 166
                            }
                        ],
                        "text": "Finally, the speech recognizer developed for the NIST 2004 Rich Transcription evaluation is a complex combination of several components from two research institutes: BBN Technologies in Cambridge and LIMSI\u2013CNRS in France (Prasad et al., 2004)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quick transcription and automatic segmentation of the fisher"
            },
            "venue": {
                "fragments": [],
                "text": "Transactions on Acoustics, Speech, and Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 177
                            }
                        ],
                        "text": "There is an additional gain of about 0.3% for the second and third systems, although they are based on the hypothesis of the first system, including system combination with two BBN systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "BBN runs a two pass decoding (system B1, 2.6xRT), followed by a full LIMSI decode that adapts on this result (system L1, 2.7xRT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "BBN runs then another two pass decode (system B2, 2.1xRT) by adapting on the ROVER of L1 and B2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "LIMSI and BBN also developed a joint system for the English broadcast news task of the 2004 NIST evaluation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 218
                            }
                        ],
                        "text": "\u2022 CTS transcripts without breath noise (21.2 M words): 2.9 M words of Switchboard transcriptions provided by the Mississippi State University, 18.3 M words of Fisher data transcribed by WordWave and distributed by BBN (Kimball et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "LIMSI and BBN in Cambridge developed a tightly integrated combination of five BBN and three LIMSI speech recognition systems for the 2004 NIST English CTS evaluation (Prasad et al., 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 166
                            }
                        ],
                        "text": "Finally, the speech recognizer developed for the NIST 2004 Rich Transcription evaluation is a complex combination of several components from two research institutes: BBN Technologies in Cambridge and LIMSI\u2013CNRS in France (Prasad et al., 2004)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quick transcription and automatic segmentation of the fisher conversational telephone speech corpus"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2004 Rich Transcriptions Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121477075"
                        ],
                        "name": "Rohit Prasad",
                        "slug": "Rohit-Prasad",
                        "structuredName": {
                            "firstName": "Rohit",
                            "lastName": "Prasad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rohit Prasad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145819149"
                        ],
                        "name": "Spyros Matsoukas",
                        "slug": "Spyros-Matsoukas",
                        "structuredName": {
                            "firstName": "Spyros",
                            "lastName": "Matsoukas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Spyros Matsoukas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809707"
                        ],
                        "name": "C. Kao",
                        "slug": "C.-Kao",
                        "structuredName": {
                            "firstName": "Chia-Lin",
                            "lastName": "Kao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2606816"
                        ],
                        "name": "Jeff Z. Ma",
                        "slug": "Jeff-Z.-Ma",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Ma",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Z. Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2314900"
                        ],
                        "name": "Dongxin Xu",
                        "slug": "Dongxin-Xu",
                        "structuredName": {
                            "firstName": "Dongxin",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongxin Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041369"
                        ],
                        "name": "Thomas Colthurst",
                        "slug": "Thomas-Colthurst",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Colthurst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Colthurst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028300167"
                        ],
                        "name": "G. Thattai",
                        "slug": "G.-Thattai",
                        "structuredName": {
                            "firstName": "Govind",
                            "lastName": "Thattai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Thattai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3353221"
                        ],
                        "name": "O. Kimball",
                        "slug": "O.-Kimball",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Kimball",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kimball"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204681"
                        ],
                        "name": "L. Lamel",
                        "slug": "L.-Lamel",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Lamel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lamel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707447"
                        ],
                        "name": "G. Adda",
                        "slug": "G.-Adda",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Adda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Adda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50653913"
                        ],
                        "name": "F. Lef\u00e8vre",
                        "slug": "F.-Lef\u00e8vre",
                        "structuredName": {
                            "firstName": "Fabrice",
                            "lastName": "Lef\u00e8vre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lef\u00e8vre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 167
                            }
                        ],
                        "text": "LIMSI and BBN in Cambridge developed a tightly integrated combination of five BBN and three LIMSI speech recognition systems for the 2004 NIST English CTS evaluation (Prasad et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 221
                            }
                        ],
                        "text": "Finally, the speech recognizer developed for the NIST 2004 Rich Transcription evaluation is a complex combination of several components from two research institutes: BBN Technologies in Cambridge and LIMSI\u2013CNRS in France (Prasad et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 228150641,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "be098860634ce06b46e7fef6bd9db9784c163f43",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-2004-20xRT-BBN/LIMSI-English-Conversational-Prasad-Matsoukas",
            "title": {
                "fragments": [],
                "text": "The 2004 20xRT BBN/LIMSI English Conversational Telephone Speech System"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145914568"
                        ],
                        "name": "Roger K. Moore",
                        "slug": "Roger-K.-Moore",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Moore",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger K. Moore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60353286,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "8bdb9879a6d60f5dd47dfb6541c3a57d46abe5be",
            "isKey": false,
            "numCitedBy": 752,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Speech-and-Language-Moore",
            "title": {
                "fragments": [],
                "text": "Computer Speech and Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53796860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ade4934db522fe6d634ff6f48887da46eedb4d1",
            "isKey": false,
            "numCitedBy": 902,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-distributed-representations-of-concepts.-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning distributed representations of concepts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748407"
                        ],
                        "name": "J. Bellegarda",
                        "slug": "J.-Bellegarda",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Bellegarda",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bellegarda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 77
                            }
                        ],
                        "text": "This idea was also applied to statistical language modeling, for example in (Bellegarda, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12976399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2928de5400a920a6a29af41821c680cef5d35f91",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-latent-semantic-analysis-framework-for-large-Span-Bellegarda",
            "title": {
                "fragments": [],
                "text": "A latent semantic analysis framework for large-Span language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079911577"
                        ],
                        "name": "Blockin Blockin",
                        "slug": "Blockin-Blockin",
                        "structuredName": {
                            "firstName": "Blockin",
                            "lastName": "Blockin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Blockin Blockin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 115
                            }
                        ],
                        "text": "Therefore several speed-up techniques have been proposed in the literature, in particular parallel implementations (Bengio et al., 2003), resampling techniques (Bengio and S\u00e9n\u00e9cal, 2003) and hierarchical decomposition (Morin and Bengio, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 116
                            }
                        ],
                        "text": "Recently, a new approach was proposed based on a continuous representation of the words (Bengio et al., 2000, 2001; Bengio et al., 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 116
                            }
                        ],
                        "text": "Therefore several speed-up techniques have been proposed in the literature, in particular parallel implementations (Bengio et al., 2003), resampling techniques (Bengio and Se\u0301ne\u0301cal, 2003) and hierarchical decomposition (Morin and Bengio, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2606135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b388f0151ab37adb3d57738b8f52a3f943f86c8",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Quick-Training-of-Probabilistic-Neural-Nets-by-Blockin",
            "title": {
                "fragments": [],
                "text": "Quick Training of Probabilistic Neural Nets by Importance Sampling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 61
                            }
                        ],
                        "text": "Bi- and trigram models using neural networks were studied by Castro and Polvoreda (2001); Castro and Prat (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist n-gram models by using MLPs"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Human Language Technology Conference"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 67
                            }
                        ],
                        "text": "A similar decomposition is used in statistical machine translation (Brown et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 266
                            }
                        ],
                        "text": "\u2026today common practice to introduce the acoustic model P(xjw) and the language model P(w) when searching for the best word sequence w*:\n0885-2 doi:10\nq Th E-m\nw \u00bc arg max w P \u00f0wjx\u00de \u00bc arg max w P\u00f0xjw\u00deP \u00f0w\u00de: \u00f01\u00de\nA similar decomposition is used in statistical machine translation (Brown et al., 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neurocomputing: Algorithms, Architectures and Applicationss"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 124
                            }
                        ],
                        "text": "The first application of a neural network language model to large vocabulary continuous speech recognition was performed by Schwenk (2001), Schwenk and Gauvain (2002), showing word error reductions on the DARPA HUB5 conversational telephone speech recognition task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Language modeling in the continuous domain"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Quick transcription and automatic segmentation of the fisher conversational telephone speech corpus"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2004 Rich Transcriptions Workshop"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A cache-based natural language model for speech reproduction"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 45
                            }
                        ],
                        "text": "Both systems are described in more detail in Gauvain et al. (2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Where are we in transcribing BN French? In: Proceedings of Eurospeech"
            },
            "venue": {
                "fragments": [],
                "text": "Where are we in transcribing BN French? In: Proceedings of Eurospeech"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 45
                            }
                        ],
                        "text": "Both systems are described in more detail in Gauvain et al. (2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Where Are We In Transcribing BN French"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Where are we in transcribing BN French ?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 272
                            }
                        ],
                        "text": "In the domain of language modeling for continuous speech recognition, a large body of interesting work has been carried out and many new approaches have been developed, for instance structured language models (Chelba and Jelinek, 2000) or maximum entropy language models (Rosenfeld, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "All rights reserved."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2004 Rich Transcriptions Workshop"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 29,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 74,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Continuous-space-language-models-Schwenk/0fcc184b3b90405ec3ceafd6a4007c749df7c363?sort=total-citations"
}