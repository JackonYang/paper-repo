{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7406434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14d2d9b2e4c29fe105bfbb31f9749b60690303a7",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs. We encourage the system to find sparse features by using a Student-t distribution to model each filter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters, the system learns a topographic map in which the orientation, spatial frequency and location of the filters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters. Once the model has been learned it can be used as a prior to derive the \"iterated Wiener filter\" for the purpose of denoising images."
            },
            "slug": "Learning-Sparse-Topographic-Representations-with-of-Welling-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning Sparse Topographic Representations with Products of Student-t Distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs is proposed and used as a prior to derive the \"iterated Wiener filter\" for the purpose of denoising images."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633389"
                        ],
                        "name": "Yan Karklin",
                        "slug": "Yan-Karklin",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Karklin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Karklin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 0
                            }
                        ],
                        "text": "Karklin and Lewicki (2003, 2005) also propose a hierarchical extension to ICA that involves a second hidden layer of marginally independent sparsely active units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "I.e. when we take the Boltzmann distribution with the energies defined in equation 37 we recover the joints and marginals specified by  Karklin and Lewicki (2003, 2005) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 140
                            }
                        ],
                        "text": "(5.16)\nWhen we take the Boltzmann distribution with the energies defined in equation 5.16, we recover the joints and marginals specified by Karklin and Lewicki (2003, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Karklin and Lewicki (2003, 2005)  also propose a hierarchical extension to ICA that involves a second hidden layer of marginally independent sparsely active units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9596650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3222c9d0e3414970de58169a96528c495184189",
            "isKey": true,
            "numCitedBy": 128,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The theoretical principles that underlie the representation and computation of higher-order structure in natural images are poorly understood. Recently, there has been considerable interest in using information theoretic techniques, such as independent component analysis, to derive representations for natural images that are optimal in the sense of coding efficiency. Although these approaches have been successful in explaining properties of neural representations in the early visual pathway and visual cortex, because they are based on a linear model, the types of image structure that can be represented are very limited. Here, we present a hierarchical probabilistic model for learning higher-order statistical regularities in natural images. This non-linear model learns an efficient code that describes variations in the underlying probabilistic density. When applied to natural images the algorithm yields coarse-coded, sparse-distributed representations of abstract image properties such as object location, scale and texture. This model offers a novel description of higher-order image structure and could provide theoretical insight into the response properties and computational functions of lower level cortical visual areas."
            },
            "slug": "Learning-higher-order-structures-in-natural-images-Karklin-Lewicki",
            "title": {
                "fragments": [],
                "text": "Learning higher-order structures in natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A hierarchical probabilistic model for learning higher-order statistical regularities in natural images is presented and could provide theoretical insight into the response properties and computational functions of lower level cortical visual areas."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47590737"
                        ],
                        "name": "O. Schwartz",
                        "slug": "O.-Schwartz",
                        "structuredName": {
                            "firstName": "Odelia",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2454057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd2793662fe36fd79e34eca845c2d3fa10f7a3aa",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian scale mixture models offer a top-down description of signal generation that captures key bottom-up statistical characteristics of filter responses to images. However, the pattern of dependence among the filters for this class of models is prespecified. We propose a novel extension to the gaussian scale mixturemodel that learns the pattern of dependence from observed inputs and thereby induces a hierarchical representation of these inputs. Specifically, we propose that inputs are generated by gaussian variables (modeling local filter structure), multiplied by a mixer variable that is assigned probabilistically to each input from a set of possible mixers. We demonstrate inference of both components of the generative model, for synthesized data and for different classes of natural images, such as a generic ensemble and faces. For natural images, the mixer variable assignments show invariances resembling those of complex cells in visual cortex; the statistics of the gaussian components of the model are in accord with the outputs of divisive normalization models. We also show how our model helps interrelate a wide range of models of image statistics and cortical processing."
            },
            "slug": "Soft-Mixer-Assignment-in-a-Hierarchical-Generative-Schwartz-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Soft Mixer Assignment in a Hierarchical Generative Model of Natural Scene Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A novel extension to the gaussian scale mixture model that learns the pattern of dependence from observed inputs and thereby induces a hierarchical representation of these inputs, and shows how this model helps interrelate a wide range of models of image statistics and cortical processing."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 118
                            }
                        ],
                        "text": "We note that a similar manipulation is also applied by most practitioners dealing with overcomplete ICA models (e.g., Olshausen & Field, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We are able to recreate the success of ICA based models like, for example, Bell and Sejnowski (1995, 1997),  Olshausen and Field (1996, 1997) , Hoyer and Hyvarinen (2000), Hyvarinen et al. (2001), and Hyvarinen and Hoyer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 111
                            }
                        ],
                        "text": "We are able to recreate the success such of ICA-based models as, for example, Bell and Sejnowski (1995, 1997), Olshausen and Field (1996, 1997), Hoyer and Hyvarinen (2000), Hyvarinen, Hoyer, & Inki (2001), and Hyvarinen and Hoyer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We are able to recreate the success such of ICA-based models as, for example,  Bell and Sejnowski (1995, 1997) , Olshausen and Field (1996, 1997), Hoyer and Hyvarinen (2000), Hyvarinen, Hoyer, & Inki (2001), and Hyvarinen and Hoyer (2001).,As we will show in section 4, when trained on a large collection of natural image patches, the linear components { Ji } behave similarly to the learned basis functions in ICA and grow to resemble the well-known Gabor-like receptive fields of simple cells found in the visual cortex ( Bell & Sejnowski, 1997 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 273
                            }
                        ],
                        "text": "\u2026show in section 4, when trained on a large collection of natural image patches, the linear components { Ji } behave similarly to the learned basis functions in ICA and grow to resemble the well-known Gabor-like receptive fields of simple cells found in the visual cortex (Bell & Sejnowski, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 78
                            }
                        ],
                        "text": "We are able to recreate the success such of ICA-based models as, for example, Bell and Sejnowski (1995, 1997), Olshausen and Field (1996, 1997), Hoyer and Hyvarinen (2000), Hyvarinen, Hoyer, & Inki (2001), and Hyvarinen and Hoyer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6219133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca1d23be869380ac9e900578c601c2d1febcc0c9",
            "isKey": false,
            "numCitedBy": 2373,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-\u201cindependent-components\u201d-of-natural-scenes-are-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "The \u201cindependent components\u201d of natural scenes are edge filters"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The basic form for a GSM density on a variable, g, can be given as follows ( Wainwright and Simoncelli, 2000 ),"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 146
                            }
                        ],
                        "text": "\u2026& Mallows, 1974;\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/18/2/381/816474/089976606775093936.pdf by guest on 06 Septem ber 2021\nWainwright & Simoncelli, 2000; Wainwright, Simoncelli, & Willsky, 2000) with a particular (complicated) form of scaling function.13\nThe basic form\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We can consider the complete version of our model as a Gaussian scale mixture (Andrews and Mallows, 1974;  Wainwright and Simoncelli, 2000;  Wainwright et al., 2000) with a particular (complicated) form of scaling function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 143
                            }
                        ],
                        "text": "\u20262000) with a particular (complicated) form of scaling function.13\nThe basic form for a GSM density on a variable, g, can be given as follows (Wainwright & Simoncelli, 2000),\npGSM(g) = \u222b \u221e\n\u2212\u221e\n1\n(2\u03c0 ) N 2 |cQ| 12\nexp ( \u2212g T (cQ)\u22121g\n2\n) \u03c6c(c)dc, (5.1)\nwhere c is a nonnegative scalar variate\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1260875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092d70604a76fa691503cdc41449ec1ddc87630f",
            "isKey": true,
            "numCitedBy": 531,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The statistics of photographic images, when represented using multiscale (wavelet) bases, exhibit two striking types of non-Gaussian behavior. First, the marginal densities of the coefficients have extended heavy tails. Second, the joint densities exhibit variance dependencies not captured by second-order models. We examine properties of the class of Gaussian scale mixtures, and show that these densities can accurately characterize both the marginal and joint distributions of natural image wavelet coefficients. This class of model suggests a Markov structure, in which wavelet coefficients are linked by hidden scaling variables corresponding to local image structure. We derive an estimator for these hidden variables, and show that a nonlinear \"normalization\" procedure can be used to Gaussianize the coefficients."
            },
            "slug": "Scale-Mixtures-of-Gaussians-and-the-Statistics-of-Wainwright-Simoncelli",
            "title": {
                "fragments": [],
                "text": "Scale Mixtures of Gaussians and the Statistics of Natural Images"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "an analogous role to the tree-structured cascade process in  Wainwright et al. (2000) , and determine the correlations between the different scaling coefficients."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 142
                            }
                        ],
                        "text": "This is the distribution that results if we draw c from \u03c6c(c) and a variable v from a multidimensional gaussian NV(0, Q) and then take g = \u221acv.\nWainwright et al. (2000) discuss a more sophisticated model in which the distributions of coefficients in a wavelet decomposition for images are described\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We can consider the complete version of our model as a Gaussian scale mixture (Andrews and Mallows, 1974; Wainwright and Simoncelli, 2000;  Wainwright et al., 2000 ) with a particular (complicated) form of scaling function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, a notable difference in this respect is that  Wainwright et al. (2000)  assume a fixed tree structure for the dependencies whereas our model is more flexible in that the interactions through the W parameters can be learned."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " Wainwright et al. (2000)  discuss a more sophisticated model in which the distributions of coefficients in a wavelet decomposition for images are described by a GSM which has a separate scaling variable, ci, for each coefficient."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 109
                            }
                        ],
                        "text": "The neighborhoods defined by W in our model play an analogous role to the tree-structured cascade process in Wainwright et al. (2000) and determine the correlations between the different scaling coefficients."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12537377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f981f7de189bc43f8c697e2cbff2912dcbb7040e",
            "isKey": true,
            "numCitedBy": 18,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiresolution representations play an important role in image processing and computer vision, as well as in modeling stochastic processes. We have developed a semi-parametric class of non-Gaussian multiscale statistical processes defined by random cascades on wavelet trees. This model class is rich enough to accurately capture the remarkably regular non-Gaussian features of natural images, but sufficiently structured to permit estimation of the underlying state variables. We showed that our models accurately fit both the marginal and joint histograms of wavelet coefficients from natural images. We developed a Newton-like method for exact MAP state estimation that exploits fast algorithms for tree estimation, and hence is very efficient. Applications of this algorithm to denoising of both 1D signals and natural images were presented. The GSM-tree model class is related to a number of previous approaches to image coding and denoising."
            },
            "slug": "Random-cascades-of-Gaussian-scale-mixtures-and-use-Wainwright-Simoncelli",
            "title": {
                "fragments": [],
                "text": "Random cascades of Gaussian scale mixtures and their use in modeling natural images with application to denoising"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A Newton-like method for exact MAP state estimation that exploits fast algorithms for tree estimation, and hence is very efficient and related to a number of previous approaches to image coding and denoising are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 26
                            }
                        ],
                        "text": "(For details, we refer to Teh et al., 2003.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 155
                            }
                        ],
                        "text": "A natural way to interpret the differences between directed models (and in particular ICA models) and PoE models was provided in Hinton and Teh (2001) and Teh et al. (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 98
                            }
                        ],
                        "text": "For further details on contrastive divergence learning, we refer to the literature (Hinton, 2002; Teh et al., 2003; Yuille, 2004; Carreira-Perpinan & Hinton, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52865368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b95799a25def71b100bd12e7ebb32cbcee6590bf",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces."
            },
            "slug": "Energy-Based-Models-for-Sparse-Overcomplete-Teh-Welling",
            "title": {
                "fragments": [],
                "text": "Energy-Based Models for Sparse Overcomplete Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A new way of extending independent components analysis (ICA) to overcomplete representations that defines features as deterministic (linear) functions of the inputs and assigns energies to the features through the Boltzmann distribution."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211061"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We are able to recreate the success of ICA based models like, for example, Bell and Sejnowski (1995, 1997), Olshausen and Field (1996, 1997), Hoyer and Hyvarinen (2000), Hyvarinen et al. (2001), and  Hyvarinen and Hoyer (2001) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The results obtained by  Hyvarinen and Hoyer (2001) ; Hyvarinen et al. (2001) are very similar to those presented here in section 4. These authors also noted the similarity between elements of their model and the response properties of simple and complex cells in V1."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 210
                            }
                        ],
                        "text": "We are able to recreate the success such of ICA-based models as, for example, Bell and Sejnowski (1995, 1997), Olshausen and Field (1996, 1997), Hoyer and Hyvarinen (2000), Hyvarinen, Hoyer, & Inki (2001), and Hyvarinen and Hoyer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 157
                            }
                        ],
                        "text": "\u2026by guest on 06 Septem ber 2021\nwhere the form of the scalar function G is given by\nG(\u03c4 ) = log \u222b\n1\u221a 2\u03c0\nexp (\n1 2\nt\u03c4 ) pt(t) \u221a Hii dt. (5.8)\nThe results obtained by Hyvarinen and Hoyer (2001) and Hyvarinen et al. (2001) are very similar to those presented here in section 4."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6302770,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7c032d555a9d7096f7bb88441f10e33d3302d5be",
            "isKey": true,
            "numCitedBy": 291,
            "numCiting": 130,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-two-layer-sparse-coding-model-learns-simple-and-Hyv\u00e4rinen-Hoyer",
            "title": {
                "fragments": [],
                "text": "A two-layer sparse coding model learns simple and complex cell receptive fields and topography from natural images"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 200
                            }
                        ],
                        "text": "This phenomenon can be neatly demonstrated through the use of bow-tie plots, in which the conditional histogram of one filter output is plotted given the output value of a different filter (e.g., see Simoncelli, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 96
                            }
                        ],
                        "text": "This is the opposite strategy as the one used in, for instance, Portilla, Strela, Wainwright, & Simoncelli (2003), where the wavelet transform is fixed and the interactions between wavelet coefficients are modeled."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14163010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4646e76fe9b59e5a3ba46e388c63a268740acf89",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a parametric statistical model for visual images in the wavelet transform domain. We characterize the joint densities of coefficient magnitudes at adjacent spatial locations, adjacent orientations, and adjacent spatial scales. The model accounts for the statistics of a wide variety of visual images. As a demonstration of this, we used the model to design a progressive image encoder with state-of-the-art rate-distortion performance. We also show promising examples of image restoration and texture synthesis."
            },
            "slug": "Statistical-models-for-images:-compression,-and-Simoncelli",
            "title": {
                "fragments": [],
                "text": "Statistical models for images: compression, restoration and synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A parametric statistical model for visual images in the wavelet transform domain is presented and the joint densities of coefficient magnitudes at adjacent spatial locations, adjacent orientations, and adjacent spatial scales are characterized."
            },
            "venue": {
                "fragments": [],
                "text": "Conference Record of the Thirty-First Asilomar Conference on Signals, Systems and Computers (Cat. No.97CB36136)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We are able to recreate the success such of ICA-based models as, for example, Bell and Sejnowski (1995, 1997),  Olshausen and Field (1996, 1997) , Hoyer and Hyvarinen (2000), Hyvarinen, Hoyer, & Inki (2001), and Hyvarinen and Hoyer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 111
                            }
                        ],
                        "text": "We are able to recreate the success such of ICA-based models as, for example, Bell and Sejnowski (1995, 1997), Olshausen and Field (1996, 1997), Hoyer and Hyvarinen (2000), Hyvarinen, Hoyer, & Inki (2001), and Hyvarinen and Hoyer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14208692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2805537bec87a6177037b18f9a3a9d3f1038867b",
            "isKey": false,
            "numCitedBy": 3574,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33523605"
                        ],
                        "name": "J. Portilla",
                        "slug": "J.-Portilla",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Portilla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Portilla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051519"
                        ],
                        "name": "V. Strela",
                        "slug": "V.-Strela",
                        "structuredName": {
                            "firstName": "Vasily",
                            "lastName": "Strela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Strela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is the opposite strategy as the one used in, for instance,  Portilla et al. (2003)  where the wavelet transform is fixed and the interactions between wavelet coefficients are modelled."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52808855,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "85791491919e1f740f0e882366046acbe56fb14c",
            "isKey": false,
            "numCitedBy": 2403,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for removing noise from digital images, based on a statistical model of the coefficients of an overcomplete multiscale oriented basis. Neighborhoods of coefficients at adjacent positions and scales are modeled as the product of two independent random variables: a Gaussian vector and a hidden positive scalar multiplier. The latter modulates the local variance of the coefficients in the neighborhood, and is thus able to account for the empirically observed correlation between the coefficient amplitudes. Under this model, the Bayesian least squares estimate of each coefficient reduces to a weighted average of the local linear estimates over all possible values of the hidden multiplier variable. We demonstrate through simulations with images contaminated by additive white Gaussian noise that the performance of this method substantially surpasses that of previously published methods, both visually and in terms of mean squared error."
            },
            "slug": "Image-denoising-using-scale-mixtures-of-Gaussians-Portilla-Strela",
            "title": {
                "fragments": [],
                "text": "Image denoising using scale mixtures of Gaussians in the wavelet domain"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The performance of this method for removing noise from digital images substantially surpasses that of previously published methods, both visually and in terms of mean squared error."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281877"
                        ],
                        "name": "J. Atick",
                        "slug": "J.-Atick",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Atick",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Atick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513847"
                        ],
                        "name": "A. Redlich",
                        "slug": "A.-Redlich",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Redlich",
                            "middleNames": [
                                "Norman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Redlich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 1
                            }
                        ],
                        "text": "(Atick & Redlich, 1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17515861,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "841cd4a6cac86fa0cfb0e8542eac5ed164f23f50",
            "isKey": false,
            "numCitedBy": 711,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "By examining the experimental data on the statistical properties of natural scenes together with (retinal) contrast sensitivity data, we arrive at a first principle, theoretical hypothesis for the purpose of retinal processing and its relationship to an animal's environment. We argue that the retinal goal is to transform the visual input as much as possible into a statistically independent basis as the first step in creating a redundancy reduced representation in the cortex, as suggested by Barlow. The extent of this whitening of the input is limited, however, by the need to suppress input noise. Our explicit theoretical solutions for the retinal filters also show a simple dependence on mean stimulus luminance: they predict an approximate Weber law at low spatial frequencies and a De Vries-Rose law at high frequencies. Assuming that the dominant source of noise is quantum, we generate a family of contrast sensitivity curves as a function of mean luminance. This family is compared to psychophysical data."
            },
            "slug": "What-Does-the-Retina-Know-about-Natural-Scenes-Atick-Redlich",
            "title": {
                "fragments": [],
                "text": "What Does the Retina Know about Natural Scenes?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that the retinal goal is to transform the visual input as much as possible into a statistically independent basis as the first step in creating a redundancy reduced representation in the cortex, as suggested by Barlow."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, in the overcomplete setting (more latent variables than observables) product of experts models are essentially different from overcomplete ICA models ( Lewicki and Sejnowski, 2000 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 261
                            }
                        ],
                        "text": "\u2026setting (more latent variables than\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/18/2/381/816474/089976606775093936.pdf by guest on 06 Septem ber 2021\nobservables), product of experts models are essentially different from overcomplete ICA models (Lewicki & Sejnowski, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6254191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42d906c733f273109c0ed716a5ef6e2a379beb26",
            "isKey": false,
            "numCitedBy": 1255,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "In an overcomplete basis, the number of basis vectors is greater than the dimensionality of the input, and the representation of an input is not a unique combination of basis vectors. Overcomplete representations have been advocated because they have greater robustness in the presence of noise, can be sparser, and can have greater flexibility in matching structure in the data. Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary visual cortex. Previous work has focused on finding the best representation of a signal using a fixed overcomplete basis (or dictionary). We present an algorithm for learning an overcomplete basis by viewing it as probabilistic model of the observed data. We show that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency. This can be viewed as a generalization of the technique of independent component analysis and provides a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
            },
            "slug": "Learning-Overcomplete-Representations-Lewicki-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning Overcomplete Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency and provide a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39092098"
                        ],
                        "name": "Y. Wu",
                        "slug": "Y.-Wu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Wu",
                            "middleNames": [
                                "Nian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2171181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a5f775f7490f410692bb224ff021da97d5e0ddc",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a statistical theory for texture modeling. This theory combines filtering theory and Markov random field modeling through the maximum entropy principle, and interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view. Our theory characterizes the ensemble of images I with the same texture appearance by a probability distribution f(I) on a random field, and the objective of texture modeling is to make inference about f(I), given a set of observed texture examples.In our theory, texture modeling consists of two steps. (1) A set of filters is selected from a general filter bank to capture features of the texture, these filters are applied to observed texture images, and the histograms of the filtered images are extracted. These histograms are estimates of the marginal distributions of f( I). This step is called feature extraction. (2) The maximum entropy principle is employed to derive a distribution p(I), which is restricted to have the same marginal distributions as those in (1). This p(I) is considered as an estimate of f( I). This step is called feature fusion. A stepwise algorithm is proposed to choose filters from a general filter bank. The resulting model, called FRAME (Filters, Random fields And Maximum Entropy), is a Markov random field (MRF) model, but with a much enriched vocabulary and hence much stronger descriptive ability than the previous MRF models used for texture modeling. Gibbs sampler is adopted to synthesize texture images by drawing typical samples from p(I), thus the model is verified by seeing whether the synthesized texture images have similar visual appearances to the texture images being modeled. Experiments on a variety of 1D and 2D textures are described to illustrate our theory and to show the performance of our algorithms. These experiments demonstrate that many textures which are previously considered as from different categories can be modeled and synthesized in a common framework."
            },
            "slug": "Filters,-Random-Fields-and-Maximum-Entropy-(FRAME):-Zhu-Wu",
            "title": {
                "fragments": [],
                "text": "Filters, Random Fields and Maximum Entropy (FRAME): Towards a Unified Theory for Texture Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The resulting model, called FRAME (Filters, Random fields And Maximum Entropy), is a Markov random field (MRF) model, but with a much enriched vocabulary and hence much stronger descriptive ability than the previous MRF models used for texture modeling."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13220267,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "064dd12847dcaa69ae758c93bfd026f54d575599",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Under-complete models, which derive lower dimensional representations of input data, are valuable in domains in which the number of input dimensions is very large, such as data consisting of a temporal sequence of images. This paper presents the under-complete product of experts (UPoE), where each expert models a one-dimensional projection of the data. Maximum-likelihood learning rules for this model constitute a tractable and exact algorithm for learning under-complete independent components. The learning rules for this model coincide with approximate learning rules proposed earlier for under-complete independent component analysis (UICA) models. This paper also derives an efficient sequential learning algorithm from this model and discusses its relationship to sequential independent component analysis (ICA), projection pursuit density estimation, and feature induction algorithms for additive random field models. This paper demonstrates the efficacy of these novel algorithms on high-dimensional continuous datasets."
            },
            "slug": "Probabilistic-sequential-independent-components-Welling-Zemel",
            "title": {
                "fragments": [],
                "text": "Probabilistic sequential independent components analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents the under-complete product of experts (UPoE), where each expert models a one-dimensional projection of the data and derives an efficient sequential learning algorithm from this model and discusses its relationship to sequential independent component analysis (ICA), projection pursuit density estimation, and feature induction algorithms for additive random field models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211061"
                        ],
                        "name": "P. Hoyer",
                        "slug": "P.-Hoyer",
                        "structuredName": {
                            "firstName": "Patrik",
                            "lastName": "Hoyer",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 145
                            }
                        ],
                        "text": "We are able to recreate the success such of ICA-based models as, for example, Bell and Sejnowski (1995, 1997), Olshausen and Field (1996, 1997), Hoyer and Hyvarinen (2000), Hyvarinen, Hoyer, & Inki (2001), and Hyvarinen and Hoyer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We are able to recreate the success of ICA based models like, for example, Bell and Sejnowski (1995, 1997), Olshausen and Field (1996, 1997),  Hoyer and Hyvarinen (2000) , Hyvarinen et al. (2001), and Hyvarinen and Hoyer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 959667,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "032bb6ea9b99b3adb9b7f0e45a62c9f37f9e1bc5",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has shown that independent component analysis (ICA) applied to feature extraction from natural image data yields features resembling Gabor functions and simple-cell receptive fields. This article considers the effects of including chromatic and stereo information. The inclusion of colour leads to features divided into separate red/green, blue/yellow, and bright/dark channels. Stereo image data, on the other hand, leads to binocular receptive fields which are tuned to various disparities. The similarities between these results and the observed properties of simple cells in the primary visual cortex are further evidence for the hypothesis that visual cortical neurons perform some type of redundancy reduction, which was one of the original motivations for ICA in the first place. In addition, ICA provides a principled method for feature extraction from colour and stereo images; such features could be used in image processing operations such as denoising and compression, as well as in pattern recognition."
            },
            "slug": "Independent-component-analysis-applied-to-feature-Hoyer-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Independent component analysis applied to feature extraction from colour and stereo images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The similarities between these results and the observed properties of simple cells in the primary visual cortex are further evidence for the hypothesis that visual cortical neurons perform some type of redundancy reduction, which was one of the original motivations for ICA in the first place."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474176"
                        ],
                        "name": "J. H. Hateren",
                        "slug": "J.-H.-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hateren",
                            "middleNames": [
                                "H.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46498936"
                        ],
                        "name": "A. Schaaf",
                        "slug": "A.-Schaaf",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "Schaaf",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schaaf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For reference, similar plots for linear spatial receptive fields measured in vivo are given in Ringach (2002) and  van Hateren and van der Schaaf (1998) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15666050,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1",
            "isKey": false,
            "numCitedBy": 802,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed."
            },
            "slug": "Independent-component-filters-of-natural-images-in-Hateren-Schaaf",
            "title": {
                "fragments": [],
                "text": "Independent component filters of natural images compared with simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis on a large set of natural images: there is no match, however, in calculated and measured distributions for the peak of the spatial frequency response."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3741160"
                        ],
                        "name": "J. V. van Hateren",
                        "slug": "J.-V.-van-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "van Hateren",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. van Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51922762"
                        ],
                        "name": "A. van der Schaaf",
                        "slug": "A.-van-der-Schaaf",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "van der Schaaf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. van der Schaaf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 114
                            }
                        ],
                        "text": "For reference, similar plots for linear spatial receptive fields measured in vivo are given in Ringach (2002) and van Hateren and van der Schaaf (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1789554,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "117175a54263cc2ae693fe82c9b3fe0553931cd8",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed."
            },
            "slug": "Independent-component-filters-of-natural-images-in-Hateren-Schaaf",
            "title": {
                "fragments": [],
                "text": "Independent component filters of natural images compared with simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis on a large set of natural images: there is no match, however, in calculated and measured distributions for the peak of the spatial frequency response."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Royal Society of London. Series B: Biological Sciences"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633389"
                        ],
                        "name": "Yan Karklin",
                        "slug": "Yan-Karklin",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Karklin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Karklin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 0
                            }
                        ],
                        "text": "Karklin and Lewicki (2003, 2005) also propose a hierarchical extension to ICA that involves a second hidden layer of marginally independent sparsely active units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 140
                            }
                        ],
                        "text": "(5.16)\nWhen we take the Boltzmann distribution with the energies defined in equation 5.16, we recover the joints and marginals specified by Karklin and Lewicki (2003, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 490453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68f58d5b4b4797955b5965f10d424764bd6ee839",
            "isKey": true,
            "numCitedBy": 173,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Capturing statistical regularities in complex, high-dimensional data is an important problem in machine learning and signal processing. Models such as principal component analysis (PCA) and independent component analysis (ICA) make few assumptions about the structure in the data and have good scaling properties, but they are limited to representing linear statistical regularities and assume that the distribution of the data is stationary. For many natural, complex signals, the latent variables often exhibit residual dependencies as well as nonstationary statistics. Here we present a hierarchical Bayesian model that is able to capture higher-order nonlinear structure and represent nonstationary data distributions. The model is a generalization of ICA in which the basis function coefficients are no longer assumed to be independent; instead, the dependencies in their magnitudes are captured by a set of density components. Each density component describes a common pattern of deviation from the marginal density of the pattern ensemble; in different combinations, they can describe nonstationary distributions. Adapting the model to image or audio data yields a nonlinear, distributed code for higher-order statistical regularities that reflect more abstract, invariant properties of the signal."
            },
            "slug": "A-Hierarchical-Bayesian-Model-for-Learning-in-Karklin-Lewicki",
            "title": {
                "fragments": [],
                "text": "A Hierarchical Bayesian Model for Learning Nonlinear Statistical Regularities in Nonstationary Natural Signals"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A hierarchical Bayesian model is presented that is able to capture higher-order nonlinear structure and represent nonstationary data distributions and Adapting the model to image or audio data yields a nonlinear, distributed code for higher- order statistical regularities that reflect more abstract, invariant properties of the signal."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145369890"
                        ],
                        "name": "Richard E. Turner",
                        "slug": "Richard-E.-Turner",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Turner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard E. Turner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2283363"
                        ],
                        "name": "M. Sahani",
                        "slug": "M.-Sahani",
                        "structuredName": {
                            "firstName": "Maneesh",
                            "lastName": "Sahani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sahani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 140248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c0787a74d43dede8d35fb3609aa87f958fbe6fc",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The brain extracts useful features from a maelstrom of sensory information, and a fundamental goal of theoretical neuroscience is to work out how it does so. One proposed feature extraction strategy is motivated by the observation that the meaning of sensory data, such as the identity of a moving visual object, is often more persistent than the activation of any single sensory receptor. This notion is embodied in the slow feature analysis (SFA) algorithm, which uses slowness as a heuristic by which to extract semantic information from multidimensional time series. Here, we develop a probabilistic interpretation of this algorithm, showing that inference and learning in the limiting case of a suitable probabilistic model yield exactly the results of SFA. Similar equivalences have proved useful in interpreting and extending comparable algorithms such as independent component analysis. For SFA, we use the equivalent probabilistic model as a conceptual springboard with which to motivate several novel extensions to the algorithm."
            },
            "slug": "A-Maximum-Likelihood-Interpretation-for-Slow-Turner-Sahani",
            "title": {
                "fragments": [],
                "text": "A Maximum-Likelihood Interpretation for Slow Feature Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A probabilistic interpretation of the slow feature analysis (SFA) algorithm is developed, showing that inference and learning in the limiting case of a suitable Probabilistic model yield exactly the results of SFA."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3355277"
                        ],
                        "name": "D. Ringach",
                        "slug": "D.-Ringach",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Ringach",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ringach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 758,
                                "start": 95
                            }
                        ],
                        "text": "For reference, similar plots for linear spatial receptive fields measured in vivo are given in Ringach (2002) and van Hateren and van der Schaaf (1998). The plots are all reasonable qualitative matches to those shown for the \u201creal\u201d V1 receptive fields as shown, for instance, in Ringach (2002). They also help to indicate the effects of representational overcompleteness. With increasing overcompleteness, the coverage in the spaces of location, spatial frequency, and orientation becomes denser and more uniform, while at the same time, the distribution of receptive fields shapes remains unchanged. Further, the more overcomplete models give better coverage in lower spatial frequencies that are not directly represented in complete models. Ringach (2002) reports that the distribution of shapes from ICA or sparse coding can be a poor fit to the data from real cells, the main problem being that there are too few cells near the origin of the plot, which corresponds roughly to cells with smaller aspect ratios and small numbers of cycles in their receptive fields."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Ringach (2002) reports that the distribution of shapes from ICA or sparse coding can be a poor fit to the data from real cells, the main problem being that there are too few cells near the origin of the plot, which corresponds roughly to cells with smaller aspect ratios and small numbers of cycles\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 95
                            }
                        ],
                        "text": "For reference, similar plots for linear spatial receptive fields measured in vivo are given in Ringach (2002) and van Hateren and van der Schaaf (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 67
                            }
                        ],
                        "text": "(F) For comparison, we include data from real macaque experiments (Ringach, 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 124
                            }
                        ],
                        "text": "The plots are all reasonable qualitative matches to those shown for the \u201creal\u201d V1 receptive fields as shown, for instance, in Ringach (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 61
                            }
                        ],
                        "text": "(E) A plot of \u201cnormalized width\u201d versus \u201cnormalized length\u201d (cf. Ringach, 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 95
                            }
                        ],
                        "text": "For reference, similar plots for linear spatial receptive fields measured in vivo are given in Ringach (2002) and van Hateren and van der Schaaf (1998). The plots are all reasonable qualitative matches to those shown for the \u201creal\u201d V1 receptive fields as shown, for instance, in Ringach (2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "A binary product of experts model was introduced under the name harmonium in Smolensky (1986). A learning algorithm based on projection pursuit was proposed in Freund and Haussler (1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 95
                            }
                        ],
                        "text": "For reference, similar plots for linear spatial receptive fields measured in vivo are given in Ringach (2002) and van Hateren and van der Schaaf (1998). The plots are all reasonable qualitative matches to those shown for the \u201creal\u201d V1 receptive fields as shown, for instance, in Ringach (2002). They also help to indicate the effects of representational overcompleteness."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11216620,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "44c6c861bb1148fde2350c6397fbe5925fafc923",
            "isKey": true,
            "numCitedBy": 299,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "I present measurements of the spatial structure of simple-cell receptive fields in macaque primary visual cortex (area V1). Similar to previous findings in cat area 17, the spatial profile of simple-cell receptive fields in the macaque is well described by two-dimensional Gabor functions. A population analysis reveals that the distribution of spatial profiles in primary visual cortex lies approximately on a one-parameter family of filter shapes. Surprisingly, the receptive fields cluster into even- and odd-symmetry classes with a tendency for neurons that are well tuned in orientation and spatial frequency to have odd-symmetric receptive fields. The filter shapes predicted by two recent theories of simple-cell receptive field function, independent component analysis and sparse coding, are compared with the data. Both theories predict receptive fields with a larger number of subfields than observed in the experimental data. In addition, these theories do not generate receptive fields that are broadly tuned in orientation and low-pass in spatial frequency, which are commonly seen in monkey V1. The implications of these results for our understanding of image coding and representation in primary visual cortex are discussed."
            },
            "slug": "Spatial-structure-and-symmetry-of-simple-cell-in-Ringach",
            "title": {
                "fragments": [],
                "text": "Spatial structure and symmetry of simple-cell receptive fields in macaque primary visual cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Measurements of the spatial structure of simple-cell receptive fields in macaque primary visual cortex show a tendency for neurons that are well tuned in orientation and spatial frequency to have odd-symmetric receptive fields, and a population analysis reveals that the distribution of spatial profiles inPrimary visual cortex lies approximately on a one-parameter family of filter shapes."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 65
                            }
                        ],
                        "text": "A learning algorithm based on projection pursuit was proposed in Freund and Haussler (1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 65
                            }
                        ],
                        "text": "A learning algorithm based on projection pursuit was proposed in Freund and Haussler (1992). In addition to binary models (Hinton, 2002), the gaussian case been studied (Williams, Agakov, & Felderhof, 2001; Marks & Movellan, 2001; Williams & Agakov, 2002; Welling, Agakov, & Williams, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 64
                            }
                        ],
                        "text": "PoT models were touched on briefly in Teh, Welling, Osindero, & Hinton (2003), and in this letter, we present the basic formulation in more detail, provide hierarchical and topographic extensions, and give an efficient learning algorithm employing auxiliary variables and Gibbs sampling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13456135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "939d584316be99e2db3fec3fbf7d71f22a477f67",
            "isKey": true,
            "numCitedBy": 340,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model can be represented by a particular type of Boltzmann machine with a bipartite graph structure that we call the combination machine. This machine is closely related to the Harmonium model defined by Smolensky. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters of the model. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images."
            },
            "slug": "Unsupervised-Learning-of-Distributions-of-Binary-Freund-Haussler",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Distributions of Binary Vectors Using 2-Layer Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "It is shown that arbitrary distributions of binary vectors can be approximated by the combination model and shown how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6433677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98a3c337a435553add253eb1af71eb9fc998bf5e",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a way of modeling high-dimensional data vectors by using an unsupervised, nonlinear, multilayer neural network in which the activity of each neuron-like unit makes an additive contribution to a global energy score that indicates how surprised the network is by the data vector. The connection weights that determine how the activity of each unit depends on the activities in earlier layers are learned by minimizing the energy assigned to data vectors that are actually observed and maximizing the energy assigned to \"confabulations\" that are generated by perturbing an observed data vector in a direction that decreases its energy under the current model."
            },
            "slug": "Unsupervised-Discovery-of-Nonlinear-Structure-Using-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "Unsupervised Discovery of Nonlinear Structure Using Contrastive Backpropagation"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A way of modeling high-dimensional data vectors by using an unsupervised, nonlinear, multilayer neural network in which the activity of each neuron-like unit makes an additive contribution to a global energy score that indicates how surprised the network is by the data vector."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34749896"
                        ],
                        "name": "Tim K. Marks",
                        "slug": "Tim-K.-Marks",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Marks",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim K. Marks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741200"
                        ],
                        "name": "J. Movellan",
                        "slug": "J.-Movellan",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Movellan",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movellan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 114
                            }
                        ],
                        "text": "In addition to binary models (Hinton, 2002), the gaussian case been studied (Williams, Agakov, & Felderhof, 2001; Marks & Movellan, 2001; Williams & Agakov, 2002; Welling, Agakov, & Williams, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "osindero@cs.toronto.edu"
                    },
                    "intents": []
                }
            ],
            "corpusId": 208899622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ba5455aaeb6ace02ea3ed47801d9ccb38513347",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Hinton (in press) recently proposed a learning algorithm called contrastive divergence learning for a class of probabilistic models called product of experts (PoE). Whereas in standard mixture models the \u201cbeliefs\u201d of individual experts are averaged, in PoEs the \u201cbeliefs\u201d are multiplied together and then renormalized. One advantage of this approach is that the combined beliefs can be much sharper than the individual beliefs of each expert. It has been shown that a restricted version of the Boltzmann machine, in which there are no lateral connections between hidden units or between observation units, is a PoE. In this paper we generalize these results to diffusion networks, a continuous-time, continuous-state version of the Boltzmann machine. We show that when the unit activation functions are linear, this PoE architecture is equivalent to a factor analyzer. This result suggests novel non-linear generalizations of factor analysis and independent component analysis that could be implemented using interactive neural circuitry."
            },
            "slug": "Diffusion-Networks,-Products-of-Experts,-and-Factor-Marks-Movellan",
            "title": {
                "fragments": [],
                "text": "Diffusion Networks, Products of Experts, and Factor Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that when the unit activation functions are linear, this PoE architecture is equivalent to a factor analyzer, which suggests novel non-linear generalizations of factor analysis and independent component analysis that could be implemented using interactive neural circuitry."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3288675"
                        ],
                        "name": "F. Agakov",
                        "slug": "F.-Agakov",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Agakov",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Agakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2410998"
                        ],
                        "name": "Stephen N. Felderhof",
                        "slug": "Stephen-N.-Felderhof",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Felderhof",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen N. Felderhof"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "osindero@cs.toronto.edu"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15646913,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f5066fc1c1174d69184fd7c7bcea3f4a3a8ede3",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Below we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of 1-factor PPCA models and (3) a products of experts construction for an AR(1) process."
            },
            "slug": "Products-of-Gaussians-Williams-Agakov",
            "title": {
                "fragments": [],
                "text": "Products of Gaussians"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "PoE models in which each expert is a Gaussian are considered, which give rise to probabilistic Minor Components Analysis, products of 1-factor PPCA models and a products of experts construction for an AR(1) process."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49914012"
                        ],
                        "name": "A. Hsu",
                        "slug": "A.-Hsu",
                        "structuredName": {
                            "firstName": "Anne",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8859935,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "1857771eebe51a0d1e49286d8f175ac13c989bc5",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-unsupervised-learning-model-of-neural-in-kittens-Hsu-Dayan",
            "title": {
                "fragments": [],
                "text": "An unsupervised learning model of neural plasticity: Orientation selectivity in goggle-reared kittens"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 251
                            }
                        ],
                        "text": "D ow nloaded from http://direct.m it.edu/neco/article-pdf/18/2/381/816474/089976606775093936.pdf by guest on 06 Septem ber 2021\ndivergence as outlined in section 3.2, and the ICA model was trained using the exact gradient of the log likelihood (as in Bell & Sejnowski, 1995 for instance)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 102
                            }
                        ],
                        "text": "In the complete case, these gradients turn out to be of the same form as the update rules proposed in Bell and Sejnowski (1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 78
                            }
                        ],
                        "text": "We are able to recreate the success such of ICA-based models as, for example, Bell and Sejnowski (1995, 1997), Olshausen and Field (1996, 1997), Hoyer and Hyvarinen (2000), Hyvarinen, Hoyer, & Inki (2001), and Hyvarinen and Hoyer (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 78
                            }
                        ],
                        "text": "We are able to recreate the success such of ICA-based models as, for example, Bell and Sejnowski (1995, 1997), Olshausen and Field (1996, 1997), Hoyer and Hyvarinen (2000), Hyvarinen, Hoyer, & Inki (2001), and Hyvarinen and Hoyer (2001). Our model provides computationally motivated accounts for the form of simple cell and complex cell receptive fields, as well as for the basic layout of cortical topographic maps for location, orientation, spatial frequency, and spatial phase."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 186
                            }
                        ],
                        "text": "In the case of an equal number of observables, {xi }, and latent variables, {yi } (the so-called complete representation), the PoT model is formally equivalent to square, noiseless ICA (Bell & Sejnowski, 1995) with Student-t priors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 102
                            }
                        ],
                        "text": "In the complete case, these gradients turn out to be of the same form as the update rules proposed in Bell and Sejnowski (1995). However, the gradients for the parameters W and \u03b1 are much harder to compute."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": true,
            "numCitedBy": 8758,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 42
                            }
                        ],
                        "text": "However, contrastive divergence learning (Hinton, 2002) (see section 3.2) has opened the way to apply these models to large-scale applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "Section 3 describes how to learn within the PoE framework using the contrastive divergence (CD) algorithm (Hinton, 2002) (with the appendix providing the background material for running the necessary Markov chain Monte Carlo sampling)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 175
                            }
                        ],
                        "text": "In product models, the product distribution is typically much sharper than the distributions of the individual experts1, which is a major advantage for high-dimensional data (Hinton, 2002; Welling, Zemel, & Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 30
                            }
                        ],
                        "text": "In addition to binary models (Hinton, 2002), the gaussian case been studied (Williams, Agakov, & Felderhof, 2001; Marks & Movellan, 2001; Williams & Agakov, 2002; Welling, Agakov, & Williams, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 60
                            }
                        ],
                        "text": "The basic model we study here is a form of PoE suggested by Hinton and Teh (2001), where the experts are given by generalized Student-t distributions:"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 84
                            }
                        ],
                        "text": "For further details on contrastive divergence learning, we refer to the literature (Hinton, 2002; Teh et al., 2003; Yuille, 2004; Carreira-Perpinan & Hinton, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 51
                            }
                        ],
                        "text": "Product of expert models (PoEs) were introduced in Hinton (2002) as an alternative method of combining expert models into one joint model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 41
                            }
                        ],
                        "text": "However, contrastive divergence learning (Hinton, 2002) (see section 3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 25
                            }
                        ],
                        "text": "7 Contrastive divergence (Hinton, 2002), replaces the MCMC samples in these Monte Carlo estimates with samples from brief MCMC runs, which were initialized at the data cases."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "\u2026for every step of learning, and a fairly large number of samples may be needed to reduce the variance in the estimates.7 Contrastive divergence (Hinton, 2002), replaces the MCMC samples in these Monte Carlo estimates with samples from brief MCMC runs, which were initialized at the data cases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 129
                            }
                        ],
                        "text": "A natural way to interpret the differences between directed models (and in particular ICA models) and PoE models was provided in Hinton and Teh (2001) and Teh et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 215
                            }
                        ],
                        "text": "compute gradients; however, in the general case of overcomplete or hierarchical PoT\u2019s, we are required to employ an approximation scheme, and the preferred method in this article will be contrastive divergence (CD) (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 65
                            }
                        ],
                        "text": "We will begin with a brief overview of product of expert models (Hinton, 2002), before presenting the basic product of Student-t model (Welling, Hinton, & Osindero, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 129
                            }
                        ],
                        "text": "A natural way to interpret the differences between directed models (and in particular ICA models) and PoE models was provided in Hinton and Teh (2001) and Teh et al. (2003). Whereas directed models intuitively have a topdown interpretation (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 247
                            }
                        ],
                        "text": "\u2026by guest on 06 Septem ber 2021\ncompute gradients; however, in the general case of overcomplete or hierarchical PoT\u2019s, we are required to employ an approximation scheme, and the preferred method in this article will be contrastive divergence (CD) (Hinton, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4572,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144883899"
                        ],
                        "name": "B. Willmore",
                        "slug": "B.-Willmore",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Willmore",
                            "middleNames": [
                                "D.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Willmore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70815589"
                        ],
                        "name": "D. Tolhurst",
                        "slug": "D.-Tolhurst",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Tolhurst",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tolhurst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 161
                            }
                        ],
                        "text": "More generally, although representations in an overcomplete PoT are sparse, there is also some redundancy; the PoT population response is typically less sparse (Willmore & Tolhurst, 2001) than a causal model with an equivalent prior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "More generally, although representations in an overcomplete PoT are sparse there is also some redundancy; the PoT population response is typically less sparse ( Willmore and Tolhurst, 2001 ) than an causal model with an \u201cequivalent\u201d prior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5769420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8906d996956916c0369ee9c467eab5b4f6bf7e0",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "It is often suggested that efficient neural codes for natural visual information should be `sparse'. However, the term `sparse' has been used in two different ways - firstly to describe codes in which few neurons are active at any time (`population sparseness'), and secondly to describe codes in which each neuron's lifetime response distribution has high kurtosis (`lifetime sparseness'). Although these ideas are related, they are not identical, and the most common measure of lifetime sparseness - the kurtosis of the lifetime response distributions of the neurons - provides no information about population sparseness. We have measured the population sparseness and lifetime kurtosis of several biologically inspired coding schemes. We used three measures of population sparseness (population kurtosis, Treves-Rolls sparseness and `activity sparseness'), and found them to be in close agreement with one another. However, we also measured the lifetime kurtosis of the cells in each code. We found that lifetime kurtosis is uncorrelated with population sparseness for the codes we used. Lifetime kurtosis is not, therefore, a useful measure of the population sparseness of a code. Moreover, the Gabor-like codes, which are often assumed to have high population sparseness (since they have high lifetime kurtosis), actually turned out to have rather low population sparseness. Surprisingly, principal components filters produced the codes with the highest population sparseness."
            },
            "slug": "Characterizing-the-sparseness-of-neural-codes-Willmore-Tolhurst",
            "title": {
                "fragments": [],
                "text": "Characterizing the sparseness of neural codes"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is found that lifetime kurtosis is not a useful measure of the population sparseness of a code, and the Gabor-like codes, which are often assumed to have high populationSparseness, actually turned out to have rather low population spARSeness."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3288675"
                        ],
                        "name": "F. Agakov",
                        "slug": "F.-Agakov",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Agakov",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Agakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9829128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "500a6f7fa91921a461ec41424ea2fd0ea8800b7b",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Principal components analysis (PCA) is one of the most widely used techniques in machine learning and data mining. Minor components analysis (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution. In this paper we present a probabilistic model for \"extreme components analysis\" (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components. For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA. We describe an efficient algorithm to solve for the globally optimal solution. For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components. In general, the solution admits a combination of both. In experiments we explore the properties of XCA on some synthetic and real-world datasets."
            },
            "slug": "Extreme-Components-Analysis-Welling-Agakov",
            "title": {
                "fragments": [],
                "text": "Extreme Components Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A probabilistic model for \"extreme components analysis\" (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400347470"
                        ],
                        "name": "M. A. Carreira-Perpi\u00f1\u00e1n",
                        "slug": "M.-A.-Carreira-Perpi\u00f1\u00e1n",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Carreira-Perpi\u00f1\u00e1n",
                            "middleNames": [
                                "\u00c1."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Carreira-Perpi\u00f1\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For further details on contrastive divergence learning we refer to the literature (Hinton, 2002; Teh et al., 2003; Yuille, 2004;  Carreira-Perpinan and Hinton, 2005 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 130
                            }
                        ],
                        "text": "For further details on contrastive divergence learning, we refer to the literature (Hinton, 2002; Teh et al., 2003; Yuille, 2004; Carreira-Perpinan & Hinton, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17861266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e270bfa5b662c531a61a5b274da636603c23a734",
            "isKey": false,
            "numCitedBy": 705,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum-likelihood (ML) learning of Markov random fields is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates, but Hinton (2002) showed that if the Markov chain is only run for a few steps, the learning can still work well and it approximately minimizes a different function called \u201ccontrastive divergence\u201d (CD). CD learning has been successfully applied to various types of random fields. Here, we study the properties of CD learning and show that it provides biased estimates in general, but that the bias is typically very small. Fast CD learning can therefore be used to get close to an ML solution and slow ML learning can then be used to fine-tune the CD solution. Consider a probability distribution over a vector x (assumed discrete w.l.o.g.) and with parameters W p(x;W) = 1 Z(W) e (1) where Z(W) = \u2211 x e \u2212E(x;W) is a normalisation constant and E(x;W) is an energy function. This class of random-field distributions has found many practical applications (Li, 2001; Winkler, 2002; Teh et al., 2003; He et al., 2004). Maximum-likelihood (ML) learning of the parameters W given an iid sample X = {xn}n=1 can be done by gradient ascent: W = W + \u03b7 \u2202L(W;X ) \u2202W \u2223"
            },
            "slug": "On-Contrastive-Divergence-Learning-Carreira-Perpi\u00f1\u00e1n-Hinton",
            "title": {
                "fragments": [],
                "text": "On Contrastive Divergence Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The properties of CD learning are studied and it is shown that it provides biased estimates in general, but that the bias is typically very small."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 37
                            }
                        ],
                        "text": "We use a style of display as used in Hyvarinen et al. (2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 154
                            }
                        ],
                        "text": "In this section we show that in the complete case, the topographic PoT model is isomorphic to the model optimized (but not the one initially proposed) by Hyvarinen et al. (2001) in their work on topographic ICA (tICA)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 75
                            }
                        ],
                        "text": "While our model is closely related to some previous work, most prominently Hyvarinen et al. (2001), it bestows a different interpretation on the learned features, is different in its formulation, and describes rather different statistical relations in the overcomplete case."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 188
                            }
                        ],
                        "text": "\u2026by guest on 06 Septem ber 2021\nwhere the form of the scalar function G is given by\nG(\u03c4 ) = log \u222b\n1\u221a 2\u03c0\nexp (\n1 2\nt\u03c4 ) pt(t) \u221a Hii dt. (5.8)\nThe results obtained by Hyvarinen and Hoyer (2001) and Hyvarinen et al. (2001) are very similar to those presented here in section 4."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 46
                            }
                        ],
                        "text": "Figure 8: Graphical model for topographic ICA (Hyvarinen et al., 2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 47
                            }
                        ],
                        "text": "Their model is of the general form proposed in Hyvarinen et al. (2001) but uses a different functional dependency between the first and second hidden layers to that employed in the topographic ICA model that Hyvarinen et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 194
                            }
                        ],
                        "text": "These are then used to set the variance of the sources, s, and conditioned on these scaling variates, the components in the second layer\nt\nx\nA\ns\n\u03c3\nH\nFigure 8: Graphical model for topographic ICA (Hyvarinen et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 47
                            }
                        ],
                        "text": "Their model is of the general form proposed in Hyvarinen et al. (2001) but uses a different functional dependency between the first and second hidden layers to that employed in the topographic ICA model that Hyvarinen et al. (2001) fully develop."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 3
                            }
                        ],
                        "text": "If Hyvarinen et al. (2001) were to make their model overcomplete, there would no longer be a deterministic relationship between their sources s and x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 72
                            }
                        ],
                        "text": "The modifications described next were inspired by a similar proposal in Hyvarinen et al. (2001) named topographic ICA."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1585328,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9c07182933e7d8f308292300a63c4b95864d8ff5",
            "isKey": true,
            "numCitedBy": 453,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "In ordinary independent component analysis, the components are assumed to be completely independent, and they do not necessarily have any meaningful order relationships. In practice, however, the estimated independent components are often not at all independent. We propose that this residual dependence structure could be used to define a topo-graphic order for the components. In particular, a distance between two components could be defined using their higher-order correlations, and this distance could be used to create a topographic representation. Thus, we obtain a linear decomposition into approximately independent components, where the dependence of two components is approximated by the proximity of the components in the topographic representation."
            },
            "slug": "Topographic-Independent-Component-Analysis-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Topographic Independent Component Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A linear decomposition is obtained into approximately independent components, where the dependence of two components is approximated by the proximity of the components in the topographic representation."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3288675"
                        ],
                        "name": "F. Agakov",
                        "slug": "F.-Agakov",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Agakov",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Agakov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 138
                            }
                        ],
                        "text": "In addition to binary models (Hinton, 2002), the gaussian case been studied (Williams, Agakov, & Felderhof, 2001; Marks & Movellan, 2001; Williams & Agakov, 2002; Welling, Agakov, & Williams, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14004203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9b93a69f2ff9d1e5243a4a0cbc71813f8ea1d02",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The Boltzmann machine (BM) learning rule for random field models with latent variables can be problematic to use in practice. These problems have (at least partially) been attributed to the negative phase in BM learning where a Gibbs sampling chain should be run to equilibrium. Hinton (1999, 2000) has introduced an alternative called contrastive divergence (CD) learning where the chain is run for only 1 step. In this paper we analyse the mean and variance of the parameter update obtained after steps of Gibbs sampling for a simple Gaussian BM. For this model our analysis shows that CD learning produces (as expected) a biased estimate of the true parameter update. We also show that the variance does usually increase with and quantify this behaviour."
            },
            "slug": "An-analysis-of-contrastive-divergence-learning-in-Williams-Agakov",
            "title": {
                "fragments": [],
                "text": "An analysis of contrastive divergence learning in gaussian boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper analyses the mean and variance of the parameter update obtained after steps of Gibbs sampling for a simple Gaussian BM and shows that CD learning produces (as expected) a biased estimate of the true parameter update."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "osindero@cs.toronto.edu"
                    },
                    "intents": []
                }
            ],
            "corpusId": 7735158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f18e18d436c51868a2cba5c7df3859986d6ba40",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting algorithms and successful applications thereof abound for classification and regression learning problems, but not for unsupervised learning. We propose a sequential approach to adding features to a random field model by training them to improve classification performance between the data and an equal-sized sample of \"negative examples\" generated from the model's current estimate of the data density. Training in each boosting round proceeds in three stages: first we sample negative examples from the model's current Boltzmann distribution. Next, a feature is trained to improve classification performance between data and negative examples. Finally, a coefficient is learned which determines the importance of this feature relative to ones already in the pool. Negative examples only need to be generated once to learn each new feature. The validity of the approach is demonstrated on binary digits and continuous synthetic data."
            },
            "slug": "Self-Supervised-Boosting-Welling-Zemel",
            "title": {
                "fragments": [],
                "text": "Self Supervised Boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A sequential approach to adding features to a random field model by training them to improve classification performance between the data and an equal-sized sample of \"negative examples\" generated from the model's current estimate of the data density is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 77
                            }
                        ],
                        "text": "A binary product of experts model was introduced under the name harmonium in Smolensky (1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A binary product of experts model was first introduced under the name \u201charmonium\u201d in  Smolensky (1986) ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 533055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f7476037408ac3d993f5088544aab427bc319c1",
            "isKey": false,
            "numCitedBy": 1949,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : At this early stage in the development of cognitive science, methodological issues are both open and central. There may have been times when developments in neuroscience, artificial intelligence, or cognitive psychology seduced researchers into believing that their discipline was on the verge of discovering the secret of intelligence. But a humbling history of hopes disappointed has produced the realization that understanding the mind will challenge the power of all these methodologies combined. The work reported in this chapter rests on the conviction that a methodology that has a crucial role to play in the development of cognitive science is mathematical analysis. The success of cognitive science, like that of many other sciences, will, I believe, depend upon the construction of a solid body of theoretical results: results that express in a mathematical language the conceptual insights of the field; results that squeeze all possible implications out of those insights by exploiting powerful mathematical techniques. This body of results, which I will call the theory of information processing, exists because information is a concept that lends itself to mathematical formalization. One part of the theory of information processing is already well-developed. The classical theory of computation provides powerful and elegant results about the notion of effective procedure, including languages for precisely expressing them and theoretical machines for realizing them."
            },
            "slug": "Information-processing-in-dynamical-systems:-of-Smolensky",
            "title": {
                "fragments": [],
                "text": "Information processing in dynamical systems: foundations of harmony theory"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The work reported in this chapter rests on the conviction that a methodology that has a crucial role to play in the development of cognitive science is mathematical analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 146
                            }
                        ],
                        "text": "\u2026to mixture of expert models, where individual models are combined additively, PoEs combine expert opinions multiplicatively as follows (see also Heskes, 1998),\nPPoE(x|\u03b8) = 1Z(\u03b8 ) M\u220f\ni=1 pi (x|\u03b8i ), (2.1)\nwhere Z(\u03b8 ) is the global normalization constant and pi (\u00b7) are the individual expert\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 51
                            }
                        ],
                        "text": "Product of expert models (PoEs) were introduced in Hinton (2002) as an alternative method of combining expert models into one joint model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11312042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d919e2bf6e87607ea7de4cde1da6c77f0ea46fa3",
            "isKey": true,
            "numCitedBy": 91,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple linear averaging of the outputs of several networks as e.g. in bagging [3], seems to follow naturally from a bias/variance decomposition of the sum-squared error. The sum-squared error of the average model is a quadratic function of the weighting factors assigned to the networks in the ensemble [7], suggesting a quadratic programming algorithm for finding the \"optimal\" weighting factors. \n \nIf we interpret the output of a network as a probability statement, the sum-squared error corresponds to minus the loglikelihood or the Kullback-Leibler divergence, and linear averaging of the outputs to logarithmic averaging of the probability statements: the logarithmic opinion pool. \n \nThe crux of this paper is that this whole story about model averaging, bias/variance decompositions, and quadratic programming to find the optimal weighting factors, is not specific for the sum-squared error, but applies to the combination of probability statements of any kind in a logarithmic opinion pool, as long as the Kullback-Leibler divergence plays the role of the error measure. As examples we treat model averaging for classification models under a cross-entropy error measure and models for estimating variances."
            },
            "slug": "Selecting-Weighting-Factors-in-Logarithmic-Opinion-Heskes",
            "title": {
                "fragments": [],
                "text": "Selecting Weighting Factors in Logarithmic Opinion Pools"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This whole story about model averaging, bias/variance decompositions, and quadratic programming to find the optimal weighting factors, is not specific for the sum-squared error, but applies to the combination of probability statements of any kind in a logarithmic opinion pool, as long as the Kullback-Leibler divergence plays the role of the error measure."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713336"
                        ],
                        "name": "G. Goodhill",
                        "slug": "G.-Goodhill",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Goodhill",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Goodhill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1850006,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "dab21ff76a6f707e50750205dd0b09627b65f812",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 189,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Contributions-of-Theoretical-Modeling-to-the-of-Map-Goodhill",
            "title": {
                "fragments": [],
                "text": "Contributions of Theoretical Modeling to the Understanding of Neural Map Development"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2656437"
                        ],
                        "name": "M. West",
                        "slug": "M.-West",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "West",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. West"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 84
                            }
                        ],
                        "text": "We can consider the complete version of our model as a gaussian scale mixture (GSM; Andrews & Mallows, 1974;\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/18/2/381/816474/089976606775093936.pdf by guest on 06 Septem ber 2021\nWainwright & Simoncelli, 2000; Wainwright, Simoncelli, &\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We can consider the complete version of our model as a Gaussian scale mixture ( Andrews and Mallows, 1974;  Wainwright and Simoncelli, 2000; Wainwright et al., 2000) with a particular (complicated) form of scaling function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44372930,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a88c4014da492520eceff8b359d981d6ef4b8e5e",
            "isKey": false,
            "numCitedBy": 737,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The exponential power family of distributions of Box & Tiao (1973) is shown to be a subset of the class of scale mixtures of normals. The corresponding mixing distributions are explicitly obtained, identifying a close relationship between the exponential power family and a further class of normal scale mixtures, namely the stable distributions."
            },
            "slug": "On-scale-mixtures-of-normal-distributions-West",
            "title": {
                "fragments": [],
                "text": "On scale mixtures of normal distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The basic model we study here is a form of PoE suggested by  Hinton and Teh (2001) , where the experts are given by generalized Student-t distributions:,A natural way to interpret the differences between directed models (and in particular ICA models) and PoE models was provided in  Hinton and Teh (2001)  and Teh et al. (2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 60
                            }
                        ],
                        "text": "The basic model we study here is a form of PoE suggested by Hinton and Teh (2001), where the experts are given by generalized Student-t distributions:\ny = Jx (2.3)\npi (yi |\u03b1i ) \u221d 1( 1 + 12 y2i )\u03b1i ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 129
                            }
                        ],
                        "text": "A natural way to interpret the differences between directed models (and in particular ICA models) and PoE models was provided in Hinton and Teh (2001) and Teh et al. (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9951467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66e65f81f1f76fb3a7c8ab2d813362b924e2fa9b",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations."
            },
            "slug": "Discovering-Multiple-Constraints-that-are-Satisfied-Hinton-Teh",
            "title": {
                "fragments": [],
                "text": "Discovering Multiple Constraints that are Frequently Approximately Satisfied"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work describes three methods of learning products of constraints using a heavy-tailed probability distribution for the violations of constraint violations."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A tractable probabilisticmodel for projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Conference on Uncertainty in Artificial Intelligence"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extreme components analy"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A tractable probabilistic model for projec"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A tractable probabilistic model for projection pursuit"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Conference on Uncertainty in Artificial Intelligence"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Random cascades"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic sequential independent"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 116
                            }
                        ],
                        "text": "For further details on contrastive divergence learning, we refer to the literature (Hinton, 2002; Teh et al., 2003; Yuille, 2004; Carreira-Perpinan & Hinton, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comment on contrastive divergence"
            },
            "venue": {
                "fragments": [],
                "text": "Los Angeles: Department Statistics and Psychology"
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 13,
            "methodology": 25,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 47,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Topographic-Product-Models-Applied-to-Natural-Scene-Osindero-Welling/d8d01934cb26064b253dbd0f1627519133c3df3e?sort=total-citations"
}