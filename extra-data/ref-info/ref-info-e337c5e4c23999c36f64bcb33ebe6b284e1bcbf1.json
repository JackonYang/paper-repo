{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32384143"
                        ],
                        "name": "D. Popovici",
                        "slug": "D.-Popovici",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Popovici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Popovici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 197
                            }
                        ],
                        "text": "\u2026of recent work on scaling up DBN and sparse coding algorithms, sometimes with entire research papers \ndevoted to ingenious methods devised speci.cally for each of these models (Hinton et al., 2006; Bengio \net al., 2006; Murray &#38; Kreutz-Delgado, 2006; Lee et al., 2006; Kavukcuoglu et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 13
                            }
                        ],
                        "text": "Partly due to \nsuch daunting computational require\u00adments, typical applications of DBNs and sparse cod\u00ading considered \nin the literature generally contain many fewer free parameters (e.g., see Table 1), or are trained on \na fraction of the available input examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14201947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
            "isKey": false,
            "numCitedBy": 3433,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "slug": "Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin",
            "title": {
                "fragments": [],
                "text": "Greedy Layer-Wise Training of Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785346"
                        ],
                        "name": "Roger B. Grosse",
                        "slug": "Roger-B.-Grosse",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Grosse",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger B. Grosse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2615814"
                        ],
                        "name": "R. Ranganath",
                        "slug": "R.-Ranganath",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Ranganath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ranganath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 133
                            }
                        ],
                        "text": "If over\u00adlapping patches \nare tiled one pixel apart, this model is identical to the convolutional RBM model (Desjardins &#38; Bengio, \n2008; Lee et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Bengio, Y., Lamblin, P., Popovici, \nD., &#38; Larochelle, H. (2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 70
                            }
                        ],
                        "text": "Consistent with previous results, we found \nthat Goto BLAS was faster (Bengio, 2007), so we report CPU results us\u00ading it."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "Desjardins, G., &#38; Bengio, Y. (2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Bengio, Y. (2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "If overlapping patches are tiled one pixel apart, this model is identical to the convolutional RBM model (Desjardins & Bengio, 2008; Lee et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12008458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e80f755bcbf10479afd2338cec05211fdbd325c",
            "isKey": false,
            "numCitedBy": 2510,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images."
            },
            "slug": "Convolutional-deep-belief-networks-for-scalable-of-Lee-Grosse",
            "title": {
                "fragments": [],
                "text": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The convolutional deep belief network is presented, a hierarchical generative model which scales to realistic image sizes and is translation-invariant and supports efficient bottom-up and top-down probabilistic inference."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 120
                            }
                        ],
                        "text": "Unfortunately, with current algorithms, \nparameter learning can take weeks using a conventional implementation on a single CPU."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 176
                            }
                        ],
                        "text": "\u2026of recent work on scaling up DBN and sparse coding algorithms, sometimes with entire research papers \ndevoted to ingenious methods devised speci.cally for each of these models (Hinton et al., 2006; Bengio \net al., 2006; Murray &#38; Kreutz-Delgado, 2006; Lee et al., 2006; Kavukcuoglu et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 36
                            }
                        ],
                        "text": "Published source Application Params Hinton et al., 2006 Hinton &#38; Salakhutdinov \nSalakhutdinov &#38; Hinton Ranzato &#38; Szummer Digit images Face images Sem. hashing Text 1.6mn 3.8mn \n2.6mn 3mn Our model 100mn the model is large, but not otherwise (Lee et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13408,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779306"
                        ],
                        "name": "Chaitanya Ekanadham",
                        "slug": "Chaitanya-Ekanadham",
                        "structuredName": {
                            "firstName": "Chaitanya",
                            "lastName": "Ekanadham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaitanya Ekanadham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 160
                            }
                        ],
                        "text": "Following previous work, we used Gaussian visible units and binary hidden units, and trained a sparse RBM by adding an additional penalty term to the objective (Lee et al., 2007)\u2014however, these modifications do not affect the running time results significantly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 161
                            }
                        ],
                        "text": "Following previ\u00adous work, we used Gaussian visible \nunits and binary hidden units, and trained a sparse RBM by adding an additional penalty term to the objective \n(Lee et al., 2007) however, these modi.cations do not a.ect the running time results signi.cantly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12589862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "202cbbf671743aefd380d2f23987bd46b9caaf97",
            "isKey": false,
            "numCitedBy": 1028,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or \"deep,\" structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Specifically, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge filters, similar to the Gabor functions known to model V1 cell receptive fields. Further, the second layer in our model encodes correlations of the first layer responses in the data. Specifically, it picks up both colinear (\"contour\") features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex \"corner\" features matches well with the results from the Ito & Komatsu's study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features."
            },
            "slug": "Sparse-deep-belief-net-model-for-visual-area-V2-Lee-Ekanadham",
            "title": {
                "fragments": [],
                "text": "Sparse deep belief net model for visual area V2"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An unsupervised learning model is presented that faithfully mimics certain properties of visual area V2 and the encoding of these more complex \"corner\" features matches well with the results from the Ito & Komatsu's study of biological V2 responses, suggesting that this sparse variant of deep belief networks holds promise for modeling more higher-order features."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Experimental Results: We again compare our method against a multicore CPU baseline (Lee et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 240
                            }
                        ],
                        "text": "Published source Application Params Hinton et al., 2006 Hinton &#38; Salakhutdinov \nSalakhutdinov &#38; Hinton Ranzato &#38; Szummer Digit images Face images Sem. hashing Text 1.6mn 3.8mn \n2.6mn 3mn Our model 100mn the model is large, but not otherwise (Lee et al., 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This suggests an alternating minimization algorithm with two steps: first, keeping b fixed, we optimize over a, which leads to an L1regularized least squares problem, that can be solved using custom-designed solvers (Efron et al., 2004; Lee et al., 2006; Andrew & Gao, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Then, we keep a fixed, and optimize over b using convex optimization techniques (Lee et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 236
                            }
                        ],
                        "text": "This suggests an alternating minimization algorithm with \ntwo steps: .rst, keep\u00ading b .xed, we optimize over a, which leads to an L1 \u00adregularized least squares \nproblem, that can be solved using custom-designed solvers (Efron et al., 2004; Lee et al., 2006; Andrew \n&#38; Gao, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The basis vectors are found by solving the following optimization problem (Lee et al., 2006):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 75
                            }
                        ],
                        "text": "The basis vectors are found by solving the following optimization problem (Lee \net al., 2006):  1(i)(i) minimizeb,a Ix(i) - j abj I2 + \u00df |a| 2 i j i,jj s.t. Ibj I= 1, .j ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 248
                            }
                        ],
                        "text": "There has been a \nlot of recent work on scaling up DBN and sparse coding algorithms, sometimes with entire research papers \ndevoted to ingenious methods devised speci.cally for each of these models (Hinton et al., 2006; Bengio \net al., 2006; Murray &#38; Kreutz-Delgado, 2006; Lee et al., 2006; Kavukcuoglu et al., 2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This problem has recently received wide attention because of its robust feature selection properties (Tibshirani, 1996; Ng, 2004), and custom algorithms have been designed to solve it (Efron et al., 2004; Lee et al., 2006; Andrew & Gao, 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There has been a lot of recent work on scaling up DBN and sparse coding algorithms, sometimes with entire research papers devoted to ingenious methods devised specifically for each of these models (Hinton et al., 2006; Bengio et al., 2006; Murray & Kreutz-Delgado, 2006; Lee et al., 2006; Kavukcuoglu et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 17
                            }
                        ],
                        "text": "Murray, J. F., &#38; Kreutz-Delgado, K. (2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 205
                            }
                        ],
                        "text": "This problem has recently received wide attention because \nof its robust feature selection properties (Tibshirani, 1996; Ng, 2004), and custom algorithms have been \ndesigned to solve it (Efron et al., 2004; Lee et al., 2006; Andrew &#38; Gao, 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 84
                            }
                        ],
                        "text": "Experimental Results: We again compare our method \nagainst a multicore CPU baseline (Lee et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 80
                            }
                        ],
                        "text": "Then, we keep a .xed, and optimize over b using convex optimization techniques (Lee \net al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 249
                            }
                        ],
                        "text": "\u2026of recent work on scaling up DBN and sparse coding algorithms, sometimes with entire research papers \ndevoted to ingenious methods devised speci.cally for each of these models (Hinton et al., 2006; Bengio \net al., 2006; Murray &#38; Kreutz-Delgado, 2006; Lee et al., 2006; Kavukcuoglu et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "the model is large, but not otherwise (Lee et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 303727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e64a9960734215e2b1866ea3cb723ffa5585ac14",
            "isKey": false,
            "numCitedBy": 2683,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons."
            },
            "slug": "Efficient-sparse-coding-algorithms-Lee-Battle",
            "title": {
                "fragments": [],
                "text": "Efficient sparse coding algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "These algorithms are applied to natural images and it is demonstrated that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31052092"
                        ],
                        "name": "J. Murray",
                        "slug": "J.-Murray",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Murray",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395421758"
                        ],
                        "name": "K. Kreutz-Delgado",
                        "slug": "K.-Kreutz-Delgado",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Kreutz-Delgado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kreutz-Delgado"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 218
                            }
                        ],
                        "text": "\u2026of recent work on scaling up DBN and sparse coding algorithms, sometimes with entire research papers \ndevoted to ingenious methods devised speci.cally for each of these models (Hinton et al., 2006; Bengio \net al., 2006; Murray &#38; Kreutz-Delgado, 2006; Lee et al., 2006; Kavukcuoglu et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 34
                            }
                        ],
                        "text": "Partly due to \nsuch daunting computational require\u00adments, typical applications of DBNs and sparse cod\u00ading considered \nin the literature generally contain many fewer free parameters (e.g., see Table 1), or are trained on \na fraction of the available input examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3040578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbc18f70c3a85586ce90ef71bd9f2ada23f2df7f",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "Images can be coded accurately using a sparse set of vectors from a learned overcomplete dictionary, with potential applications in image compression and feature selection for pattern recognition. We present a survey of algorithms that perform dictionary learning and sparse coding and make three contributions. First, we compare our overcomplete dictionary learning algorithm (FOCUSS-CNDL) with overcomplete independent component analysis (ICA). Second, noting that once a dictionary has been learned in a given domain the problem becomes one of choosing the vectors to form an accurate, sparse representation, we compare a recently developed algorithm (sparse Bayesian learning with adjustable variance Gaussians, SBL-AVG) to well known methods of subset selection: matching pursuit and FOCUSS. Third, noting that in some cases it may be necessary to find a non-negative sparse coding, we present a modified version of the FOCUSS algorithm that can find such non-negative codings. Efficient parallel implementations in VLSI could make these algorithms more practical for many applications."
            },
            "slug": "Learning-Sparse-Overcomplete-Codes-for-Images-Murray-Kreutz-Delgado",
            "title": {
                "fragments": [],
                "text": "Learning Sparse Overcomplete Codes for Images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A survey of algorithms that perform dictionary learning and sparse coding is presented and a modified version of the FOCUSS algorithm is presented that can find a non-negative sparse coding in some cases."
            },
            "venue": {
                "fragments": [],
                "text": "J. VLSI Signal Process."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2551676"
                        ],
                        "name": "Cheng-Tao Chu",
                        "slug": "Cheng-Tao-Chu",
                        "structuredName": {
                            "firstName": "Cheng-Tao",
                            "lastName": "Chu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Tao Chu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109828167"
                        ],
                        "name": "Sang Kyun Kim",
                        "slug": "Sang-Kyun-Kim",
                        "structuredName": {
                            "firstName": "Sang",
                            "lastName": "Kim",
                            "middleNames": [
                                "Kyun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang Kyun Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47904256"
                        ],
                        "name": "Yi-An Lin",
                        "slug": "Yi-An-Lin",
                        "structuredName": {
                            "firstName": "Yi-An",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi-An Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117163611"
                        ],
                        "name": "YuanYuan Yu",
                        "slug": "YuanYuan-Yu",
                        "structuredName": {
                            "firstName": "YuanYuan",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "YuanYuan Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720184"
                        ],
                        "name": "G. Bradski",
                        "slug": "G.-Bradski",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Bradski",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Bradski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746638"
                        ],
                        "name": "K. Olukotun",
                        "slug": "K.-Olukotun",
                        "structuredName": {
                            "firstName": "Kunle",
                            "lastName": "Olukotun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Olukotun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 269
                            }
                        ],
                        "text": "\u2026such as logistic regres\u00adsion, linear SVMs and others can be easily imple\u00admented in parallel \non multicore architectures, by hav\u00ading each core perform the required computations for a subset of input \nexamples, and then combining the re\u00adsults centrally (Dean &#38; Ghemawat, 2004; Chu et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 134
                            }
                        ],
                        "text": "The map-reduce framework (Dean &#38; Ghe\u00admawat, 2004) has been successfully \napplied to par\u00adallelize a class of machine learning algorithms (Chu et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 349,
                                "start": 308
                            }
                        ],
                        "text": "Recent work has shown that several popular learning algorithms such as logistic regression, linear SVMs and others can be easily implemented in parallel on multicore architectures, by having each core perform the required computations for a subset of input examples, and then combining the results centrally (Dean & Ghemawat, 2004; Chu et al., 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 12
                            }
                        ],
                        "text": "Dean, J., &#38; \nGhemawat, S. (2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 133
                            }
                        ],
                        "text": "The map-reduce framework (Dean & Ghemawat, 2004) has been successfully applied to parallelize a class of machine learning algorithms (Chu et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 316
                            }
                        ],
                        "text": "Recent work has shown that several popular learning \nalgorithms such as logistic regres\u00adsion, linear SVMs and others can be easily imple\u00admented in parallel \non multicore architectures, by hav\u00ading each core perform the required computations for a subset of input \nexamples, and then combining the re\u00adsults centrally (Dean &#38; Ghemawat, 2004; Chu et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 183466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38aff6df1accc456f6cda7d16d4b9ecf418ef21e",
            "isKey": false,
            "numCitedBy": 1069,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain \"summation form,\" which allows them to be easily parallelized on multicore computers. We adapt Google's map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors."
            },
            "slug": "Map-Reduce-for-Machine-Learning-on-Multicore-Chu-Kim",
            "title": {
                "fragments": [],
                "text": "Map-Reduce for Machine Learning on Multicore"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that algorithms that fit the Statistical Query model can be written in a certain \"summation form,\" which allows them to be easily parallelized on multicore computers and shows basically linear speedup with an increasing number of processors."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 177
                            }
                        ],
                        "text": "Introduction We consider two well-known \nunsupervised learning models, deep belief networks (DBNs) and sparse cod\u00ading, that can learn hierarchical \nrepresentations of their input (Olshausen &#38; Field, 1996; Hinton &#38; Salakhutdi\u00adnov, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 267
                            }
                        ],
                        "text": "\u2026of recent work on scaling up DBN and sparse coding algorithms, sometimes with entire research papers \ndevoted to ingenious methods devised speci.cally for each of these models (Hinton et al., 2006; Bengio \net al., 2006; Murray &#38; Kreutz-Delgado, 2006; Lee et al., 2006; Kavukcuoglu et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 83
                            }
                        ],
                        "text": "Partly due to \nsuch daunting computational require\u00adments, typical applications of DBNs and sparse cod\u00ading considered \nin the literature generally contain many fewer free parameters (e.g., see Table 1), or are trained on \na fraction of the available input examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5931210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks."
            },
            "slug": "Fast-Inference-in-Sparse-Coding-Algorithms-with-to-Kavukcuoglu-Ranzato",
            "title": {
                "fragments": [],
                "text": "Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes a simple and efficient algorithm to learn basis functions, which provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409971380"
                        ],
                        "name": "Ben Packer",
                        "slug": "Ben-Packer",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Packer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Packer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6692382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "isKey": false,
            "numCitedBy": 1610,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation."
            },
            "slug": "Self-taught-learning:-transfer-learning-from-data-Raina-Battle",
            "title": {
                "fragments": [],
                "text": "Self-taught learning: transfer learning from unlabeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data to form a succinct input representation and significantly improve classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061322266"
                        ],
                        "name": "David M. Bradley",
                        "slug": "David-M.-Bradley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Bradley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Bradley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 162
                            }
                        ],
                        "text": "Such a higher-level representation can then be applied to classi.cation \ntasks, where it leads to good results even with limited labeled data (Raina et al., 2007; Bradley &#38; \nBagnell, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4086703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3a9654a830fc891b015b1799b14eb43e40b292e",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1) that promotes sparsity. We show how smoother priors can preserve the benefits of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efficiently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and find that online optimization of the parameters of the KL-regularized model can significantly improve prediction performance."
            },
            "slug": "Differentiable-Sparse-Coding-Bagnell-Bradley",
            "title": {
                "fragments": [],
                "text": "Differentiable Sparse Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work shows how smoother priors can preserve the benefits of these sparse priors while adding stability to the Maximum A-Posteriori estimate, and finds that online optimization of the parameters of the KL-regularized model can significantly improve prediction performance."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "Olshausen, B. A., &#38; Field, D. J. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 181
                            }
                        ],
                        "text": "We consider two well-known unsuper\u00advised learning models, deep belief networks (DBNs) \nand sparse coding, that have recently been applied to a .urry of machine learning applications (Hinton \n&#38; Salakhutdinov, 2006; Raina et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that can learn hierarchical representations of their input (Olshausen & Field, 1996;  Hinton & Salakhutdinov, 2006 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications ( Hinton & Salakhutdinov, 2006;  Raina et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 113
                            }
                        ],
                        "text": "Sparse Coding Sparse coding is an algorithm for \nconstructing suc\u00adcinct representations of input data (Olshausen &#38; Field, (i) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 202
                            }
                        ],
                        "text": "Introduction We consider two well-known \nunsupervised learning models, deep belief networks (DBNs) and sparse cod\u00ading, that can learn hierarchical \nrepresentations of their input (Olshausen &#38; Field, 1996; Hinton &#38; Salakhutdi\u00adnov, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": true,
            "numCitedBy": 14641,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809791"
                        ],
                        "name": "K. Chellapilla",
                        "slug": "K.-Chellapilla",
                        "structuredName": {
                            "firstName": "Kumar",
                            "lastName": "Chellapilla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chellapilla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2568084"
                        ],
                        "name": "Sidd Puri",
                        "slug": "Sidd-Puri",
                        "structuredName": {
                            "firstName": "Sidd",
                            "lastName": "Puri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sidd Puri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "GPUs have been applied to certain problems in machine learning, including SVMs (Catanzaro et al., 2008), and supervised learning in convolutional networks ( Chellapilla et al., 2006 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 156
                            }
                        ],
                        "text": "GPUs have been applied to certain problems in ma\u00adchine learning, including SVMs (Catanzaro et al., 2008), \nand supervised learning in convolutional net\u00adworks (Chellapilla et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14936779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cc157afda51873c30b195fff56e917b9c06b853",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional neural networks (CNNs) are well known for producing state-of-the-art recognizers for document processing [1]. However, they can be difficult to implement and are usually slower than traditional multi-layer perceptrons (MLPs). We present three novel approaches to speeding up CNNs: a) unrolling convolution, b) using BLAS (basic linear algebra subroutines), and c) using GPUs (graphic processing units). Unrolled convolution converts the processing in each convolutional layer (both forward-propagation and back-propagation) into a matrix-matrix product. The matrix-matrix product representation of CNNs makes their implementation as easy as MLPs. BLAS is used to efficiently compute matrix products on the CPU. We also present a pixel shader based GPU implementation of CNNs. Results on character recognition problems indicate that unrolled convolution with BLAS produces a dramatic 2.4X\u22123.0X speedup. The GPU implementation is even faster and produces a 3.1X\u22124.1X speedup."
            },
            "slug": "High-Performance-Convolutional-Neural-Networks-for-Chellapilla-Puri",
            "title": {
                "fragments": [],
                "text": "High Performance Convolutional Neural Networks for Document Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Three novel approaches to speeding up CNNs are presented: a) unrolling convolution, b) using BLAS (basic linear algebra subroutines), and c) using GPUs (graphic processing units)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "This problem has recently received wide attention because of its robust feature selection properties (Tibshirani, 1996; Ng, 2004), and custom algorithms have been designed to solve it (Efron et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 120
                            }
                        ],
                        "text": "This problem has recently received wide attention because \nof its robust feature selection properties (Tibshirani, 1996; Ng, 2004), and custom algorithms have been \ndesigned to solve it (Efron et al., 2004; Lee et al., 2006; Andrew &#38; Gao, 2007)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11258400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee6275a84962a0ffd6212585e4f7ee7ffb2b068a",
            "isKey": false,
            "numCitedBy": 1635,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn \"well,\") grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invariant algorithm---including logistic regression with L2 regularization, SVMs, and neural networks trained by backpropagation---has a worst case sample complexity that grows at least linearly in the number of irrelevant features."
            },
            "slug": "Feature-selection,-L1-vs.-L2-regularization,-and-Ng",
            "title": {
                "fragments": [],
                "text": "Feature selection, L1 vs. L2 regularization, and rotational invariance"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A lower-bound is given showing that any rotationally invariant algorithm---including logistic regression with L1 regularization, SVMs, and neural networks trained by backpropagation---has a worst case sample complexity that grows at least linearly in the number of irrelevant features."
            },
            "venue": {
                "fragments": [],
                "text": "Twenty-first international conference on Machine learning  - ICML '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1501682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a",
            "isKey": false,
            "numCitedBy": 1260,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Semantic-hashing-Salakhutdinov-Hinton",
            "title": {
                "fragments": [],
                "text": "Semantic hashing"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Approx. Reason."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66739595"
                        ],
                        "name": "Holger Hofling",
                        "slug": "Holger-Hofling",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hofling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Hofling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15413966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d50b597c475e87e03e8630e381011cb46e460ad8",
            "isKey": false,
            "numCitedBy": 1889,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider \u201cone-at-a-time\u201d coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the \u201cfused lasso,\u201d however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems. 1. Introduction. In this paper we consider statistical models that lead to convex optimization problems with inequality constraints. Typically, the optimization for these problems is carried out using a standard quadratic programming algorithm. The purpose of this paper is to explore \u201cone-at-a-time\u201d coordinate-wise descent algorithms for these problems. The equivalent of a coordinate descent algorithm has been proposed for the L1-penalized regression (lasso) in the literature, but it is not commonly used. Moreover, coordinate-wise algorithms seem too simple, and they are not often used in convex optimization, perhaps because they only work in specialized problems. We ourselves never appreciated the value of coordinate descent methods for convex statistical problems before working on this paper. In this paper we show that coordinate descent is very competitive with the wellknown LARS (or homotopy) procedure in large lasso problems, can deliver a path of solutions efficiently, and can be applied to many other convex statistical problems such as the garotte and elastic net. We then go on to explore a nonseparable problem in which coordinate-wise descent does not work\u2014the \u201cfused lasso.\u201d We derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to"
            },
            "slug": "PATHWISE-COORDINATE-OPTIMIZATION-Friedman-Hastie",
            "title": {
                "fragments": [],
                "text": "PATHWISE COORDINATE OPTIMIZATION"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that coordinate descent is very competitive with the well-known LARS procedure in large lasso problems, can deliver a path of solutions efficiently, and can be applied to many other convex statistical problems such as the garotte and elastic net."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4570,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301680"
                        ],
                        "name": "Bryan Catanzaro",
                        "slug": "Bryan-Catanzaro",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Catanzaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan Catanzaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789372"
                        ],
                        "name": "N. Sundaram",
                        "slug": "N.-Sundaram",
                        "structuredName": {
                            "firstName": "Narayanan",
                            "lastName": "Sundaram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sundaram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732330"
                        ],
                        "name": "K. Keutzer",
                        "slug": "K.-Keutzer",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Keutzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Keutzer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 79
                            }
                        ],
                        "text": "GPUs have been applied to certain problems in machine learning, including SVMs (Catanzaro et al., 2008), and supervised learning in convolutional networks (Chellapilla et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 80
                            }
                        ],
                        "text": "GPUs have been applied to certain problems in ma\u00adchine learning, including SVMs (Catanzaro et al., 2008), \nand supervised learning in convolutional net\u00adworks (Chellapilla et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Recent work has shown that several popular learning \nalgorithms such as logistic regres\u00adsion, linear SVMs and others can be easily imple\u00admented in parallel \non multicore architectures, by hav\u00ading each core perform the required computations for a subset of input \nexamples, and then combining the re\u00adsults centrally (Dean &#38; Ghemawat, 2004; Chu et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2127615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2635f61333900a6b4cd9b5db5d4c3bc31363b2ff",
            "isKey": false,
            "numCitedBy": 402,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent developments in programmable, highly parallel Graphics Processing Units (GPUs) have enabled high performance implementations of machine learning algorithms. We describe a solver for Support Vector Machine training running on a GPU, using the Sequential Minimal Optimization algorithm and an adaptive first and second order working set selection heuristic, which achieves speedups of 9-35x over LIBSVM running on a traditional processor. We also present a GPU-based system for SVM classification which achieves speedups of 81-138x over LIBSVM (5-24x over our own CPU based SVM classifier)."
            },
            "slug": "Fast-support-vector-machine-training-and-on-Catanzaro-Sundaram",
            "title": {
                "fragments": [],
                "text": "Fast support vector machine training and classification on graphics processors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A solver for Support Vector Machine training run on a GPU, using the Sequential Minimal Optimization algorithm and an adaptive first and second order working set selection heuristic, which achieves speedups of 9-35x over LIBSVM running on a traditional processor."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2151537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18862760ac708a589afa5848ab55931996db1b28",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding good representations of text documents is crucial in information retrieval and classification systems. Today the most popular document representation is based on a vector of word counts in the document. This representation neither captures dependencies between related words, nor handles synonyms or polysemous words. In this paper, we propose an algorithm to learn text document representations based on semi-supervised autoencoders that are stacked to form a deep network. The model can be trained efficiently on partially labeled corpora, producing very compact representations of documents, while retaining as much class information and joint word statistics as possible. We show that it is advantageous to exploit even a few labeled samples during training."
            },
            "slug": "Semi-supervised-learning-of-compact-document-with-Ranzato-Szummer",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning of compact document representations with deep networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An algorithm to learn text document representations based on semi-supervised autoencoders that are stacked to form a deep network that can be trained efficiently on partially labeled corpora, producing very compact representations of documents, while retaining as much class information and joint word statistics as possible."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 242
                            }
                        ],
                        "text": "\u2026processing language modeling and \nspelling correction it has been shown that sim\u00adple, classical models can outperform newer, more com\u00adplex \nmodels, just because the simple models can be tractably learnt using orders of magnitude more input data \n(Banko &#38; Brill, 2001; Brants et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 18
                            }
                        ],
                        "text": "For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup \nover previous methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6645623,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7628b62d64d2e5c33a13a5a473bc41b2391c1ebc",
            "isKey": false,
            "numCitedBy": 693,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost."
            },
            "slug": "Scaling-to-Very-Very-Large-Corpora-for-Natural-Banko-Brill",
            "title": {
                "fragments": [],
                "text": "Scaling to Very Very Large Corpora for Natural Language Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper examines methods for effectively exploiting very large corpora when labeled data comes at a cost, and evaluates the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambigsuation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474176"
                        ],
                        "name": "J. H. Hateren",
                        "slug": "J.-H.-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hateren",
                            "middleNames": [
                                "H.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46498936"
                        ],
                        "name": "A. Schaaf",
                        "slug": "A.-Schaaf",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "Schaaf",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schaaf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Consistent with previous results, we found that Goto BLAS was faster (Bengio, 2007), so we report CPU results using it. As input, we used a large dataset of natural images ( van Hateren & van der Schaa, 1997 ) and obtained input examples by randomly extracting square image patches of the required size."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15666050,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1",
            "isKey": false,
            "numCitedBy": 802,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed."
            },
            "slug": "Independent-component-filters-of-natural-images-in-Hateren-Schaaf",
            "title": {
                "fragments": [],
                "text": "Independent component filters of natural images compared with simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis on a large set of natural images: there is no match, however, in calculated and measured distributions for the peak of the spatial frequency response."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 186
                            }
                        ],
                        "text": "{1, ..., \nn} where the .rst term in the objective function encour\u00ad (i) ages good reconstruction (x(i) j bj aj \n), and the second term encourages sparsity by penalizing non\u00adzero activations (Tibshirani, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "where the first term in the objective function encourages good reconstruction (x(i) P j bja (i) j ), and the second term encourages sparsity by penalizing nonzero activations ( Tibshirani, 1996 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 102
                            }
                        ],
                        "text": "This problem has recently received wide attention because \nof its robust feature selection properties (Tibshirani, 1996; Ng, 2004), and custom algorithms have been \ndesigned to solve it (Efron et al., 2004; Lee et al., 2006; Andrew &#38; Gao, 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This problem has recently received wide attention because of its robust feature selection properties ( Tibshirani, 1996;  Ng, 2004), and custom algorithms have been designed to solve it (Efron et al., 2004; Lee et al., 2006; Andrew & Gao, 2007)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16162039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5",
            "isKey": true,
            "numCitedBy": 36476,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described."
            },
            "slug": "Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Regression Shrinkage and Selection via the Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new method for estimation in linear models called the lasso, which minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937132"
                        ],
                        "name": "Mark J. Harris",
                        "slug": "Mark-J.-Harris",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Harris",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark J. Harris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2914921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5420ed51f5a22895d9a308b1fac0172a8232ac82",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In the past, graphics processors were special-purpose hardwired application accelerators, suitable only for conventional graphics applications. Modern GPUs are fully programmable, massively parallel floating point processors. In this talk I will describe NVIDIA's scalable, highly parallel many-core GPU architecture and how CUDA software for GPU computing delivers high throughput for data-intensive processing. I will discuss how CUDA is reinvigorating research on data-parallel algorithms, reducing time to scientific discovery, and enabling a variety of compute-intensive industrial applications of GPUs beyond computer graphics."
            },
            "slug": "Many-core-GPU-computing-with-NVIDIA-CUDA-Harris",
            "title": {
                "fragments": [],
                "text": "Many-core GPU computing with NVIDIA CUDA"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This talk will describe NVIDIA's scalable, highly parallel many-core GPU architecture and how CUDA software for GPU computing delivers high throughput for data-intensive processing."
            },
            "venue": {
                "fragments": [],
                "text": "ICS '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2814173"
                        ],
                        "name": "P. Gelsinger",
                        "slug": "P.-Gelsinger",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Gelsinger",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gelsinger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 234
                            }
                        ],
                        "text": "Meanwhile, \nthe raw clock speed of single CPUs has begun to hit a hardware power limit, and most of the growth in \nprocessing power is increasingly ob\u00adtained by throwing together multiple CPU cores, in\u00adstead of speeding \nup a single core (Gelsinger, 2001; Frank, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 79
                            }
                        ],
                        "text": "In our view, if the goal is to deploy better machine learning \napplications, the di.culty of learning large models is a severe limitation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23130525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0367336a8d841ae09043cc6bb3c29900059283ba",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "As designs become more complex, technology scaling more difficult, and power issues more pressing, \"business as usual\" no longer suffices, if the industry is to continue its long-standing tradition of microprocessor innovation. To provide means for system performance advancements and power management, there must be focus on all aspects of the computing platform-architecture, micro-architecture, bus memory, and I/O performance-much more than in the past. Multithreading and multi-core computer micro-architectures will increase both general-purpose and networking processor MIPS. Transaction-focused server processors will benefit from large on-die caches. Special-purpose architectures and circuit techniques will be required to deliver performance with higher efficiency. Future microprocessors will evolve as integration of DSP capabilities becomes imperative to enable such applications as media-rich communications, computer vision, and speech recognition. These advances in processing natural data will lead to a change in the computing paradigm from today's data-based, machine-based computing to tomorrow's knowledge-based, human-based computing. As the Internet becomes more integral to businesses and consumers, there will be new uses for and users of microprocessors. All this can be accomplished only with wired and wireless high-bandwidth Internet connectivity, driven by high-performance computer servers to fulfil the demand of computing in the Internet economy."
            },
            "slug": "Microprocessors-for-the-new-millennium:-Challenges,-Gelsinger",
            "title": {
                "fragments": [],
                "text": "Microprocessors for the new millennium: Challenges, opportunities, and new frontiers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Future microprocessors will evolve as integration of DSP capabilities becomes imperative to enable such applications as media-rich communications, computer vision, and speech recognition, which will lead to a change in the computing paradigm from today's data-based, machine-based computing to tomorrow's knowledge- based, human- based computing."
            },
            "venue": {
                "fragments": [],
                "text": "2001 IEEE International Solid-State Circuits Conference. Digest of Technical Papers. ISSCC (Cat. No.01CH37177)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050725"
                        ],
                        "name": "O. Yasar",
                        "slug": "O.-Yasar",
                        "structuredName": {
                            "firstName": "Osman",
                            "lastName": "Yasar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Yasar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87558455"
                        ],
                        "name": "Y. Deng",
                        "slug": "Y.-Deng",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2914613"
                        ],
                        "name": "R. E. Tuzun",
                        "slug": "R.-E.-Tuzun",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tuzun",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. E. Tuzun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102971399"
                        ],
                        "name": "D. Saltz",
                        "slug": "D.-Saltz",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Saltz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Saltz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195721081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f5cc984ba23ea682036cd3abf8f26a7553f4cd3",
            "isKey": false,
            "numCitedBy": 1133,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "New-trends-in-high-performance-computing-Yasar-Deng",
            "title": {
                "fragments": [],
                "text": "New trends in high performance computing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144301481"
                        ],
                        "name": "R. C. Whaley",
                        "slug": "R.-C.-Whaley",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Whaley",
                            "middleNames": [
                                "Clinton"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Whaley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719457"
                        ],
                        "name": "A. Petitet",
                        "slug": "A.-Petitet",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Petitet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Petitet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708869"
                        ],
                        "name": "J. Dongarra",
                        "slug": "J.-Dongarra",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Dongarra",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dongarra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Consistent with previous results, we found \nthat Goto BLAS was faster (Bengio, 2007), so we report CPU results us\u00ading it."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Package Architecture 576x1024 \n1024x4096 2304x16000 4096x11008 Goto BLAS Goto BLAS Single CPU Dual-core CPU 563s 497s 3638s 2987s 172803s \n93586s 223741s 125381s GPU GPU Speedup 38.6s 12.9x 184s 16.2x 1376s 68.0x 1726s 72.6x Table 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 115
                            }
                        ],
                        "text": "The CPU-based method was \nimplemented using two highly optimized multithreaded linear algebra pack\u00adages: ATLAS BLAS (Whaley et \nal., 2001) and Goto BLAS (Goto &#38; Van De Geijn, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 114
                            }
                        ],
                        "text": "The CPU-based method was implemented using two highly optimized multithreaded linear algebra packages: ATLAS BLAS (Whaley et al., 2001) and Goto BLAS (Goto & Van De Geijn, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "High\u00adperformance implementation of the level-3 BLAS."
                    },
                    "intents": []
                }
            ],
            "corpusId": 159954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c649fed768cf39c83531eb4f17082a51bfedb70f",
            "isKey": true,
            "numCitedBy": 1152,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automated-empirical-optimizations-of-software-and-Whaley-Petitet",
            "title": {
                "fragments": [],
                "text": "Automated empirical optimizations of software and the ATLAS project"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784037"
                        ],
                        "name": "T. Brants",
                        "slug": "T.-Brants",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Brants",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brants"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054252"
                        ],
                        "name": "Ashok Popat",
                        "slug": "Ashok-Popat",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Popat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashok Popat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2092025743"
                        ],
                        "name": "P. Xu",
                        "slug": "P.-Xu",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 263
                            }
                        ],
                        "text": "\u2026processing language modeling and \nspelling correction it has been shown that sim\u00adple, classical models can outperform newer, more com\u00adplex \nmodels, just because the simple models can be tractably learnt using orders of magnitude more input data \n(Banko &#38; Brill, 2001; Brants et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Brie.y, memory \naccess requests from threads in a block are said to be coalesced if the threads access memory in sequence \n(i.e., the k-th thread accesses the k-th consecutive location in memory).1 When memory accesses are coalesced, \nthe hardware can perform them in parallel for all stream processors, and the e.ective access speed (between \nthe stream processors and the global memory) is several times faster than the access speed between a \nCPU and RAM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 353,
                                "start": 350
                            }
                        ],
                        "text": "To take a speci.c case study, \nfor two widely-studied statistical learning tasks in natural language processing language modeling and \nspelling correction it has been shown that sim\u00adple, classical models can outperform newer, more com\u00adplex \nmodels, just because the simple models can be tractably learnt using orders of magnitude more input data \n(Banko &#38; Brill, 2001; Brants et al., 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Banko, M., &#38; Brill, E. (2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 39
                            }
                        ],
                        "text": "For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup \nover previous methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Brie.y, an RBM contains a set of stochastic hidden units \nh that are fully connected in an undirected model to a set of stochas\u00adtic visible units x. Assuming binary-valued \nunits, the RBM de.nes the following joint distribution: P (x, h) . exp i,j xiwijhj +i cixi +j bj hj \nwhere the weights w and biases b and c are parame\u00adters to be tuned."
                    },
                    "intents": []
                }
            ],
            "corpusId": 633992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba786c46373892554b98df42df7af6f5da343c9d",
            "isKey": true,
            "numCitedBy": 533,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus."
            },
            "slug": "Large-Language-Models-in-Machine-Translation-Brants-Popat",
            "title": {
                "fragments": [],
                "text": "Large Language Models in Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Systems, methods, and computer program products for machine translation are provided for backoff score determination as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056396557"
                        ],
                        "name": "Kazushige Goto",
                        "slug": "Kazushige-Goto",
                        "structuredName": {
                            "firstName": "Kazushige",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazushige Goto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9151878"
                        ],
                        "name": "R. V. D. Geijn",
                        "slug": "R.-V.-D.-Geijn",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Geijn",
                            "middleNames": [
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. V. D. Geijn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The CPU-based method was implemented using two highly optimized multithreaded linear algebra packages: ATLAS BLAS (Whaley et al., 2001) and Goto BLAS ( Goto & Van De Geijn, 2008 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14722514,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "62fdc203262e62af885c757c2549444d5aa2c620",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple but highly effective approach for transforming high-performance implementations on cache-based architectures of matrix-matrix multiplication into implementations of other commonly used matrix-matrix computations (the level-3 BLAS) is presented. Exceptional performance is demonstrated on various architectures."
            },
            "slug": "High-performance-implementation-of-the-level-3-BLAS-Goto-Geijn",
            "title": {
                "fragments": [],
                "text": "High-performance implementation of the level-3 BLAS"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A simple but highly effective approach for transforming high-performance implementations on cache-based architectures of matrix-Matrix multiplication into implementations of other commonly used matrix-matrix computations (the level-3 BLAS) is presented."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2869762"
                        ],
                        "name": "D. Frank",
                        "slug": "D.-Frank",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Frank",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Frank"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 251
                            }
                        ],
                        "text": "Meanwhile, \nthe raw clock speed of single CPUs has begun to hit a hardware power limit, and most of the growth in \nprocessing power is increasingly ob\u00adtained by throwing together multiple CPU cores, in\u00adstead of speeding \nup a single core (Gelsinger, 2001; Frank, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 96
                            }
                        ],
                        "text": "In our view, if the goal is to deploy better machine learning \napplications, the di.culty of learning large models is a severe limitation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18017088,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "c86566872d3e5f896b4a8bfb489e8b4886059355",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The scaling of CMOS technology has progressed rapidly for three decades, but may soon come to an end because of power-dissipation constraints. The primary problem is static power dissipation, which is caused by leakage currents arising from quantum tunneling and thermal excitations. The details of these effects, along with other scaling issues, are discussed in the context of their dependence on application. On the basis of these considerations, the limits of CMOS scaling are estimated for various application scenarios."
            },
            "slug": "Power-constrained-CMOS-scaling-limits-Frank",
            "title": {
                "fragments": [],
                "text": "Power-constrained CMOS scaling limits"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The limits of CMOS scaling are estimated for various application scenarios because of power-dissipation constraints, which are caused by leakage currents arising from quantum tunneling and thermal excitations."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 269
                            }
                        ],
                        "text": "\u2026such as logistic regres\u00adsion, linear SVMs and others can be easily imple\u00admented in parallel \non multicore architectures, by hav\u00ading each core perform the required computations for a subset of input \nexamples, and then combining the re\u00adsults centrally (Dean &#38; Ghemawat, 2004; Chu et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 134
                            }
                        ],
                        "text": "The map-reduce framework (Dean &#38; Ghe\u00admawat, 2004) has been successfully \napplied to par\u00adallelize a class of machine learning algorithms (Chu et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 204
                            }
                        ],
                        "text": "\u2026processing language modeling and \nspelling correction it has been shown that sim\u00adple, classical models can outperform newer, more com\u00adplex \nmodels, just because the simple models can be tractably learnt using orders of magnitude more input data \n(Banko &#38; Brill, 2001; Brants et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 12
                            }
                        ],
                        "text": "Dean, J., &#38; \nGhemawat, S. (2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 316
                            }
                        ],
                        "text": "Recent work has shown that several popular learning \nalgorithms such as logistic regres\u00adsion, linear SVMs and others can be easily imple\u00admented in parallel \non multicore architectures, by hav\u00ading each core perform the required computations for a subset of input \nexamples, and then combining the re\u00adsults centrally (Dean &#38; Ghemawat, 2004; Chu et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mapreduce for machine learning on multicore. Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Mapreduce for machine learning on multicore. Neural Information Processing Systems"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121570279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c7c5595dc7a1f5d360acf5c360ca1ca49536ba5",
            "isKey": false,
            "numCitedBy": 7890,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates."
            },
            "slug": "Least-angle-regression-Efron-Hastie",
            "title": {
                "fragments": [],
                "text": "Least angle regression"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Source: NVIDIA CUDA Programming Guide) Large-scale Deep Unsupervised Learning Rajat Raina"
            },
            "venue": {
                "fragments": [],
                "text": "CPU NVIDIA GPU"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "-0249, and by the Office of Naval Research under MURI N000140710747"
            },
            "venue": {
                "fragments": [],
                "text": "-0249, and by the Office of Naval Research under MURI N000140710747"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semisupervised learning of compact document representations with deep networks. International Conference on Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Semisupervised learning of compact document representations with deep networks. International Conference on Machine Learning"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 69
                            }
                        ],
                        "text": "Consistent with previous results, we found that Goto BLAS was faster (Bengio, 2007), so we report CPU results using it."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 70
                            }
                        ],
                        "text": "Consistent with previous results, we found \nthat Goto BLAS was faster (Bengio, 2007), so we report CPU results us\u00ading it."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Speeding up stochastic gradient descent"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems Workshop on Efficient Machine Learning"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 223
                            }
                        ],
                        "text": "This problem has recently received wide attention because \nof its robust feature selection properties (Tibshirani, 1996; Ng, 2004), and custom algorithms have been \ndesigned to solve it (Efron et al., 2004; Lee et al., 2006; Andrew &#38; Gao, 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 254
                            }
                        ],
                        "text": "This suggests an alternating minimization algorithm with \ntwo steps: .rst, keep\u00ading b .xed, we optimize over a, which leads to an L1 \u00adregularized least squares \nproblem, that can be solved using custom-designed solvers (Efron et al., 2004; Lee et al., 2006; Andrew \n&#38; Gao, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scalable training of L1regularized log-linear models"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Machine Learning (pp. 33\u201340)"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Differentiable sparse coding. Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Differentiable sparse coding. Neural Information Processing Systems"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scalable training of L 1 regularized log-linear models"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Machine Learning"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient sparse coding algorithms. Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Efficient sparse coding algorithms. Neural Information Processing Systems"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic Hashing. SIGIR Workshop on Information Retrieval and Applications of Graphical Models"
            },
            "venue": {
                "fragments": [],
                "text": "Semantic Hashing. SIGIR Workshop on Information Retrieval and Applications of Graphical Models"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Empirical evaluation of convolutional RBMs for vision"
            },
            "venue": {
                "fragments": [],
                "text": "Empirical evaluation of convolutional RBMs for vision"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ACM Trans. Math. Softw"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Math. Softw"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 13,
            "methodology": 19,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Large-scale-deep-unsupervised-learning-using-Raina-Madhavan/e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1?sort=total-citations"
}