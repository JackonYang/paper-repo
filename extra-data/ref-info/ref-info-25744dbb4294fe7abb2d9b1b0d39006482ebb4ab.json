{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242194"
                        ],
                        "name": "Ghulum Bakiri",
                        "slug": "Ghulum-Bakiri",
                        "structuredName": {
                            "firstName": "Ghulum",
                            "lastName": "Bakiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ghulum Bakiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 0
                            }
                        ],
                        "text": "Dietterich & Bakiri (1991, 1995) have shown that the * * * * * * * * * * * 0 -10 -20 -30 10 C4.5 Multiclass Gl as s Vo we l PO S So yb ea n Au dio log y IS OL ET Le tte r NE Tt alk Pe rf or m an ce r el at iv e to M ul tic la ss C4.5 one-per-class C4.5 ECOCFigure 1: Performance of the one-per-class\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8268247,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ce4103e5bc275498adc81c422777ea404b5e599",
            "isKey": false,
            "numCitedBy": 247,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying large collections of training examples of the form \u2329Xi, f(Xi)\u232a. Existing approaches to this problem include (a) direct application of multiclass algorithms such as the decision-tree algorithms ID3 and CART, (b) application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and (c) application of binary concept learning algorithms with distributed output codes such as those employed by Sejnowski and Rosenberg in the NETtalk system. This paper compares these three approaches to a new technique in which BCH error-correcting codes are employed as a distributed output representation. We show that these output representations improve the performance of ID3 on the NETtalk task and of backpropagation on an isolated-letter speech-recognition task. These results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "slug": "Error-Correcting-Output-Codes:-A-General-Method-for-Dietterich-Bakiri",
            "title": {
                "fragments": [],
                "text": "Error-Correcting Output Codes: A General Method for Improving Multiclass Inductive Learning Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is demonstrated that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242194"
                        ],
                        "name": "Ghulum Bakiri",
                        "slug": "Ghulum-Bakiri",
                        "structuredName": {
                            "firstName": "Ghulum",
                            "lastName": "Bakiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ghulum Bakiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 89
                            }
                        ],
                        "text": "Previous experimental research (Bakiri, 1991; Diet-terich & Bakiri, 1991; Wettschereck & Dietterich,1992; Dietterich & Bakiri, 1995) has shown thaterror-correcting output coding uniformly improvesthe classi cation accuracy of decision tree and neural network classi ers when compared with the stan-dard approaches to k-class learning problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 106
                            }
                        ],
                        "text": "Previous experimental research (Bakiri, 1991; Diet-terich & Bakiri, 1991; Wettschereck & Dietterich,1992; Dietterich & Bakiri, 1995) has shown thaterror-correcting output coding uniformly improvesthe classi cation accuracy of decision tree and neural network classi ers when compared with the\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 0
                            }
                        ],
                        "text": "Dietterich & Bakiri (1991, 1995) have shown that the * * * * * * * * * * * 0 -10 -20 -30 10 C4.5 Multiclass Gl as s Vo we l PO S So yb ea n Au dio log y IS OL ET Le tte r NE Tt alk Pe rf or m an ce r el at iv e to M ul tic la ss C4.5 one-per-class C4.5 ECOCFigure 1: Performance of the one-per-class\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 241
                            }
                        ],
                        "text": "Hence, homo-geneous voting can reduce variance but not bias.4 ECOC and VotingTo apply the error-correcting output coding methodto the problem in Figure 2, we constructed a 31-biterror-correcting code (via the \\exhaustive\" methoddescribed in Dietterich & Bakiri, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 47109072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d221bbcbd20c7157e4500f942de8ceec490f8936",
            "isKey": false,
            "numCitedBy": 2852,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "slug": "Solving-Multiclass-Learning-Problems-via-Output-Dietterich-Bakiri",
            "title": {
                "fragments": [],
                "text": "Solving Multiclass Learning Problems via Error-Correcting Output Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is demonstrated that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 264
                            }
                        ],
                        "text": "\u2026that is,voting multiple hypotheses that have been con-structed by di erent learning algorithms applied tothe same problem (Bates & Granger, 1969; Makri-dakis & Winkler, 1983; Clemen, 1989; Schapire,1990; Hampshire II & Waibel, 1990; Zhang, Mesirov,& Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15635352,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32a86f4c74d69fc9dc7d43a6b561d39d6bfa8292",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Using-Decision-Trees-to-Improve-Case-Based-Learning-Cardie",
            "title": {
                "fragments": [],
                "text": "Using Decision Trees to Improve Case-Based Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507150"
                        ],
                        "name": "M. Perrone",
                        "slug": "M.-Perrone",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Perrone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Perrone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8884630"
                        ],
                        "name": "L. Cooper",
                        "slug": "L.-Cooper",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Cooper",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cooper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 234
                            }
                        ],
                        "text": "\u2026hypotheses (e.g., as a result of di erent randomseeds in neural network learning or di erent trainingsets in decision tree learning), and then voting thesehypotheses (Hansen & Salamon, 1990; LeBlanc &Tibshirani, 1993; Perrone, 1993; Perrone & Cooper,1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10408361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "595640253ffdfd12e04ac57bd78753f936a7cfad",
            "isKey": false,
            "numCitedBy": 899,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This paper presents a general theoretical framework for ensemble methods of constructing significantly improved regression estimates. Given a population of regression estimators, the authors construct a hybrid estimator that is as good or better in the mean square error sense than any estimator in the population. They argue that the ensemble method presented has several properties: (1) it efficiently uses all the networks of a population -- none of the networks need to be discarded; (2) it efficiently uses all of the available data for training without over-fitting; (3) it inherently performs regularization by smoothing in functional space, which helps to avoid over-fitting; (4) it utilizes local minima to construct improved estimates whereas other neural network algorithms are hindered by local minima; (5) it is ideally suited for parallel computation; (6) it leads to a very useful and natural measure of the number of distinct estimators in a population; and (7) the optimal parameters of the ensemble estimator are given in closed form. Experimental results show that the ensemble method dramatically improves neural network performance on difficult real-world optical character recognition tasks."
            },
            "slug": "When-Networks-Disagree:-Ensemble-Methods-for-Hybrid-Perrone-Cooper",
            "title": {
                "fragments": [],
                "text": "When Networks Disagree: Ensemble Methods for Hybrid Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the ensemble method dramatically improves neural network performance on difficult real-world optical character recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507150"
                        ],
                        "name": "M. Perrone",
                        "slug": "M.-Perrone",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Perrone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Perrone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 219
                            }
                        ],
                        "text": "\u2026hypotheses (e.g., as a result of di erent randomseeds in neural network learning or di erent trainingsets in decision tree learning), and then voting thesehypotheses (Hansen & Salamon, 1990; LeBlanc &Tibshirani, 1993; Perrone, 1993; Perrone & Cooper,1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116953896,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6b7f418ca3583c0657e868bee9d121d1d3a0ad6a",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 149,
            "paperAbstract": {
                "fragments": [],
                "text": "A general theoretical framework for Monte Carlo averaging methods of improving regression estimates is presented with application to neural network classification and time series prediction. Given a population of regression estimators, it is shown how to construct a hybrid estimator which is as good as or better than, in the MSE sense, any estimator in the population. \nIt is argued that the ensemble method presented has several properties: It efficiently uses all the regressors of a population--none need be discarded. It efficiently uses all the available data for training without over-fitting. It inherently performs regularization by smoothing in functional space which helps to avoid over-fitting. It utilizes local minima to construct improved estimates whereas other regression algorithms are hindered by local minima. It is ideally suited for parallel computation. It leads to a very useful and natural measure of the number of distinct estimators in a population. The optimal parameters of the ensemble estimator are given in closed form. \nIt is shown that this result derives from the notion of convexity and can be applied to a wide variety of optimization algorithms including: Mean Square Error, a general class of $L\\sb{p}$-norm cost functions, Maximum Likelihood Estimation, Maximum Entropy, Maximum Mutual Information, the Kullback-Leibler Information (Cross Entropy), Penalized Maximum Likelihood Estimation and Smoothing Splines. \nThe connection to Bayesian Inference is discussed. \nExperimental results on the NIST OCR database, the Turk and Pentland human face database and sunspot time series prediction are presented which demonstrate that the ensemble method dramatically improves regression performance on real-world classification tasks."
            },
            "slug": "Improving-regression-estimation:-Averaging-methods-Perrone",
            "title": {
                "fragments": [],
                "text": "Improving regression estimation: Averaging methods for variance reduction with extensions to general convex measure optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Experimental results are presented which demonstrate that the ensemble method dramatically improves regression performance on real-world classification tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782539"
                        ],
                        "name": "D. Wettschereck",
                        "slug": "D.-Wettschereck",
                        "structuredName": {
                            "firstName": "Dietrich",
                            "lastName": "Wettschereck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wettschereck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 74
                            }
                        ],
                        "text": "Previous experimental research (Bakiri, 1991; Diet-terich & Bakiri, 1991; Wettschereck & Dietterich,1992; Dietterich & Bakiri, 1995) has shown thaterror-correcting output coding uniformly improvesthe classi cation accuracy of decision tree and neural network classi ers when compared with the\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 6472763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "810fa6f8958bcce4b9e9d7b17fddf8de6bfc4591",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Three methods for improving the performance of (gaussian) radial basis function (RBF) networks were tested on the NETtalk task. In RBF, a new example is classified by computing its Euclidean distance to a set of centers chosen by unsupervised methods. The application of supervised learning to learn a non-Euclidean distance metric was found to reduce the error rate of RBF networks, while supervised learning of each center's variance resulted in inferior performance. The best improvement in accuracy was achieved by networks called generalized radial basis function (GRBF) networks. In GRBF, the center locations are determined by supervised learning. After training on 1000 words, RBF classifies 56.5% of letters correct, while GRBF scores 73.4% letters correct (on a separate test set). From these and other experiments, we conclude that supervised learning of center locations can be very important for radial basis function learning."
            },
            "slug": "Improving-the-Performance-of-Radial-Basis-Function-Wettschereck-Dietterich",
            "title": {
                "fragments": [],
                "text": "Improving the Performance of Radial Basis Function Networks by Learning Center Locations"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is concluded that supervised learning of center locations can be very important for radial basis function learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766683"
                        ],
                        "name": "R. Meir",
                        "slug": "R.-Meir",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Meir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Meir"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 184
                            }
                        ],
                        "text": "First, as Breiman(1994) has recently shown, decision tree algorithmssuch as CART and C4.5 have high variance|thatis, the hypotheses produced by these algorithms canchange substantially with small changes in the train-ing set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 272
                            }
                        ],
                        "text": "\u2026hypotheses (e.g., as a result of di erent randomseeds in neural network learning or di erent trainingsets in decision tree learning), and then voting thesehypotheses (Hansen & Salamon, 1990; LeBlanc &Tibshirani, 1993; Perrone, 1993; Perrone & Cooper,1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13882988,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d93a80db1fbb3a09fb5caeacb72f44e8726dcbe1",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the eeect of combining several least squares estimators on the expected performance of a regression problem. Computing the exact bias and variance curves as a function of the sample size we are able to quantitatively compare the eeect of the combination on the bias and variance separately, and thus on the expected error which is the sum of the two. First, we show that by splitting the data set into several independent parts and training each estimator on a diierent subset, the performance can in some cases be signiicantly improved. We nd three basic regions of interest. For a small number of noisy samples the estimation quality is dramatically improved by combining several independent estimators. For intermediate sample sizes, however, the eeect of combining estimators can in fact be deletarious, tending to increase the bias too much. For large sample sizes both the single and the combined estimator approach the same limit. Our results are derived analytically for the case of linear least-squares regression, and are valid for systems of large input dimensions. A deenite conclusion of our work is that substantial improvement in the quality of least-squares estimation is possible by decreasing the variance at the cost of an increase in bias. This gain is especially pronounced for small and noisy data sets. We stress, however, that the approach of estimator combination is not a panacea for constructing improved estimators and must be applied with care."
            },
            "slug": "Bias,-variance-and-the-combination-of-estimators;-Meir",
            "title": {
                "fragments": [],
                "text": "Bias, variance and the combination of estimators; The case of linear least squares"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that substantial improvement in the quality of least-squares estimation is possible by decreasing the variance at the cost of an increase in bias, especially pronounced for small and noisy data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507150"
                        ],
                        "name": "M. Perrone",
                        "slug": "M.-Perrone",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Perrone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Perrone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 410,
                                "start": 398
                            }
                        ],
                        "text": "There has been an explosion of papers showing thatthe classi cation performance of learning algorithmscan be signi cantly improved by generating multi-ple hypotheses (e.g., as a result of di erent randomseeds in neural network learning or di erent trainingsets in decision tree learning), and then voting thesehypotheses (Hansen & Salamon, 1990; LeBlanc &Tibshirani, 1993; Perrone, 1993; Perrone & Cooper,1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 257
                            }
                        ],
                        "text": "\u2026hypotheses (e.g., as a result of di erent randomseeds in neural network learning or di erent trainingsets in decision tree learning), and then voting thesehypotheses (Hansen & Salamon, 1990; LeBlanc &Tibshirani, 1993; Perrone, 1993; Perrone & Cooper,1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12802977,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab3e4a391f4436b38b3aa968f4ea6b2fb48e19d7",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The past several years have seen a tremendous growth in the complexity of the recognition, estimation and control tasks expected of neural networks. In solving these tasks, one is faced with a large variety of learning algorithms and a vast selection of possible network architectures. After all the training, how does one know which is the best network? This decision is further complicated by the fact that standard techniques can be severely limited by problems such as over-fitting, data sparsity and local optima. The usual solution to these problems is a winner-take-all cross-validatory model selection. However, recent experimental and theoretical work indicates that we can improve performance by considering methods for combining neural networks."
            },
            "slug": "Putting-It-All-Together:-Methods-for-Combining-Perrone",
            "title": {
                "fragments": [],
                "text": "Putting It All Together: Methods for Combining Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experimental and theoretical work indicates that the performance of neural networks can be improved by considering methods for combining neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49850030"
                        ],
                        "name": "M. LeBlanc",
                        "slug": "M.-LeBlanc",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "LeBlanc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. LeBlanc"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 102
                            }
                        ],
                        "text": "First, as Breiman(1994) has recently shown, decision tree algorithmssuch as CART and C4.5 have high variance|thatis, the hypotheses produced by these algorithms canchange substantially with small changes in the train-ing set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17882137,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d4c2e0b1b9b9d2671ab4be128878a26d0d4a748c",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider the problem of how to combine a collection of general regression fit vectors to obtain a better predictive model. The individual fits may be from subset linear regression, ridge regression, or something more complex like a neural network. We develop a general framework for this problem and examine a cross-validation\u2014based proposal called \u201cmodel mix\u201d or \u201cstacking\u201d in this context. We also derive combination methods based on the bootstrap and analytic methods and compare them in examples. Finally, we apply these ideas to classification problems where the estimated combination weights can yield insight into the structure of the problem."
            },
            "slug": "Combining-Estimates-in-Regression-and-LeBlanc-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Combining Estimates in Regression and Classification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242194"
                        ],
                        "name": "Ghulum Bakiri",
                        "slug": "Ghulum-Bakiri",
                        "structuredName": {
                            "firstName": "Ghulum",
                            "lastName": "Bakiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ghulum Bakiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 32
                            }
                        ],
                        "text": "Previous experimental research (Bakiri, 1991; Diet-terich & Bakiri, 1991; Wettschereck & Dietterich,1992; Dietterich & Bakiri, 1995) has shown thaterror-correcting output coding uniformly improvesthe classi cation accuracy of decision tree and neural network classi ers when compared with the\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 13
                            }
                        ],
                        "text": "Dietterich & Bakiri (1991, 1995) have shown that the * * * * * * * * * * * 0 -10 -20 -30 10 C4.5 Multiclass Gl as s Vo we l PO S So yb ea n Au dio log y IS OL ET Le tte r NE Tt alk Pe rf or m an ce r el at iv e to M ul tic la ss C4.5 one-per-class C4.5 ECOCFigure 1: Performance of the one-per-class\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60756409,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb0427eeeda79a97e90392c16b40e51e41c5f9d2",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of mapping spelled English words into strings of phonemes and stresses (\"reading aloud\") has many practical applications. Several commercial systems perform this task by applying a knowledge base of expert-supplied letter-to-sound rules. This dissertation presents a set of machine learning methods for automatically constructing letter-to-sound rules by analyzing a dictionary of words and their pronunciations. Taken together, these methods provide a substantial performance improvement over the best commercial system--DECtalk from Digital Equipment Corporation. In a performance test, the learning methods were trained on a dictionary of 19,002 words. Then, human subjects were asked to compare the performance of the resulting letter-to-sound rules against the dictionary for an additional 1,000 words not used during training. In a blind procedure, the subjects rated the pronunciations of both the learned rules and the DECtalk rules according to whether they were noticably different from the dictionary pronunciation. The error rate for the learned rules was 28.8% (288 words noticeably different), while the error rate for the DECtalk rules was 44.3% (433 words noticeably different). If, instead of using human judges, were required that the pronunciations of the letter-to-sound rules exactly match the dictionary to be counted correct, then the error rate for our learned rules is 35.2% and the error rate for DECtalk is 63.6%. Similar results were observed at the level of individual letters, phonemes, and stresses. \nTo achieve these results, several techniques were combined. The key learning technique represents the output classes by the codewords of an error-correcting code. Boolean concept learning methods, such as the standard ID3 decision-tree algorithm, can be applied to learn the individual bits of these codewords. This converts the muticlass learning problem into a number of boolean concept learning problems. This method is shown to be superior to several other methods: multiclass ID3, one-tree-per-class ID3, the domain-specific distributed code employed by T. Sejnowski and C. Rosenberg in their NETtalk system, and a method developed by D. Wolpert. Similar results in the domain of isolated-letter speech recognition with the backpropagation algorithm show that error-correcting output codes provide a domain-independent, algorithm-independent approach to multiclass learning problems."
            },
            "slug": "Converting-English-text-to-speech:-a-machine-Bakiri-Dietterich",
            "title": {
                "fragments": [],
                "text": "Converting English text to speech: a machine learning approach"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A set of machine learning methods for automatically constructing letter-to-sound rules by analyzing a dictionary of words and their pronunciations are presented, showing that error-correcting output codes provide a domain-independent, algorithm-independent approach to multiclass learning problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35191677"
                        ],
                        "name": "J. Hampshire",
                        "slug": "J.-Hampshire",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hampshire",
                            "middleNames": [
                                "B."
                            ],
                            "suffix": "II"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hampshire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1975998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b4081a0b5d775172269854c07dadb0c07977806",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present single- and multispeaker recognition results for the voiced stop consonants /b, d, g/ using time-delay neural networks (TDNN), a new objective function for training these networks, and a simple arbitration scheme for improved classification accuracy. With these enhancements a median 24% reduction in the number of misclassifications made by TDNNs trained with the traditional backpropagation objective function is achieved. This redundant results in /b, d, g/ recognition rates that consistently exceed 98% for TDNNs trained with individual speakers; it yields a 98.1% recognition rate for a TDNN trained with three male speakers.<<ETX>>"
            },
            "slug": "A-novel-objective-function-for-improved-phoneme-Hampshire-Waibel",
            "title": {
                "fragments": [],
                "text": "A novel objective function for improved phoneme recognition using time delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present single- and multispeaker recognition results for the voiced stop consonants /b, d, g/ using time-delay neural networks (TDNN), a new objective function for training these networks, and a simple arbitration scheme for improved classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276105"
                        ],
                        "name": "Spyros Makridakis",
                        "slug": "Spyros-Makridakis",
                        "structuredName": {
                            "firstName": "Spyros",
                            "lastName": "Makridakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Spyros Makridakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830895"
                        ],
                        "name": "R. L. Winkler",
                        "slug": "R.-L.-Winkler",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Winkler",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. L. Winkler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53077627,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "5d78e11a344abaa563d5b90d511a5e631b0ce301",
            "isKey": false,
            "numCitedBy": 589,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "An alternative to using a single forecasting method is to average the forecasts obtained from several methods. In this paper we investigate empirically the impact of the number and choice of forecasting methods on the accuracy of simple averages. It is concluded that the forecasting accuracy improves, and that the variability of accuracy among different combinations decreases, as the number of methods in the average increases. Thus, combining forecasts seems to be a reasonable practical alternative when, as is often the case, a \"true\" model of the data-generating process or a single \"best\" forecasting method cannot be or is not, for whatever reasons, identified."
            },
            "slug": "Averages-of-Forecasts:-Some-Empirical-Results-Makridakis-Winkler",
            "title": {
                "fragments": [],
                "text": "Averages of Forecasts: Some Empirical Results"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 136
                            }
                        ],
                        "text": "Extending notions of bias and variance to classi -cation problems is not straightforward, and many\nalternative approaches are possible (Efron, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120750007,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b7bd8ec9ed208323617540a5e4a4a94bb898dd4f",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider regression situations for which the response variable is dichotomous. The most common analysis fits successively richer linear logistic models and measures the residual variation from the model by minus twice the maximized log likelihood. General measures of residual variation are considered here, including ordinary squared error and prediction error as well as the log likelihood. All of these are shown to be satisfactory in a certain primitive sense, unlike quantitative regression theory where only squared error is logically satisfactory. The relation of Goodman and Kruskal's measures of categorical association to the theory of penalty functions and probability elicitation is demonstrated."
            },
            "slug": "Regression-and-ANOVA-with-Zero-One-Data:-Measures-Efron",
            "title": {
                "fragments": [],
                "text": "Regression and ANOVA with Zero-One Data: Measures of Residual Variation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2558625"
                        ],
                        "name": "R. Clemen",
                        "slug": "R.-Clemen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Clemen",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Clemen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "\u2026that is,voting multiple hypotheses that have been con-structed by di erent learning algorithms applied tothe same problem (Bates & Granger, 1969; Makri-dakis & Winkler, 1983; Clemen, 1989; Schapire,1990; Hampshire II & Waibel, 1990; Zhang, Mesirov,& Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 153675289,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "efcba4b3b8c571d017e10cfebe6343acb1b7ce4f",
            "isKey": false,
            "numCitedBy": 2204,
            "numCiting": 166,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-forecasts:-A-review-and-annotated-Clemen",
            "title": {
                "fragments": [],
                "text": "Combining forecasts: A review and annotated bibliography"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 40
                            }
                        ],
                        "text": "Some learning algorithms, such as C4.5 (Quinlan,1993a) and CART (Breiman, Friedman, Olshen, &Stone, 1984), can solve such k-way classi cationproblems directly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 278
                            }
                        ],
                        "text": "\u2026that is,voting multiple hypotheses that have been con-structed by di erent learning algorithms applied tothe same problem (Bates & Granger, 1969; Makri-dakis & Winkler, 1983; Clemen, 1989; Schapire,1990; Hampshire II & Waibel, 1990; Zhang, Mesirov,& Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 101
                            }
                        ],
                        "text": "Any learning algorithmthat can handle two-class problems, such as the deci-sion tree algorithm C4.5 (Quinlan, 1993a), can thenbe applied to learn each of these L problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2327460,
            "fieldsOfStudy": [
                "Computer Science",
                "Education",
                "Economics"
            ],
            "id": "4f9e6b13e22ae8f3d77b1f5d1c946179e3abfd64",
            "isKey": false,
            "numCitedBy": 653,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-Instance-Based-and-Model-Based-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "Combining Instance-Based and Model-Based Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50982390"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727782"
                        ],
                        "name": "J. Mesirov",
                        "slug": "J.-Mesirov",
                        "structuredName": {
                            "firstName": "Jill",
                            "lastName": "Mesirov",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mesirov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19003314,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31c5dcffbac6cfe1f656b29d05a1c9abb4bbd327",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hybrid-system-for-protein-secondary-structure-Zhang-Mesirov",
            "title": {
                "fragments": [],
                "text": "Hybrid system for protein secondary structure prediction."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32647802"
                        ],
                        "name": "J. M. Bates",
                        "slug": "J.-M.-Bates",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bates",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Bates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49566894"
                        ],
                        "name": "C. Granger",
                        "slug": "C.-Granger",
                        "structuredName": {
                            "firstName": "Clive",
                            "lastName": "Granger",
                            "middleNames": [
                                "William",
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Granger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 148
                            }
                        ],
                        "text": "\u2026non-homogeneous voting, that is,voting multiple hypotheses that have been con-structed by di erent learning algorithms applied tothe same problem (Bates & Granger, 1969; Makri-dakis & Winkler, 1983; Clemen, 1989; Schapire,1990; Hampshire II & Waibel, 1990; Zhang, Mesirov,& Waltz, 1992; Cardie,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14178367,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "ac849c9294e5e0c34b3f159045ca0042f1fc439c",
            "isKey": false,
            "numCitedBy": 2892,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Two separate sets of forecasts of airline passenger data have been combined to form a composite set of forecasts. The main conclusion is that the composite set of forecasts can yield lower mean-square error than either of the original forecasts. Past errors of each of the original forecasts are used to determine the weights to attach to these two original forecasts in forming the combined forecasts, and different methods of deriving these weights are examined."
            },
            "slug": "The-Combination-of-Forecasts-Bates-Granger",
            "title": {
                "fragments": [],
                "text": "The Combination of Forecasts"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 31
                            }
                        ],
                        "text": "Previous experimental research (Bakiri, 1991; Dietterich & Bakiri, 1991; Wettschereck & Dietterich, 1992; Dietterich & Bakiri, 1995) has shown that error-correcting output coding uniformly improves the classi cation accuracy of decision tree and neural network classi ers when compared with the standard approaches to k-class learning problems."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 32
                            }
                        ],
                        "text": "Previous experimental research (Bakiri, 1991; Diet-terich & Bakiri, 1991; Wettschereck & Dietterich,1992; Dietterich & Bakiri, 1995) has shown thaterror-correcting output coding uniformly improvesthe classi cation accuracy of decision tree and neural network classi ers when compared with the\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 13
                            }
                        ],
                        "text": "Dietterich & Bakiri (1991, 1995) have shown that the * * * * * * * * * * * 0 -10 -20 -30 10 C4.5 Multiclass Gl as s Vo we l PO S So yb ea n Au dio log y IS OL ET Le tte r NE Tt alk Pe rf or m an ce r el at iv e to M ul tic la ss C4.5 one-per-class C4.5 ECOCFigure 1: Performance of the one-per-class\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Converting English text to speech"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 40
                            }
                        ],
                        "text": "Some learning algorithms, such as C4.5 (Quinlan,1993a) and CART (Breiman, Friedman, Olshen, &Stone, 1984), can solve such k-way classi cationproblems directly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 278
                            }
                        ],
                        "text": "\u2026that is,voting multiple hypotheses that have been con-structed by di erent learning algorithms applied tothe same problem (Bates & Granger, 1969; Makri-dakis & Winkler, 1983; Clemen, 1989; Schapire,1990; Hampshire II & Waibel, 1990; Zhang, Mesirov,& Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 101
                            }
                        ],
                        "text": "Any learning algorithmthat can handle two-class problems, such as the deci-sion tree algorithm C4.5 (Quinlan, 1993a), can thenbe applied to learn each of these L problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Empirical Learning"
            },
            "venue": {
                "fragments": [],
                "text": "C4.5: Programs for Empirical Learning"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 264
                            }
                        ],
                        "text": "\u2026that is,voting multiple hypotheses that have been con-structed by di erent learning algorithms applied tothe same problem (Bates & Granger, 1969; Makri-dakis & Winkler, 1983; Clemen, 1989; Schapire,1990; Hampshire II & Waibel, 1990; Zhang, Mesirov,& Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 378,
                                "start": 207
                            }
                        ],
                        "text": "Some papers have also reported improved performance through non-homogeneous voting, that is, voting multiple hypotheses that have been constructed by di erent learning algorithms applied to the same problem (Bates & Granger, 1969; Makridakis & Winkler, 1983; Clemen, 1989; Schapire, 1990; Hampshire II & Waibel, 1990; Zhang, Mesirov, & Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using decision trees to improve"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "\u2026that is,voting multiple hypotheses that have been con-structed by di erent learning algorithms applied tothe same problem (Bates & Granger, 1969; Makri-dakis & Winkler, 1983; Clemen, 1989; Schapire,1990; Hampshire II & Waibel, 1990; Zhang, Mesirov,& Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 378,
                                "start": 207
                            }
                        ],
                        "text": "Some papers have also reported improved performance through non-homogeneous voting, that is, voting multiple hypotheses that have been constructed by di erent learning algorithms applied to the same problem (Bates & Granger, 1969; Makridakis & Winkler, 1983; Clemen, 1989; Schapire, 1990; Hampshire II & Waibel, 1990; Zhang, Mesirov, & Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining forcasts: A"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 136
                            }
                        ],
                        "text": "Extending notions of bias and variance to classi -cation problems is not straightforward, and many\nalternative approaches are possible (Efron, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 134
                            }
                        ],
                        "text": "Extending notions of bias and variance to classi cation problems is not straightforward, and many alternative approaches are possible (Efron, 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regression and ANOVA with zero"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 410,
                                "start": 398
                            }
                        ],
                        "text": "There has been an explosion of papers showing thatthe classi cation performance of learning algorithmscan be signi cantly improved by generating multi-ple hypotheses (e.g., as a result of di erent randomseeds in neural network learning or di erent trainingsets in decision tree learning), and then voting thesehypotheses (Hansen & Salamon, 1990; LeBlanc &Tibshirani, 1993; Perrone, 1993; Perrone & Cooper,1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 153
                            }
                        ],
                        "text": ", as a result of di erent random seeds in neural network learning or di erent training sets in decision tree learning), and then voting these hypotheses (Hansen & Salamon, 1990; LeBlanc & Tibshirani, 1993; Perrone, 1993; Perrone & Cooper, 1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 257
                            }
                        ],
                        "text": "\u2026hypotheses (e.g., as a result of di erent randomseeds in neural network learning or di erent trainingsets in decision tree learning), and then voting thesehypotheses (Hansen & Salamon, 1990; LeBlanc &Tibshirani, 1993; Perrone, 1993; Perrone & Cooper,1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Putting it all together: Meth"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 153
                            }
                        ],
                        "text": ", as a result of di erent random seeds in neural network learning or di erent training sets in decision tree learning), and then voting these hypotheses (Hansen & Salamon, 1990; LeBlanc & Tibshirani, 1993; Perrone, 1993; Perrone & Cooper, 1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 219
                            }
                        ],
                        "text": "\u2026hypotheses (e.g., as a result of di erent randomseeds in neural network learning or di erent trainingsets in decision tree learning), and then voting thesehypotheses (Hansen & Salamon, 1990; LeBlanc &Tibshirani, 1993; Perrone, 1993; Perrone & Cooper,1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving regression estima"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "\u2026that is,voting multiple hypotheses that have been con-structed by di erent learning algorithms applied tothe same problem (Bates & Granger, 1969; Makri-dakis & Winkler, 1983; Clemen, 1989; Schapire,1990; Hampshire II & Waibel, 1990; Zhang, Mesirov,& Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining forcasts: A review and annotated bibliography"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. of Forecasting"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 153
                            }
                        ],
                        "text": ", as a result of di erent random seeds in neural network learning or di erent training sets in decision tree learning), and then voting these hypotheses (Hansen & Salamon, 1990; LeBlanc & Tibshirani, 1993; Perrone, 1993; Perrone & Cooper, 1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 272
                            }
                        ],
                        "text": "\u2026hypotheses (e.g., as a result of di erent randomseeds in neural network learning or di erent trainingsets in decision tree learning), and then voting thesehypotheses (Hansen & Salamon, 1990; LeBlanc &Tibshirani, 1993; Perrone, 1993; Perrone & Cooper,1993; Perrone, 1994; Meir, 1994; Breiman, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bias, variance, and the combination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 190
                            }
                        ],
                        "text": "\u2026that is,voting multiple hypotheses that have been con-structed by di erent learning algorithms applied tothe same problem (Bates & Granger, 1969; Makri-dakis & Winkler, 1983; Clemen, 1989; Schapire,1990; Hampshire II & Waibel, 1990; Zhang, Mesirov,& Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 378,
                                "start": 207
                            }
                        ],
                        "text": "Some papers have also reported improved performance through non-homogeneous voting, that is, voting multiple hypotheses that have been constructed by di erent learning algorithms applied to the same problem (Bates & Granger, 1969; Makridakis & Winkler, 1983; Clemen, 1989; Schapire, 1990; Hampshire II & Waibel, 1990; Zhang, Mesirov, & Waltz, 1992; Cardie, 1993; Quinlan, 1993b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The strength of weak learn"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 4,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 28,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Error-Correcting-Output-Coding-Corrects-Bias-and-Kong-Dietterich/25744dbb4294fe7abb2d9b1b0d39006482ebb4ab?sort=total-citations"
}