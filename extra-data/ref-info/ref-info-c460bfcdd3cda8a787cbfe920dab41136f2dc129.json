{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2517825"
                        ],
                        "name": "J. Krishnamurthy",
                        "slug": "J.-Krishnamurthy",
                        "structuredName": {
                            "firstName": "Jayant",
                            "lastName": "Krishnamurthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krishnamurthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 68
                            }
                        ],
                        "text": "This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5633240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ffc1d8ec7b86a01b047d2c1ce66708a496a3f8a",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependency-parsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-the-art accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form."
            },
            "slug": "Weakly-Supervised-Training-of-Semantic-Parsers-Krishnamurthy-Mitchell",
            "title": {
                "fragments": [],
                "text": "Weakly Supervised Training of Semantic Parsers"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work presents a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences, and demonstrates recovery of this richer structure by extracting logical forms from natural language queries against Freebase."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2616463"
                        ],
                        "name": "Panupong Pasupat",
                        "slug": "Panupong-Pasupat",
                        "structuredName": {
                            "firstName": "Panupong",
                            "lastName": "Pasupat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Panupong Pasupat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 84
                            }
                        ],
                        "text": "Of the two mentioned problems, spuriousness has attracted relatively less attention (Pasupat and Liang, 2016; Guu et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 143
                            }
                        ],
                        "text": "Pasupat and Liang (2015) used manual rules to prune unlikely programs on the WIKITABLEQUESTIONS dataset, and then later utilized crowdsourcing (Pasupat and Liang, 2016) to eliminate spurious programs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2434931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d6cd38755e2cf2bd43e75c532a201e20ede128b",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A core problem in learning semantic parsers from denotations is picking out consistent logical forms--those that yield the correct denotation--from a combinatorially large space. To control the search space, previous work relied on restricted set of rules, which limits expressivity. In this paper, we consider a much more expressive class of logical forms, and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms. Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance. To address this, we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms. On the WikiTableQuestions dataset, we increase the coverage of answerable questions from 53.5% to 76%, and the additional crowdsourced supervision lets us rule out 92.1% of spurious logical forms."
            },
            "slug": "Inferring-Logical-Forms-From-Denotations-Pasupat-Liang",
            "title": {
                "fragments": [],
                "text": "Inferring Logical Forms From Denotations"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper generates fictitious worlds and uses crowdsourced denotations on these worlds to filter out spurious logical forms, and shows how to use dynamic programming to efficiently represent the complete set of consistent logical forms."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091768"
                        ],
                        "name": "Kelvin Guu",
                        "slug": "Kelvin-Guu",
                        "structuredName": {
                            "firstName": "Kelvin",
                            "lastName": "Guu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kelvin Guu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2616463"
                        ],
                        "name": "Panupong Pasupat",
                        "slug": "Panupong-Pasupat",
                        "structuredName": {
                            "firstName": "Panupong",
                            "lastName": "Pasupat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Panupong Pasupat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7301007"
                        ],
                        "name": "E. Liu",
                        "slug": "E.-Liu",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Liu",
                            "middleNames": [
                                "Zheran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 110
                            }
                        ],
                        "text": "Of the two mentioned problems, spuriousness has attracted relatively less attention (Pasupat and Liang, 2016; Guu et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 147
                            }
                        ],
                        "text": "First, we use beam search at decoding time and when training from weak supervision (see Section 4), as was done in prior work (Liang et al., 2017; Guu et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 54
                            }
                        ],
                        "text": "We base our model on the semantic parser presented by Guu et al. (2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 133
                            }
                        ],
                        "text": "We develop a formal language for visual reasoning, inspired by Johnson et al. (2017b), and a semantic parser based on recent work by Guu et al. (2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Guu et al. (2017) proposed RANDOMER, a method for increasing exploration and handling spuriousness by adding randomness to beam search and a proposing a \u201cmeritocratic\u201d weighting scheme for gradients."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 55
                            }
                        ],
                        "text": "To train a weakly-supervised semantic parser we follow Guu et al. (2017) and treat the program z as a latent variable that is approximately marginalized."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 125
                            }
                        ],
                        "text": "In the weakly-supervised parser we encourage exploration at training time by using meritocratic gradient updates with \u03b2 = 0.5 (Guu et al., 2017), where the weight of a program with correct denotation is an interpolation of the model\nprobability distribution and a uniform distribution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9268430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ae84940e212e2e3ca132c7aa0878baf0fda06cd",
            "isKey": true,
            "numCitedBy": 158,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task."
            },
            "slug": "From-Language-to-Programs:-Bridging-Reinforcement-Guu-Pasupat",
            "title": {
                "fragments": [],
                "text": "From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "The goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available, and a new algorithm is presented that guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145246869"
                        ],
                        "name": "Chen Liang",
                        "slug": "Chen-Liang",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750652"
                        ],
                        "name": "Jonathan Berant",
                        "slug": "Jonathan-Berant",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Berant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Berant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713121"
                        ],
                        "name": "Kenneth D. Forbus",
                        "slug": "Kenneth-D.-Forbus",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Forbus",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth D. Forbus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1914797"
                        ],
                        "name": "N. Lao",
                        "slug": "N.-Lao",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Lao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Lao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 82
                            }
                        ],
                        "text": ", 2013), but recently denotations have been used to train neural semantic parsers (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 205
                            }
                        ],
                        "text": "\u2026on traditional loglinear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013), but recently denotations have been used to train neural semantic parsers as well (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Liang et al. (2017) also suggested to use a cache for adding correct programs to the beam, but they did not use any example abstraction, and only stored full programs as a way to stabilize learning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 127
                            }
                        ],
                        "text": "First, we use beam search at decoding time and when training from weak supervision (see Section 4), as was done in prior work (Liang et al., 2017; Guu et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2742513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ca430d4640afa4a3838371a08f8f418284bdb7c",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural \u201cprogrammer\u201d, i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \u201ccomputer\u201d, i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge."
            },
            "slug": "Neural-Symbolic-Machines:-Learning-Semantic-Parsers-Liang-Berant",
            "title": {
                "fragments": [],
                "text": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A Neural Symbolic Machine is introduced, which contains a neural \u201cprogrammer\u201d that maps language utterances to programs and utilizes a key-variable memory to handle compositionality, and a symbolic \u201ccomputer\u201d, i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3167681"
                        ],
                        "name": "Yoav Artzi",
                        "slug": "Yoav-Artzi",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Artzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Artzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 68
                            }
                        ],
                        "text": "This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9963298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cde902f11b0870c695428d865a35eb819b1d24b7",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "The context in which language is used provides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning. It also enables algorithms that learn while executing instructions, for example by trying to replicate human actions. Experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60% more instruction sets relative to the previous state of the art."
            },
            "slug": "Weakly-Supervised-Learning-of-Semantic-Parsers-for-Artzi-Zettlemoyer",
            "title": {
                "fragments": [],
                "text": "Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This paper shows semantic parsing can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision."
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the Association for Computational Linguistics"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50649676"
                        ],
                        "name": "J. Clarke",
                        "slug": "J.-Clarke",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Clarke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Clarke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2877164"
                        ],
                        "name": "Dan Goldwasser",
                        "slug": "Dan-Goldwasser",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Goldwasser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Goldwasser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 52
                            }
                        ],
                        "text": "Early work focused on traditional loglinear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013), but recently denotations have been used to train neural semantic parsers as well (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 68
                            }
                        ],
                        "text": "This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 82
                            }
                        ],
                        "text": "This has led to a significant body of work on weakly-supervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and\n\u2217 Authors equally contributed to this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 52
                            }
                        ],
                        "text": "Early work focused on traditional log-linear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), but recently denotations have been used to train neural semantic parsers (Liang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5667590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92fb5e045bb23f13c11d8bb277925013b24b5930",
            "isKey": true,
            "numCitedBy": 249,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers."
            },
            "slug": "Driving-Semantic-Parsing-from-the-World\u2019s-Response-Clarke-Goldwasser",
            "title": {
                "fragments": [],
                "text": "Driving Semantic Parsing from the World\u2019s Response"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper develops two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world and reformulates the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing the parser to scale better using less supervision."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750652"
                        ],
                        "name": "Jonathan Berant",
                        "slug": "Jonathan-Berant",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Berant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Berant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059149862"
                        ],
                        "name": "A. Chou",
                        "slug": "A.-Chou",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Chou",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Chou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34765463"
                        ],
                        "name": "Roy Frostig",
                        "slug": "Roy-Frostig",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Frostig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roy Frostig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 119
                            }
                        ],
                        "text": "Early work focused on traditional loglinear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013), but recently denotations have been used to train neural semantic parsers as well (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 68
                            }
                        ],
                        "text": "This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 183
                            }
                        ],
                        "text": "This has led to a significant body of work on weakly-supervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and\n\u2217 Authors equally contributed to this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6401679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b29447ba499507a259ae9d8f685d60cc1597d7d3",
            "isKey": false,
            "numCitedBy": 1337,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline."
            },
            "slug": "Semantic-Parsing-on-Freebase-from-Question-Answer-Berant-Chou",
            "title": {
                "fragments": [],
                "text": "Semantic Parsing on Freebase from Question-Answer Pairs"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This paper trains a semantic parser that scales up to Freebase and outperforms their state-of-the-art parser on the dataset of Cai and Yates (2013), despite not having annotated logical forms."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2517825"
                        ],
                        "name": "J. Krishnamurthy",
                        "slug": "J.-Krishnamurthy",
                        "structuredName": {
                            "firstName": "Jayant",
                            "lastName": "Krishnamurthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krishnamurthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2697425"
                        ],
                        "name": "Pradeep Dasigi",
                        "slug": "Pradeep-Dasigi",
                        "structuredName": {
                            "firstName": "Pradeep",
                            "lastName": "Dasigi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pradeep Dasigi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40642935"
                        ],
                        "name": "Matt Gardner",
                        "slug": "Matt-Gardner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Gardner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 82
                            }
                        ],
                        "text": ", 2013), but recently denotations have been used to train neural semantic parsers (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 225
                            }
                        ],
                        "text": "\u2026on traditional loglinear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013), but recently denotations have been used to train neural semantic parsers as well (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 199
                            }
                        ],
                        "text": "Second, we utilize our semantic typing system to only construct programs that are syntactically valid, and substantially prune the program search space (similar to the type constraints introduced by Krishnamurthy et al. (2017))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1675452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c6f58ed0ebf379858c0bbe02c53ee51b3eb398a",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new semantic parsing model for answering compositional questions on semi-structured Wikipedia tables. Our parser is an encoder-decoder neural network with two key technical innovations: (1) a grammar for the decoder that only generates well-typed logical forms; and (2) an entity embedding and linking module that identifies entity mentions while generalizing across tables. We also introduce a novel method for training our neural model with question-answer supervision. On the WikiTableQuestions data set, our parser achieves a state-of-the-art accuracy of 43.3% for a single model and 45.9% for a 5-model ensemble, improving on the best prior score of 38.7% set by a 15-model ensemble. These results suggest that type constraints and entity linking are valuable components to incorporate in neural semantic parsers."
            },
            "slug": "Neural-Semantic-Parsing-with-Type-Constraints-for-Krishnamurthy-Dasigi",
            "title": {
                "fragments": [],
                "text": "Neural Semantic Parsing with Type Constraints for Semi-Structured Tables"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A new semantic parsing model for answering compositional questions on semi-structured Wikipedia tables with a state-of-the-art accuracy and type constraints and entity linking are valuable components to incorporate in neural semantic parsers."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33945581"
                        ],
                        "name": "Chunyang Xiao",
                        "slug": "Chunyang-Xiao",
                        "structuredName": {
                            "firstName": "Chunyang",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunyang Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2954698"
                        ],
                        "name": "Marc Dymetman",
                        "slug": "Marc-Dymetman",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Dymetman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Dymetman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794075"
                        ],
                        "name": "Claire Gardent",
                        "slug": "Claire-Gardent",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Gardent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Gardent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16911296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcf16c08a41009d9f9174c6f72b2ff534232c147",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query. Building on recent work by (Wang et al., 2015), the interpretable logical forms, which are structured objects obeying certain constraints, are enumerated by an underlying grammar and are paired with their canonical realizations. In order to use sequence prediction, we need to sequentialize these logical forms. We compare three sequentializations: a direct linearization of the logical form, a linearization of the associated canonical realization, and a sequence consisting of derivation steps relative to the underlying grammar. We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNN-based sequential predictor. Our experiments show important improvements over previous results for the same dataset, and also demonstrate the advantage of incorporating the grammatical constraints."
            },
            "slug": "Sequence-based-Structured-Prediction-for-Semantic-Xiao-Dymetman",
            "title": {
                "fragments": [],
                "text": "Sequence-based Structured Prediction for Semantic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query and shows how grammatical constraints on the derivation sequence can easily be integrated inside the RNN-based sequential predictor."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15652489"
                        ],
                        "name": "T. Kwiatkowski",
                        "slug": "T.-Kwiatkowski",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Kwiatkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kwiatkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2890423"
                        ],
                        "name": "Eunsol Choi",
                        "slug": "Eunsol-Choi",
                        "structuredName": {
                            "firstName": "Eunsol",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eunsol Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3167681"
                        ],
                        "name": "Yoav Artzi",
                        "slug": "Yoav-Artzi",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Artzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Artzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 93
                            }
                        ],
                        "text": "Early work focused on traditional loglinear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013), but recently denotations have been used to train neural semantic parsers as well (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 68
                            }
                        ],
                        "text": "This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 157
                            }
                        ],
                        "text": "This has led to a significant body of work on weakly-supervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and\n\u2217 Authors equally contributed to this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 52
                            }
                        ],
                        "text": "Early work focused on traditional log-linear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), but recently denotations have been used to train neural semantic parsers (Liang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14341841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2ac51e10a3510eadac5eac5e4fb828f086fab88",
            "isKey": true,
            "numCitedBy": 317,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology. For example, even simple phrases such as \u2018daughter\u2019 and \u2018number of people living in\u2019 cannot be directly represented in Freebase, whose ontology instead encodes facts about gender, parenthood, and population. In this paper, we introduce a new semantic parsing approach that learns to resolve such ontological mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus."
            },
            "slug": "Scaling-Semantic-Parsers-with-On-the-Fly-Ontology-Kwiatkowski-Choi",
            "title": {
                "fragments": [],
                "text": "Scaling Semantic Parsers with On-the-Fly Ontology Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new semantic parsing approach that learns to resolve ontological mismatches, which is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075292388"
                        ],
                        "name": "P. Liang",
                        "slug": "P.-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 73
                            }
                        ],
                        "text": "Early work focused on traditional loglinear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013), but recently denotations have been used to train neural semantic parsers as well (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017;\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 68
                            }
                        ],
                        "text": "This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 103
                            }
                        ],
                        "text": "This has led to a significant body of work on weakly-supervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and\n\u2217 Authors equally contributed to this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 52
                            }
                        ],
                        "text": "Early work focused on traditional log-linear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), but recently denotations have been used to train neural semantic parsers (Liang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 340852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ecd3e00bbbfd94446c3adc9c6878de27e250f7c",
            "isKey": true,
            "numCitedBy": 568,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose we want to build a system that answers a natural language question by representing its semantics as a logical forxm and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive.Our goal is to instead learn a semantic parser from question\u2013answer pairs, where the logical form is modeled as a latent variable. We develop a new semantic formalism, dependency-based compositional semantics (DCS) and define a log-linear distribution over DCS logical forms. The model parameters are estimated using a simple procedure that alternates between beam search and numerical optimization. On two standard semantic parsing benchmarks, we show that our system obtains comparable accuracies to even state-of-the-art systems that do require annotated logical forms."
            },
            "slug": "Learning-Dependency-Based-Compositional-Semantics-Liang-Jordan",
            "title": {
                "fragments": [],
                "text": "Learning Dependency-Based Compositional Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new semantic formalism, dependency-based compositional semantics (DCS) is developed and a log-linear distribution over DCS logical forms is defined and it is shown that the system obtains comparable accuracies to even state-of-the-art systems that do require annotated logical forms."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422908"
                        ],
                        "name": "Robin Jia",
                        "slug": "Robin-Jia",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robin Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7218315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7eac64a8410976759445cce235469163d23ee65",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision."
            },
            "slug": "Data-Recombination-for-Neural-Semantic-Parsing-Jia-Liang",
            "title": {
                "fragments": [],
                "text": "Data Recombination for Neural Semantic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Data recombination improves the accuracy of the RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1941442"
                        ],
                        "name": "Jianpeng Cheng",
                        "slug": "Jianpeng-Cheng",
                        "structuredName": {
                            "firstName": "Jianpeng",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianpeng Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145732771"
                        ],
                        "name": "Siva Reddy",
                        "slug": "Siva-Reddy",
                        "structuredName": {
                            "firstName": "Siva",
                            "lastName": "Reddy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siva Reddy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714061"
                        ],
                        "name": "V. Saraswat",
                        "slug": "V.-Saraswat",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Saraswat",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Saraswat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 82
                            }
                        ],
                        "text": ", 2013), but recently denotations have been used to train neural semantic parsers (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 278
                            }
                        ],
                        "text": "\u2026on traditional loglinear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013), but recently denotations have been used to train neural semantic parsers as well (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2423360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db5be062d8b26b2e636b57cc8ca9761b673ec0e6",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones."
            },
            "slug": "Learning-Structured-Natural-Language-for-Semantic-Cheng-Reddy",
            "title": {
                "fragments": [],
                "text": "Learning Structured Natural Language Representations for Semantic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A neural semantic parser which is interpretable and scalable, which converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2616463"
                        ],
                        "name": "Panupong Pasupat",
                        "slug": "Panupong-Pasupat",
                        "structuredName": {
                            "firstName": "Panupong",
                            "lastName": "Pasupat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Panupong Pasupat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9027681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b41e95c8c97846d5ca4c11ef79d7814499cc9663",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available."
            },
            "slug": "Compositional-Semantic-Parsing-on-Semi-Structured-Pasupat-Liang",
            "title": {
                "fragments": [],
                "text": "Compositional Semantic Parsing on Semi-Structured Tables"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes a logical-form driven parsing algorithm guided by strong typing constraints and shows that it obtains significant improvements over natural baselines and is made publicly available."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710450"
                        ],
                        "name": "J. Zelle",
                        "slug": "J.-Zelle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Zelle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 96
                            }
                        ],
                        "text": "supervised learning, where training examples included pairs of language utterances and programs (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 263135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7c0e47f8b768258b7d536c21b218e6c46ab8791",
            "isKey": false,
            "numCitedBy": 642,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application."
            },
            "slug": "Learning-to-Parse-Database-Queries-Using-Inductive-Zelle-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning to Parse Database Queries Using Inductive Logic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart, and provide direct evidence of the utility of an empirical approach at the level of a complete natural language application."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 2"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12728987,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "774113732db34ce0b797fc3dcceded811fb6edbc",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar\u2014for example allowing flexible word order, or insertion of lexical items\u2014 with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006)."
            },
            "slug": "Online-Learning-of-Relaxed-CCG-Grammars-for-Parsing-Zettlemoyer-Collins",
            "title": {
                "fragments": [],
                "text": "Online Learning of Relaxed CCG Grammars for Parsing to Logical Form"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar\u2014for example allowing flexible word order, or insertion of lexical items\u2014 with learned costs."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 449252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74fe7ec751cd50295b15cfd46389a8fefb37c414",
            "isKey": false,
            "numCitedBy": 874,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of mapping natural language sentences to lambda\u2013calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains."
            },
            "slug": "Learning-to-Map-Sentences-to-Logical-Form:-with-Zettlemoyer-Collins",
            "title": {
                "fragments": [],
                "text": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A learning algorithm is described that takes as input a training set of sentences labeled with expressions in the lambda calculus and induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874347"
                        ],
                        "name": "Ronghang Hu",
                        "slug": "Ronghang-Hu",
                        "structuredName": {
                            "firstName": "Ronghang",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronghang Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112400"
                        ],
                        "name": "Jacob Andreas",
                        "slug": "Jacob-Andreas",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Andreas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Andreas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2903226"
                        ],
                        "name": "Kate Saenko",
                        "slug": "Kate-Saenko",
                        "structuredName": {
                            "firstName": "Kate",
                            "lastName": "Saenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kate Saenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 113
                            }
                        ],
                        "text": "We define a programming language over the KB that is more amenable to spatial reasoning, inspired by work on the CLEVR dataset (Johnson et al., 2017b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 164
                            }
                        ],
                        "text": "CNLVR as a semantic parsing problem: our goal is to translate language utterances into programs that will be executed against the KB to determine their correctness (Johnson et al., 2017b; Hu et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 118
                            }
                        ],
                        "text": "Treating the problem of visual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR (Hu et al., 2017; Johnson et al., 2017b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 108
                            }
                        ],
                        "text": "We provide a full description of all program tokens, their arguments and return types in Appendix A.\nUnlike CLEVR, CNLVR requires substantial set-theoretic reasoning (utterances refer to various aspects of sets of items in one of the three boxes in the image), which required extending the language described by Johnson et al. (2017b) to include set operators and lambda abstraction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 121
                            }
                        ],
                        "text": "Visual reasoning has attracted considerable attention, with the release of datasets such as VQA (Antol et al., 2015) and CLEVR (Johnson et al., 2017a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 227
                            }
                        ],
                        "text": "Given the input KB, it is easy to view CNLVR as a semantic parsing problem: our goal is to translate language utterances into programs that will be executed against the KB to determine their correctness (Johnson et al., 2017b; Hu et al., 2017)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 102
                            }
                        ],
                        "text": "Treating visual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR (Hu et al., 2017; Johnson et al., 2017b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a396a6febdacb84340d139096455e67049ac1e22",
            "isKey": true,
            "numCitedBy": 430,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer \u201cis there an equal number of balls and boxes?\u201d we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture [3, 2] implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50% relative to state-of-theart attentional approaches, while discovering interpretable network architectures specialized for each question."
            },
            "slug": "Learning-to-Reason:-End-to-End-Module-Networks-for-Hu-Andreas",
            "title": {
                "fragments": [],
                "text": "Learning to Reason: End-to-End Module Networks for Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "End-to-End Module Networks are proposed, which learn to reason by directly predicting instance-specific network layouts without the aid of a parser, and achieve an error reduction of nearly 50% relative to state-of-theart attentional approaches."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054915766"
                        ],
                        "name": "Qingqing Cai",
                        "slug": "Qingqing-Cai",
                        "structuredName": {
                            "firstName": "Qingqing",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingqing Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3321874"
                        ],
                        "name": "A. Yates",
                        "slug": "A.-Yates",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Yates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yates"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 68
                            }
                        ],
                        "text": "This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2265838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80c2d8c691b09f8b4e53f512b9d2641b49fda935",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised training procedures for semantic parsers produce high-quality semantic parsers, but they have difficulty scaling to large databases because of the sheer number of logical constants for which they must see labeled training data. We present a technique for developing semantic parsers for large databases based on a reduction to standard supervised training algorithms, schema matching, and pattern learning. Leveraging techniques from each of these areas, we develop a semantic parser for Freebase that is capable of parsing questions with an F1 that improves by 0.42 over a purely-supervised learning algorithm."
            },
            "slug": "Large-scale-Semantic-Parsing-via-Schema-Matching-Cai-Yates",
            "title": {
                "fragments": [],
                "text": "Large-scale Semantic Parsing via Schema Matching and Lexicon Extension"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A semantic parser for Freebase is developed based on a reduction to standard supervised training algorithms, schema matching, and pattern learning that is capable of parsing questions with an F1 that improves by 0.42 over a purely-supervised learning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108641"
                        ],
                        "name": "Rohit J. Kate",
                        "slug": "Rohit-J.-Kate",
                        "structuredName": {
                            "firstName": "Rohit",
                            "lastName": "Kate",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rohit J. Kate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8075439"
                        ],
                        "name": "Y. W. Wong",
                        "slug": "Y.-W.-Wong",
                        "structuredName": {
                            "firstName": "Yuk",
                            "lastName": "Wong",
                            "middleNames": [
                                "Wah"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. W. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 25
                            }
                        ],
                        "text": "(Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005, 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 96
                            }
                        ],
                        "text": "supervised learning, where training examples included pairs of language utterances and programs (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7396224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dd9fd6a45afd266d48255c398429e01ea4fd6db",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for inducing transformation rules that map natural-language sentences into a formal query or command language. The approach assumes a formal grammar for the target representation language and learns transformation rules that exploit the non-terminal symbols in this grammar. The learned transformation rules incrementally map a natural-language sentence or its syntactic parse tree into a parse-tree for the target formal language. Experimental results are presented for two corpora. one which maps English instructions into an existing formal coaching language for simulated RoboCup soccer agents, and another which maps English U.S.-geography questions into a database query language. We show that our method performs overall better and faster than previous approaches in both domains."
            },
            "slug": "Learning-to-Transform-Natural-to-Formal-Languages-Kate-Wong",
            "title": {
                "fragments": [],
                "text": "Learning to Transform Natural to Formal Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper presents a method for inducing transformation rules that map natural-language sentences into a formal query or command language and shows that this method performs overall better and faster than previous approaches in both domains."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40354648"
                        ],
                        "name": "Maxim Rabinovich",
                        "slug": "Maxim-Rabinovich",
                        "structuredName": {
                            "firstName": "Maxim",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maxim Rabinovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144872294"
                        ],
                        "name": "Mitchell Stern",
                        "slug": "Mitchell-Stern",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Stern",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mitchell Stern"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 82
                            }
                        ],
                        "text": ", 2013), but recently denotations have been used to train neural semantic parsers (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 253
                            }
                        ],
                        "text": "\u2026on traditional loglinear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013), but recently denotations have been used to train neural semantic parsers as well (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13529592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c1e874c3b67510a3215e535f5646b362de5bc89",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering."
            },
            "slug": "Abstract-Syntax-Networks-for-Code-Generation-and-Rabinovich-Stern",
            "title": {
                "fragments": [],
                "text": "Abstract Syntax Networks for Code Generation and Semantic Parsing"
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 66
                            }
                        ],
                        "text": "In their work, they use an standard encoder-decoder architecture (Sutskever et al., 2014) to define a distribution p\u03b8(z | x, k)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 56
                            }
                        ],
                        "text": "In their work, they used an encoderdecoder architecture (Sutskever et al., 2014) to define a distribution p\u03b8(z | x)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7961699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "isKey": false,
            "numCitedBy": 14881,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
            },
            "slug": "Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals",
            "title": {
                "fragments": [],
                "text": "Sequence to Sequence Learning with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure, and finds that reversing the order of the words in all source sentences improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73710317"
                        ],
                        "name": "B. Hariharan",
                        "slug": "B.-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803520"
                        ],
                        "name": "L. V. D. Maaten",
                        "slug": "L.-V.-D.-Maaten",
                        "structuredName": {
                            "firstName": "Laurens",
                            "lastName": "Maaten",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. V. D. Maaten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 128
                            }
                        ],
                        "text": "We define a programming language over the KB that is more amenable to spatial reasoning, inspired by work on the CLEVR dataset (Johnson et al., 2017b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 135
                            }
                        ],
                        "text": "Treating the problem of visual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR (Hu et al., 2017; Johnson et al., 2017b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 108
                            }
                        ],
                        "text": "We provide a full description of all program tokens, their arguments and return types in Appendix A.\nUnlike CLEVR, CNLVR requires substantial set-theoretic reasoning (utterances refer to various aspects of sets of items in one of the three boxes in the image), which required extending the language described by Johnson et al. (2017b) to include set operators and lambda abstraction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 227
                            }
                        ],
                        "text": "\u2026in Appendix A.\nUnlike CLEVR, CNLVR requires substantial set-theoretic reasoning (utterances refer to various aspects of sets of items in one of the three boxes in the image), which required extending the language described by Johnson et al. (2017b) to include set operators and lambda abstraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 128
                            }
                        ],
                        "text": "Visual reasoning has attracted considerable attention, with the release of datasets such as VQA (Antol et al., 2015) and CLEVR (Johnson et al., 2017a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 63
                            }
                        ],
                        "text": "We develop a formal language for visual reasoning, inspired by Johnson et al. (2017b), and a semantic parser based on recent work by Guu et al. (2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 204
                            }
                        ],
                        "text": "Given the input KB, it is easy to view CNLVR as a semantic parsing problem: our goal is to translate language utterances into programs that will be executed against the KB to determine their correctness (Johnson et al., 2017b; Hu et al., 2017)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15458100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03eb382e04cca8cca743f7799070869954f1402a",
            "isKey": true,
            "numCitedBy": 1223,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover short-comings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations."
            },
            "slug": "CLEVR:-A-Diagnostic-Dataset-for-Compositional-and-Johnson-Hariharan",
            "title": {
                "fragments": [],
                "text": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a diagnostic dataset that tests a range of visual reasoning abilities and uses this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801949"
                        ],
                        "name": "Aishwarya Agrawal",
                        "slug": "Aishwarya-Agrawal",
                        "structuredName": {
                            "firstName": "Aishwarya",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aishwarya Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8553015"
                        ],
                        "name": "Jiasen Lu",
                        "slug": "Jiasen-Lu",
                        "structuredName": {
                            "firstName": "Jiasen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiasen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963421"
                        ],
                        "name": "Stanislaw Antol",
                        "slug": "Stanislaw-Antol",
                        "structuredName": {
                            "firstName": "Stanislaw",
                            "lastName": "Antol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanislaw Antol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067793408"
                        ],
                        "name": "Margaret Mitchell",
                        "slug": "Margaret-Mitchell",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 97
                            }
                        ],
                        "text": "Visual reasoning has attracted considerable attention, with the release of datasets such as VQA (Antol et al., 2015) and CLEVR (Johnson et al., 2017a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "Visual reasoning has attracted considerable attention, with datasets such as VQA (Antol et al., 2015) and CLEVR (Johnson et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3180429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
            "isKey": false,
            "numCitedBy": 2887,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing $$\\sim $$\u223c0.25\u00a0M images, $$\\sim $$\u223c0.76\u00a0M questions, and $$\\sim $$\u223c10\u00a0M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa)."
            },
            "slug": "VQA:-Visual-Question-Answering-Agrawal-Lu",
            "title": {
                "fragments": [],
                "text": "VQA: Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The task of free-form and open-ended Visual Question Answering (VQA) is proposed, given an image and a natural language question about the image, the task is to provide an accurate natural language answer."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335364"
                        ],
                        "name": "Dzmitry Bahdanau",
                        "slug": "Dzmitry-Bahdanau",
                        "structuredName": {
                            "firstName": "Dzmitry",
                            "lastName": "Bahdanau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dzmitry Bahdanau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 272
                            }
                        ],
                        "text": "The utterance x is encoded using a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) that creates a contextualized representation hi for every utterance token xi, and the decoder is a feed-forward network combined with an attention mechanism over the encoder outputs (Bahdanau et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 268
                            }
                        ],
                        "text": "The utterance x is encoded with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) that creates a contextualized representation hi for every utterance token xi, and the decoder is a feed-forward network combined with attention mechanism over the encoder outputs (Bahdanau et al., 2015)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11212020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "isKey": false,
            "numCitedBy": 19342,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."
            },
            "slug": "Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho",
            "title": {
                "fragments": [],
                "text": "Neural Machine Translation by Jointly Learning to Align and Translate"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and it is proposed to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73710317"
                        ],
                        "name": "B. Hariharan",
                        "slug": "B.-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803520"
                        ],
                        "name": "L. V. D. Maaten",
                        "slug": "L.-V.-D.-Maaten",
                        "structuredName": {
                            "firstName": "Laurens",
                            "lastName": "Maaten",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. V. D. Maaten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50196944"
                        ],
                        "name": "Judy Hoffman",
                        "slug": "Judy-Hoffman",
                        "structuredName": {
                            "firstName": "Judy",
                            "lastName": "Hoffman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Judy Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 128
                            }
                        ],
                        "text": "We define a programming language over the KB that is more amenable to spatial reasoning, inspired by work on the CLEVR dataset (Johnson et al., 2017b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 164
                            }
                        ],
                        "text": "CNLVR as a semantic parsing problem: our goal is to translate language utterances into programs that will be executed against the KB to determine their correctness (Johnson et al., 2017b; Hu et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 135
                            }
                        ],
                        "text": "Treating the problem of visual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR (Hu et al., 2017; Johnson et al., 2017b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 108
                            }
                        ],
                        "text": "We provide a full description of all program tokens, their arguments and return types in Appendix A.\nUnlike CLEVR, CNLVR requires substantial set-theoretic reasoning (utterances refer to various aspects of sets of items in one of the three boxes in the image), which required extending the language described by Johnson et al. (2017b) to include set operators and lambda abstraction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 227
                            }
                        ],
                        "text": "\u2026in Appendix A.\nUnlike CLEVR, CNLVR requires substantial set-theoretic reasoning (utterances refer to various aspects of sets of items in one of the three boxes in the image), which required extending the language described by Johnson et al. (2017b) to include set operators and lambda abstraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 128
                            }
                        ],
                        "text": "Visual reasoning has attracted considerable attention, with the release of datasets such as VQA (Antol et al., 2015) and CLEVR (Johnson et al., 2017a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 63
                            }
                        ],
                        "text": "We develop a formal language for visual reasoning, inspired by Johnson et al. (2017b), and a semantic parser based on recent work by Guu et al. (2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 204
                            }
                        ],
                        "text": "Given the input KB, it is easy to view CNLVR as a semantic parsing problem: our goal is to translate language utterances into programs that will be executed against the KB to determine their correctness (Johnson et al., 2017b; Hu et al., 2017)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 102
                            }
                        ],
                        "text": "Treating visual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR (Hu et al., 2017; Johnson et al., 2017b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31319559,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e17cf6a339fd071ad222062f868e882ef4120a4",
            "isKey": true,
            "numCitedBy": 413,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings."
            },
            "slug": "Inferring-and-Executing-Programs-for-Visual-Johnson-Hariharan",
            "title": {
                "fragments": [],
                "text": "Inferring and Executing Programs for Visual Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32849969"
                        ],
                        "name": "Alane Suhr",
                        "slug": "Alane-Suhr",
                        "structuredName": {
                            "firstName": "Alane",
                            "lastName": "Suhr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alane Suhr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35084211"
                        ],
                        "name": "M. Lewis",
                        "slug": "M.-Lewis",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Lewis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053174592"
                        ],
                        "name": "James Yeh",
                        "slug": "James-Yeh",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Yeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Yeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3167681"
                        ],
                        "name": "Yoav Artzi",
                        "slug": "Yoav-Artzi",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Artzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Artzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 48
                            }
                        ],
                        "text": "We also compare to the best results reported by Suhr et al. (2017) when taking the KB as input, which is a maximum entropy classifier (MAXENT)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 89
                            }
                        ],
                        "text": "Recently, the Cornell Natural Language for Visual Reasoning corpus (CNLVR) was released (Suhr et al., 2017), and presented a difficult problem of spuriousness."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "Recently, the Cornell Natural Language for Visual Reasoning corpus (CNLVR) was released (Suhr et al., 2017), and has presented an opportunity to better investigate the problem of spuriousness."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19435386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9e28863c7fb963b40a379c5a4e0da00eb031933",
            "isKey": true,
            "numCitedBy": 129,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research."
            },
            "slug": "A-Corpus-of-Natural-Language-for-Visual-Reasoning-Suhr-Lewis",
            "title": {
                "fragments": [],
                "text": "A Corpus of Natural Language for Visual Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method of crowdsourcing linguistically-diverse data, and an analysis of the data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3365603"
                        ],
                        "name": "D. Andor",
                        "slug": "D.-Andor",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Andor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Andor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114577307"
                        ],
                        "name": "Chris Alberti",
                        "slug": "Chris-Alberti",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Alberti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Alberti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145045509"
                        ],
                        "name": "David Weiss",
                        "slug": "David-Weiss",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3091861"
                        ],
                        "name": "Aliaksei Severyn",
                        "slug": "Aliaksei-Severyn",
                        "structuredName": {
                            "firstName": "Aliaksei",
                            "lastName": "Severyn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aliaksei Severyn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3377551"
                        ],
                        "name": "A. Presta",
                        "slug": "A.-Presta",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Presta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Presta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144422385"
                        ],
                        "name": "K. Ganchev",
                        "slug": "K.-Ganchev",
                        "structuredName": {
                            "firstName": "Kuzman",
                            "lastName": "Ganchev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ganchev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754497"
                        ],
                        "name": "Slav Petrov",
                        "slug": "Slav-Petrov",
                        "structuredName": {
                            "firstName": "Slav",
                            "lastName": "Petrov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Slav Petrov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123052390"
                        ],
                        "name": "Michael Collins",
                        "slug": "Michael-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 134
                            }
                        ],
                        "text": "is a locally-normalized model that provides a distribution for every decoded token, and thus might suffer from the label bias problem (Andor et al., 2016; Lafferty et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2952144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4be0dd53aa1c751219fa6f19fed8a6324f6d2766",
            "isKey": false,
            "numCitedBy": 529,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models."
            },
            "slug": "Globally-Normalized-Transition-Based-Neural-Andor-Alberti",
            "title": {
                "fragments": [],
                "text": "Globally Normalized Transition-Based Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A globally normalized transition-based neural network model that achieves state-of-the-art part- of-speech tagging, dependency parsing and sentence compression results is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 67
                            }
                        ],
                        "text": "Formally, this is equivalent to a synchronous context-free grammar (Chiang, 2005) that has a rule for generat-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 384994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad3d2f463916784d0c14a19936c1544309a0a440",
            "isKey": false,
            "numCitedBy": 1298,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system."
            },
            "slug": "A-Hierarchical-Phrase-Based-Model-for-Statistical-Chiang",
            "title": {
                "fragments": [],
                "text": "A Hierarchical Phrase-Based Model for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information, which can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 61
                            }
                        ],
                        "text": "We initialize word embeddings by running the CBOW algorithm (Mikolov et al., 2013) on the training data and then optimize them end-to-end."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 42
                            }
                        ],
                        "text": "Word embeddings are initialized from CBOW (Mikolov et al., 2013) trained on the training data, and are then optimized end-toend."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5959482,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "330da625c15427c6e42ccfa3b747fb29e5835bf0",
            "isKey": false,
            "numCitedBy": 21884,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
            },
            "slug": "Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen",
            "title": {
                "fragments": [],
                "text": "Efficient Estimation of Word Representations in Vector Space"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Two novel model architectures for computing continuous vector representations of words from very large data sets are proposed and it is shown that these vectors provide state-of-the-art performance on the authors' test set for measuring syntactic and semantic word similarities."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 31
                            }
                        ],
                        "text": "For optimization, Adam is used (Kingma and Ba, 2014)), with learning rate of 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90064,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13409,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3119801"
                        ],
                        "name": "Xavier Glorot",
                        "slug": "Xavier-Glorot",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Glorot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier Glorot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 93
                            }
                        ],
                        "text": "In the supervised parser, the parameters other than word embeddings are randomly initialized (Glorot and Bengio, 2010), while in the weakly-supervised parser we initialize parameters using the learned parameters of the supervised model to warm-start the model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5575601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b71ac1e9fb49420d13e084ac67254a0bbd40f83f",
            "isKey": false,
            "numCitedBy": 12433,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact)."
            },
            "slug": "Understanding-the-difficulty-of-training-deep-Glorot-Bengio",
            "title": {
                "fragments": [],
                "text": "Understanding the difficulty of training deep feedforward neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "The utterance x is encoded with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) that creates a contextualized representation hi for every utterance token xi, and the decoder is a feed-forward network combined with attention mechanism over the encoder outputs (Bahdanau et al., 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 55
                            }
                        ],
                        "text": "The utterance x is encoded using a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) that creates a contextualized representation hi for every utterance token xi, and the decoder is a feed-forward network combined with an attention mechanism over the encoder outputs (Bahdanau et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "First, an utterance representation x\u0302 is computed by the BiLSTM encoder:\nhFi = LSTM(h F i\u22121, \u03c6x(xi)), hBi = LSTM(h B i+1, \u03c6x(xi)),\nhi = [h F i ;h B i ],\nx\u0302 = [hF|x|;h B 1 ],\nwhere \u03c6x(\u00b7) provides utterance embeddings and \u2019;\u2019 corresponds to concatenation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 275
                            }
                        ],
                        "text": "Then decoding produces the program token by token:\nqt = relu(Wq[x\u0302; zt\u2212K\u22121:t\u22121]),\n\u03b1t \u221d exp(q>t W\u03b1hi), ct = \u2211 i \u03b1i \u00b7 hi,\np\u03b8(zt | x, z1:t\u22121) \u221d exp(\u03c6z(zt)>Ws[qt; ct]),\nwhere \u03c6z(\u00b7) provides program embeddings, zi:j = (zi, . . . , zj), and the matrices Wq,W\u03b1,Ws are\nlearned parameters (along with the LSTM parameters)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Experimental details Our encoder is a BiLSTM where the hidden state dimension for each LSTM is 30."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long shortterm memory"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation 9(8):1735\u2013 1780."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ditional random fields : Probabilistic models for segmenting and labeling data"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Machine Learning ( ICML )"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 134
                            }
                        ],
                        "text": "is a locally-normalized model that provides a distribution for every decoded token, and thus might suffer from the label bias problem (Andor et al., 2016; Lafferty et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conditional random fields: Probabilistic models for segmenting and labeling data"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Machine Learning (ICML). pages 282\u2013289."
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ditional random fields : Probabilistic models for segmenting and labeling data"
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Machine Learning ( ICML )"
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 19,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 37,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Weakly-Supervised-Semantic-Parsing-with-Abstract-Goldman-Latcinnik/c460bfcdd3cda8a787cbfe920dab41136f2dc129?sort=total-citations"
}