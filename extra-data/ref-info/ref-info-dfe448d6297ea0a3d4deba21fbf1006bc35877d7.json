{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38945194"
                        ],
                        "name": "Dieu-Thu Le",
                        "slug": "Dieu-Thu-Le",
                        "structuredName": {
                            "firstName": "Dieu-Thu",
                            "lastName": "Le",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dieu-Thu Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145040726"
                        ],
                        "name": "R. Bernardi",
                        "slug": "R.-Bernardi",
                        "structuredName": {
                            "firstName": "Raffaella",
                            "lastName": "Bernardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bernardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823362"
                        ],
                        "name": "J. Uijlings",
                        "slug": "J.-Uijlings",
                        "structuredName": {
                            "firstName": "Jasper",
                            "lastName": "Uijlings",
                            "middleNames": [
                                "R.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Uijlings"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 259
                            }
                        ],
                        "text": "Language in Vision: The community has been incorporating natural language into computer vision over the last decade to great success, from generating sentences from images [19], producing visual models from sentences [36, 33], and aiding in contextual models [26, 21] to name a few."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 72
                            }
                        ],
                        "text": "The language model in this case is acting as a type of contextual model [26, 21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2402982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21b8a7b863e85159e22a3794f4f02921e26529eb",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of human action recognition. Typically, visual action recognition systems need visual training examples for all actions that one wants to recognize. However, the total number of possible actions is staggering as not only are there many types of actions but also many possible objects for each action type. Normally, visual training examples are needed for all actions of this combinatorial explosion of possibilities. To address this problem, this paper is a first attempt to propose a general framework for unseen action recognition in still images by exploiting both visual and language models. Based on objects recognized in images by means of visual features, the system suggests the most plausible actions exploiting off-the-shelf language models. All components in the framework are trained on universal datasets, hence the system is general, flexible, and able to recognize actions for which no visual training example has been provided. This paper shows that our model yields good performance on unseen action recognition. It even outperforms a state-of-the-art Bag-of-Words model in a realistic scenario where few visual training examples are available."
            },
            "slug": "Exploiting-language-models-to-recognize-unseen-Le-Bernardi",
            "title": {
                "fragments": [],
                "text": "Exploiting language models to recognize unseen actions"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper is a first attempt to propose a general framework for unseen action recognition in still images by exploiting both visual and language models based on objects recognized in images by means of visual features."
            },
            "venue": {
                "fragments": [],
                "text": "ICMR '13"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 115
                            }
                        ],
                        "text": "Common Knowledge: There are promising efforts in progress to acquire common sense for use in computer vision tasks [35, 5, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10554419,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "051830b0ea58d1568f19ec3297e301d9789c9a76",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity."
            },
            "slug": "Bringing-Semantics-into-Focus-Using-Visual-Zitnick-Parikh",
            "title": {
                "fragments": [],
                "text": "Bringing Semantics into Focus Using Visual Abstraction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper creates 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions and thoroughly analyzes this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46925940"
                        ],
                        "name": "Dan Song",
                        "slug": "Dan-Song",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731447"
                        ],
                        "name": "Nikolaos Kyriazis",
                        "slug": "Nikolaos-Kyriazis",
                        "structuredName": {
                            "firstName": "Nikolaos",
                            "lastName": "Kyriazis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikolaos Kyriazis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725054"
                        ],
                        "name": "I. Oikonomidis",
                        "slug": "I.-Oikonomidis",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Oikonomidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Oikonomidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976711"
                        ],
                        "name": "Chavdar Papazov",
                        "slug": "Chavdar-Papazov",
                        "structuredName": {
                            "firstName": "Chavdar",
                            "lastName": "Papazov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chavdar Papazov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689415"
                        ],
                        "name": "Antonis A. Argyros",
                        "slug": "Antonis-A.-Argyros",
                        "structuredName": {
                            "firstName": "Antonis",
                            "lastName": "Argyros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonis A. Argyros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791260"
                        ],
                        "name": "Darius Burschka",
                        "slug": "Darius-Burschka",
                        "structuredName": {
                            "firstName": "Darius",
                            "lastName": "Burschka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darius Burschka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731490"
                        ],
                        "name": "D. Kragic",
                        "slug": "D.-Kragic",
                        "structuredName": {
                            "firstName": "Danica",
                            "lastName": "Kragic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kragic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 133
                            }
                        ],
                        "text": "Action Prediction: There have been several works in robotics that predicts a person\u2019s imminent next action from a sequence of images [31, 23, 15, 8, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10238652,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a818c58ca10bab45aa53168de7afa0e7f431c57",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The main contribution of this paper is a probabilistic method for predicting human manipulation intention from image sequences of human-object interaction. Predicting intention amounts to inferring the imminent manipulation task when human hand is observed to have stably grasped the object. Inference is performed by means of a probabilistic graphical model that encodes object grasping tasks over the 3D state of the observed scene. The 3D state is extracted from RGB-D image sequences by a novel vision-based, markerless hand-object 3D tracking framework. To deal with the high-dimensional state-space and mixed data types (discrete and continuous) involved in grasping tasks, we introduce a generative vector quantization method using mixture models and self-organizing maps. This yields a compact model for encoding of grasping actions, able of handling uncertain and partial sensory data. Experimentation showed that the model trained on simulated data can provide a potent basis for accurate goal-inference with partial and noisy observations of actual real-world demonstrations. We also show a grasp selection process, guided by the inferred human intention, to illustrate the use of the system for goal-directed grasp imitation."
            },
            "slug": "Predicting-human-intention-in-visual-observations-Song-Kyriazis",
            "title": {
                "fragments": [],
                "text": "Predicting human intention in visual observations of hand/object interactions"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A probabilistic method for predicting human manipulation intention from image sequences of human-object interaction, and a grasp selection process, guided by the inferred human intention, to illustrate the use of the system for goal-directed grasp imitation."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Robotics and Automation"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038685"
                        ],
                        "name": "S. Divvala",
                        "slug": "S.-Divvala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Divvala",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Divvala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 115
                            }
                        ],
                        "text": "Common Knowledge: There are promising efforts in progress to acquire common sense for use in computer vision tasks [35, 5, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7748515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0ab8aa7a5b684532b4ff30f8d34b35a99759a46",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition is graduating from labs to real-world applications. While it is encouraging to see its potential being tapped, it brings forth a fundamental challenge to the vision researcher: scalability. How can we learn a model for any concept that exhaustively covers all its appearance variations, while requiring minimal or no human supervision for compiling the vocabulary of visual variance, gathering the training images and annotations, and learning the models? In this paper, we introduce a fully-automated approach for learning extensive models for a wide range of variations (e.g. actions, interactions, attributes and beyond) within any concept. Our approach leverages vast resources of online books to discover the vocabulary of variance, and intertwines the data collection and modeling steps to alleviate the need for explicit human supervision in training the models. Our approach organizes the visual knowledge about a concept in a convenient and useful way, enabling a variety of applications across vision and NLP. Our online system has been queried by users to learn models for several interesting concepts including breakfast, Gandhi, beautiful, etc. To date, our system has models available for over 50, 000 variations within 150 concepts, and has annotated more than 10 million images with bounding boxes."
            },
            "slug": "Learning-Everything-about-Anything:-Visual-Concept-Divvala-Farhadi",
            "title": {
                "fragments": [],
                "text": "Learning Everything about Anything: Webly-Supervised Visual Concept Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A fully-automated approach for learning extensive models for a wide range of variations within any concept, which leverages vast resources of online books to discover the vocabulary of variance, and intertwines the data collection and modeling steps to alleviate the need for explicit human supervision in training the models."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909300"
                        ],
                        "name": "Lucy Vanderwende",
                        "slug": "Lucy-Vanderwende",
                        "structuredName": {
                            "firstName": "Lucy",
                            "lastName": "Vanderwende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucy Vanderwende"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 217
                            }
                        ],
                        "text": "Language in Vision: The community has been incorporating natural language into computer vision over the last decade to great success, from generating sentences from images [19], producing visual models from sentences [36, 33], and aiding in contextual models [26, 21] to name a few."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5642345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c39f56c3c21c3972c362f8e752be57a50c41f4f",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentences that describe visual scenes contain a wide variety of information pertaining to the presence of objects, their attributes and their spatial relations. In this paper we learn the visual features that correspond to semantic phrases derived from sentences. Specifically, we extract predicate tuples that contain two nouns and a relation. The relation may take several forms, such as a verb, preposition, adjective or their combination. We model a scene using a Conditional Random Field (CRF) formulation where each node corresponds to an object, and the edges to their relations. We determine the potentials of the CRF using the tuples extracted from the sentences. We generate novel scenes depicting the sentences' visual meaning by sampling from the CRF. The CRF is also used to score a set of scenes for a text-based image retrieval task. Our results show we can generate (retrieve) scenes that convey the desired semantic meaning, even when scenes (queries) are described by multiple sentences. Significant improvement is found over several baseline approaches."
            },
            "slug": "Learning-the-Visual-Interpretation-of-Sentences-Zitnick-Parikh",
            "title": {
                "fragments": [],
                "text": "Learning the Visual Interpretation of Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper extracts predicate tuples that contain two nouns and a relation from sentences to generate novel scenes depicting the sentences' visual meaning by sampling from the Conditional Random Field (CRF)."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944655894"
                        ],
                        "name": "Jacob Walker",
                        "slug": "Jacob-Walker",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Walker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Walker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 51
                            }
                        ],
                        "text": "There also has been work in forecasting activities [16, 32] and early event detection [11], but they are interested in predicting the future in videos while we wish to explain the motivations of actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1303771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc0bb8f933e514dd9441e3082a34a9f129e35500",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a conceptually simple but surprisingly powerful method for visual prediction which combines the effectiveness of mid-level visual elements with temporal modeling. Our framework can be learned in a completely unsupervised manner from a large collection of videos. However, more importantly, because our approach models the prediction framework on these mid-level elements, we can not only predict the possible motion in the scene but also predict visual appearances - how are appearances going to change with time. This yields a visual \"hallucination\" of probable events on top of the scene. We show that our method is able to accurately predict and visualize simple future events, we also show that our approach is comparable to supervised methods for event prediction."
            },
            "slug": "Patch-to-the-Future:-Unsupervised-Visual-Prediction-Walker-Gupta",
            "title": {
                "fragments": [],
                "text": "Patch to the Future: Unsupervised Visual Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper presents a conceptually simple but surprisingly powerful method for visual prediction which combines the effectiveness of mid-level visual elements with temporal modeling and shows that it is comparable to supervised methods for event prediction."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2635321"
                        ],
                        "name": "Josiah Wang",
                        "slug": "Josiah-Wang",
                        "structuredName": {
                            "firstName": "Josiah",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josiah Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686341"
                        ],
                        "name": "K. Markert",
                        "slug": "K.-Markert",
                        "structuredName": {
                            "firstName": "Katja",
                            "lastName": "Markert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Markert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 217
                            }
                        ],
                        "text": "Language in Vision: The community has been incorporating natural language into computer vision over the last decade to great success, from generating sentences from images [19], producing visual models from sentences [36, 33], and aiding in contextual models [26, 21] to name a few."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1801271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a251dac6589a83e0bbcf9bef9a80c21222aeecbb",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the task of learning models for visual object recognition from natural language descriptions alone. The approach contributes to the recognition of fine-grain object categories, such as animal and plant species, where it may be difficult to collect many images for training, but where textual descriptions of visual attributes are readily available. As an example we tackle recognition of butterfly species, learning models from descriptions in an online nature guide. We propose natural language processing methods for extracting salient visual attributes from these descriptions to use as \u2018templates\u2019 for the object categories, and apply vision methods to extract corresponding attributes from test images. A generative model is used to connect textual terms in the learnt templates to visual attributes. We report experiments comparing the performance of humans and the proposed method on a dataset of ten butterfly categories."
            },
            "slug": "Learning-Models-for-Object-Recognition-from-Natural-Wang-Markert",
            "title": {
                "fragments": [],
                "text": "Learning Models for Object Recognition from Natural Language Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes natural language processing methods for extracting salient visual attributes from natural language descriptions to use as \u2018templates\u2019 for the object categories, and applies vision methods to extract corresponding attributes from test images."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564333"
                        ],
                        "name": "Girish Kulkarni",
                        "slug": "Girish-Kulkarni",
                        "structuredName": {
                            "firstName": "Girish",
                            "lastName": "Kulkarni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Girish Kulkarni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128210"
                        ],
                        "name": "Visruth Premraj",
                        "slug": "Visruth-Premraj",
                        "structuredName": {
                            "firstName": "Visruth",
                            "lastName": "Premraj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Visruth Premraj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985883"
                        ],
                        "name": "S. Dhar",
                        "slug": "S.-Dhar",
                        "structuredName": {
                            "firstName": "Sagnik",
                            "lastName": "Dhar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50341924"
                        ],
                        "name": "Siming Li",
                        "slug": "Siming-Li",
                        "structuredName": {
                            "firstName": "Siming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "Language in Vision: The community has been incorporating natural language into computer vision over the last decade to great success, from generating sentences from images [19], producing visual models from sentences [36, 33], and aiding in contextual models [26, 21] to name a few."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10116609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "169b847e69c35cfd475eb4dcc561a24de11762ca",
            "isKey": false,
            "numCitedBy": 483,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work."
            },
            "slug": "Baby-talk:-Understanding-and-generating-simple-Kulkarni-Premraj",
            "title": {
                "fragments": [],
                "text": "Baby talk: Understanding and generating simple image descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision that is very effective at producing relevant sentences for images."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1834047"
                        ],
                        "name": "Jungseock Joo",
                        "slug": "Jungseock-Joo",
                        "structuredName": {
                            "firstName": "Jungseock",
                            "lastName": "Joo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jungseock Joo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145391253"
                        ],
                        "name": "Weixin Li",
                        "slug": "Weixin-Li",
                        "structuredName": {
                            "firstName": "Weixin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weixin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2991300"
                        ],
                        "name": "Francis F. Steen",
                        "slug": "Francis-F.-Steen",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Steen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francis F. Steen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3138078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0fabb120db2471edafcf4b3a63977e4be032b4f",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce the novel problem of understanding visual persuasion. Modern mass media make extensive use of images to persuade people to make commercial and political decisions. These effects and techniques are widely studied in the social sciences, but behavioral studies do not scale to massive datasets. Computer vision has made great strides in building syntactical representations of images, such as detection and identification of objects. However, the pervasive use of images for communicative purposes has been largely ignored. We extend the significant advances in syntactic analysis in computer vision to the higher-level challenge of understanding the underlying communicative intent implied in images. We begin by identifying nine dimensions of persuasive intent latent in images of politicians, such as \"socially dominant, \" \"energetic, \" and \"trustworthy, \" and propose a hierarchical model that builds on the layer of syntactical attributes, such as \"smile\" and \"waving hand, \" to predict the intents presented in the images. To facilitate progress, we introduce a new dataset of 1, 124 images of politicians labeled with ground-truth intents in the form of rankings. This study demonstrates that a systematic focus on visual persuasion opens up the field of computer vision to a new class of investigations around mediated images, intersecting with media analysis, psychology, and political communication."
            },
            "slug": "Visual-Persuasion:-Inferring-Communicative-Intents-Joo-Li",
            "title": {
                "fragments": [],
                "text": "Visual Persuasion: Inferring Communicative Intents of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This study demonstrates that a systematic focus on visual persuasion opens up the field of computer vision to a new class of investigations around mediated images, intersecting with media analysis, psychology, and political communication."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21161348"
                        ],
                        "name": "Chris L. Baker",
                        "slug": "Chris-L.-Baker",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Baker",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris L. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276622"
                        ],
                        "name": "R. Saxe",
                        "slug": "R.-Saxe",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Saxe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Saxe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 133
                            }
                        ],
                        "text": "However, we are inspired by the observation that humans have the remarkable cognitive ability to think about other people\u2019s thinking [2, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 161
                            }
                        ],
                        "text": "Psychophysics researchers hypothesize that our capacity to reliably infer another person\u2019s motivation stems from our ability to impute our own beliefs to others [2, 30] and there may even be regions of the brain dedicated to this task [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1560164,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7dd51cef9bd43d495a12d10b7d0846f9bd60d9fa",
            "isKey": false,
            "numCitedBy": 680,
            "numCiting": 115,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Action-understanding-as-inverse-planning-Baker-Saxe",
            "title": {
                "fragments": [],
                "text": "Action understanding as inverse planning"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37991449"
                        ],
                        "name": "Kris M. Kitani",
                        "slug": "Kris-M.-Kitani",
                        "structuredName": {
                            "firstName": "Kris",
                            "lastName": "Kitani",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kris M. Kitani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753269"
                        ],
                        "name": "Brian D. Ziebart",
                        "slug": "Brian-D.-Ziebart",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ziebart",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian D. Ziebart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 51
                            }
                        ],
                        "text": "There also has been work in forecasting activities [16, 32] and early event detection [11], but they are interested in predicting the future in videos while we wish to explain the motivations of actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8979634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d8a5addbd17d2c7c8043d8877234675da19938a",
            "isKey": false,
            "numCitedBy": 653,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the task of inferring the future actions of people from noisy visual input. We denote this task activity forecasting. To achieve accurate activity forecasting, our approach models the effect of the physical environment on the choice of human actions. This is accomplished by the use of state-of-the-art semantic scene understanding combined with ideas from optimal control theory. Our unified model also integrates several other key elements of activity analysis, namely, destination forecasting, sequence smoothing and transfer learning. As proof-of-concept, we focus on the domain of trajectory-based activity analysis from visual input. Experimental results demonstrate that our model accurately predicts distributions over future actions of individuals. We show how the same techniques can improve the results of tracking algorithms by leveraging information about likely goals and trajectories."
            },
            "slug": "Activity-Forecasting-Kitani-Ziebart",
            "title": {
                "fragments": [],
                "text": "Activity Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The unified model uses state-of-the-art semantic scene understanding combined with ideas from optimal control theory to achieve accurate activity forecasting and shows how the same techniques can improve the results of tracking algorithms by leveraging information about likely goals and trajectories."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39717886"
                        ],
                        "name": "Xinlei Chen",
                        "slug": "Xinlei-Chen",
                        "structuredName": {
                            "firstName": "Xinlei",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinlei Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781242"
                        ],
                        "name": "Abhinav Shrivastava",
                        "slug": "Abhinav-Shrivastava",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Shrivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhinav Shrivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 115
                            }
                        ],
                        "text": "Common Knowledge: There are promising efforts in progress to acquire common sense for use in computer vision tasks [35, 5, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12350611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53e4ab9730e983242a3409c7bf1af945041a6563",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose NEIL (Never Ending Image Learner), a computer program that runs 24 hours per day and 7 days per week to automatically extract visual knowledge from Internet data. NEIL uses a semi-supervised learning algorithm that jointly discovers common sense relationships (e.g., \"Corolla is a kind of/looks similar to Car\", \"Wheel is a part of Car\") and labels instances of the given visual categories. It is an attempt to develop the world's largest visual structured knowledge base with minimum human labeling effort. As of 10th October 2013, NEIL has been continuously running for 2.5 months on 200 core cluster (more than 350K CPU hours) and has an ontology of 1152 object categories, 1034 scene categories and 87 attributes. During this period, NEIL has discovered more than 1700 relationships and has labeled more than 400K visual instances."
            },
            "slug": "NEIL:-Extracting-Visual-Knowledge-from-Web-Data-Chen-Shrivastava",
            "title": {
                "fragments": [],
                "text": "NEIL: Extracting Visual Knowledge from Web Data"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "NEIL (Never Ending Image Learner), a computer program that runs 24 hours per day and 7 days per week to automatically extract visual knowledge from Internet data, is proposed in an attempt to develop the world's largest visual structured knowledge base with minimum human labeling effort."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276622"
                        ],
                        "name": "R. Saxe",
                        "slug": "R.-Saxe",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Saxe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Saxe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931482"
                        ],
                        "name": "N. Kanwisher",
                        "slug": "N.-Kanwisher",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Kanwisher",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kanwisher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 133
                            }
                        ],
                        "text": "However, we are inspired by the observation that humans have the remarkable cognitive ability to think about other people\u2019s thinking [2, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 161
                            }
                        ],
                        "text": "Psychophysics researchers hypothesize that our capacity to reliably infer another person\u2019s motivation stems from our ability to impute our own beliefs to others [2, 30] and there may even be regions of the brain dedicated to this task [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206118958,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8b175fab5d56791fcb017c149350a66f1a36e7b8",
            "isKey": false,
            "numCitedBy": 1912,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "People-thinking-about-thinking-people-The-role-of-Saxe-Kanwisher",
            "title": {
                "fragments": [],
                "text": "People thinking about thinking people The role of the temporo-parietal junction in \u201ctheory of mind\u201d"
            },
            "venue": {
                "fragments": [],
                "text": "NeuroImage"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723948"
                        ],
                        "name": "H. Koppula",
                        "slug": "H.-Koppula",
                        "structuredName": {
                            "firstName": "Hema",
                            "lastName": "Koppula",
                            "middleNames": [
                                "Swetha"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Koppula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681995"
                        ],
                        "name": "Ashutosh Saxena",
                        "slug": "Ashutosh-Saxena",
                        "structuredName": {
                            "firstName": "Ashutosh",
                            "lastName": "Saxena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashutosh Saxena"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 133
                            }
                        ],
                        "text": "Action Prediction: There have been several works in robotics that predicts a person\u2019s imminent next action from a sequence of images [31, 23, 15, 8, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1121245,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50a00d4fa9bf2e7bff37bc944ac48b403f5eb097",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "An important aspect of human perception is anticipation, which we use extensively in our day-to-day activities when interacting with other humans as well as with our surroundings. Anticipating which activities will a human do next (and how) can enable an assistive robot to plan ahead for reactive responses. Furthermore, anticipation can even improve the detection accuracy of past activities. The challenge, however, is two-fold: We need to capture the rich context for modeling the activities and object affordances, and we need to anticipate the distribution over a large space of future human activities. In this work, we represent each possible future using an anticipatory temporal conditional random field (ATCRF) that models the rich spatial-temporal relations through object affordances. We then consider each ATCRF as a particle and represent the distribution over the potential futures using a set of particles. In extensive evaluation on CAD-120 human activity RGB-D dataset, we first show that anticipation improves the state-of-the-art detection results. We then show that for new subjects (not seen in the training set), we obtain an activity anticipation accuracy (defined as whether one of top three predictions actually happened) of 84.1, 74.4 and 62.2 percent for an anticipation time of 1, 3 and 10 seconds respectively. Finally, we also show a robot using our algorithm for performing a few reactive responses."
            },
            "slug": "Anticipating-Human-Activities-Using-Object-for-Koppula-Saxena",
            "title": {
                "fragments": [],
                "text": "Anticipating Human Activities Using Object Affordances for Reactive Robotic Response"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work represents each possible future using an anticipatory temporal conditional random field (ATCRF) that models the rich spatial-temporal relations through object affordances and represents each ATCRF as a particle and represents the distribution over the potential futures using a set of particles."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754666"
                        ],
                        "name": "R. Poppe",
                        "slug": "R.-Poppe",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Poppe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Poppe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 38
                            }
                        ],
                        "text": "We refer readers to excellent surveys [27, 1] for a full review."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 77451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d984b580e02da76cd4d991953e6d430fadf3d578",
            "isKey": false,
            "numCitedBy": 2105,
            "numCiting": 212,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-survey-on-vision-based-human-action-recognition-Poppe",
            "title": {
                "fragments": [],
                "text": "A survey on vision-based human action recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145566573"
                        ],
                        "name": "Richard Kelley",
                        "slug": "Richard-Kelley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Kelley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Kelley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3237874"
                        ],
                        "name": "L. Wigand",
                        "slug": "L.-Wigand",
                        "structuredName": {
                            "firstName": "Liesl",
                            "lastName": "Wigand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wigand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067535175"
                        ],
                        "name": "Brian Hamilton",
                        "slug": "Brian-Hamilton",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Hamilton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Hamilton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2596177"
                        ],
                        "name": "Katie Browne",
                        "slug": "Katie-Browne",
                        "structuredName": {
                            "firstName": "Katie",
                            "lastName": "Browne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katie Browne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3254699"
                        ],
                        "name": "M. Nicolescu",
                        "slug": "M.-Nicolescu",
                        "structuredName": {
                            "firstName": "Monica",
                            "lastName": "Nicolescu",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nicolescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144338216"
                        ],
                        "name": "M. Nicolescu",
                        "slug": "M.-Nicolescu",
                        "structuredName": {
                            "firstName": "Mircea",
                            "lastName": "Nicolescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nicolescu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 133
                            }
                        ],
                        "text": "Action Prediction: There have been several works in robotics that predicts a person\u2019s imminent next action from a sequence of images [31, 23, 15, 8, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8841583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53e5c0b907b3782c0a7de5c2fcf22a640572a231",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Effective human-robot interaction requires systems that can accurately infer and predict human intentions. In this paper, we introduce a system that uses stacked denoising autoencoders to perform intent recognition. We introduce the intent recognition problem, provide an overview of deep architectures in machine learning, and outline the components of our system. We also provide preliminary results for our system's performance."
            },
            "slug": "Deep-networks-for-predicting-human-intent-with-to-Kelley-Wigand",
            "title": {
                "fragments": [],
                "text": "Deep networks for predicting human intent with respect to objects"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A system that uses stacked denoising autoencoders to perform intent recognition and an overview of deep architectures in machine learning is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931073"
                        ],
                        "name": "J. Elfring",
                        "slug": "J.-Elfring",
                        "structuredName": {
                            "firstName": "Jos",
                            "lastName": "Elfring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elfring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694041"
                        ],
                        "name": "R. V. D. Molengraft",
                        "slug": "R.-V.-D.-Molengraft",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Molengraft",
                            "middleNames": [
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. V. D. Molengraft"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145871082"
                        ],
                        "name": "M. Steinbuch",
                        "slug": "M.-Steinbuch",
                        "structuredName": {
                            "firstName": "Maarten",
                            "lastName": "Steinbuch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steinbuch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14986697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3598c576819b885e99c57667a029363c7233bf8",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-intentions-for-improved-human-motion-Elfring-Molengraft",
            "title": {
                "fragments": [],
                "text": "Learning intentions for improved human motion prediction"
            },
            "venue": {
                "fragments": [],
                "text": "2013 16th International Conference on Advanced Robotics (ICAR)"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48811777"
                        ],
                        "name": "Dan Xie",
                        "slug": "Dan-Xie",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143856428"
                        ],
                        "name": "S. Todorovic",
                        "slug": "S.-Todorovic",
                        "structuredName": {
                            "firstName": "Sinisa",
                            "lastName": "Todorovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Todorovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There also has been work in forecasting activities [16, 32], inferring goals [35], and early event detection [11], but they are interested in predicting the future in videos while we wish to explain the motivations of actions of people in images in the wild."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15150618,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4a3be1de7ea7b07cf6ac98398e8b9bea6cb2dfe",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene. Functional objects do not have discriminative appearance and shape, but they affect behavior of people in the scene. For example, they \"attract\" people to approach them for satisfying certain needs (e.g., vending machines could quench thirst), or \"repel\" people to avoid them (e.g., grass lawns). Therefore, functional objects can be viewed as \"dark matter\", emanating \"dark energy\" that affects people's trajectories in the video. To detect \"dark matter\" and infer their \"dark energy\" field, we extend the Lagrangian mechanics. People are treated as particle-agents with latent intents to approach \"dark matter\" and thus satisfy their needs, where their motions are subject to a composite \"dark energy\" field of all functional objects in the scene. We make the assumption that people take globally optimal paths toward the intended \"dark matter\" while avoiding latent obstacles. A Bayesian framework is used to probabilistically model: people's trajectories and intents, constraint map of the scene, and locations of functional objects. A data-driven Markov Chain Monte Carlo (MCMC) process is used for inference. Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people's trajectories in unobserved parts of the video footage."
            },
            "slug": "Inferring-\"Dark-Matter\"-and-\"Dark-Energy\"-from-Xie-Todorovic",
            "title": {
                "fragments": [],
                "text": "Inferring \"Dark Matter\" and \"Dark Energy\" from Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene by extending the Lagrangian mechanics to probabilistically model people's trajectories and intents, constraint map of the scene, and locations of functional objects."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4675758"
                        ],
                        "name": "Mitesh Patel",
                        "slug": "Mitesh-Patel",
                        "structuredName": {
                            "firstName": "Mitesh",
                            "lastName": "Patel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mitesh Patel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2484138"
                        ],
                        "name": "C. Ek",
                        "slug": "C.-Ek",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Ek",
                            "middleNames": [
                                "Henrik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731447"
                        ],
                        "name": "Nikolaos Kyriazis",
                        "slug": "Nikolaos-Kyriazis",
                        "structuredName": {
                            "firstName": "Nikolaos",
                            "lastName": "Kyriazis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikolaos Kyriazis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689415"
                        ],
                        "name": "Antonis A. Argyros",
                        "slug": "Antonis-A.-Argyros",
                        "structuredName": {
                            "firstName": "Antonis",
                            "lastName": "Argyros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonis A. Argyros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703903"
                        ],
                        "name": "J. V. Mir\u00f3",
                        "slug": "J.-V.-Mir\u00f3",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Mir\u00f3",
                            "middleNames": [
                                "Valls"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. Mir\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731490"
                        ],
                        "name": "D. Kragic",
                        "slug": "D.-Kragic",
                        "structuredName": {
                            "firstName": "Danica",
                            "lastName": "Kragic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kragic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 259
                            }
                        ],
                        "text": "Language in Vision: The community has been incorporating natural language into computer vision over the last decade to great success, from generating sentences from images [19], producing visual models from sentences [36, 33], and aiding in contextual models [26, 21] to name a few."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 72
                            }
                        ],
                        "text": "The language model in this case is acting as a type of contextual model [26, 21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2868854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ff92fe91874c3f7ebdfee20babf89108583ad6c",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we use a Hierarchical Hidden Markov Model (HHMM) to represent and learn complex activities/task performed by humans/robots in everyday life. Action primitives are used as a grammar to represent complex human behaviour and learn the interactions and behaviour of human/robots with different objects. The main contribution is the use of a probabilistic model capable of representing behaviours at multiple levels of abstraction to support the proposed hypothesis. The hierarchical nature of the model allows decomposition of the complex task into simple action primitives. The framework is evaluated with data collected for tasks of everyday importance performed by a human user."
            },
            "slug": "Language-for-learning-complex-human-object-Patel-Ek",
            "title": {
                "fragments": [],
                "text": "Language for learning complex human-object interactions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A probabilistic model capable of representing behaviours at multiple levels of abstraction to support the proposed hypothesis and decomposition of the complex task into simple action primitives is used."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Robotics and Automation"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276622"
                        ],
                        "name": "R. Saxe",
                        "slug": "R.-Saxe",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Saxe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Saxe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144366429"
                        ],
                        "name": "S. Carey",
                        "slug": "S.-Carey",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Carey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931482"
                        ],
                        "name": "N. Kanwisher",
                        "slug": "N.-Kanwisher",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Kanwisher",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kanwisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 235
                            }
                        ],
                        "text": "Psychophysics researchers hypothesize that our capacity to reliably infer another person\u2019s motivation stems from our ability to impute our own beliefs to others [2, 30] and there may even be regions of the brain dedicated to this task [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Interestingly, recent work in psychophysics provides evidence that there is a region in our brain devoted to this task [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10165816,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "9762e0211c6f86cdbf5a7bb55718407d5f71069e",
            "isKey": false,
            "numCitedBy": 668,
            "numCiting": 201,
            "paperAbstract": {
                "fragments": [],
                "text": "Evidence from developmental psychology suggests that understanding other minds constitutes a special domain of cognition with at least two components: an early-developing system for reasoning about goals, perceptions, and emotions, and a later-developing system for representing the contents of beliefs. Neuroimaging reinforces and elaborates upon this view by providing evidence that (a) domain-specific brain regions exist for representing belief contents, (b) these regions are apparently distinct from other regions engaged in reasoning about goals and actions (suggesting that the two developmental stages reflect the emergence of two distinct systems, rather than the elaboration of a single system), and (c) these regions are distinct from brain regions engaged in inhibitory control and in syntactic processing. The clear neural distinction between these processes is evidence that belief attribution is not dependent on either inhibitory control or syntax, but is subserved by a specialized neural system for theory of mind."
            },
            "slug": "Understanding-other-minds:-linking-developmental-Saxe-Carey",
            "title": {
                "fragments": [],
                "text": "Understanding other minds: linking developmental psychology and functional neuroimaging."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Neuroimaging reinforces and elaborates upon this view by providing evidence that domain-specific brain regions exist for representing belief contents, and these regions are apparently distinct from other regions engaged in reasoning about goals and actions."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of psychology"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3324073"
                        ],
                        "name": "Catharine L. R. McGhan",
                        "slug": "Catharine-L.-R.-McGhan",
                        "structuredName": {
                            "firstName": "Catharine",
                            "lastName": "McGhan",
                            "middleNames": [
                                "L.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catharine L. R. McGhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527114"
                        ],
                        "name": "A. Nasir",
                        "slug": "A.-Nasir",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Nasir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nasir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144729148"
                        ],
                        "name": "E. Atkins",
                        "slug": "E.-Atkins",
                        "structuredName": {
                            "firstName": "Ella",
                            "lastName": "Atkins",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Atkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 133
                            }
                        ],
                        "text": "Action Prediction: There have been several works in robotics that predicts a person\u2019s imminent next action from a sequence of images [31, 23, 15, 8, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18209503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2756579b05ce72ca2d902225571431e018260866",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a system for modeling human task-level intent through the use of Markov Decision Processes (MDPs). To maintain safety and efficiency during physicallyproximal human-robot collaboration, it is necessary for both human and robot to communicate or otherwise deconflict physical actions. Human-state aware robot intelligence is necessary to facilitate this. However, physical action deconfliction without explicit communication requires a robot to estimate a human (or robotic) companion\u2019s current action(s) and goal priorities, and then use this information to predict their intended future action sequence. Models tailored to a particular human can also enable online human intent prediction. We call the former a \u2018simulated human\u2019 model \u2013 one that is non-specific and generalized to statistical norms of human reaction obtained from human subject testing. The latter we call a \u2018human matching\u2019 model \u2013 one that attempts to produce the same output as a particular human subject, requiring online learning for improved accuracy. We propose the creation of \u2018simulated human\u2019 and \u2018human matching\u2019 models in this manuscript as a means for a robot to intelligently predict a human companion\u2019s intended future actions. We develop a Human Intent Prediction (HIP) system, which can model human choice, to satisfy these needs. This system, when given a time history of previous actions as input, predicts the most likely action a human agent will next make to a robot\u2019s task scheduling system. Our HIP system is applied to an intra-vehicle activity (IVA) space robotics application. We use data from preliminary human subject testing to formulate and populate our models in an offline learning process that illustrates how the models can adapt to better predict intent as new training data is incorporated."
            },
            "slug": "Human-Intent-Prediction-Using-Markov-Decision-McGhan-Nasir",
            "title": {
                "fragments": [],
                "text": "Human Intent Prediction Using Markov Decision Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A Human Intent Prediction (HIP) system, which can model human choice, predicts the most likely action a human agent will next make to a robot\u2019s task scheduling system and is applied to an intra-vehicle activity (IVA) space robotics application."
            },
            "venue": {
                "fragments": [],
                "text": "Infotech@Aerospace"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11696,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "In order to give the vision-only baseline access to other visual concepts, we concatenate its features with a ground truth object bank [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 591187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6286a82f72f632672c1890f3dd6bbb15b8e5168b",
            "isKey": false,
            "numCitedBy": 996,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classification; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classifiers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efficient and scalable for large scene datasets, and reveal semantically meaningful feature patterns."
            },
            "slug": "Object-Bank:-A-High-Level-Image-Representation-for-Li-Su",
            "title": {
                "fragments": [],
                "text": "Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A high-level image representation, called the Object Bank, is proposed, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 65
                            }
                        ],
                        "text": "We then combine these labels with state-ofthe-art image features [18, 12] to train data-driven classifiers that predict a person\u2019s motivation from images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 115
                            }
                        ],
                        "text": "We computed features from the second to last layer in the AlexNet convolutional neural network trained on ImageNet [18, 12] due to their state-of-the-art performance on other visual recognition problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 80973,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049736"
                        ],
                        "name": "Sergey Karayev",
                        "slug": "Sergey-Karayev",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Karayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Karayev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1799558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "isKey": false,
            "numCitedBy": 13757,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia."
            },
            "slug": "Caffe:-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer",
            "title": {
                "fragments": [],
                "text": "Caffe: Convolutional Architecture for Fast Feature Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698158"
                        ],
                        "name": "Minh Hoai Nguyen",
                        "slug": "Minh-Hoai-Nguyen",
                        "structuredName": {
                            "firstName": "Minh",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Hoai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minh Hoai Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143867160"
                        ],
                        "name": "F. D. L. Torre",
                        "slug": "F.-D.-L.-Torre",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Torre",
                            "middleNames": [
                                "De",
                                "la"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. D. L. Torre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "There also has been work in forecasting activities [16, 32] and early event detection [11], but they are interested in predicting the future in videos while we wish to explain the motivations of actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8386677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b72756c4d4237a857e1a764c876e82a82edd128c",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "The need for early detection of temporal events from sequential data arises in a wide spectrum of applications ranging from human-robot interaction to video security. While temporal event detection has been extensively studied, early detection is a relatively unexplored problem. This paper proposes a maximum-margin framework for training temporal event detectors to recognize partial events, enabling early detection. Our method is based on Structured Output SVM, but extends it to accommodate sequential data. Experiments on datasets of varying complexity, for detecting facial expressions, hand gestures, and human activities, demonstrate the benefits of our approach."
            },
            "slug": "Max-Margin-Early-Event-Detectors-Nguyen-Torre",
            "title": {
                "fragments": [],
                "text": "Max-Margin Early Event Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes a maximum-margin framework for training temporal event detectors to recognize partial events, enabling early detection, based on Structured Output SVM, but extends it to accommodate sequential data."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50256971"
                        ],
                        "name": "Thomas Finley",
                        "slug": "Thomas-Finley",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Finley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Finley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3166569"
                        ],
                        "name": "C. Yu",
                        "slug": "C.-Yu",
                        "structuredName": {
                            "firstName": "Chun-Nam",
                            "lastName": "Yu",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "This optimization is equivalent to a structured SVM and can be solved by efficient off-the-shelf solvers [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14211670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f30aba767d71c1db5ea70b041d9fcc2b9b1ddad4",
            "isKey": false,
            "numCitedBy": 1083,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminative training approaches like structural SVMs have shown much promise for building highly complex and accurate models in areas like natural language processing, protein structure prediction, and information retrieval. However, current training algorithms are computationally expensive or intractable on large datasets. To overcome this bottleneck, this paper explores how cutting-plane methods can provide fast training not only for classification SVMs, but also for structural SVMs. We show that for an equivalent \u201c1-slack\u201d reformulation of the linear SVM training problem, our cutting-plane method has time complexity linear in the number of training examples. In particular, the number of iterations does not depend on the number of training examples, and it is linear in the desired precision and the regularization parameter. Furthermore, we present an extensive empirical evaluation of the method applied to binary classification, multi-class classification, HMM sequence tagging, and CFG parsing. The experiments show that the cutting-plane algorithm is broadly applicable and fast in practice. On large datasets, it is typically several orders of magnitude faster than conventional training methods derived from decomposition methods like SVM-light, or conventional cutting-plane methods. Implementations of our methods are available at www.joachims.org."
            },
            "slug": "Cutting-plane-training-of-structural-SVMs-Joachims-Finley",
            "title": {
                "fragments": [],
                "text": "Cutting-plane training of structural SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper explores how cutting-plane methods can provide fast training not only for classification SVMs, but also for structural SVMs and presents an extensive empirical evaluation of the method applied to binary classification, multi-class classification, HMM sequence tagging, and CFG parsing."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705627"
                        ],
                        "name": "J. Aggarwal",
                        "slug": "J.-Aggarwal",
                        "structuredName": {
                            "firstName": "Jake",
                            "lastName": "Aggarwal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aggarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766489"
                        ],
                        "name": "M. Ryoo",
                        "slug": "M.-Ryoo",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Ryoo",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ryoo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5388357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56ca1bcc0ee88770e86554ce54471130c9acf0e3",
            "isKey": false,
            "numCitedBy": 1761,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "Human activity recognition is an important area of computer vision research. Its applications include surveillance systems, patient monitoring systems, and a variety of systems that involve interactions between persons and electronic devices such as human-computer interfaces. Most of these applications require an automated recognition of high-level activities, composed of multiple simple (or atomic) actions of persons. This article provides a detailed overview of various state-of-the-art research papers on human activity recognition. We discuss both the methodologies developed for simple human actions and those for high-level activities. An approach-based taxonomy is chosen that compares the advantages and limitations of each approach. Recognition methodologies for an analysis of the simple actions of a single person are first presented in the article. Space-time volume approaches and sequential approaches that represent and recognize activities directly from input images are discussed. Next, hierarchical recognition methodologies for high-level activities are presented and compared. Statistical approaches, syntactic approaches, and description-based approaches for hierarchical recognition are discussed in the article. In addition, we further discuss the papers on the recognition of human-object interactions and group activities. Public datasets designed for the evaluation of the recognition methodologies are illustrated in our article as well, comparing the methodologies' performances. This review will provide the impetus for future research in more productive areas."
            },
            "slug": "Human-activity-analysis-Aggarwal-Ryoo",
            "title": {
                "fragments": [],
                "text": "Human activity analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article provides a detailed overview of various state-of-the-art research papers on human activity recognition, discussing both the methodologies developed for simple human actions and those for high-level activities."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Comput. Surv."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "2While the rest of this baseline uses Crammer and Singer\u2019s multiclass SVM [6], we found a one-vs-rest strategy worked better for the fully automatic baseline (median rank 30 for Crammer and Singer, and median rank 23 for one-vs-rest)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "(2)While the rest of this baseline uses Crammer and Singer\u2019s multiclass SVM [6], we found a one-vs-rest strategy worked better for the fully automatic baseline (median rank 30 for Crammer and Singer, and median rank 23 for one-vs-rest)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "This optimization is equivalent to a structured SVM and can be solved by efficient off-the-shelf solvers [13]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10151608,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "cfc6d0c8260594ebc5dd20ee558d29b1014ed41a",
            "isKey": false,
            "numCitedBy": 2190,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy."
            },
            "slug": "On-the-Algorithmic-Implementation-of-Multiclass-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper describes the algorithmic implementation of multiclass kernel-based vector machines using a generalized notion of the margin to multiclass problems, and describes an efficient fixed-point algorithm for solving the reduced optimization problems and proves its convergence."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376506"
                        ],
                        "name": "Payman Yadollahpour",
                        "slug": "Payman-Yadollahpour",
                        "structuredName": {
                            "firstName": "Payman",
                            "lastName": "Yadollahpour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Payman Yadollahpour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405492658"
                        ],
                        "name": "Abner Guzm\u00e1n-Rivera",
                        "slug": "Abner-Guzm\u00e1n-Rivera",
                        "structuredName": {
                            "firstName": "Abner",
                            "lastName": "Guzm\u00e1n-Rivera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abner Guzm\u00e1n-Rivera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490189"
                        ],
                        "name": "Gregory Shakhnarovich",
                        "slug": "Gregory-Shakhnarovich",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Shakhnarovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Shakhnarovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 154
                            }
                        ],
                        "text": "For both learning and evaluation, we require theK-best solutions, which can be done efficiently with approximate approaches such as K-best MAP estimation [3, 20] or sampling techniques [28, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11207149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5deff8a3cd0e2b24713d2449ab54710334af763a",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Much effort has been directed at algorithms for obtaining the highest probability (MAP) configuration in probabilistic (random field) models. In many situations, one could benefit from additional high-probability solutions. Current methods for computing the M most probable configurations produce solutions that tend to be very similar to the MAP solution and each other. This is often an undesirable property. In this paper we propose an algorithm for the Diverse M-Best problem, which involves finding a diverse set of highly probable solutions under a discrete probabilistic model. Given a dissimilarity function measuring closeness of two solutions, our formulation involves maximizing a linear combination of the probability and dissimilarity to previous solutions. Our formulation generalizes the M-Best MAP problem and we show that for certain families of dissimilarity functions we can guarantee that these solutions can be found as easily as the MAP solution."
            },
            "slug": "Diverse-M-Best-Solutions-in-Markov-Random-Fields-Batra-Yadollahpour",
            "title": {
                "fragments": [],
                "text": "Diverse M-Best Solutions in Markov Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes an algorithm for the Diverse M-Best problem, which involves finding a diverse set of highly probable solutions under a discrete probabilistic model and shows that for certain families of dissimilarity functions the authors can guarantee that these solutions can be found as easily as the MAP solution."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702066"
                        ],
                        "name": "Kenneth Heafield",
                        "slug": "Kenneth-Heafield",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Heafield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Heafield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Using state-of-the-art language models [10] estimated on billions of webpages [4], we are able to acquire common knowledge about people\u2019s experiences, such as their interactions with objects, their environments, and their motivations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 88
                            }
                        ],
                        "text": "In our experiments, we query a 5-gram language model estimated on billions of web-pages [4, 10] to form each L(\u00b7)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8313873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "883d1d06d857a85a0e64bb19f0b17d56f2cc9d7b",
            "isKey": false,
            "numCitedBy": 1173,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. The Probing data structure uses linear probing hash tables and is designed for speed. Compared with the widely-used SRILM, our Probing model is 2.4 times as fast while using 57% of the memory. The Trie data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. Trie simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline. Our code is open-source, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations."
            },
            "slug": "KenLM:-Faster-and-Smaller-Language-Model-Queries-Heafield",
            "title": {
                "fragments": [],
                "text": "KenLM: Faster and Smaller Language Model Queries"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "KenLM is a library that implements two data structures for efficient language model queries, reducing both time and memory costs and is integrated into the Moses, cdec, and Joshua translation systems."
            },
            "venue": {
                "fragments": [],
                "text": "WMT@EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064348170"
                        ],
                        "name": "C. Buck",
                        "slug": "C.-Buck",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Buck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702066"
                        ],
                        "name": "Kenneth Heafield",
                        "slug": "Kenneth-Heafield",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Heafield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Heafield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31965747"
                        ],
                        "name": "B. V. Ooyen",
                        "slug": "B.-V.-Ooyen",
                        "structuredName": {
                            "firstName": "Bas",
                            "lastName": "Ooyen",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. V. Ooyen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9709731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e4fb17fff38a7834af5b4eaafcbbde02bf00975",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We contribute 5-gram counts and language models trained on the Common Crawl corpus, a collection over 9 billion web pages. This release improves upon the Google n-gram counts in two key ways: the inclusion of low-count entries and deduplication to reduce boilerplate. By preserving singletons, we were able to use Kneser-Ney smoothing to build large language models. This paper describes how the corpus was processed with emphasis on the problems that arise in working with data at this scale. Our unpruned Kneser-Ney English $5$-gram language model, built on 975 billion deduplicated tokens, contains over 500 billion unique n-grams. We show gains of 0.5-1.4 BLEU by using large language models to translate into various languages."
            },
            "slug": "N-gram-Counts-and-Language-Models-from-the-Common-Buck-Heafield",
            "title": {
                "fragments": [],
                "text": "N-gram Counts and Language Models from the Common Crawl"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This release improves upon the Google n-gram counts in two key ways: the inclusion of low-count entries and deduplication to reduce boilerplate, and the use of Kneser-Ney smoothing to build large language models."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145945413"
                        ],
                        "name": "H. Wimmer",
                        "slug": "H.-Wimmer",
                        "structuredName": {
                            "firstName": "Heinz",
                            "lastName": "Wimmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wimmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2274473"
                        ],
                        "name": "J. Perner",
                        "slug": "J.-Perner",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Perner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Perner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "Humans may be able to make such remarkable inferences partially due to cognitive skills known as the theory of mind [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17014009,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "75ccbdbeef122c70f0fd317984da890792b23ece",
            "isKey": false,
            "numCitedBy": 5222,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beliefs-about-beliefs:-Representation-and-function-Wimmer-Perner",
            "title": {
                "fragments": [],
                "text": "Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "After merging similar words using WordNet [24], workers annotated a total of 79 unique motivations, 7 actions, 43 objects, and 112 scenes on 792 images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1671874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "68c03788224000794d5491ab459be0b2a2c38677",
            "isKey": false,
            "numCitedBy": 13891,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
            },
            "slug": "WordNet:-A-Lexical-Database-for-English-Miller",
            "title": {
                "fragments": [],
                "text": "WordNet: A Lexical Database for English"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "WordNet1 provides a more effective combination of traditional lexicographic information and modern computing, and is an online lexical database designed for use under program control."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318588"
                        ],
                        "name": "Jake Porway",
                        "slug": "Jake-Porway",
                        "structuredName": {
                            "firstName": "Jake",
                            "lastName": "Porway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jake Porway"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11840582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7e407b68d822311480bd25fcd15070aa5b863c5",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel Markov Chain Monte Carlo (MCMC) inference algorithm called C4-Clustering with Cooperative and Competitive Constraints-for computing multiple solutions from posterior probabilities defined on graphical models, including Markov random fields (MRF), conditional random fields (CRF), and hierarchical models. The graphs may have both positive and negative edges for cooperative and competitive constraints. C4 is a probabilistic clustering algorithm in the spirit of Swendsen-Wang [34]. By turning the positive edges on/off probabilistically, C4 partitions the graph into a number of connected components (ccps) and each ccp is a coupled subsolution with nodes connected by positive edges. Then, by turning the negative edges on/off probabilistically, C4 obtains composite ccps (called cccps) with competing ccps connected by negative edges. At each step, C4 flips the labels of all nodes in a cccp so that nodes in each ccp keep the same label while different ccps are assigned different labels to observe both positive and negative constraints. Thus, the algorithm can jump between multiple competing solutions (or modes of the posterior probability) in a single or a few steps. It computes multiple distinct solutions to preserve the intrinsic ambiguities and avoids premature commitments to a single solution that may not be valid given later context. C4 achieves a mixing rate faster than existing MCMC methods, such as various Gibbs samplers [15], [26] and Swendsen-Wang cuts [2], [34]. It is also more \u201cdynamic\u201d than common optimization methods such as ICM [3], LBP [21], [37], and graph cuts [4], [20]. We demonstrate the C4 algorithm in line drawing interpretation, scene labeling, and object recognition."
            },
            "slug": "C^4:-Exploring-Multiple-Solutions-in-Graphical-by-Porway-Zhu",
            "title": {
                "fragments": [],
                "text": "C^4: Exploring Multiple Solutions in Graphical Models by Cluster Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The C4 algorithm can jump between multiple competing solutions (or modes of the posterior probability) in a single or a few steps and computes multiple distinct solutions to preserve the intrinsic ambiguities and avoids premature commitments to a single solution that may not be valid given later context."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2172077"
                        ],
                        "name": "E. Lawler",
                        "slug": "E.-Lawler",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Lawler",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lawler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 154
                            }
                        ],
                        "text": "For both learning and evaluation, we require theK-best solutions, which can be done efficiently with approximate approaches such as K-best MAP estimation [3, 20] or sampling techniques [28, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120048011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee565531f315d30dc0e9ea77c8a31c3185afd2f1",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A general procedure is presented for computing the best, 2nd best,..., Kth best solutions to a given discrete optimization problem. If the number of computational steps required to find an optimal solution to a problem with n(0, 1) variables is c(n), then the amount of computation required to obtain the if best solutions is O(K nc (n)). The procedure specializes to published procedures of Murty and of Yen for the assignment problem and the shortest path problem, respectively. A method is presented for reducing the required amount of storage by a factor of n, compared with the algorithms of Murty and of Yen. It is shown how the K shortest (loopless) paths in an n-node network with positive and negative arcs can be computed with an amount of computation which is O(Kn 3 ). This represents an improvement by a factor of n, compared with Yen's algorithm."
            },
            "slug": "A-PROCEDURE-FOR-COMPUTING-THE-K-BEST-SOLUTIONS-TO-Lawler",
            "title": {
                "fragments": [],
                "text": "A PROCEDURE FOR COMPUTING THE K BEST SOLUTIONS TO DISCRETE OPTIMIZATION PROBLEMS AND ITS APPLICATION TO THE SHORTEST PATH PROBLEM"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how the K shortest (loopless) paths in an n-node network with positive and negative arcs can be computed with an amount of computation which is O(Kn 3 )."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40756124"
                        ],
                        "name": "J. Einasto",
                        "slug": "J.-Einasto",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Einasto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Einasto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119112532,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "4006beb2a9874a5b39e5c5e3f564b7408ab7c137",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "I review the development of the concept of dark matter. The dark matter story passed through several stages, from a minor observational puzzle to a major challenge for theory of elementary particles. Modern data suggest that dark matter is the dominant matter component in the Universe and that it consists of some unknown non-baryonic particles. Dark matter is the dominant matter component in the Universe; therefore, properties of dark matter particles determine the structure of the cosmic web."
            },
            "slug": "Dark-Matter-Einasto",
            "title": {
                "fragments": [],
                "text": "Dark Matter"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1619078806"
                        ],
                        "name": "A. ADoefaa",
                        "slug": "A.-ADoefaa",
                        "structuredName": {
                            "firstName": "A",
                            "lastName": "ADoefaa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. ADoefaa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1619192001"
                        ],
                        "name": "H. P. Doetsch",
                        "slug": "H.-P.-Doetsch",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Doetsch",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. P. Doetsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1619263939"
                        ],
                        "name": "Draweng Table",
                        "slug": "Draweng-Table",
                        "structuredName": {
                            "firstName": "Draweng",
                            "lastName": "Table",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Draweng Table"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1619192001"
                        ],
                        "name": "H. P. Doetsch",
                        "slug": "H.-P.-Doetsch",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Doetsch",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. P. Doetsch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Motivation in Vision: Perhaps the most related to our paper is work that predicts the persuasive motivation of the photographer who captured an image [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207877176,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d8daa2f46de22a0bf06ec5174d4fdbc650d4239b",
            "isKey": false,
            "numCitedBy": 143621,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "OF THE DISCLOSURE A gas Spring for a drawing table which has a cylinder, a piston in the cylinder, and a piston rod projecting from the piston through one end wall of the cylinder, the other end wall being imperforate. Gas under pressure fills the cylinder and may be released from the chamber adjacent the piston rod if the latter is almost fully expelled from the cylinder against a compression spring through a chan nel having orifices in the piston and in the piston rod. Additional gas is admitted to the cylinder of a modified Spring when the piston rod is almost fully retracted into the cylinder and the piston strikes a normally closed valve Separating the cylinder cavity from a storage chamber. SLSSSMSSSLSSSSTSSSTSS The present invention relates to a weight-compensating and equilibrium-maintaining device, and it is the principal object of this invention to provide a very simple and inexpensive device in the form of a so-called gas spring for compensating the weight or maintaining the equi librium of any device or apparatus which is adjustable to different levels or inclinations and for permitting such adjustments to be carried out with the least possible physi cal effort. Among the numerous types of devices and appa ratus to which the present invention may be applied may be mentioned especially: drawing tables, X-ray apparatus, hair driers, tilting doors and windows, covers for large freezer chests, and so forth. In connection with such devices or apparatus it is con ventionai to balance their weight or to maintain their equi librium by the provision of counterweights or coil springs. The employment of counterweights has the disadvantage that they require considerable space and also considerably increase the weight of the entire apparatus. The use of coil springs, on the other hand, has the disadvantage that the operations of compressing or expanding such springs require a considerable force and either require or result in considerable changes inforce which have to be compen sated by special mechanical means such as levers, cams, or other force-transmitting means which considerably in crease the cost of the respective apparatus. Another device which has previously been employed for the above-mentioned purposes is a so-called gas spring which consists of a pneumatic cylinder and piston unit in which the cylinder is filled with a pressure gas, for exam ple, compressed air. Although very successful when spe cially designed for a specific apparatus, these gas springs have the disadvantage that each of them has a very par ticular spring characteristic and that therefore a large number of different gas springs have to be produced and be held available for compensating the different forces of different devices or apparatus and even for compen sating differences in force which might be due to inac curacies of manufacture of an individual apparatus of a series thereof. When such gas springs are to be installed, for example, on a drawing table, it is evident that dif ferent gas springs would be required either for merely balancing the weight of the drawing board itself or for 0."
            },
            "slug": "F-ADoefaa-Doetsch",
            "title": {
                "fragments": [],
                "text": "F"
            },
            "venue": {
                "fragments": [],
                "text": "The Herodotus Encyclopedia"
            },
            "year": 1934
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116657183"
                        ],
                        "name": "Miao Li",
                        "slug": "Miao-Li",
                        "structuredName": {
                            "firstName": "Miao",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miao Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118899136"
                        ],
                        "name": "Xiao-Dong Li",
                        "slug": "Xiao-Dong-Li",
                        "structuredName": {
                            "firstName": "Xiao-Dong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-Dong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118511079"
                        ],
                        "name": "Shuang Wang",
                        "slug": "Shuang-Wang",
                        "structuredName": {
                            "firstName": "Shuang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46394601"
                        ],
                        "name": "Yi Wang",
                        "slug": "Yi-Wang",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 118524981,
            "fieldsOfStudy": [
                "Physics",
                "Education"
            ],
            "id": "0e5ecccc372e5aa92f700b0f0fe08c98efb17d0c",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 403,
            "paperAbstract": {
                "fragments": [],
                "text": "Miao Li, Xiao-Dong Li, Shuang Wang and Yi Wang 1 Institute of Theoretical Physics, Chinese Academy of Sciences Beijing 100190, China 2 Kavli Institute for Theoretical Physics, Key Laboratory of Frontiers in Theoretical Physics Beijing 100190, China 3 Department of Modern Physics, University of Science and Technology of China Hefei 230026, China 4 Physics Department, McGill University Montreal, H3A2T8, Canada"
            },
            "slug": "Dark-Energy-Li-Li",
            "title": {
                "fragments": [],
                "text": "Dark Energy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141732326"
                        ],
                        "name": "Jianguo Zhang",
                        "slug": "Jianguo-Zhang",
                        "structuredName": {
                            "firstName": "Jianguo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguo Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "Using the images from PASCAL VOC 2012 [9] containing a person, we instructed workers on Amazon Mechanical Turk to annotate each person with their action, the object with which they are interacting, the scene, and their best prediction of the motivation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 63925014,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "0ec48ac86456cea3d6d6172ca81ef68e98b21a61",
            "isKey": false,
            "numCitedBy": 3322,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-PASCAL-Visual-Object-Classes-Challenge-Zhang",
            "title": {
                "fragments": [],
                "text": "The PASCAL Visual Object Classes Challenge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 148
                            }
                        ],
                        "text": "Action Prediction: There have been several works in robotics that predicts a person\u2019s imminent next action from a sequence of images [31, 23, 15, 8, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17548695,
            "fieldsOfStudy": [],
            "id": "1f7c9d1b4cfe403a6de8fe638bc64b3d384d493d",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning intentions for improved human motion prediction"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "Capitalizing on the theory of mind, we are able to instruct a crowd of workers to annotate why people are likely undertaking actions in photographs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Caffe: An open source convolutional architecture for fast feature embedding"
            },
            "venue": {
                "fragments": [],
                "text": "Caffe: An open source convolutional architecture for fast feature embedding"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 65
                            }
                        ],
                        "text": "We then combine these labels with state-ofthe-art image features [18, 12] to train data-driven classifiers that predict a person\u2019s motivation from images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 115
                            }
                        ],
                        "text": "We computed features from the second to last layer in the AlexNet convolutional neural network trained on ImageNet [18, 12] due to their state-of-the-art performance on other visual recognition problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Caffe: An open source convolutional architecture for fast feature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Grante : Inference and estimation for discrete factor graph model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Grante: Inference and estimation for discrete factor graph model"
            },
            "venue": {
                "fragments": [],
                "text": "Grante: Inference and estimation for discrete factor graph model"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 25,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 45,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Inferring-the-Why-in-Images-Pirsiavash-Vondrick/dfe448d6297ea0a3d4deba21fbf1006bc35877d7?sort=total-citations"
}