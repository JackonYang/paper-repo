{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764577"
                        ],
                        "name": "Alberto H. F. Laender",
                        "slug": "Alberto-H.-F.-Laender",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Laender",
                            "middleNames": [
                                "H.",
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto H. F. Laender"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1396785771"
                        ],
                        "name": "B. Ribeiro-Neto",
                        "slug": "B.-Ribeiro-Neto",
                        "structuredName": {
                            "firstName": "Berthier",
                            "lastName": "Ribeiro-Neto",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ribeiro-Neto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690426"
                        ],
                        "name": "A. D. Silva",
                        "slug": "A.-D.-Silva",
                        "structuredName": {
                            "firstName": "Altigran",
                            "lastName": "Silva",
                            "middleNames": [
                                "Soares",
                                "da"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. D. Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065541874"
                        ],
                        "name": "Juliana S. Teixeira",
                        "slug": "Juliana-S.-Teixeira",
                        "structuredName": {
                            "firstName": "Juliana",
                            "lastName": "Teixeira",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juliana S. Teixeira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 30
                            }
                        ],
                        "text": "17 3.2 Layer cake comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 20
                            }
                        ],
                        "text": "3 1.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Our contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.4 Organization of the survey . . . . . . . . . . . . . . . . . . . . . . . .\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 7717201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b84720c6a517c7db44c2ff54a082e1a98b765eed",
            "isKey": true,
            "numCitedBy": 782,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last few years, several works in the literature have addressed the problem of data extraction from Web pages. The importance of this problem derives from the fact that, once extracted, the data can be handled in a way similar to instances of a traditional database. The approaches proposed in the literature to address the problem of Web data extraction use techniques borrowed from areas such as natural language processing, languages and grammars, machine learning, information retrieval, databases, and ontologies. As a consequence, they present very distinct features and capabilities which make a direct comparison difficult to be done. In this paper, we propose a taxonomy for characterizing Web data extraction fools, briefly survey major Web data extraction tools described in the literature, and provide a qualitative analysis of them. Hopefully, this work will stimulate other studies aimed at a more comprehensive analysis of data extraction approaches and tools for Web data."
            },
            "slug": "A-brief-survey-of-web-data-extraction-tools-Laender-Ribeiro-Neto",
            "title": {
                "fragments": [],
                "text": "A brief survey of web data extraction tools"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A taxonomy for characterizing Web data extraction fools is proposed, a survey of major web data extraction tools described in the literature is briefly surveyed, and a qualitative analysis of them is provided."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720956"
                        ],
                        "name": "Chia-Hui Chang",
                        "slug": "Chia-Hui-Chang",
                        "structuredName": {
                            "firstName": "Chia-Hui",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chia-Hui Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2484984"
                        ],
                        "name": "Mohammed Kayed",
                        "slug": "Mohammed-Kayed",
                        "structuredName": {
                            "firstName": "Mohammed",
                            "lastName": "Kayed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohammed Kayed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570984"
                        ],
                        "name": "M. Girgis",
                        "slug": "M.-Girgis",
                        "structuredName": {
                            "firstName": "Moheb",
                            "lastName": "Girgis",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girgis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40241708"
                        ],
                        "name": "K. Shaalan",
                        "slug": "K.-Shaalan",
                        "structuredName": {
                            "firstName": "Khaled",
                            "lastName": "Shaalan",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shaalan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19] introduced a tridimensional categorization of Web Data Extraction systems, based on task difficulties, techniques used and degree of automation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206742377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cc310717f422592c3cc9a046943777468c14358",
            "isKey": false,
            "numCitedBy": 877,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "The Internet presents a huge amount of useful information which is usually formatted for its users, which makes it difficult to extract relevant data from various sources. Therefore, the availability of robust, flexible information extraction (IE) systems that transform the Web pages into program-friendly structures such as a relational database will become a great necessity. Although many approaches for data extraction from Web pages have been developed, there has been limited effort to compare such tools. Unfortunately, in only a few cases can the results generated by distinct tools be directly compared since the addressed extraction tasks are different. This paper surveys the major Web data extraction approaches and compares them in three dimensions: the task domain, the automation degree, and the techniques used. The criteria of the first dimension explain why an IE system fails to handle some Web sites of particular structures. The criteria of the second dimension classify IE systems based on the techniques used. The criteria of the third dimension measure the degree of automation for IE systems. We believe these criteria provide qualitatively measures to evaluate various IE approaches"
            },
            "slug": "A-Survey-of-Web-Information-Extraction-Systems-Chang-Kayed",
            "title": {
                "fragments": [],
                "text": "A Survey of Web Information Extraction Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper surveys the major Web data extraction approaches and compares them in three dimensions: the task domain, the automation degree, and the techniques used and believes these criteria provide qualitatively measures to evaluate various IE approaches."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40343421"
                        ],
                        "name": "Yanhong Zhai",
                        "slug": "Yanhong-Zhai",
                        "structuredName": {
                            "firstName": "Yanhong",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanhong Zhai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145321667"
                        ],
                        "name": "B. Liu",
                        "slug": "B.-Liu",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 35
                            }
                        ],
                        "text": "It has been recently formalized by Zhai and Liu [126, 127] and the authors also developed a Web Data Extraction system based on it."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 48
                            }
                        ],
                        "text": "It has been recently formalized by Zhai and Liu [126,127] and the authors also developed a Web Data Extraction system based on it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 506970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "468c8699f772ce84dec9c656e71bd52e15c1889a",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the problem of structured data extraction from arbitrary Web pages. The objective of the proposed research is to automatically segment data records in a page, extract data items/fields from these records, and store the extracted data in a database. Existing methods addressing the problem can be classified into three categories. Methods in the first category provide some languages to facilitate the construction of data extraction systems. Methods in the second category use machine learning techniques to learn wrappers (which are data extraction programs) from human labeled examples. Manual labeling is time-consuming and is hard to scale to a large number of sites on the Web. Methods in the third category are based on the idea of automatic pattern discovery. However, multiple pages that conform to a common schema are usually needed as the input. In this paper, we propose a novel and effective technique (called DEPTA) to perform the task of Web data extraction automatically. The method consists of two steps: 1) identifying individual records in a page and 2) aligning and extracting data items from the identified records. For step 1, a method based on visual information and tree matching is used to segment data records. For step 2, a novel partial alignment technique is proposed. This method aligns only those data items in a pair of records that can be aligned with certainty, making no commitment on the rest of the items. Experimental results obtained using a large number of Web pages from diverse domains show that the proposed two-step technique is highly effective"
            },
            "slug": "Structured-Data-Extraction-from-the-Web-Based-on-Zhai-Liu",
            "title": {
                "fragments": [],
                "text": "Structured Data Extraction from the Web Based on Partial Tree Alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A novel and effective technique to perform the task of Web data extraction automatically, called DEPTA, which consists of two steps: identifying individual records in a page and aligning and extracting data items from the identified records."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2000882"
                        ],
                        "name": "Wolfgang Gatterbauer",
                        "slug": "Wolfgang-Gatterbauer",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Gatterbauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Gatterbauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 117
                            }
                        ],
                        "text": "Web Data Extraction systems are a broad class of software applications targeting at extracting data from Web sources [81,11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[11] is the most recently updated review on the discipline as of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12454926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbe337fd5b87eb325aa16f23fa71cda50827e01f",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "SYNONYMS web data extraction toolkit, web information extraction system, wrapper generator, wrapper generator toolkit, web macros, web scraper. DEFINITION A web data extraction system is a software system that automatically and repeatedly extracts data from web pages with changing content and delivers the extracted data to a database or some other application. The task of web data extraction performed by such a system is usually divided into five different functions: (1) web interaction, which comprises mainly the navigation to usually predetermined target web pages containing the desired information; (2) support for wrapper generation and execution, where a wrapper is a program that identifies the desired data on target pages, extracts the data and transforms it into a structured format; (3) scheduling, which allows repeated application of previously generated wrappers to their respective target pages; (4) data transformation, which includes filtering, transforming, refining, and integrating data extracted from one or more sources and structuring the result according to a desired output format (usually XML or relational tables); and (5) delivering the resulting structured data to external applications such as database management systems, data warehouses, business software systems, content management systems, decision support systems, RSS publishers, email servers, or SMS servers. Alternatively, the output can be used to generate new web services out of existing and continually changing web sources."
            },
            "slug": "Web-Data-Extraction-System-Baumgartner-Gatterbauer",
            "title": {
                "fragments": [],
                "text": "Web Data Extraction System"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A web data extraction system is a software system that automatically and repeatedly extracts data from web pages with changing content and delivers the extracted data to a database or some other application."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Database Systems"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40343421"
                        ],
                        "name": "Yanhong Zhai",
                        "slug": "Yanhong-Zhai",
                        "structuredName": {
                            "firstName": "Yanhong",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanhong Zhai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145321667"
                        ],
                        "name": "B. Liu",
                        "slug": "B.-Liu",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 35
                            }
                        ],
                        "text": "It has been recently formalized by Zhai and Liu [126, 127] and the authors also developed a Web Data Extraction system based on it."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 48
                            }
                        ],
                        "text": "It has been recently formalized by Zhai and Liu [126,127] and the authors also developed a Web Data Extraction system based on it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 90
                            }
                        ],
                        "text": "Let us analyze the behavior of the algorithm with an example often used in the literature [122,126,39] to explain the simple tree matching (see Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12750207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f49e35b6a85b8d81d2c9d9e26e8bf19dd94fad3a",
            "isKey": false,
            "numCitedBy": 600,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the problem of extracting data from a Web page that contains several structured data records. The objective is to segment these data records, extract data items/fields from them and put the data in a database table. This problem has been studied by several researchers. However, existing methods still have some serious limitations. The first class of methods is based on machine learning, which requires human labeling of many examples from each Web site that one is interested in extracting data from. The process is time consuming due to the large number of sites and pages on the Web. The second class of algorithms is based on automatic pattern discovery. These methods are either inaccurate or make many assumptions. This paper proposes a new method to perform the task automatically. It consists of two steps, (1) identifying individual data records in a page, and (2) aligning and extracting data items from the identified data records. For step 1, we propose a method based on visual information to segment data records, which is more accurate than existing methods. For step 2, we propose a novel partial alignment technique based on tree matching. Partial alignment means that we align only those data fields in a pair of data records that can be aligned (or matched) with certainty, and make no commitment on the rest of the data fields. This approach enables very accurate alignment of multiple data records. Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately."
            },
            "slug": "Web-data-extraction-based-on-partial-tree-alignment-Zhai-Liu",
            "title": {
                "fragments": [],
                "text": "Web data extraction based on partial tree alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819393"
                        ],
                        "name": "Fotis Kokkoras",
                        "slug": "Fotis-Kokkoras",
                        "structuredName": {
                            "firstName": "Fotis",
                            "lastName": "Kokkoras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fotis Kokkoras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2280041"
                        ],
                        "name": "Konstantinos Ntonas",
                        "slug": "Konstantinos-Ntonas",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Ntonas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konstantinos Ntonas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143804820"
                        ],
                        "name": "Nick Bassiliades",
                        "slug": "Nick-Bassiliades",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Bassiliades",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nick Bassiliades"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "DEiXTo [72] utilizes a simplified tree matching algorithm which is easy for the end-user to follow, thus enabling her to easily build well-engineered, DOM-based, extraction patterns, using the visual tools provided."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14852130,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6a932ab36a564ebf945d00cf2ef1b2404293ce0",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Web data extraction (or web scraping) is the process of collecting unstructured or semi-structured information from the World Wide Web, at different levels of automation. It is an important, valuable and practical approach towards web reuse while at the same time can serve the transition of the web to the semantic web, by providing the structured data required by the latter. In this paper we present DEiXTo, a web data extraction suite that provides an arsenal of features aiming at designing and deploying well-engineered extraction tasks. We focus on presenting the core pattern matching algorithm and the overall architecture, which allows programming of custom-made solutions for hard extraction tasks. DEiXTo consists of both freeware and open source components."
            },
            "slug": "DEiXTo:-a-web-data-extraction-suite-Kokkoras-Ntonas",
            "title": {
                "fragments": [],
                "text": "DEiXTo: a web data extraction suite"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents DEiXTo, a web data extraction suite that provides an arsenal of features aiming at designing and deploying well-engineered extraction tasks, and focuses on presenting the core pattern matching algorithm and the overall architecture, which allows programming of custom-made solutions for hard extraction tasks."
            },
            "venue": {
                "fragments": [],
                "text": "BCI '13"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38699354"
                        ],
                        "name": "W. Meng",
                        "slug": "W.-Meng",
                        "structuredName": {
                            "firstName": "Weiyi",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662719"
                        ],
                        "name": "Hongkun Zhao",
                        "slug": "Hongkun-Zhao",
                        "structuredName": {
                            "firstName": "Hongkun",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongkun Zhao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 167
                            }
                        ],
                        "text": "Just for now, we generically define the concept of wrapper as a procedure extracting unstructured information from a source and transforming them into structured data [Zhao 2007; Irmak and Suel 2006]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 114900748,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30341cb4e4c3ea7722a474dccfa514a141906dfb",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The deep web, which is estimated about 500 times larger than that of the surface web, is extremely under-utilized. Researchers have been working on various issues towards the building of large-scale deep web applications, which aim at unleashing the real power of the deep web. One of the key issues facing large-scale deep applications is the extraction and understanding of the data returned by deep web sites. In order to utilize the data in deep web sites, we need to extract the data (search result records) from the search result pages, which are web pages that contain both the data of interest and other unrelated content, returned by the deep web sites. Data extraction from web pages is generally a very hard problem. The performances of existing researches in the literature are far from satisfactory. \nThis dissertation studies the problem of extracting search result records from search engine returned pages in both the deep web sites and the surface web sites. A method that combines both the visual content features and the HTML tag structures the result pages is proposed to generate wrappers for the extraction of search result records. This novel technique archives significantly better performance than that of the state-of-the-art researches. \nTo extract search result records from categorized result pages requires maintaining the section-record relationships. Major issues like section boundaries and optional sections make achieving a good performance difficult. We introduce a novel method based on the content properties of search result records and the dynamic properties of sections. \nA search result record usually consists of multiple data units. The semi-structured nature of search result records makes the data units extraction a hard problem. The mismatches between the HTML tag structures and the data structure of search result records as well as the optional and disjunctive data units further limit the performance. We introduce a novel directed acyclic graph representation of search result record templates, which can be used to extract data units from search result records. An effective machine learning and statistics based algorithm that extracts templates from search result records is also presented."
            },
            "slug": "Automatic-wrapper-generation-for-the-extraction-of-Meng-Zhao",
            "title": {
                "fragments": [],
                "text": "Automatic wrapper generation for the extraction of search result records from search engines"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel directed acyclic graph representation of search result record templates, which can be used to extract data units from search result records is introduced, which is significantly better performance than that of the state-of-the-art researches."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363437"
                        ],
                        "name": "M. Ceresna",
                        "slug": "M.-Ceresna",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Ceresna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ceresna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219094"
                        ],
                        "name": "Gerald Ledermuller",
                        "slug": "Gerald-Ledermuller",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Ledermuller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerald Ledermuller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "The most advanced Web Data Extraction systems support the extraction of data from pages reached by deep Web navigation [6]: they simulate the activity of users clicking on DOM elements of pages, through macros or, more simply, by filling HTML forms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15520727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6027f4b6cb497480c8516af537ae0fca20299e04",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In literature, data extraction techniques for HTML and semi-structured data in general have been exhaustively studied and a number of automatic and semi-automatic approaches proposed. However, in real-life scenarios data extraction capabilities are only one half of the game. Password-protected sites, cookies, non-HTML data formats, JavaScript, session IDs, Web form iterations and dynamic changes on Web sites are the obstacles that make Web data extraction difficult in real-life application scenarios. We propose, based on current Lixto technology, a novel approach that introduces action-based Web navigation sequence recording and replaying and its close integration with extraction technologies. On the one hand, the technical innovation is the embedding of the Mozilla browser into the Lixto visual wrapper with the advantage of the support of a large number of Web standards and an open-source API to permit close interaction of Lixto with Mozilla. On the other hand, we develop a navigation language and explore its close interaction with Elog, the extraction language of Lixto. Current research status and sample screenshots are given. The paper closes with a description of two application domains where deep Web navigation capabilities play a crucial role, that is automotive B2B Web platforms and business intelligence scenarios"
            },
            "slug": "DeepWeb-Navigation-in-Web-Data-Extraction-Baumgartner-Ceresna",
            "title": {
                "fragments": [],
                "text": "DeepWeb Navigation in Web Data Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel approach is proposed that introduces action-based Web navigation sequence recording and replaying and its close integration with extraction technologies and develops a navigation language and explores its close interaction with Elog, the extraction language of Lixto."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153481005"
                        ],
                        "name": "M. Herzog",
                        "slug": "M.-Herzog",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Herzog",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Herzog"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 0
                            }
                        ],
                        "text": "[9,12,10] deeply analyzed how to apply Web Data Extraction techniques and tools to improve the process of acquiring market information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 156
                            }
                        ],
                        "text": "Commercial frameworks offer a full IDE (Integrated Development Environment) with functionalities described in the previous section (e.g. Denodo, Kapowtech, Lixto and Mozenda)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 63
                            }
                        ],
                        "text": "The Elog language is used as the core extraction method of the Lixto Visual Wrapper system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 25
                            }
                        ],
                        "text": "Some commercial systems, Lixto for first but also Kapow Mashup Server (described below), include a Graphical User Interface for fully visual and interactive navigation of HTML pages, integrated with data extraction tools."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "One of the first Web Data Extraction platforms relying on a cloud architecture is Lixto [12]: in it cloud clients receive a wrapper as well as extraction parameters and they pass back retrieved data; these data are not stored on cloud instances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 59
                            }
                        ],
                        "text": "The authors implemented this approach in a commercial tool \u2013 Lixto, describing how the pipeline of the wrapper generation has been modified to allow the wrappers to automatically detect and adapt their functioning to structural modifications occurring to Web pages [41]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 210
                            }
                        ],
                        "text": "Regarding the definition of deep Web navigations, such as form fillouts, a number of tools offer VCR-style recording the human browsing and replaying the recorded navigation sequence (e.g., Chickenfoot, iOpus, Lixto)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17623968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "895b7f65610899a4fe811a4aaac121279ced1118",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Online market intelligence (OMI), in particular competitive intelligence for product pricing, is a very important application area for Web data extraction. However, OMI presents non-trivial challenges to data extraction technology. Sophisticated and highly parameterized navigation and extraction tasks are required. On-the-fly data cleansing is necessary in order two identify identical products from different suppliers. It must be possible to smoothly define data flow scenarios that merge and filter streams of extracted data stemming from several Web sites and store the resulting data into a data warehouse, where the data is subjected to market intelligence analytics. Finally, the system must be highly scalable, in order to be able to extract and process massive amounts of data in a short time. Lixto (www.lixto.com), a company offering data extraction tools and services, has been providing OMI solutions for several customers. In this paper we show how Lixto has tackled each of the above challenges by improving and extending its original data extraction software. Most importantly, we show how high scalability is achieved through cloud computing. This paper also features a case study from the computers and electronics market."
            },
            "slug": "Scalable-Web-Data-Extraction-for-Online-Market-Baumgartner-Gottlob",
            "title": {
                "fragments": [],
                "text": "Scalable Web Data Extraction for Online Market Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper shows how Lixto has tackled each of the above challenges by improving and extending its original data extraction software and shows how high scalability is achieved through cloud computing."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2221116"
                        ],
                        "name": "G. Gkotsis",
                        "slug": "G.-Gkotsis",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Gkotsis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gkotsis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144844260"
                        ],
                        "name": "K. Stepanyan",
                        "slug": "K.-Stepanyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Stepanyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stepanyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722663"
                        ],
                        "name": "A. Cristea",
                        "slug": "A.-Cristea",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Cristea",
                            "middleNames": [
                                "Ioana"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cristea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145799400"
                        ],
                        "name": "M. Joy",
                        "slug": "M.-Joy",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Joy",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Joy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": ", structured XML documents allowing to access the content of a Web page) plus the hypertext of the blog [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24279312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2671a97b09dc8456844f8434b2ea67a4a374064",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Data extraction from the web is notoriously hard. Of the types of resources available on the web, weblogs are becoming increasingly important due to the continued growth of the blogosphere, but remain poorly explored. Past approaches to data extraction from weblogs have often involved manual intervention and suffer from low scalability. This paper proposes a fully automated information extraction methodology based on the use of web feeds and processing of HTML. The approach includes a model for generating a wrapper that exploits web feeds for deriving a set of extraction rules automatically. Instead of performing a pairwise comparison between posts, the model matches the values of the web feeds against their corresponding HTML elements retrieved from multiple weblog posts. It adopts a probabilistic approach for deriving a set of rules and automating the process of wrapper generation. An evaluation of the model is conducted on a dataset of 2,393 posts and the results (92% accuracy) show that the proposed technique enables robust extraction of weblog properties and can be applied across the blogosphere for applications such as improved information retrieval and more robust web preservation initiatives."
            },
            "slug": "Self-supervised-Automated-Wrapper-Generation-for-Gkotsis-Stepanyan",
            "title": {
                "fragments": [],
                "text": "Self-supervised Automated Wrapper Generation for Weblog Data Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Results show that the proposed technique enables robust extraction of weblog properties and can be applied across the blogosphere for applications such as improved information retrieval and more robust web preservation initiatives."
            },
            "venue": {
                "fragments": [],
                "text": "BNCOD"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39665544"
                        ],
                        "name": "D. C. Reis",
                        "slug": "D.-C.-Reis",
                        "structuredName": {
                            "firstName": "Davi",
                            "lastName": "Reis",
                            "middleNames": [
                                "de",
                                "Castro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. C. Reis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285220"
                        ],
                        "name": "P. B. Golgher",
                        "slug": "P.-B.-Golgher",
                        "structuredName": {
                            "firstName": "Paulo",
                            "lastName": "Golgher",
                            "middleNames": [
                                "Braz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. B. Golgher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690426"
                        ],
                        "name": "A. D. Silva",
                        "slug": "A.-D.-Silva",
                        "structuredName": {
                            "firstName": "Altigran",
                            "lastName": "Silva",
                            "middleNames": [
                                "Soares",
                                "da"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. D. Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764577"
                        ],
                        "name": "Alberto H. F. Laender",
                        "slug": "Alberto-H.-F.-Laender",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Laender",
                            "middleNames": [
                                "H.",
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto H. F. Laender"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 50
                            }
                        ],
                        "text": "The worst case time complexity of the approach of [102] is still O\u00f0nodes\u00f0A\u00de nodes\u00f0B\u00de\u00de but, as shown by the authors, it works much faster because only restricted top-down mappings are managed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3343581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83cc20d078920efa6cc3876a83065beddb592474",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The Web poses itself as the largest data repository ever available in the history of humankind. Major efforts have been made in order to provide efficient access to relevant information within this huge repository of data. Although several techniques have been developed to the problem of Web data extraction, their use is still not spread, mostly because of the need for high human intervention and the low quality of the extraction results.In this paper, we present a domain-oriented approach to Web data extraction and discuss its application to automatically extracting news from Web sites. Our approach is based on a highly efficient tree structure analysis that produces very effective results. We have tested our approach with several important Brazilian on-line news sites and achieved very precise results, correctly extracting 87.71% of the news in a set of 4088 pages distributed among 35 different sites."
            },
            "slug": "Automatic-web-news-extraction-using-tree-edit-Reis-Golgher",
            "title": {
                "fragments": [],
                "text": "Automatic web news extraction using tree edit distance"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A domain-oriented approach to Web data extraction is presented and its application to automatically extracting news from Web sites is discussed, based on a highly efficient tree structure analysis that produces very effective results."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145887845"
                        ],
                        "name": "D. M. Campbell",
                        "slug": "D.-M.-Campbell",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Campbell",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Campbell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158090475"
                        ],
                        "name": "Y. S. Jiang",
                        "slug": "Y.-S.-Jiang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Jiang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. S. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742211"
                        ],
                        "name": "Stephen W. Liddle",
                        "slug": "Stephen-W.-Liddle",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Liddle",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen W. Liddle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143743807"
                        ],
                        "name": "Yiu-Kai Ng",
                        "slug": "Yiu-Kai-Ng",
                        "structuredName": {
                            "firstName": "Yiu-Kai",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiu-Kai Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144259075"
                        ],
                        "name": "D. Quass",
                        "slug": "D.-Quass",
                        "structuredName": {
                            "firstName": "Dallan",
                            "lastName": "Quass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Quass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108985589"
                        ],
                        "name": "Randy D. Smith",
                        "slug": "Randy-D.-Smith",
                        "structuredName": {
                            "firstName": "Randy",
                            "lastName": "Smith",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Randy D. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Relying on a set of primitives to compare with the structure of the given page, these tools can find one or more objects in the page matching the primitive items [Embley et al. 1999; Gatterbauer et al. 2007]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3217704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "156a5992d338d2bf7ba8f5af50be8985fabe93fb",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Conceptual-Model-Based-Data-Extraction-from-Web-Embley-Campbell",
            "title": {
                "fragments": [],
                "text": "Conceptual-Model-Based Data Extraction from Multiple-Record Web Pages"
            },
            "venue": {
                "fragments": [],
                "text": "Data Knowl. Eng."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48898287"
                        ],
                        "name": "Emilio Ferrara",
                        "slug": "Emilio-Ferrara",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ferrara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Ferrara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, it has been adopted in several scenarios [66,126,127,129,39\u201341]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "More recently, Ferrara and Baumgartner [39\u201341] developed a system of automatic wrapper adaptation (a kind of maintenance that occur to modify Web wrappers according to the new structure of the Web pages) relying on the analysis of structural similarities between different versions of the same Web page using a tree-edit algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "sented in [39\u201341]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8069631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4217c582edebc044edd6afd1ea343743eb4b294f",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Nowadays, the huge amount of information distributed through the Web motivates studying techniques to \nbe adopted in order to extract relevant data in an ef\ufb01cient and reliable way. Both academia and enterprises \ndeveloped several approaches of Web data extraction, for example using techniques of arti\ufb01cial intelligence or \nmachine learning. Some commonly adopted procedures, namely wrappers, ensure a high degree of precision \nof information extracted from Web pages, and, at the same time, have to prove robustness in order not to \ncompromise quality and reliability of data themselves. \nIn this paper we focus on some experimental aspects related to the robustness of the data extraction process \nand the possibility of automatically adapting wrappers. We discuss the implementation of algorithms for \n\ufb01nding similarities between two different version of a Web page, in order to handle modi\ufb01cations, avoiding \nthe failure of data extraction tasks and ensuring reliability of information extracted. Our purpose is to evaluate \nperformances, advantages and draw-backs of our novel system of automatic wrapper adaptation."
            },
            "slug": "Design-of-Automatically-Adaptable-Web-Wrappers-Ferrara-Baumgartner",
            "title": {
                "fragments": [],
                "text": "Design of Automatically Adaptable Web Wrappers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The performance, advantages and draw-backs of the novel system of automatic wrapper adaptation are evaluated to evaluate the robustness of the data extraction process and the possibility of automatically adapting wrappers."
            },
            "venue": {
                "fragments": [],
                "text": "ICAART"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770124"
                        ],
                        "name": "Sunita Sarawagi",
                        "slug": "Sunita-Sarawagi",
                        "structuredName": {
                            "firstName": "Sunita",
                            "lastName": "Sarawagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunita Sarawagi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 63
                            }
                        ],
                        "text": "A relevant survey on Information Extraction is due to Sarawagi [105] and, in our opinion, anybody who intends to approach this discipline should read it."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 181
                            }
                        ],
                        "text": "In particular, two classes of strategies emerged [64]: learning techniques and knowledge engineering techniques \u2013 also called learning-based and rule-based approaches, respectively [105]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64422510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4510a11fce6444f952ce06adbe721ac8c457b5e2",
            "isKey": false,
            "numCitedBy": 447,
            "numCiting": 202,
            "paperAbstract": {
                "fragments": [],
                "text": "The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem. \n \nThis review is a survey of information extraction research of over two decades from these diverse communities. We create a taxonomy of the field along various dimensions derived from the nature of the extraction task, the techniques used for extraction, the variety of input resources exploited, and the type of output produced. We elaborate on rule-based and statistical methods for entity and relationship extraction. In each case we highlight the different kinds of models for capturing the diversity of clues driving the recognition process and the algorithms for training and efficiently deploying the models. We survey techniques for optimizing the various steps in an information extraction pipeline, adapting to dynamic data, integrating with existing entities and handling uncertainty in the extraction process."
            },
            "slug": "Information-Extraction-Sarawagi",
            "title": {
                "fragments": [],
                "text": "Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A taxonomy of the field is created along various dimensions derived from the nature of the extraction task, the techniques used for extraction, the variety of input resources exploited, and the type of output produced to survey techniques for optimizing the various steps in an information extraction pipeline."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Databases"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143976873"
                        ],
                        "name": "Xiaofeng Meng",
                        "slug": "Xiaofeng-Meng",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070032066"
                        ],
                        "name": "Dongdong Hu",
                        "slug": "Dongdong-Hu",
                        "structuredName": {
                            "firstName": "Dongdong",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongdong Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49673312"
                        ],
                        "name": "Chen Li",
                        "slug": "Chen-Li",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[89], and it is called schema-guided wrapper maintenance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8850461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a87f5e35b745e377a5fd9de9daba986e2ab902fd",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting data from Web pages using wrappers is a fundamental problem arising in a large variety of applications of vast practical interests. There are two main issues relevant to Web-data extraction, namely wrapper generation and wrapper maintenance. In this paper, we propose a novel schema-guided approach to the problem of automatic wrapper maintenance. It is based on the observation that despite various page changes, many important features of the pages are preserved, such as syntactic patterns, annotations, and hyperlinks of the extracted data items. Our approach uses these preserved features to identify the locations of the desired values in the changed pages, and repair wrappers correspondingly by inducing semantic blocks from the HTML tree. Our intensive experiments on real Web sites show that the proposed approach can effectively maintain wrappers to extract desired data with high accuracies."
            },
            "slug": "Schema-guided-wrapper-maintenance-for-web-data-Meng-Hu",
            "title": {
                "fragments": [],
                "text": "Schema-guided wrapper maintenance for web-data extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes a novel schema-guided approach to the problem of automatic wrapper maintenance, based on the observation that despite various page changes, many important features of the pages are preserved, such as syntactic patterns, annotations, and hyperlinks of the extracted data items."
            },
            "venue": {
                "fragments": [],
                "text": "WIDM '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32452484"
                        ],
                        "name": "A. Campi",
                        "slug": "A.-Campi",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Campi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Campi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153481005"
                        ],
                        "name": "M. Herzog",
                        "slug": "M.-Herzog",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Herzog",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Herzog"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 157
                            }
                        ],
                        "text": ", the seamless integration of Web applications into a corporate infrastructure or service oriented landscape by generating Web services from given Web sites [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38949182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "215eb8dc01265010aa8229894c3799efa91a2a66",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Web data extraction is an enabling technique in the search computing scenario. In this chapter, we first review the state of the art in wrapper technologies focusing on how wrapper generators can be used to create unified services that integrate data from Web Applications and Web services in various domains. Next, we describe the Lixto approach and we present the Lixto Suite as one example of Web Process Integration. Finally, application areas and future challenges and the usage of wrapper technologies in the search computing context is discussed."
            },
            "slug": "Web-Data-Extraction-for-Service-Creation-Baumgartner-Campi",
            "title": {
                "fragments": [],
                "text": "Web Data Extraction for Service Creation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This chapter reviews the state of the art in wrapper technologies focusing on how wrapper generators can be used to create unified services that integrate data from Web Applications and Web services in various domains and describes the Lixto approach."
            },
            "venue": {
                "fragments": [],
                "text": "SeCO Workshop"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 105
                            }
                        ],
                        "text": "At the Enterprise level, Web Data Extraction techniques emerge as a key tool to perform data analysis in Business and Competitive Intelligence systems as well as for business process re-engineering."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "[9, 12, 10] deeply analyzed how to apply Web Data Extraction techniques and tools to improve the process of acquiring market information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 59
                            }
                        ],
                        "text": "This requirement is particularly stringent in the field of Business and Competitive Intelligence because a company needs to perform timely analysis of market conditions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 264
                            }
                        ],
                        "text": "Web Data Extraction systems find extensive use in a wide range of applications including the analysis of textbased documents available to a company (like e-mails, support forums, technical and legal documentation, and so on), Business and Competitive Intelligence [9], crawling of Social Web platforms [17, 52], BioInformatics [99] and so on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 25
                            }
                        ],
                        "text": "This process is known as Competitive Intelligence [22, 125] and it is crucial to quickly identify the opportunities provided by the market, to anticipate the decisions of the competitors as well as to learn from their faults and successes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 35
                            }
                        ],
                        "text": "24 4.1.5 Business Intelligence and Competitive Intelligence . . . . . . . . . . . . . . . . . . . . 24 4.1.6 Web process integration and channel management . . . . . . . . . . . . . . . . . . . . 25 4.1.7 Functional Web application testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.1.8 Comparison shopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5505039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68070ecd9f60a405ddd3183784331eb0d32abe3a",
            "isKey": true,
            "numCitedBy": 44,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge about market developments and competitor activities on the market becomes more and more a critical success factor for enterprises. The World Wide Web provides public domain information which can be retrieved for example from Web sites or online shops. The extraction from semi-structured information sources is mostly done manually and is therefore very time consuming. This paper describes how public information can be extracted automatically from Web sites, transformed into structured data formats, and used for data analysis in Business Intelligence systems."
            },
            "slug": "Web-Data-Extraction-for-Business-Intelligence:-The-Gottlob",
            "title": {
                "fragments": [],
                "text": "Web Data Extraction for Business Intelligence: The Lixto Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "How public information can be extracted automatically from Web sites, transformed into structured data formats, and used for data analysis in Business Intelligence systems is described."
            },
            "venue": {
                "fragments": [],
                "text": "BTW"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762910"
                        ],
                        "name": "X. Phan",
                        "slug": "X.-Phan",
                        "structuredName": {
                            "firstName": "Xuan",
                            "lastName": "Phan",
                            "middleNames": [
                                "Hieu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060737928"
                        ],
                        "name": "S. Horiguchi",
                        "slug": "S.-Horiguchi",
                        "structuredName": {
                            "firstName": "Susumu",
                            "lastName": "Horiguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Horiguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719940"
                        ],
                        "name": "T. Ho",
                        "slug": "T.-Ho",
                        "structuredName": {
                            "firstName": "Tu",
                            "lastName": "Ho",
                            "middleNames": [
                                "Bao"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Statistical Machine Learning systems were also developed, relying on conditional models [98] or adaptive search [114] as an alternative solution to human knowledge and interaction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "Automating the access to Web pages as well as the localization of their elements is one of the most important features included in last Web Data Extraction systems [98]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17951124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "370ce64e790caa458e19d903714c463b26ef9d31",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting data on the Web is an important information extraction task. Most existing approaches rely on wrappers which require human knowledge and user interaction during extraction. This paper proposes the use of conditional models as an alternative solution to this task. Deriving the strength of conditional models like maximum entropy and maximum entropy Markov models, our method offers three major advantages: the full automation, the ability to incorporate various non-independent, overlapping features of different hypertext representations, and the ability to deal with missing and disordered data fields. The experimental results on a wide range of e-commercial websites with different layouts show that our method can achieve a satisfactory trade-off between automation and accuracy, and also provide a practical application of automated data extraction from the Web."
            },
            "slug": "Automated-data-extraction-from-the-web-with-models-Phan-Horiguchi",
            "title": {
                "fragments": [],
                "text": "Automated data extraction from the web with conditional models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deriving the strength of conditional models like maximum entropy and maximum entropy Markov models, this method offers three major advantages: the full automation, the ability to incorporate various non-independent, overlapping features of different hypertext representations, and the able to deal with missing and disordered data fields."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Bus. Intell. Data Min."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48898287"
                        ],
                        "name": "Emilio Ferrara",
                        "slug": "Emilio-Ferrara",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ferrara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Ferrara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 265
                            }
                        ],
                        "text": "The authors implemented this approach in a commercial tool \u2013 Lixto, describing how the pipeline of the wrapper generation has been modified to allow the wrappers to automatically detect and adapt their functioning to structural modifications occurring to Web pages [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16313935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a5add45b25830c12c51444b54abd95fc3f1c7cf",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The amount of information available on the Web grows at an incredible high rate. Systems and procedures devised to extract these data from Web sources already exist, and different approaches and techniques have been investigated during the last years. On the one hand, reliable solutions should provide robust algorithms of Web data mining which could automatically face possible malfunctioning or failures. On the other, in literature there is a lack of solutions about the maintenance of these systems. Procedures that extract Web data may be strictly interconnected with the structure of the data source itself; thus, malfunctioning or acquisition of corrupted data could be caused, for example, by structural modifications of data sources brought by their owners. Nowadays, verification of data integrity and maintenance are mostly manually managed, in order to ensure that these systems work correctly and reliably. In this paper we propose a novel approach to create procedures able to extract data from Web sources - the so called Web wrappers - which can face possible malfunctioning caused by modifications of the structure of the data source, and can automatically repair themselves."
            },
            "slug": "Intelligent-Self-repairable-Web-Wrappers-Ferrara-Baumgartner",
            "title": {
                "fragments": [],
                "text": "Intelligent Self-repairable Web Wrappers"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel approach is proposed to create procedures able to extract data from Web sources - the so called Web wrappers - which can face possible malfunctioning caused by modifications of the structure of the data source, and can automatically repair themselves."
            },
            "venue": {
                "fragments": [],
                "text": "AI*IA"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105757"
                        ],
                        "name": "Utku Irmak",
                        "slug": "Utku-Irmak",
                        "structuredName": {
                            "firstName": "Utku",
                            "lastName": "Irmak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Utku Irmak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691664"
                        ],
                        "name": "Torsten Suel",
                        "slug": "Torsten-Suel",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Suel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Torsten Suel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 132
                            }
                        ],
                        "text": "Eventually, extracted data might be post-processed, converted in the most convenient structured format and stored for further usage [130,63]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7725389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "425e7268794d7116ee505dc7da3bf59f7a6c5375",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "While much of the data on the web is unstructured in nature, there is also a significant amount of embedded structured data, such as product information on e-commerce sites or stock data on financial sites. A large amount of research has focused on the problem of generating wrappers, i.e., software tools that allow easy and robust extraction of structured data from text and HTML sources. In many applications, such as comparison shopping, data has to be extracted from many different sources, making manual coding of a wrapper for each source impractical. On the other hand, fully automatic approaches are often not reliable enough, resulting in low quality of the extracted data.We describe a complete system for semi-automatic wrapper generation that can be trained on different data sources in a simple interactive manner. Our goal is to minimize the amount of user effort for training reliable wrappers through design of a suitable training interface that is implemented based on a powerful underlying extraction language and a set of training and ranking algorithms. Our experiments show that our system achieves reliable extraction with a very small amount of user effort."
            },
            "slug": "Interactive-wrapper-generation-with-minimal-user-Irmak-Suel",
            "title": {
                "fragments": [],
                "text": "Interactive wrapper generation with minimal user effort"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The goal is to minimize the amount of user effort for training reliable wrappers through design of a suitable training interface that is implemented based on a powerful underlying extraction language and a set of training and ranking algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145325584"
                        ],
                        "name": "Jiawei Han",
                        "slug": "Jiawei-Han",
                        "structuredName": {
                            "firstName": "Jiawei",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiawei Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46833537"
                        ],
                        "name": "M. Kamber",
                        "slug": "M.-Kamber",
                        "structuredName": {
                            "firstName": "Micheline",
                            "lastName": "Kamber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kamber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "n about products, customers, competitors with the goal of helping the managers of a company in decisional processes is commonly called Competitive Intelligence, and is strictly related to data mining [58]. Zanasi [125] was the rst to introduce the possibility of acquiring these data, through data mining processes, on public domain information. Chen et al. [22] developed a platform, that works more lik"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52869656,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c36991759325bedd19f69264f72d1cbf59a6158c",
            "isKey": false,
            "numCitedBy": 14116,
            "numCiting": 445,
            "paperAbstract": {
                "fragments": [],
                "text": "The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data"
            },
            "slug": "Data-Mining:-Concepts-and-Techniques-Han-Kamber",
            "title": {
                "fragments": [],
                "text": "Data Mining: Concepts and Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This book presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects, and provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 50
                            }
                        ],
                        "text": "The first example of hybrid system is provided by RoadRunner [28, 27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 66
                            }
                        ],
                        "text": "The first example of this class of systems is given by RoadRunner [28,27], a template-based system that automatically generates templates to extract data by matching features from different pages in the same domain."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 21
                            }
                        ],
                        "text": "The main strength of RoadRunner is that it is oriented to data-intensive websites based on templates or regular structures."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 289
                            }
                        ],
                        "text": "Hence, screen-scraping made back its way into novel Web Data extraction frameworks, using methods from document understanding and spatial reasoning such as the approaches of the TamCrow project [73], of the ABBA project [37] spatial XPath extensions [95] and rendition-based extensions in RoadRunner to detect labels [29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 20
                            }
                        ],
                        "text": "Another strength of RoadRunner is its high-quality open-source implementation (see: www.dia.uniroma3.it/db/roadRunner/), that provides a high degree of reliability of the extraction system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 61
                            }
                        ],
                        "text": "The first example of hybrid system is provided by RoadRunner [28,27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 13
                            }
                        ],
                        "text": "Essentially, RoadRunner can extract relevant information from any Web site containing at least two Web pages with a similar structure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 55
                            }
                        ],
                        "text": "The first example of this class of systems is given by RoadRunner [28, 27], a template-based system that automatically generates templates to extract data by matching features from different pages in the same domain."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 15
                            }
                        ],
                        "text": "In particular, RoadRunner can work using information provided by users, in the form of labeled example pages, or also by automatically labeling Web pages (such as WIEN), to build a training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 0
                            }
                        ],
                        "text": "RoadRunner relies on the idea of working with two HTML pages at the same time in order to discover patterns analyzing similarities and differences between structure and content of each pair of pages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 154
                            }
                        ],
                        "text": "Since usually Web pages are dynamically generated starting from template, and relevant data are positioned in the same (or in similar) areas of the page, RoadRunner is able to exploit this characteristic to identify relevant pieces of information, and, at the same time, taking into account small differences due to missing values or other mismatches."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6572841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a47888c0243cac0b173c2748d8ed1b0a2a15fdd8",
            "isKey": true,
            "numCitedBy": 183,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction from websites is nowadays a relevant problem, usually performed by software modules called wrappers. A key requirement is that the wrapper generation process should be automated to the largest extent, in order to allow for large-scale extraction tasks even in presence of changes in the underlying sites. So far, however, only semi-automatic proposals have appeared in the literature.We present a novel approach to information extraction from websites, which reconciles recent proposals for supervised wrapper induction with the more traditional field of grammar inference. Grammar inference provides a promising theoretical framework for the study of unsupervised---that is, fully automatic---wrapper generation algorithms. However, due to some unrealistic assumptions on the input, these algorithms are not practically applicable to Web information extraction tasks.The main contributions of the article stand in the definition of a class of regular languages, called the prefix mark-up languages, that abstract the structures usually found in HTML pages, and in the definition of a polynomial-time unsupervised learning algorithm for this class. The article shows that, differently from other known classes, prefix mark-up languages and the associated algorithm can be practically used for information extraction purposes.A system based on the techniques described in the article has been implemented in a working prototype. We present some experimental results on known Websites, and discuss opportunities and limitations of the proposed approach."
            },
            "slug": "Automatic-information-extraction-from-large-Crescenzi-Mecca",
            "title": {
                "fragments": [],
                "text": "Automatic information extraction from large websites"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A novel approach to information extraction from websites is presented, which reconciles recent proposals for supervised wrapper induction with the more traditional field of grammar inference, and shows that, differently from other known classes, prefix mark-up languages and the associated algorithm can be practically used for information extraction purposes."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624228"
                        ],
                        "name": "Christoph E. Koch",
                        "slug": "Christoph-E.-Koch",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Koch",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph E. Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "The first powerful wrapping language has been formalized by Gottlob and Koch [55]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15880864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d3f0fed7427aa4ae43424eb76ec65a8c1f2e164",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The Web wrapping proble, i.e., the problem of extracting structured information from HTML documents, is one of great practical importance. The often observed information overload that users of the Web experience witnesses the lack of intelligent and encompassing Web services that provide high-quality collected and value-added inforamtion. The Web wrapping problem has been addressed by a significant amount of research work. Previous work can be classified into two categories, depending on whether the HTML input is regarded as a sequential character string (e.g., [34, 27, 24, 30, 23]) or a pre-parsed document tree (for instance, [35, 25, 22, 29, 3, 2, 26]). The latter category of work thus assumes that systems may make use of an existing HTML parser as a front and."
            },
            "slug": "Logic-based-web-information-extraction-Gottlob-Koch",
            "title": {
                "fragments": [],
                "text": "Logic-based web information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The Web wrapping problem has been addressed by a significant amount of research work and it is suggested that systems may make use of an existing HTML parser as a front and."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874808"
                        ],
                        "name": "Tim Furche",
                        "slug": "Tim-Furche",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Furche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Furche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24604135"
                        ],
                        "name": "G. Grasso",
                        "slug": "G.-Grasso",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Grasso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grasso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3347054"
                        ],
                        "name": "Omer Gunes",
                        "slug": "Omer-Gunes",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Gunes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Gunes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146371097"
                        ],
                        "name": "Xiaonan Guo",
                        "slug": "Xiaonan-Guo",
                        "structuredName": {
                            "firstName": "Xiaonan",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaonan Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48322444"
                        ],
                        "name": "A. Kravchenko",
                        "slug": "A.-Kravchenko",
                        "structuredName": {
                            "firstName": "Andrey",
                            "lastName": "Kravchenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kravchenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35203250"
                        ],
                        "name": "G. Orsi",
                        "slug": "G.-Orsi",
                        "structuredName": {
                            "firstName": "Giorgio",
                            "lastName": "Orsi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Orsi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766300"
                        ],
                        "name": "C. Schallhart",
                        "slug": "C.-Schallhart",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Schallhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schallhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34565417"
                        ],
                        "name": "A. Sellers",
                        "slug": "A.-Sellers",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Sellers",
                            "middleNames": [
                                "Jon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sellers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119128164"
                        ],
                        "name": "Cheng Wang",
                        "slug": "Cheng-Wang",
                        "structuredName": {
                            "firstName": "Cheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10082324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce83f2034c9689254139a2fd8f8c4154c9041e81",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Search engines are the sinews of the web. These sinews have become strained, however: Where the web's function once was a mix of library and yellow pages, it has become the central marketplace for information of almost any kind. We search more and more for objects with specific characteristics, a car with a certain mileage, an affordable apartment close to a good school, or the latest accessory for our phones. Search engines all too often fail to provide reasonable answers, making us sift through dozens of websites with thousands of offers--never to be sure a better offer isn't just around the corner. What search engines are missing is understanding of the objects and their attributes published on websites. Automatically identifying and extracting these objects is akin to alchemy: transforming unstructured web information into highly structured data with near perfect accuracy. With DIADEM we present a formula for this transformation, but at a price: DIADEM identifies and extracts data from a website with high accuracy. The price is that for this task we need to provide DIADEM with extensive knowledge about the ontology and phenomenology of the domain, i.e., about entities (and relations) and about the representation of these entities in the textual, structural, and visual language of a website of this domain. In this demonstration, we demonstrate with a first prototype of DIADEM that, in contrast to alchemists, DIADEM has developed a viable formula."
            },
            "slug": "DIADEM:-domain-centric,-intelligent,-automated-data-Furche-Gottlob",
            "title": {
                "fragments": [],
                "text": "DIADEM: domain-centric, intelligent, automated data extraction methodology"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "In this demonstration, it is demonstrated with a first prototype of DIADEM that, in contrast to alchemists, DIADem has developed a viable formula for transforming unstructured web information into highly structured data with near perfect accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "WWW"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2000882"
                        ],
                        "name": "Wolfgang Gatterbauer",
                        "slug": "Wolfgang-Gatterbauer",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Gatterbauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Gatterbauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308385"
                        ],
                        "name": "Paul Bohunsky",
                        "slug": "Paul-Bohunsky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Bohunsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Bohunsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153481005"
                        ],
                        "name": "M. Herzog",
                        "slug": "M.-Herzog",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Herzog",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Herzog"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738409"
                        ],
                        "name": "Bernhard Kr\u00fcpl",
                        "slug": "Bernhard-Kr\u00fcpl",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Kr\u00fcpl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Kr\u00fcpl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36806626"
                        ],
                        "name": "B. Pollak",
                        "slug": "B.-Pollak",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Pollak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pollak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Relying on a set of primitives to compare with the structure of the given page, these tools can find one or more objects in the page matching the primitive items [Embley et al. 1999; Gatterbauer et al. 2007]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17356112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95c9961c73db64837fd6b8dbda2f0b246fed6812",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, information extraction from web tables has focused on small, more or less homogeneous corpora, often based on assumptions about the use of <table> tags. A multitude of different HTML implementations of web tables make these approaches difficult to scale. In this paper, we approach the problem of domain-independent information extraction from web tables by shifting our attention from the tree-based representation of webpages to a variation of the two-dimensional visual box model used by web browsers to display the information on the screen. The there by obtained topological and style information allows us to fill the gap created by missing domain-specific knowledge about content and table templates. We believe that, in a future step, this approach can become the basis for a new way of large-scale knowledge acquisition from the current \"Visual Web."
            },
            "slug": "Towards-domain-independent-information-extraction-Gatterbauer-Bohunsky",
            "title": {
                "fragments": [],
                "text": "Towards domain-independent information extraction from web tables"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shifts attention from the tree-based representation of webpages to a variation of the two-dimensional visual box model used by web browsers to display the information on the screen and believes that this approach can become the basis for a new way of large-scale knowledge acquisition from the current \"Visual Web\"."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145468294"
                        ],
                        "name": "Wolfgang May",
                        "slug": "Wolfgang-May",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "May",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang May"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2508720"
                        ],
                        "name": "Rainer Himmer\u00f6der",
                        "slug": "Rainer-Himmer\u00f6der",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Himmer\u00f6der",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rainer Himmer\u00f6der"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809410"
                        ],
                        "name": "G. Lausen",
                        "slug": "G.-Lausen",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Lausen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716911"
                        ],
                        "name": "Bertram Lud\u00e4scher",
                        "slug": "Bertram-Lud\u00e4scher",
                        "structuredName": {
                            "firstName": "Bertram",
                            "lastName": "Lud\u00e4scher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bertram Lud\u00e4scher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34619970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c34b2c06fe17c6a2b316d2153fa03d297fd5cb1e",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of information extraction from the Web is to provide an integrated view on heterogeneous information sources via a common data model and query language. A main problem with current approaches is that they rely on very different formalisms sind tools for wrappers and mediators, thus leading to an \"impedance mismatch\" between the wrapper and mediator level. In contrast, our approach integrates wrapping and mediation in a unified framework based on an object-oriented data model which represents both the Web structure and the data of the application domain. Wrappers and mediators are written in a rule-based object-oriented language which is augmented with features for Web access and structured document analysis, i.e., pattern matching by regular expressions and SGML parsing. In this paper, we develop generic, reusable rule patterns for typical extraction, integration, and restructuring tasks using this framework. We show the practicability of our approach by using the FLORID system [10]."
            },
            "slug": "A-Unified-Framework-for-Wrapping,-Mediating-and-the-May-Himmer\u00f6der",
            "title": {
                "fragments": [],
                "text": "A Unified Framework for Wrapping, Mediating and Restructuring Information from the Web"
            },
            "venue": {
                "fragments": [],
                "text": "ER"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738409"
                        ],
                        "name": "Bernhard Kr\u00fcpl",
                        "slug": "Bernhard-Kr\u00fcpl",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Kr\u00fcpl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Kr\u00fcpl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2269127"
                        ],
                        "name": "R. R. Fayzrakhmanov",
                        "slug": "R.-R.-Fayzrakhmanov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Fayzrakhmanov",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. R. Fayzrakhmanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48047481"
                        ],
                        "name": "Wolfgang Holzinger",
                        "slug": "Wolfgang-Holzinger",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Holzinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Holzinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075579935"
                        ],
                        "name": "Mathias Panzenb\u00f6ck",
                        "slug": "Mathias-Panzenb\u00f6ck",
                        "structuredName": {
                            "firstName": "Mathias",
                            "lastName": "Panzenb\u00f6ck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathias Panzenb\u00f6ck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "frameworks, using methods from document understanding and spatial reasoning such as the approaches of the TamCrow project [75], of the ABBA project [37] spatial XPath extensions [95] and rendition-based extensions in RoadRunner to detect labels [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8591166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cf9a29f3d0ee46804b678e3858be783a1cb25fc",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "On today's Web, designers take huge efforts to create visually rich websites that boast a magnitude of interactive elements. Contrarily, most web information extraction (WIE) algorithms are still based on attributed tree methods which struggle to deal with this complexity. In this paper, we introduce a versatile model to represent web documents. The model is based on gestalt theory principles---trying to capture the most important aspects in a formally exact way. It (i) represents and unifies access to visual layout, content and functional aspects; (ii) is implemented with semantic web techniques that can be leveraged for i.e. automatic reasoning. Considering the visual appearance of a web page, we view it as a collection of gestalt figures---based on gestalt primitives---each representing a specific design pattern, be it navigation menus or news articles. Based on this model, we introduce our WIE methodology, a re-engineering process involving design patterns, statistical distributions and text content properties. The complete framework consists of the UOM model, which formalizes the mentioned components, and the MANM layer that hints on structure and serialization, providing document re-packaging foundations. Finally, we discuss how we have applied and evaluated our model in the area of web accessibility."
            },
            "slug": "A-versatile-model-for-web-page-representation,-and-Kr\u00fcpl-Fayzrakhmanov",
            "title": {
                "fragments": [],
                "text": "A versatile model for web page representation, information extraction and content re-packaging"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A versatile model to represent web documents based on gestalt theory principles, which represents and unifies access to visual layout, content and functional aspects, and is implemented with semantic web techniques that can be leveraged for i.e. automatic reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "DocEng '11"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655430"
                        ],
                        "name": "Bing Liu",
                        "slug": "Bing-Liu",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "The reasoning above is formally encoded in the definition of mapping, presented by [82]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59258568,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "802dcd4c80fe905a7a3823805e87ed519aca9f7a",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Web information extraction is the problem of extracting target information items from Web pages. There are two general problems: extracting information from natural language text and extracting structured data from Web pages. This chapter focuses on extracting structured data. A program for extracting such data is usually called a wrapper. Extracting information from text is studied mainly in the natural language processing community."
            },
            "slug": "Structured-Data-Extraction:-Wrapper-Generation-Liu",
            "title": {
                "fragments": [],
                "text": "Structured Data Extraction: Wrapper Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This chapter focuses on extracting structured data, which is the problem of extracting target information items from Web pages by using a program for extracting such data, usually called a wrapper."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685478"
                        ],
                        "name": "J. Turmo",
                        "slug": "J.-Turmo",
                        "structuredName": {
                            "firstName": "Jordi",
                            "lastName": "Turmo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Turmo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2734693"
                        ],
                        "name": "A. Ageno",
                        "slug": "A.-Ageno",
                        "structuredName": {
                            "firstName": "Alicia",
                            "lastName": "Ageno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ageno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016839"
                        ],
                        "name": "Neus Catal\u00e0",
                        "slug": "Neus-Catal\u00e0",
                        "structuredName": {
                            "firstName": "Neus",
                            "lastName": "Catal\u00e0",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neus Catal\u00e0"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 112
                            }
                        ],
                        "text": "Statistical Machine Learning systems were also developed, relying on conditional models [98] or adaptive search [114] as an alternative solution to human knowledge and interaction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11068217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef76646912d675c987af5c0fe6bd06f40886ff0d",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 193,
            "paperAbstract": {
                "fragments": [],
                "text": "The growing availability of online textual sources and the potential number of applications of knowledge acquisition from textual data has lead to an increase in Information Extraction (IE) research. Some examples of these applications are the generation of data bases from documents, as well as the acquisition of knowledge useful for emerging technologies like question answering, information integration, and others related to text mining. However, one of the main drawbacks of the application of IE refers to its intrinsic domain dependence. For the sake of reducing the high cost of manually adapting IE applications to new domains, experiments with different Machine Learning (ML) techniques have been carried out by the research community. This survey describes and compares the main approaches to IE and the different ML techniques used to achieve Adaptive IE technology."
            },
            "slug": "Adaptive-information-extraction-Turmo-Ageno",
            "title": {
                "fragments": [],
                "text": "Adaptive information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This survey describes and compares the main approaches to IE and the different ML techniques used to achieve Adaptive IE technology."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40103167"
                        ],
                        "name": "Adam Mathes",
                        "slug": "Adam-Mathes",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Mathes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Mathes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18176830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa11849871fe719f02c75051c2ff0f9bd3134de1",
            "isKey": false,
            "numCitedBy": 976,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines user-generated metadata as implemented and applied in two web services designed to share and organize digital media to better understand grassroots classification. Metadata data about data allows systems to collocate related information, and helps users find relevant information. The creation of metadata has generally been approached in two ways: professional creation and author creation. In libraries and other organizations, creating metadata, primarily in the form of catalog records, has traditionally been the domain of dedicated professionals working with complex, detailed rule sets and vocabularies. The primary problem with this approach is scalability and its impracticality for the vast amounts of content being produced and used, especially on the World Wide Web. The apparatus and tools built around professional cataloging systems are generally too complicated for anyone without specialized training and knowledge. A second approach is for metadata to be created by authors. The movement towards creator described documents was heralded by SGML, the WWW, and the Dublin Core Metadata Initiative. There are problems with this approach as well often due to inadequate or inaccurate description, or outright deception. This paper examines a third approach: user-created metadata, where users of the documents and media create metadata for their own individual use that is also shared throughout a community. 1 The Creation of Metadata: Professionals, Content Creators, Users Metadata is often characterized as \u201cdata about data.\u201d Metadata is information, often highly structured, about documents, books, articles, photographs, or other items that is designed to support specific functions. These functions are usually"
            },
            "slug": "Folksonomies-Cooperative-Classification-and-Through-Mathes",
            "title": {
                "fragments": [],
                "text": "Folksonomies-Cooperative Classification and Communication Through Shared Metadata"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper examines user-generated metadata as implemented and applied in two web services designed to share and organize digital media to better understand grassroots classification."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751591"
                        ],
                        "name": "G. Weikum",
                        "slug": "G.-Weikum",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Weikum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Weikum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 315,
                                "start": 301
                            }
                        ],
                        "text": "We cover in particular enterprise, social and scientific applications by discussing which fields have already been approached (e.g., advertising engineering, enterprise solutions, Business and Competitive intelligence, etc.) and which are potentially going to be in the future (e.g., Bio-informatics, Web Harvesting, etc.)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 92
                            }
                        ],
                        "text": "One of the most attractive future applications of the Web Data Extraction is Web Harvesting [118]: Gatterbauer [50] defines it as \u201cthe process of gathering and integrating data from various heterogeneous Web sources\u201d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7560156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8cd6981cf944aa0a5f7207ab55b6d29dc663eccd",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "There are major trends to advance the functionality of search engines to a more expressive semantic level (e.g., [2, 4, 6, 7, 8, 9, 13, 14, 18]). This is enabled by employing large-scale information extraction [1, 11, 20] of entities and relationships from semistructured as well as natural-language Web sources. In addition, harnessing Semantic-Web-style ontologies [22] and reaching into Deep-Web sources [16] can contribute towards a grand vision of turning the Web into a comprehensive knowledge base that can be efficiently searched with high precision.\n This talk presents ongoing research towards this objective, with emphasis on our work on the YAGO knowledge base [23, 24] and the NAGA search engine [14] but also covering related projects. YAGO is a large collection of entities and relational facts that are harvested from Wikipedia and WordNet with high accuracy and reconciled into a consistent RDF-style \"semantic\" graph. For further growing YAGO from Web sources while retaining its high quality, pattern-based extraction is combined with logic-based consistency checking in a unified framework [25]. NAGA provides graph-template-based search over this data, with powerful ranking capabilities based on a statistical language model for graphs. Advanced queries and the need for ranking approximate matches pose efficiency and scalability challenges that are addressed by algorithmic and indexing techniques [15, 17].\n YAGO is publicly available and has been imported into various other knowledge-management projects including DB-pedia. YAGO shares many of its goals and methodologies with parallel projects along related lines. These include Avatar [19], Cimple/DBlife [10, 21], DBpedia [3], Know-ItAll/TextRunner [12, 5], Kylin/KOG [26, 27], and the Libra technology [18, 28] (and more). Together they form an exciting trend towards providing comprehensive knowledge bases with semantic search capabilities."
            },
            "slug": "Harvesting,-searching,-and-ranking-knowledge-on-the-Weikum",
            "title": {
                "fragments": [],
                "text": "Harvesting, searching, and ranking knowledge on the web: invited talk"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This talk presents ongoing research towards turning the Web into a comprehensive knowledge base that can be efficiently searched with high precision, with emphasis on work on the YAGO knowledge base and the NAGA search engine."
            },
            "venue": {
                "fragments": [],
                "text": "WSDM '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47666658"
                        ],
                        "name": "Hsinchun Chen",
                        "slug": "Hsinchun-Chen",
                        "structuredName": {
                            "firstName": "Hsinchun",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsinchun Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145884195"
                        ],
                        "name": "M. Chau",
                        "slug": "M.-Chau",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Chau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Chau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153325404"
                        ],
                        "name": "D. Zeng",
                        "slug": "D.-Zeng",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Zeng",
                            "middleNames": [
                                "Dajun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zeng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "\u2026Information Retrieval\n\u2217Corresponding author Email addresses: ferrarae@indiana.edu (Emilio Ferrara), pdemeo@unime.it (Pasquale De Meo), gfiumara@unime.it\n(Giacomo Fiumara), robert.baumgartner@lixto.com (Robert Baumgartner)\nPreprint submitted to Knowledge-based systems June 11, 2014\nar X\niv :1\n20 7."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206038432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "855a6b98e834dc7fb7f8bd6b6dc48a1b90c3609e",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "CI-Spider:-a-tool-for-competitive-intelligence-on-Chen-Chau",
            "title": {
                "fragments": [],
                "text": "CI Spider: a tool for competitive intelligence on the Web"
            },
            "venue": {
                "fragments": [],
                "text": "Decis. Support Syst."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "STALKER is also able to define wildcards, classes of generic tokens that are inclusive of more specific tokens."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "The last learning-based system discussed in this part is called STALKER [93]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 74
                            }
                        ],
                        "text": "The root of the EC tree is populated the sequence of all tokens (whereas, STALKER considers as token any piece of text or HTML tag in the document)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 174
                            }
                        ],
                        "text": "The extraction of elements of interest for the user is achieved by inferring a set of extraction rules on the EC tree itself \u2013 a typical example of extraction rule inferred by STALKER is the construct SkipTo(T), a directive that indicates, during the extraction phase, to skip all tokens until the first occurrence of the token T is found."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 88
                            }
                        ],
                        "text": "The main difference between these two systems is the specification of relevant data: in STALKER, a set of tokens is manually positioned on the Web page, so that to identify information that the user intend to extract."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 112
                            }
                        ],
                        "text": "In the following we shortly describe some Web Data Extraction approaches relying on Machine Learning algorithms [78,16,92,109,46,60,93]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 38
                            }
                        ],
                        "text": "This aspect ensures the capability of STALKER of handling with empty values, hierarchical structures and non ordered items."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3514097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cc263c84b85027164bd39db169f5d5959ef6822",
            "isKey": true,
            "numCitedBy": 464,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks. We introduce an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that STALKER does significantly better then other approaches; on one hand, STALKER requires up to two orders of magnitude fewer examples than other algorithms, while on the other hand it can handle information sources that could not be wrapped by existing techniques."
            },
            "slug": "A-hierarchical-approach-to-wrapper-induction-Muslea-Minton",
            "title": {
                "fragments": [],
                "text": "A hierarchical approach to wrapper induction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples that can handle information sources that could not be wrapped by existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "AGENTS '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874808"
                        ],
                        "name": "Tim Furche",
                        "slug": "Tim-Furche",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Furche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Furche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24604135"
                        ],
                        "name": "G. Grasso",
                        "slug": "G.-Grasso",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Grasso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grasso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766300"
                        ],
                        "name": "C. Schallhart",
                        "slug": "C.-Schallhart",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Schallhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schallhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34565417"
                        ],
                        "name": "A. Sellers",
                        "slug": "A.-Sellers",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Sellers",
                            "middleNames": [
                                "Jon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sellers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15445353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67ea8a15e0dbaae897611f6694e0d1835717ef38",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The evolution of the web has outpaced itself: The growing wealth of information and the increasing sophistication of interfaces necessitate automated processing. Web automation and extraction technologies have been overwhelmed by this very growth. To address this trend, we identify four key requirements of web extraction: (1) Interact with sophisticated web application interfaces, (2) Precisely capture the relevant data for most web extraction tasks, (3) Scale with the number of visited pages, and (4) Readily embed into existing web technologies. We introduce OXPath, an extension of XPath for interacting with web applications and for extracting information thus revealed. It addresses all the above requirements. OXPath\u2019s page-at-a-time evaluation guarantees memory use independent of the number of visited pages, yet remains polynomial in time. We validate experimentally the theoretical complexity and demonstrate that its evaluation is dominated by the page rendering of the underlying browser. Our experiments show that OXPath outperforms existing commercial and academic data extraction tools by a wide margin. OXPath is available under an open source license."
            },
            "slug": "OXPath:-A-Language-for-Scalable,-Memory-efficient-Furche-Gottlob",
            "title": {
                "fragments": [],
                "text": "OXPath: A Language for Scalable, Memory-efficient Data Extraction from Web Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces OXPath, an extension of XPath for interacting with web applications and for extracting information thus revealed, and demonstrates that OX path outperforms existing commercial and academic data extraction tools by a wide margin."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145161991"
                        ],
                        "name": "Xiaohua Hu",
                        "slug": "Xiaohua-Hu",
                        "structuredName": {
                            "firstName": "Xiaohua",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohua Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772322"
                        ],
                        "name": "T. Lin",
                        "slug": "T.-Lin",
                        "structuredName": {
                            "firstName": "Tsau",
                            "lastName": "Lin",
                            "middleNames": [
                                "Young"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144436171"
                        ],
                        "name": "I. Song",
                        "slug": "I.-Song",
                        "structuredName": {
                            "firstName": "Il-Yeol",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145724540"
                        ],
                        "name": "Xia Lin",
                        "slug": "Xia-Lin",
                        "structuredName": {
                            "firstName": "Xia",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xia Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728526"
                        ],
                        "name": "Illhoi Yoo",
                        "slug": "Illhoi-Yoo",
                        "structuredName": {
                            "firstName": "Illhoi",
                            "lastName": "Yoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illhoi Yoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144512721"
                        ],
                        "name": "Mark Lechner",
                        "slug": "Mark-Lechner",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Lechner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Lechner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144982626"
                        ],
                        "name": "Min Song",
                        "slug": "Min-Song",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Song"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 61
                            }
                        ],
                        "text": "social networks and communities [Mika 2007], bio-informatics [Hu et al. 2004], etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18070841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b8ef03b38c2bb52053709b7a1c2a9f554590bf5",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Automated discovery and extraction of biological knowledge from biomedical web documents has become essential because of the enormous amount of biomedical literature published each year. In this paper we present an ontology-based scalable and portable information extraction system to automatically extract biological knowledge from huge collection of online biomedical web documents. Our method integrates ontology-based semantic tagging, information extraction and data mining together, automatically learns the patterns based on a few user seed tuples, and then extract new tuples from the biomedical web documents based on the discovered patterns. A novel system SPIE (Scalable and Portable Information Extraction) is implemented and tested on the PuBMed to find the chromatin protein-protein interaction and the experimental results indicate our approach is very effective in extracting biological knowledge from huge collection of biomedical web documents."
            },
            "slug": "Ontology-Based-Scalable-and-Portable-Information-to-Hu-Lin",
            "title": {
                "fragments": [],
                "text": "Ontology-Based Scalable and Portable Information Extraction System to Extract Biological Knowledge from Huge Collection of Biomedical Web Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The method integrates ontology-based semantic tagging, information extraction and data mining together, automatically learns the patterns based on a few user seed tuples, and then extracts new tuples from the biomedical web documents based on the discovered patterns."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE/WIC/ACM International Conference on Web Intelligence (WI'04)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730314"
                        ],
                        "name": "S. Flesca",
                        "slug": "S.-Flesca",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Flesca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Flesca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718485"
                        ],
                        "name": "G. Manco",
                        "slug": "G.-Manco",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Manco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Manco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700482"
                        ],
                        "name": "E. Masciari",
                        "slug": "E.-Masciari",
                        "structuredName": {
                            "firstName": "Elio",
                            "lastName": "Masciari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Masciari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1878928"
                        ],
                        "name": "Eugenio Rende",
                        "slug": "Eugenio-Rende",
                        "structuredName": {
                            "firstName": "Eugenio",
                            "lastName": "Rende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugenio Rende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688359"
                        ],
                        "name": "A. Tagarelli",
                        "slug": "A.-Tagarelli",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Tagarelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tagarelli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[45] and Kaiser and Miksch [64] surveyed approaches, techniques and tools."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44791375,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48caaf9bfb0544b371a92ad36b1e8dca6153732f",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Nowadays several companies use the information available on the Web for a number of purposes. However, since most of this information is only available as HTML documents, several techniques that allow information from the Web to be automatically extracted have recently been defined. In this paper we review the main techniques and tools for extracting information available on the Web, devising a taxonomy of existing systems. In particular we emphasize the advantages and drawbacks of the techniques analyzed from a user point of view."
            },
            "slug": "Web-wrapper-induction:-a-brief-survey-Flesca-Manco",
            "title": {
                "fragments": [],
                "text": "Web wrapper induction: a brief survey"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper reviews the main techniques and tools for extracting information available on the Web, devising a taxonomy of existing systems and emphasizes the advantages and drawbacks of the techniques analyzed from a user point of view."
            },
            "venue": {
                "fragments": [],
                "text": "AI Commun."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777572"
                        ],
                        "name": "G. Fiumara",
                        "slug": "G.-Fiumara",
                        "structuredName": {
                            "firstName": "Giacomo",
                            "lastName": "Fiumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fiumara"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 8
                            }
                        ],
                        "text": "Fiumara [Fiumara 2007] applied these criteria to classify four new tools that are also presented here."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16490072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfbc2033e9ff349c391c1f9d888dc0e1eb5e3521",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The Web contains an enormous quantity of information which is usually formatted for human users. This makes it difficult to extract relevant content from various sources. In the last few years some authors have addressed the problem to convert Web documents from unstructured or semi-structured format into structured and therefore machine-understandable format such as, for example, XML. In this paper we briefly survey some of the most promising and recently developed extraction tools."
            },
            "slug": "Automated-Information-Extraction-from-Web-Sources-:-Fiumara",
            "title": {
                "fragments": [],
                "text": "Automated Information Extraction from Web Sources : a Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Some of the most promising and recently developed extraction tools for Web documents from unstructured or semi-structured format into structured and therefore machine-understandable format such as, for example, XML are surveyed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119254956"
                        ],
                        "name": "Yeonjung Kim",
                        "slug": "Yeonjung-Kim",
                        "structuredName": {
                            "firstName": "Yeonjung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeonjung Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2172647"
                        ],
                        "name": "Jeahyun Park",
                        "slug": "Jeahyun-Park",
                        "structuredName": {
                            "firstName": "Jeahyun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeahyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760472"
                        ],
                        "name": "Taehwan Kim",
                        "slug": "Taehwan-Kim",
                        "structuredName": {
                            "firstName": "Taehwan",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taehwan Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701401"
                        ],
                        "name": "Joongmin Choi",
                        "slug": "Joongmin-Choi",
                        "structuredName": {
                            "firstName": "Joongmin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joongmin Choi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 50
                            }
                        ],
                        "text": "In fact, it has been adopted in several scenarios [66, 126, 127, 130, 39, 40, 41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13787687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1644fc9d0e7005bd740552d4e70b733ea613ca0b",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The main issue for effective Web information extraction is how to recognize similar patterns in a Web page. Traditionally, it has been shown that pattern matching by using the HTML DOM tree is more efficient than the simple string matching approach. Nonetheless, previous tree-based pattern matching methods have problems by assuming that all HTML tags have the same values, assigning the same weight to each node in HTML trees. This paper proposes an enhanced tree matching algorithm that improves the tree edit distance method by considering the characteristics of HTML features. We assign different values to different HTML tree nodes according to their weights for displaying the corresponding data objects in the browser. Pattern matching of HTML patterns is done by obtaining the maximum mapping values of two HTML trees that are constructed with weighted node values from HTML data objects. Experiments are done over several Web commerce sites to evaluate the effectiveness of the proposed HTML tree matching algorithm."
            },
            "slug": "Web-Information-Extraction-by-HTML-Tree-Edit-Kim-Park",
            "title": {
                "fragments": [],
                "text": "Web Information Extraction by HTML Tree Edit Distance Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An enhanced tree matching algorithm that improves the tree edit distance method by considering the characteristics of HTML features and obtained the maximum mapping values of two HTML trees that are constructed with weighted node values from HTML data objects."
            },
            "venue": {
                "fragments": [],
                "text": "2007 International Conference on Convergence Information Technology (ICCIT 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34607455"
                        ],
                        "name": "Chun-Nan Hsu",
                        "slug": "Chun-Nan-Hsu",
                        "structuredName": {
                            "firstName": "Chun-Nan",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Nan Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094164369"
                        ],
                        "name": "Ming-Tzung Dung",
                        "slug": "Ming-Tzung-Dung",
                        "structuredName": {
                            "firstName": "Ming-Tzung",
                            "lastName": "Dung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Tzung Dung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 112
                            }
                        ],
                        "text": "In the following we shortly describe some Web Data Extraction approaches relying on Machine Learning algorithms [78,16,92,109,46,60,93]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "SoftMealy [60] was the first wrapper induction system specifically designed to work in the Web Data Extraction context."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17895561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9478265afd280486299a5b8f1dbaaf6769422de",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generating-Finite-State-Transducers-for-Data-from-Hsu-Dung",
            "title": {
                "fragments": [],
                "text": "Generating Finite-State Transducers for Semi-Structured Data Extraction from the Web"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46458150"
                        ],
                        "name": "Ling Liu",
                        "slug": "Ling-Liu",
                        "structuredName": {
                            "firstName": "Ling",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ling Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145932397"
                        ],
                        "name": "C. Pu",
                        "slug": "C.-Pu",
                        "structuredName": {
                            "firstName": "Calton",
                            "lastName": "Pu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114924634"
                        ],
                        "name": "Wei Han",
                        "slug": "Wei-Han",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Han"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 520140,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7430414e85f9b85095a716b68bea98b30ebaab30",
            "isKey": false,
            "numCitedBy": 533,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper describes the methodology and the software development of XWRAP, an XML-enabled wrapper construction system for semi-automatic generation of wrapper programs. By XML-enabled we mean that the metadata about information content that are implicit in the original Web pages will be extracted and encoded explicitly as XML tags in the wrapped documents. In addition, the query based content filtering process is performed against the XML documents. The XWRAP wrapper generation framework has three distinct features. First, it explicitly separates tasks of building wrappers that are specific to a Web source from the tasks that are repetitive for any source, and uses a component library to provide basic building blocks for wrapper programs. Second, it provides a user friendly interface program to allow wrapper developers to generate their wrapper code with a few mouse clicks. Third and most importantly, we introduce and develop a two-phase code generation framework. The first phase utilizes an interactive interface facility to encode the source-specific metadata knowledge identified by individual wrapper developers as declarative information extraction rules. The second phase combines the information extraction rules generated at the first phase with the XWRAP component library to construct an executable wrapper program for the given Web source. We report the initial experiments on performance of the XWRAP code generation system and the wrapper programs generated by XWRAP."
            },
            "slug": "XWRAP:-an-XML-enabled-wrapper-construction-system-Liu-Pu",
            "title": {
                "fragments": [],
                "text": "XWRAP: an XML-enabled wrapper construction system for Web information sources"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The paper describes the methodology and the software development of XWRAP, an XML-enabled wrapper construction system for semi-automatic generation of wrapper programs, and introduces and develops a two-phase code generation framework."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662719"
                        ],
                        "name": "Hongkun Zhao",
                        "slug": "Hongkun-Zhao",
                        "structuredName": {
                            "firstName": "Hongkun",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongkun Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38699354"
                        ],
                        "name": "W. Meng",
                        "slug": "W.-Meng",
                        "structuredName": {
                            "firstName": "Weiyi",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642131"
                        ],
                        "name": "Clement T. Yu",
                        "slug": "Clement-T.-Yu",
                        "structuredName": {
                            "firstName": "Clement",
                            "lastName": "Yu",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clement T. Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7856427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7370232482ce739b738d94e1e08a4a744a26fa38",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Metasearch engine, Comparison-shopping and Deep Web crawling applications need to extract search result records enwrapped in result pages returned from search engines in response to user queries. The search result records from a given search engine are usually formatted based on a template. Precisely identifying this template can greatly help extract and annotate the data units within each record correctly. In this paper, we propose a graph model to represent record template and develop a domain independent statistical method to automatically mine the record template for any search engine using sample search result records. Our approach can identify both template tags (HTML tags) and template texts (non-tag texts), and it also explicitly addresses the mismatches between the tag structures and the data structures of search result records. Our experimental results indicate that this approach is very effective."
            },
            "slug": "Mining-templates-from-search-result-records-of-Zhao-Meng",
            "title": {
                "fragments": [],
                "text": "Mining templates from search result records of search engines"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A graph model is proposed to represent record template and a domain independent statistical method to automatically mine the record template for any search engine using sample search result records is developed."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097869788"
                        ],
                        "name": "Stefan Kuhlins",
                        "slug": "Stefan-Kuhlins",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kuhlins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Kuhlins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3138904"
                        ],
                        "name": "Ross Tredwell",
                        "slug": "Ross-Tredwell",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Tredwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross Tredwell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Kuhlins and Tredwell [Kuhlins and Tredwell 2003] surveyed tools for generating wrappers already in 2003: information could not be up-to-date but analyzing the approach is still very interesting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33127570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8aa28627b2fd608e57253d454586d572a49d9db0",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Various web applications in e-business, such as online price comparisons, competition monitoring and personalised newsletters require retrieval of distributed information from the Internet. This paper examines the suitability of software toolkits for the extraction of data from web sites. The term wrapper is defined and an overview of presently available toolkits for generating wrappers is provided. In order to give a better insight into the workings of such toolkits, a detailed analysis of the non-commercial software program LAPIS is presented. An example application using this toolkit demonstrates how acceptable results can be achieved with relative ease. The functionality of the program is compared with the functionality of the commercial toolkit RoboMaker and the differences are highlighted. With the aim of providing improved ease-of-use and faster wrapper generation in mind, possible areas for further development of toolkits for automated web data extraction are discussed."
            },
            "slug": "Toolkits-for-Generating-Wrappers-Kuhlins-Tredwell",
            "title": {
                "fragments": [],
                "text": "Toolkits for Generating Wrappers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The suitability of software toolkits for the extraction of data from web sites, including RoboMaker, is examined with the aim of providing improved ease-of-use and faster wrapper generation in mind."
            },
            "venue": {
                "fragments": [],
                "text": "NetObjectDays"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923209"
                        ],
                        "name": "Nilesh N. Dalvi",
                        "slug": "Nilesh-N.-Dalvi",
                        "structuredName": {
                            "firstName": "Nilesh",
                            "lastName": "Dalvi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nilesh N. Dalvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730124"
                        ],
                        "name": "P. Bohannon",
                        "slug": "P.-Bohannon",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Bohannon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bohannon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 30
                            }
                        ],
                        "text": "To this purpose, some authors [30,31] introduced the concept of wrapper robustness: they proposed a strategy to find, among all the XPath expressions capable of extracting the same information from a Web page, the one that is less influenced by potential changes in the structure of the page and such an expression identifies the more robust wrapper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10711472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b47e5640376b1a3858e1f8119b8588d1e7517f6c",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "On script-generated web sites, many documents share common HTML tree structure, allowing wrappers to effectively extract information of interest. Of course, the scripts and thus the tree structure evolve over time, causing wrappers to break repeatedly, and resulting in a high cost of maintaining wrappers. In this paper, we explore a novel approach: we use temporal snapshots of web pages to develop a tree-edit model of HTML, and use this model to improve wrapper construction. We view the changes to the tree structure as suppositions of a series of edit operations: deleting nodes, inserting nodes and substituting labels of nodes. The tree structures evolve by choosing these edit operations stochastically. Our model is attractive in that the probability that a source tree has evolved into a target tree can be estimated efficiently--in quadratic time in the size of the trees--making it a potentially useful tool for a variety of tree-evolution problems. We give an algorithm to learn the probabilistic model from training examples consisting of pairs of trees, and apply this algorithm to collections of web-page snapshots to derive HTML-specific tree edit models. Finally, we describe a novel wrapper-construction framework that takes the tree-edit model into account, and compare the quality of resulting wrappers to that of traditional wrappers on synthetic and real HTML document examples."
            },
            "slug": "Robust-web-extraction:-an-approach-based-on-a-model-Dalvi-Bohannon",
            "title": {
                "fragments": [],
                "text": "Robust web extraction: an approach based on a probabilistic tree-edit model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper uses temporal snapshots of web pages to develop a tree-edit model of HTML, and uses this model to improve wrapper construction, and gives an algorithm to learn the probabilistic model from training examples consisting of pairs of trees."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD Conference"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48898287"
                        ],
                        "name": "Emilio Ferrara",
                        "slug": "Emilio-Ferrara",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ferrara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Ferrara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The document tree (henceforth also referred as DOM tree) has been successfully exploited for Web Data Extraction purposes in a number of techniques discussed in the following."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To this extent, some techniques, called tree-matching strategies are a good candidate to detect similarities between two tree and they will be discussed in detail in the next sections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8725466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef318be3f8330bb2886e5c9d17b16d502976a992",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Information distributed through the Web keeps growing faster day by day, and for this reason, several techniques for extracting Web data have been suggested during last years. Often, extraction tasks are performed through so called wrappers, procedures extracting information from Web pages, e.g. implementing logic-based techniques. Many fields of application today require a strong degree of robustness of wrappers, in order not to compromise assets of information or reliability of data extracted."
            },
            "slug": "Automatic-Wrapper-Adaptation-by-Tree-Edit-Distance-Ferrara-Baumgartner",
            "title": {
                "fragments": [],
                "text": "Automatic Wrapper Adaptation by Tree Edit Distance Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper presents a meta-analyses of several techniques for extracting Web data through so called wrappers, procedures extracting information from Web pages, e.g. implementing logic-based techniques."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2000882"
                        ],
                        "name": "Wolfgang Gatterbauer",
                        "slug": "Wolfgang-Gatterbauer",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Gatterbauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Gatterbauer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "One of the most attractive future applications of the Web Data Extraction is Web Harvesting [118]: Gatterbauer [50] defines it as \u2018\u2018the process of gathering and integrating data from various heterogeneous Web sources\u2019\u2019."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14886581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "649a45f22e05c35025b1e206830d461da4efe836",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "DEFINITION Web harvesting describes the process of gathering and integrating data from various heterogeneous web sources. Necessary input is an appropriate knowledge representation of the domain of interest (e.g. an ontology), together with example instances of concepts or relationships (seed knowledge). Output is structured data (e.g. in the form of a relational database) that is gathered from the Web. The term harvesting implies that, while passing over a large body of available information, the process gathers only such information that lies in the domain of interest and is, as such, relevant."
            },
            "slug": "Web-Harvesting-Gatterbauer",
            "title": {
                "fragments": [],
                "text": "Web Harvesting"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The term harvesting implies that, while passing over a large body of available information, the process gathers only such information that lies in the domain of interest and is, as such, relevant."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Database Systems"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157325247"
                        ],
                        "name": "Fei Chen",
                        "slug": "Fei-Chen",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3030274"
                        ],
                        "name": "A. Doan",
                        "slug": "A.-Doan",
                        "structuredName": {
                            "firstName": "AnHai",
                            "lastName": "Doan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146158563"
                        ],
                        "name": "Jun Yang",
                        "slug": "Jun-Yang",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709145"
                        ],
                        "name": "R. Ramakrishnan",
                        "slug": "R.-Ramakrishnan",
                        "structuredName": {
                            "firstName": "Raghu",
                            "lastName": "Ramakrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ramakrishnan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "An attempt to address this challenge has been done in [21]: in that paper, the authors suggest an incremental solution which requires to identify portions of information shared by consecutive snapshots and to reuse the information extracted from a snapshot to the subsequent one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16341437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6a95c5a75b496c396b60c79be96c40ed2e230a1",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Most current information extraction (IE) approaches have considered only static text corpora, over which we typically have to apply IE only once. Many real-world text corpora however are dynamic. They evolve over time, and to keep extracted information up to date, we often must apply IE repeatedly, to consecutive corpus snapshots. We describe Cyclex, an approach that efficiently executes such repeated IE, by recycling previous IE efforts. Specifically, given a current corpus snapshot U, Cyclex identifies text portions of U that also appear in the previous corpus snapshot V. Since Cyclex has already executed IE over V, it can now recycle the IE results of these parts, by combining these results with the results of executing IE over the remaining parts of U, to produce the complete IE results for U. Realizing Cyclex raises many challenges, including modeling information extractors, exploring the trade-off between runtime and completeness in identifying overlapping text, and making informed, cost-based decisions between redoing IE from scratch and recycling previous IE results. We describe initial solutions to these challenges, and experiments over two real-world data sets that demonstrate the utility of our approach."
            },
            "slug": "Efficient-Information-Extraction-over-Evolving-Text-Chen-Doan",
            "title": {
                "fragments": [],
                "text": "Efficient Information Extraction over Evolving Text Data"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Realizing Cyclex raises many challenges, including modeling information extractors, exploring the trade-off between runtime and completeness in identifying overlapping text, and making informed, cost-based decisions between redoing IE from scratch and recycling previous IE results."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE 24th International Conference on Data Engineering"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874808"
                        ],
                        "name": "Tim Furche",
                        "slug": "Tim-Furche",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Furche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Furche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24604135"
                        ],
                        "name": "G. Grasso",
                        "slug": "G.-Grasso",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Grasso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grasso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766300"
                        ],
                        "name": "C. Schallhart",
                        "slug": "C.-Schallhart",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Schallhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schallhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34565417"
                        ],
                        "name": "A. Sellers",
                        "slug": "A.-Sellers",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Sellers",
                            "middleNames": [
                                "Jon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sellers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "OxPath [48], which is part of the DIADEM project [47] is a declarative formalism that extends XPath to support deep Web navigation and data extraction from interactive Web sites."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 219296139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ca191b4f8507c4ba1293521d3616dd2ebdc099b",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The evolution of the web has outpaced itself: The growing wealth of information and the increasing sophistication of interfaces necessitate automated processing. Web automation and extraction technologies have been overwhelmed by this very growth. To address this trend, we identify four key requirements of web extraction: (1) Interact with sophisticated web application interfaces, (2) Precisely capture the relevant data for most web extraction tasks, (3) Scale with the number of visited pages, and (4) Readily embed into existing web technologies. We introduce OXPath, an extension of XPath for interacting with web applications and for extracting information thus revealed. It addresses all the above requirements. OXPath's page-at-a-time evaluation guarantees memory use independent of the number of visited pages, yet remains polynomial in time. We validate experimentally the theoretical complexity and demonstrate that its evaluation is dominated by the page rendering of the underlying browser. Our experiments show that OXPath outperforms existing commercial and academic data extraction tools by a wide margin. OX-Path is available under an open source license."
            },
            "slug": "OXPath-Furche-Gottlob",
            "title": {
                "fragments": [],
                "text": "OXPath"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces OXPath, an extension of XPath for interacting with web applications and for extracting information thus revealed, and demonstrates that OX path outperforms existing commercial and academic data extraction tools by a wide margin."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 50
                            }
                        ],
                        "text": "The first example of hybrid system is provided by RoadRunner [28, 27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 66
                            }
                        ],
                        "text": "The first example of this class of systems is given by RoadRunner [28,27], a template-based system that automatically generates templates to extract data by matching features from different pages in the same domain."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 21
                            }
                        ],
                        "text": "The main strength of RoadRunner is that it is oriented to data-intensive websites based on templates or regular structures."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 289
                            }
                        ],
                        "text": "Hence, screen-scraping made back its way into novel Web Data extraction frameworks, using methods from document understanding and spatial reasoning such as the approaches of the TamCrow project [73], of the ABBA project [37] spatial XPath extensions [95] and rendition-based extensions in RoadRunner to detect labels [29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 20
                            }
                        ],
                        "text": "Another strength of RoadRunner is its high-quality open-source implementation (see: www.dia.uniroma3.it/db/roadRunner/), that provides a high degree of reliability of the extraction system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 61
                            }
                        ],
                        "text": "The first example of hybrid system is provided by RoadRunner [28,27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 13
                            }
                        ],
                        "text": "Essentially, RoadRunner can extract relevant information from any Web site containing at least two Web pages with a similar structure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 55
                            }
                        ],
                        "text": "The first example of this class of systems is given by RoadRunner [28, 27], a template-based system that automatically generates templates to extract data by matching features from different pages in the same domain."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 15
                            }
                        ],
                        "text": "In particular, RoadRunner can work using information provided by users, in the form of labeled example pages, or also by automatically labeling Web pages (such as WIEN), to build a training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 0
                            }
                        ],
                        "text": "RoadRunner relies on the idea of working with two HTML pages at the same time in order to discover patterns analyzing similarities and differences between structure and content of each pair of pages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 154
                            }
                        ],
                        "text": "Since usually Web pages are dynamically generated starting from template, and relevant data are positioned in the same (or in similar) areas of the page, RoadRunner is able to exploit this characteristic to identify relevant pieces of information, and, at the same time, taking into account small differences due to missing values or other mismatches."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15075203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dd1f9f7795b31493d98d9f260d37aad07550f6e",
            "isKey": true,
            "numCitedBy": 1157,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper investigates techniques for extracting data from HTML sites through the use of automatically generated wrappers. To automate the wrapper generation and the data extraction process, the paper develops a novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach."
            },
            "slug": "RoadRunner:-Towards-Automatic-Data-Extraction-from-Crescenzi-Mecca",
            "title": {
                "fragments": [],
                "text": "RoadRunner: Towards Automatic Data Extraction from Large Web Sites"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences is developed, which confirms the feasibility of the approach on real-life data-intensive Web sites."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747414"
                        ],
                        "name": "E. Rahm",
                        "slug": "E.-Rahm",
                        "structuredName": {
                            "firstName": "Erhard",
                            "lastName": "Rahm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rahm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737944"
                        ],
                        "name": "P. Bernstein",
                        "slug": "P.-Bernstein",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bernstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 120
                            }
                        ],
                        "text": "The most powerful Web Data Extraction systems provide tools to perform automatic schema matching from multiple wrappers [100], then packaging data into a desired format (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10500613,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "580221d63ae75bdc7d68829916cf608e44a56b27",
            "isKey": false,
            "numCitedBy": 3761,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component."
            },
            "slug": "A-survey-of-approaches-to-automatic-schema-matching-Rahm-Bernstein",
            "title": {
                "fragments": [],
                "text": "A survey of approaches to automatic schema matching"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A taxonomy is presented that distinguishes between schema-level and instance-level, element- level and structure- level, and language-based and constraint-based matchers and is intended to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component."
            },
            "venue": {
                "fragments": [],
                "text": "The VLDB Journal"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730314"
                        ],
                        "name": "S. Flesca",
                        "slug": "S.-Flesca",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Flesca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Flesca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 432493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35a38da0b7079bb61ef29bb27915ada2c4665e0a",
            "isKey": false,
            "numCitedBy": 590,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present new techniques for supervised wrapper generation and automated web information extraction, and a system called Lixto implementing these techniques. Our system can generate wrappers which translate relevant pieces of HTML pages into XML. Lixto, of which a working prototype has been implemented, assists the user to semi-automatically create wrapper programs by providing a fully visual and interactive user interface. In this convenient user-interface very expressive extraction programs can be created. Internally, this functionality is reected by the new logicbased declarative language Elog. Users never have to deal with Elog and even familiarity with HTML is not required. Lixto can be used to create an \\XML-Companion\" for an HTML web page with changing content, containing the continually updated XML translation of the relevant information."
            },
            "slug": "Visual-Web-Information-Extraction-with-Lixto-Baumgartner-Flesca",
            "title": {
                "fragments": [],
                "text": "Visual Web Information Extraction with Lixto"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Lixto, of which a working prototype has been implemented, assists the user to semi-automatically create wrapper programs by providing a fully visual and interactive user interface that helps to create very expressive extraction programs."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2000882"
                        ],
                        "name": "Wolfgang Gatterbauer",
                        "slug": "Wolfgang-Gatterbauer",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Gatterbauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Gatterbauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308385"
                        ],
                        "name": "Paul Bohunsky",
                        "slug": "Paul-Bohunsky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Bohunsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Bohunsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 88
                            }
                        ],
                        "text": "In fact, a recent model of data extraction, called Visual Box Model, has been presented [74,51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6474907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c506ce95dcdd45831eb4785c638b4767e1e995e",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Tables on web pages contain a huge amount of semantically explicit information, which makes them a worthwhile target for automatic information extraction and knowledge acquisition from the Web. However, the task of table extraction from web pages is difficult, because of HTML's design purpose to convey visual instead of semantic information. In this paper, we propose a robust technique for table extraction from arbitrary web pages. This technique relies upon the positional information of visualized DOM element nodes in a browser and, hereby, separates the intricacies of code implementation from the actual intended visual appearance. The novel aspect of the proposed web table extraction technique is the effective use of spatial reasoning on the CSS2 visual box model, which shows a high level of robustness even without any form of learning (F-measure \u2248 90%). We describe the ideas behind our approach, the tabular pattern recognition algorithm operating on a double topographical grid structure and allowing for effective and robust extraction, and general observations on web tables that should be borne in mind by any automatic web table extraction mechanism."
            },
            "slug": "Table-Extraction-Using-Spatial-Reasoning-on-the-Box-Gatterbauer-Bohunsky",
            "title": {
                "fragments": [],
                "text": "Table Extraction Using Spatial Reasoning on the CSS2 Visual Box Model"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a robust technique for table extraction from arbitrary web pages that relies upon the positional information of visualized DOM element nodes in a browser and separates the intricacies of code implementation from the actual intended visual appearance."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31744438"
                        ],
                        "name": "Kushal Dave",
                        "slug": "Kushal-Dave",
                        "structuredName": {
                            "firstName": "Kushal",
                            "lastName": "Dave",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kushal Dave"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145840115"
                        ],
                        "name": "S. Lawrence",
                        "slug": "S.-Lawrence",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Lawrence",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766638"
                        ],
                        "name": "D. Pennock",
                        "slug": "D.-Pennock",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pennock",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pennock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[Dave et al. 2003] approached the problem of opinion extraction and subsequent semantic classification of reviews of products."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1469556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "338a891907dce447da9a0fa2f27221bd35164163",
            "isKey": false,
            "numCitedBy": 2314,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful."
            },
            "slug": "Mining-the-peanut-gallery:-opinion-extraction-and-Dave-Lawrence",
            "title": {
                "fragments": [],
                "text": "Mining the peanut gallery: opinion extraction and semantic classification of product reviews"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work develops a method for automatically distinguishing between positive and negative reviews and draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624228"
                        ],
                        "name": "Christoph E. Koch",
                        "slug": "Christoph-E.-Koch",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Koch",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph E. Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "The information extraction functions implemented by this wrapping language rely on monadic datalogs over trees [56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 208007332,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12516fd1ef11ea4e02e78339baa987ef0028d3ee",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on information extraction from Web pages (wrapping) has seen much activity in recent times (particularly systems implementations), but little work has been done on formally studying the expressiveness of the formalisms proposed or on the theoretical foundations of wrapping.In this paper, we first study monadic datalog as a wrapping language (over ranked or unranked tree structures). Using previous work by Neven and Schwentick, we show that this simple language is equivalent to full monadic second order logic (MSO) in its ability to specify wrappers. We believe that MSO has the right expressiveness required for Web information extraction and thus propose MSO as a yardstick for evaluating and comparing wrappers.Using the above result, we study the kernel fragment Elog- of the Elog wrapping language used in the Lixto system (a visual wrapper generator). The striking fact here is that Elog- exactly captures MSO, yet is easier to use. Indeed, programs in this language can be entirely visually specified. We also formally compare Elog to other wrapping languages proposed in the literature."
            },
            "slug": "Monadic-datalog-and-the-expressive-power-of-for-web-Gottlob-Koch",
            "title": {
                "fragments": [],
                "text": "Monadic datalog and the expressive power of languages for Web information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is believed that MSO has the right expressiveness required for Web information extraction and thus MSO is proposed as a yardstick for evaluating and comparing wrappers and is compared to other wrapping languages proposed in the literature."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110304077"
                        ],
                        "name": "Masahiro Tanaka",
                        "slug": "Masahiro-Tanaka",
                        "structuredName": {
                            "firstName": "Masahiro",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masahiro Tanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143807934"
                        ],
                        "name": "T. Ishida",
                        "slug": "T.-Ishida",
                        "structuredName": {
                            "firstName": "Toru",
                            "lastName": "Ishida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ishida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some works try to apply the ontological approach to generic domains of Web data extraction [Han 2002] or to tables [Tanaka and Ishida 2006]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11993247,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faa07deed91afcc7ed981b97d126cd696f825207",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous works on information extraction from tables make use of prior knowledge such as a cognition model of tables or lexical knowledge bases for specific domains. However, we often need to interpret table structures in each table differently and to treat lexicons in various domains to more fully utilize the broad range of tables available on the Web. The method proposed in this paper uses relations represented by structures to extract an ontology from a table. Once the interpretations of table structures are given by humans, the table structures are automatically generalized to extract relations from the whole table. We define a formal representation of generalized table structure based on the adjacency of cells and iterative structures. As the result of the comparison with a method proposed in a previous work, it was shown that our method is suited to extraction of various relations which are needed for descriptions in RDF/OWL."
            },
            "slug": "Ontology-extraction-from-tables-on-the-Web-Tanaka-Ishida",
            "title": {
                "fragments": [],
                "text": "Ontology extraction from tables on the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A formal representation of generalized table structure based on the adjacency of cells and iterative structures is defined and it was shown that this method is suited to extraction of various relations which are needed for descriptions in RDF/OWL."
            },
            "venue": {
                "fragments": [],
                "text": "International Symposium on Applications and the Internet (SAINT'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3030274"
                        ],
                        "name": "A. Doan",
                        "slug": "A.-Doan",
                        "structuredName": {
                            "firstName": "AnHai",
                            "lastName": "Doan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5151034"
                        ],
                        "name": "J. Naughton",
                        "slug": "J.-Naughton",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Naughton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Naughton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709145"
                        ],
                        "name": "R. Ramakrishnan",
                        "slug": "R.-Ramakrishnan",
                        "structuredName": {
                            "firstName": "Raghu",
                            "lastName": "Ramakrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ramakrishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2621302"
                        ],
                        "name": "A. Baid",
                        "slug": "A.-Baid",
                        "structuredName": {
                            "firstName": "Akanksha",
                            "lastName": "Baid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Baid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059102"
                        ],
                        "name": "Xiaoyong Chai",
                        "slug": "Xiaoyong-Chai",
                        "structuredName": {
                            "firstName": "Xiaoyong",
                            "lastName": "Chai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoyong Chai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157325247"
                        ],
                        "name": "Fei Chen",
                        "slug": "Fei-Chen",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117182555"
                        ],
                        "name": "Ting Chen",
                        "slug": "Ting-Chen",
                        "structuredName": {
                            "firstName": "Ting",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ting Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056679359"
                        ],
                        "name": "E. Chu",
                        "slug": "E.-Chu",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Chu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Chu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2749312"
                        ],
                        "name": "Pedro DeRose",
                        "slug": "Pedro-DeRose",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "DeRose",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro DeRose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082453316"
                        ],
                        "name": "Byron J. Gao",
                        "slug": "Byron-J.-Gao",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Gao",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Byron J. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3151128"
                        ],
                        "name": "Chaitanya S. Gokhale",
                        "slug": "Chaitanya-S.-Gokhale",
                        "structuredName": {
                            "firstName": "Chaitanya",
                            "lastName": "Gokhale",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaitanya S. Gokhale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9395892"
                        ],
                        "name": "Jiansheng Huang",
                        "slug": "Jiansheng-Huang",
                        "structuredName": {
                            "firstName": "Jiansheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiansheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34414997"
                        ],
                        "name": "Warren Shen",
                        "slug": "Warren-Shen",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Warren Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4354559"
                        ],
                        "name": "Ba-Quy Vuong",
                        "slug": "Ba-Quy-Vuong",
                        "structuredName": {
                            "firstName": "Ba-Quy",
                            "lastName": "Vuong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ba-Quy Vuong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "\u2026. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.1.14 Web (experience) archiving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.1.15 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.2 Social\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1621055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c78bf7f88ae684f241551567170433a6cb8639a",
            "isKey": true,
            "numCitedBy": 83,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the past few years, we have been trying to build an end-to-end system at Wisconsin to manage unstructured data, using extraction, integration, and user interaction. This paper describes the key information extraction (IE) challenges that we have run into, and sketches our solutions. We discuss in particular developing a declarative IE language, optimizing for this language, generating IE provenance, incorporating user feedback into the IE process, developing a novel wiki-based user interface for feedback, best-effort IE, pushing IE into RDBMSs, and more. Our work suggests that IE in managing unstructured data can open up many interesting research challenges, and that these challenges can greatly benefit from the wealth of work on managing structured data that has been carried out by the database community."
            },
            "slug": "Information-extraction-challenges-in-managing-data-Doan-Naughton",
            "title": {
                "fragments": [],
                "text": "Information extraction challenges in managing unstructured data"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The work suggests that IE in managing unstructured data can open up many interesting research challenges, and that these challenges can greatly benefit from the wealth of work on managing structured data that has been carried out by the database community."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 112
                            }
                        ],
                        "text": "In the following we shortly describe some Web Data Extraction approaches relying on Machine Learning algorithms [78,16,92,109,46,60,93]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8618254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58de638505046e7de5fe7cc0660b4c6d79247488",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning to perform information extraction in domains where linguistic processing is problematic, such as Usenet posts, email, and finger plan files. In place of syntactic and semantic information, other sources of information can be used, such as term frequency, typography, formatting, and mark-up. We describe four learning approaches to this problem, each drawn from a different paradigm: a rote learner, a term-space learner based on Naive Bayes, an approach using grammatical induction, and a relational rule learner. Experiments on 14 information extraction problems defined over four diverse document collections demonstrate the effectiveness of these approaches. Finally, we describe a multistrategy approach which combines these learners and yields performance competitive with or better than the best of them. This technique is modular and flexible, and could find application in other machine learning problems."
            },
            "slug": "Machine-Learning-for-Information-Extraction-in-Freitag",
            "title": {
                "fragments": [],
                "text": "Machine Learning for Information Extraction in Informal Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A multistrategy approach which combines these learners and yields performance competitive with or better than the best of them is described, which is modular and flexible, and could find application in other machine learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802339"
                        ],
                        "name": "M. Pazienza",
                        "slug": "M.-Pazienza",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Pazienza",
                            "middleNames": [
                                "Teresa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazienza"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 57
                            }
                        ],
                        "text": "On the wrapper induction problem, Flesca et al. [45] and Kaiser and Miksch [64] surveyed approaches, techniques and tools."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 48
                            }
                        ],
                        "text": "In particular, two classes of strategies emerge [Kaiser and Miksch 2005]: learning techniques and knowledge engineering techniques \u2013 also called as learning-based and rule-based approaches, respectively [Sarawagi 2008]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 28
                            }
                        ],
                        "text": "2004] and Kaiser and Miksch [Kaiser and Miksch 2005] surveyed approaches, techniques and tools."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1102517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06894f06b6411af67a0ffde61d27efd86a5d31c7",
            "isKey": true,
            "numCitedBy": 916,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "It seems widely agreed that IE (Information Extraction) is now a tested language technology that has reached precision+recall values that put it in about the same position as Information Retrieval and Machine Translation, both of which are widely used commercially. There is also a clear range of practical applications that would be eased by the sort of template-style data that IE provides. The problem for wider deployment of the technology is adaptability: the ability to customize IE rapidly to new domains. In this paper we discuss some methods that have been tried to ease this problem, and to create something more rapid than the bench-mark one-month figure, which was roughly what ARPA teams in IE needed to adapt an existing system by hand to a new domain of corpora and templates. An important distinction in discussing the issue is the degree to which a user can be assumed to know what is wanted, to have preexisting templates ready to hand, as opposed to a user who has a vague idea of what is needed from a corpus. We shall discuss attempts to derive templates directly from corpora; to derive knowledge structures and lexicons directly from corpora, including discussion of the recent LE project ECRAN which attempted to tune existing lexicons to new corpora. An important issue is how far established methods in Information Retrieval of tuning to a user\u2019s needs with feedback at an interface can be transferred to IE."
            },
            "slug": "Information-Extraction-Pazienza",
            "title": {
                "fragments": [],
                "text": "Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper discusses attempts to derive templates directly from corpora; to derive knowledge structures and lexicons directly from Corpora, including discussion of the recent LE project ECRAN which attempted to tune existing lexicons to new corpora."
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "One of the early approaches is WIEN [78]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "The advantages of SoftMealy with respect to WIEN are worth noting: in fact, the system was able to deal with a number of exception, such as missing values/attributes, multiple attribute values, variant attribute permutations and also with typos."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "WIEN was based on different inductive learning techniques and it was capable of automatically labeling training pages, representing de facto a hybrid system whose training process implied low human engagement."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "One of the early approaches is WIEN [76]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 112
                            }
                        ],
                        "text": "In the following we shortly describe some Web Data Extraction approaches relying on Machine Learning algorithms [78,16,92,109,46,60,93]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "Finally, the FST was fed by sequence of separators, instead of raw HTML strings (as in WIEN), so that to match tokens with contextual rules (defined to characterize a set of individual separators) to determine the state transitions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "In particular, RoadRunner can work using information provided by users, in the form of labeled example pages, or also by automatically labeling Web pages (such as WIEN), to build a training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "The flip side of the high automation of WIEN was the big number of limitations related to its inferential system: for example, the data extraction process was not capable of dealing with missing values \u2013 a case that occurs on a frequent base and posed serious limitations on the adaptability of WIEN to real-world scenarios."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11075952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f052f40a3307de1e45e11a3007a7552b36ebfc8",
            "isKey": true,
            "numCitedBy": 641,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Wrapper-induction:-Efficiency-and-expressiveness-Kushmerick",
            "title": {
                "fragments": [],
                "text": "Wrapper induction: Efficiency and expressiveness"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782651"
                        ],
                        "name": "A. Zanasi",
                        "slug": "A.-Zanasi",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Zanasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zanasi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 118
                            }
                        ],
                        "text": "At the Enterprise level, Web Data Extraction techniques emerge as a key tool to perform data analysis in Business and Competitive Intelligence systems as well as for business process re-engineering."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "Zanasi [125] was the first to introduce the possibility of acquiring these data, through data mining processes, on public domain information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 72
                            }
                        ],
                        "text": "This requirement is particularly stringent in the field of Business and Competitive Intelligence because a company needs to perform timely analysis of market conditions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 239
                            }
                        ],
                        "text": "Web Data Extraction systems find extensive use in a wide range of applications including the analysis of textbased documents available to a company (like e-mails, support forums, technical and legal documentation, and so on), Business and Competitive Intelligence [9], crawling of Social Web platforms [17, 52], BioInformatics [99] and so on."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 25
                            }
                        ],
                        "text": "This process is known as Competitive Intelligence [22, 125] and it is crucial to quickly identify the opportunities provided by the market, to anticipate the decisions of the competitors as well as to learn from their faults and successes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 35
                            }
                        ],
                        "text": "24 4.1.5 Business Intelligence and Competitive Intelligence . . . . . . . . . . . . . . . . . . . . 24 4.1.6 Web process integration and channel management . . . . . . . . . . . . . . . . . . . . 25 4.1.7 Functional Web application testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.1.8 Comparison shopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "This process is known as Competitive Intelligence [22,125] and it is crucial to quickly identify the opportunities provided by the market, to anticipate the decisions of the competitors as well as to learn from their faults and successes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 153436524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "110dcbd826f00c450d95fee3b4531637cc0cafeb",
            "isKey": true,
            "numCitedBy": 68,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A new paradigm has appeared in computer science in the recent years: data mining. The success of this new approach comes from recent advances in three different, yet connected fields: mathematics\u2014new efficient and quick algorithms have been developed; database technologies\u2014new research has allowed the improvement of databases; computer power\u2014new powerful computers and architectures have made possible the elaboration of huge data volumes. Data mining's applications to the business world are several and include database marketing, basket analysis, and crime detection. In this article, the author discusses the application of data mining to competitive intelligence analysis. \u00a9 1998 John Wiley & Sons, Inc."
            },
            "slug": "Competitive-intelligence-through-data-mining-public-Zanasi",
            "title": {
                "fragments": [],
                "text": "Competitive intelligence through data mining public sources"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The author discusses the application of data mining to competitive intelligence analysis and suggests several applications to the business world that include database marketing, basket analysis, and crime detection."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714362"
                        ],
                        "name": "F. Abel",
                        "slug": "F.-Abel",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Abel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Abel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706210"
                        ],
                        "name": "E. Herder",
                        "slug": "E.-Herder",
                        "structuredName": {
                            "firstName": "Eelco",
                            "lastName": "Herder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Herder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143779489"
                        ],
                        "name": "G. Houben",
                        "slug": "G.-Houben",
                        "structuredName": {
                            "firstName": "Geert-Jan",
                            "lastName": "Houben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Houben"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699110"
                        ],
                        "name": "N. Henze",
                        "slug": "N.-Henze",
                        "structuredName": {
                            "firstName": "Nicola",
                            "lastName": "Henze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Henze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143888651"
                        ],
                        "name": "D. Krause",
                        "slug": "D.-Krause",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Krause"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11708893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d14ea71ef9446e068469f02ddd059b084f9c72a",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to adapt functionality to their individual users, systems need information about these users. The Social Web provides opportunities to gather user data from outside the system itself. Aggregated user data may be useful to address cold-start problems as well as sparse user profiles, but this depends on the nature of individual user profiles distributed on the Social Web. For example, does it make sense to re-use Flickr profiles to recommend bookmarks in Delicious? In this article, we study distributed form-based and tag-based user profiles, based on a large dataset aggregated from the Social Web. We analyze the completeness, consistency and replication of form-based profiles, which users explicitly create by filling out forms at Social Web systems such as Twitter, Facebook and LinkedIn. We also investigate tag-based profiles, which result from social tagging activities in systems such as Flickr, Delicious and StumbleUpon: to what extent do tag-based profiles overlap between different systems, what are the benefits of aggregating tag-based profiles. Based on these insights, we developed and evaluated the performance of several cross-system user modeling strategies in the context of recommender systems. The evaluation results show that the proposed methods solve the cold-start problem and improve recommendation quality significantly, even beyond the cold-start."
            },
            "slug": "Cross-system-user-modeling-and-personalization-on-Abel-Herder",
            "title": {
                "fragments": [],
                "text": "Cross-system user modeling and personalization on the Social Web"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article analyzed the completeness, consistency and replication of form-based and tag-based user profiles, which result from social tagging activities in systems such as Flickr, Delicious and StumbleUpon, and developed and evaluated several cross-system user modeling strategies in the context of recommender systems."
            },
            "venue": {
                "fragments": [],
                "text": "User Modeling and User-Adapted Interaction"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145077269"
                        ],
                        "name": "F. Sebastiani",
                        "slug": "F.-Sebastiani",
                        "structuredName": {
                            "firstName": "Fabrizio",
                            "lastName": "Sebastiani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Sebastiani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Statistical methods are more effective and reliable in domains of unstructured data (like natural language processing problems, facts extraction from speeches, and automated text categorization [Sebastiani 2002])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3091,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b20af22b0734757d9ead382b201a65f9dd637cc",
            "isKey": false,
            "numCitedBy": 8449,
            "numCiting": 224,
            "paperAbstract": {
                "fragments": [],
                "text": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation."
            },
            "slug": "Machine-learning-in-automated-text-categorization-Sebastiani",
            "title": {
                "fragments": [],
                "text": "Machine learning in automated text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This survey discusses the main approaches to text categorization that fall within the machine learning paradigm and discusses in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099518"
                        ],
                        "name": "M. Szomszor",
                        "slug": "M.-Szomszor",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Szomszor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szomszor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737406"
                        ],
                        "name": "Iv\u00e1n Cantador",
                        "slug": "Iv\u00e1n-Cantador",
                        "structuredName": {
                            "firstName": "Iv\u00e1n",
                            "lastName": "Cantador",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iv\u00e1n Cantador"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842687"
                        ],
                        "name": "Harith Alani",
                        "slug": "Harith-Alani",
                        "structuredName": {
                            "firstName": "Harith",
                            "lastName": "Alani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Harith Alani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "Finally, Social Web users often create accounts and/or profiles in multiple platforms [112,34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 84
                            }
                        ],
                        "text": "Some tricks aiming at producing a larger and accurate dataset were also proposed in [112]: in particular, the authors observed that, in real scenarios, if a user creates accounts in several Web sites, she frequently adds a link to her accounts: so by using traditional search engines, we can find all pages linking to the homepage of a user and, by filtering these hits we can find the exact URL of the profile of a user in different platforms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 125
                            }
                        ],
                        "text": "Since both in Delicious and Flickr the users had the chance of filling a form by specifying their real names, the authors of [112] suggested to refine the candidate list by keeping only those user accounts whose real names matched exactly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 258
                            }
                        ],
                        "text": "In the field of Social Web applications, an example of single purpose application is given by applications devoted at collecting data about human activities in different Social Web platforms: for instance, the tags contributed by a user in different systems [112]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 132
                            }
                        ],
                        "text": "To the best of our knowledge, one of the first approaches facing the problem of correlating multiple user accounts was presented in [112]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11323029,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1524a73fb7702ddc1786d4b7901e991f5ba90180",
            "isKey": true,
            "numCitedBy": 81,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "As the popularity of the web increases, particularly the use of social networking sites and style sharing platforms, users are becoming increasingly connected, sharing more and more information, resources, and opinions. This vast array of information presents unique opportunities to harvest knowledge about user activities and interests through the exploitation of large-scale, complex systems. Communal tagging sites, and their respective folksonomies, are one example of such a complex system, providing huge amounts of information about users, spanning multiple domains of interest. However, the current Web infrastructure provides no mechanism for users to consolidate and exploit this information since it is spread over many desperate and unconnected resources. In this paper we compare user tag-clouds from multiple folksonomies to: (a) show how they tend to overlap, regardless of the focus of the folksonomy (b) demonstrate how this comparison helps finding and aligning the user's separate identities, and (c) show that cross-linking distributed user tag-clouds enriches users profiles. During this process, we find that significant user interests are often reflected in multiple Web2.0 profiles, even though they may operate over different domains. However, due to the free-form nature of tagging, some correlations are lost, a problem we address through the implementation and evaluation of a user tag filtering architecture."
            },
            "slug": "Correlating-user-profiles-from-multiple-Szomszor-Cantador",
            "title": {
                "fragments": [],
                "text": "Correlating user profiles from multiple folksonomies"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper compares user tag-clouds from multiple folksonomies to show how they tend to overlap, regardless of the focus of the folksonomy, and demonstrates how this comparison helps finding and aligning the user's separate identities, and shows that cross-linking distributed user tags enriches users profiles."
            },
            "venue": {
                "fragments": [],
                "text": "HT '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099518"
                        ],
                        "name": "M. Szomszor",
                        "slug": "M.-Szomszor",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Szomszor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szomszor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144300761"
                        ],
                        "name": "C. Cattuto",
                        "slug": "C.-Cattuto",
                        "structuredName": {
                            "firstName": "Ciro",
                            "lastName": "Cattuto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cattuto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842687"
                        ],
                        "name": "Harith Alani",
                        "slug": "Harith-Alani",
                        "structuredName": {
                            "firstName": "Harith",
                            "lastName": "Alani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Harith Alani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1394341279"
                        ],
                        "name": "K. O\u2019Hara",
                        "slug": "K.-O\u2019Hara",
                        "structuredName": {
                            "firstName": "Kieron",
                            "lastName": "O\u2019Hara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. O\u2019Hara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730687"
                        ],
                        "name": "A. Baldassarri",
                        "slug": "A.-Baldassarri",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Baldassarri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Baldassarri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144347993"
                        ],
                        "name": "V. Loreto",
                        "slug": "V.-Loreto",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Loreto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Loreto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776193"
                        ],
                        "name": "V. Servedio",
                        "slug": "V.-Servedio",
                        "structuredName": {
                            "firstName": "Vito",
                            "lastName": "Servedio",
                            "middleNames": [
                                "Domenico",
                                "Pietro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Servedio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "social connotation, i.e., they do not target at building a community of interacting members. A rst research work showing the benets of linking data provided by independent Web systems is provided in [113]. In that paper, the authors combined information from the Internet Movie Database (www.imdb.com) and Net ix (www.netflix.com). The IMDB is an online database containing extensive information on movie"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ie (e.g., the most important scenes, location, genres and so on). Net ix is an US-based company oering an online DVD rental service. Net ix users are allowed to rate a movie by providing a score. In [113], data from Net ix and IMDB were imported in a relational DBMS; movie titles in IMDB were correlated with movie titles in Net ix by applying string matching techniques. In this way, for each movie, th"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16349750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86bb731d152f83fdbe7e3d5f22c05c5b82e070d0",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "While the Semantic Web has evolved to support the meaningful exchange of heterogeneous data through shared and controlled conceptualisations, Web 2.0 has demonstrated that large-scale community tagging sites can enrich the semantic web with readily accessible and valuable knowledge. In this paper, we investigate the integration of a movies folksonomy with a semantic knowledge base about user-movie rentals. The folksonomy is used to enrich the knowledge base with descriptions and categorisations of movie titles, and user interests and opinions. Using tags harvested from the Internet Movie Database, and movie rating data gathered by Netflix, we perform experiments to investigate the question that folksonomy-generated movie tag-clouds can be used to construct better user profiles that reflect a user's level of interest in different kinds of movies, and therefore, provide a basis for prediction of their rating for a previously unseen movie."
            },
            "slug": "Folksonomies,-the-semantic-web,-and-movie-Szomszor-Cattuto",
            "title": {
                "fragments": [],
                "text": "Folksonomies, the semantic web, and movie recommendation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper investigates the question that folksonomy-generated movie tag-clouds can be used to construct better user profiles that reflect a user's level of interest in different kinds of movies, and therefore, provide a basis for prediction of their rating for a previously unseen movie."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1883948"
                        ],
                        "name": "A. Sahuguet",
                        "slug": "A.-Sahuguet",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Sahuguet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sahuguet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3181471"
                        ],
                        "name": "Fabien Azavant",
                        "slug": "Fabien-Azavant",
                        "structuredName": {
                            "firstName": "Fabien",
                            "lastName": "Azavant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabien Azavant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 71
                            }
                        ],
                        "text": "A notable tool implementing regular-expression-based extraction is W4F [104]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "By fully exploiting the power of regular expressions, W4F extraction rules include match and also split expressions, which separates words, annotating different elements on the same string."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "W4F adopts an annotation approach: instead of challenging users to deal with the HTML documents syntax, W4F eases the design of the wrapper by means of a wizard procedure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 115
                            }
                        ],
                        "text": "Wizards that simplify the way to specify queries are the next logical level and for instance have been used in W4F [104] and XWrap [83]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "A further step, which is the optimization of the regular expressions generated by W4F, is delegated to expert users \u2013 in fact, the tool is not always able to provide the best extraction rule."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "W4F produces the regular expression extraction rules of the annotated items and provides them to users."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7252520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba1b6020d376dd28d4b1d3598c16e9477f379113",
            "isKey": true,
            "numCitedBy": 202,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The Web has become a major conduit to information repositories of all kinds. Today, more than 80% of information published on the Web is generated by underlying databases (however access is granted through a Web gateway using forms as a query language and HTML as a display vehicle) and this proportion keeps increasing. But Web data sources also consist of standalone HTML pages hand-coded by individuals, that provide very useful information such as reviews, digests, links, etc. As for the information that also exists in underlying databases, the HTML interface is often the only one available for many would-be clients."
            },
            "slug": "Building-Light-Weight-Wrappers-for-Legacy-Web-Using-Sahuguet-Azavant",
            "title": {
                "fragments": [],
                "text": "Building Light-Weight Wrappers for Legacy Web Data-Sources Using W4F"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The Web has become a major conduit to information repositories of all kinds, but Web data sources also consist of standalone HTML pages hand-coded by individuals, that provide very useful information such as reviews, digests, links, etc."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39827458"
                        ],
                        "name": "Peiling Wang",
                        "slug": "Peiling-Wang",
                        "structuredName": {
                            "firstName": "Peiling",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peiling Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3058687"
                        ],
                        "name": "William B. Hawk",
                        "slug": "William-B.-Hawk",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Hawk",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William B. Hawk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145102313"
                        ],
                        "name": "C. Tenopir",
                        "slug": "C.-Tenopir",
                        "structuredName": {
                            "firstName": "Carol",
                            "lastName": "Tenopir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tenopir"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 79
                            }
                        ],
                        "text": "The first phase of a generic Web Data Extraction system is the Web interaction [117]: the Web Data Extraction system accesses a Web source and extracts data stored in it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2424601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea48a56aa7567b7b361613de2a3eb29e08c896eb",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Users'-interaction-with-World-Wide-Web-resources:-a-Wang-Hawk",
            "title": {
                "fragments": [],
                "text": "Users' interaction with World Wide Web resources: an exploratory study using a holistic approach"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062862"
                        ],
                        "name": "Sergio Bossa",
                        "slug": "Sergio-Bossa",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Bossa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergio Bossa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777572"
                        ],
                        "name": "G. Fiumara",
                        "slug": "G.-Fiumara",
                        "structuredName": {
                            "firstName": "Giacomo",
                            "lastName": "Fiumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fiumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785816"
                        ],
                        "name": "A. Provetti",
                        "slug": "A.-Provetti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Provetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Provetti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[Bossa et al. 2006] developed a token based lightweight Web data extraction system called Dynamo that differs from STALKER because tokens are placed during the Web pages building to identify elements on the page and relevant information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2695633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7611116473e1d43d9ee217a676a3b34e09e4aac",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new Web service architecture designed to make it possible to collect data from traditional plain HTML Web sites, aggregate and serve them in more advanced formats, e.g. as RSS feeds. To locate the relevant data in the plain HTML pages, the architecture requires the insertion of some meta tags in the commented text. Hence, the extra markup remains totally transparent to users and programs. Such annotated HTML documents are then routinely pulled by our Web service, which then aggregates the data and serves them over several channels, e.g. RSS 1.0 or 2.0. Also, a REST-style Web Service allows users to submit XQuery queries to the feeds database. Finally, we discuss scalability issues w.r.t. polling frequencies."
            },
            "slug": "A-Lightweight-Architecture-for-RSS-Polling-of-Web-Bossa-Fiumara",
            "title": {
                "fragments": [],
                "text": "A Lightweight Architecture for RSS Polling of Arbitrary Web sources"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new Web service architecture is described designed to make it possible to collect data from traditional plain HTML Web sites, aggregate and serve them in more advanced formats, e.g. as RSS feeds."
            },
            "venue": {
                "fragments": [],
                "text": "WOA"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To this extent, some techniques, called tree-matching strategies are a good candidate to detect similarities between two tree and they will be discussed in detail in the next sections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8359747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22fb3b3b2bdf768dd435eedfc5ef5155d3e56b1a",
            "isKey": false,
            "numCitedBy": 1071,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically.WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semi-structured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories."
            },
            "slug": "Learning-Information-Extraction-Rules-for-and-Free-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning Information Extraction Rules for Semi-Structured and Free Text"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences, and can also handle extraction from free text such as news stories."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682797"
                        ],
                        "name": "Avare Stewart",
                        "slug": "Avare-Stewart",
                        "structuredName": {
                            "firstName": "Avare",
                            "lastName": "Stewart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Avare Stewart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399601671"
                        ],
                        "name": "Ernesto Diaz-Aviles",
                        "slug": "Ernesto-Diaz-Aviles",
                        "structuredName": {
                            "firstName": "Ernesto",
                            "lastName": "Diaz-Aviles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ernesto Diaz-Aviles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744808"
                        ],
                        "name": "W. Nejdl",
                        "slug": "W.-Nejdl",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Nejdl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Nejdl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739660"
                        ],
                        "name": "L. Marinho",
                        "slug": "L.-Marinho",
                        "structuredName": {
                            "firstName": "Leandro",
                            "lastName": "Marinho",
                            "middleNames": [
                                "Balby"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Marinho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728442"
                        ],
                        "name": "A. Nanopoulos",
                        "slug": "A.-Nanopoulos",
                        "structuredName": {
                            "firstName": "Alexandros",
                            "lastName": "Nanopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nanopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388781075"
                        ],
                        "name": "L. Schmidt-Thieme",
                        "slug": "L.-Schmidt-Thieme",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Schmidt-Thieme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schmidt-Thieme"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 36
                            }
                        ],
                        "text": "A nice example has been provided in [111]; in that paper the authors consider users of blogs concerning music and users of Last."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 21
                            }
                        ],
                        "text": "The ultimate goal of [111] is to enrich each Social Web system by re-using tags already exploited in other environments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2500652,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "836002c2b8df148cc60919531d8ff6213c8f1e21",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The Social Web is successfully established and poised for continued growth. Web 2.0 applications such as blogs, bookmarking, music, photo and video sharing systems are among the most popular; and all of them incorporate a social aspect, i.e., users can easily share information with other users. But due to the diversity of these applications -- serving different aims -- the Social Web is ironically divided. Blog users who write about music for example, could possibly benefit from other users registered in other social systems operating within the same domain, such as a social radio station. Although these sites are two different and disconnected systems, offering distinct services to the users, the fact that domains are compatible could benefit users from both systems with interesting and multi-faceted information. In this paper we propose to automatically establish social links between distinct social systems through cross-tagging, i.e., enriching a social system with the tags of other similar social system(s). Since tags are known for increasing the prediction quality of recommender systems (RS), we propose to quantitatively evaluate the extent to which users can benefit from cross-tagging by measuring the impact of different cross-tagging approaches on tag-aware RS for personalized resource recommendations. We conduct experiments in real world data sets and empirically show the effectiveness of our approaches."
            },
            "slug": "Cross-tagging-for-personalized-open-social-Stewart-Diaz-Aviles",
            "title": {
                "fragments": [],
                "text": "Cross-tagging for personalized open social networking"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes to automatically establish social links between distinct social systems through cross-tagging, i.e., enriching a social system with the tags of other similar social system(s), and quantitatively evaluates the extent to which users can benefit from cross- Tagging by measuring the impact of different cross- tagging approaches on tag-aware RS for personalized resource recommendations."
            },
            "venue": {
                "fragments": [],
                "text": "HT '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923209"
                        ],
                        "name": "Nilesh N. Dalvi",
                        "slug": "Nilesh-N.-Dalvi",
                        "structuredName": {
                            "firstName": "Nilesh",
                            "lastName": "Dalvi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nilesh N. Dalvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683442"
                        ],
                        "name": "Ravi Kumar",
                        "slug": "Ravi-Kumar",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ravi Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144018773"
                        ],
                        "name": "Mohamed A. Soliman",
                        "slug": "Mohamed-A.-Soliman",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Soliman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed A. Soliman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 30
                            }
                        ],
                        "text": "To this purpose, some authors [30,31] introduced the concept of wrapper robustness: they proposed a strategy to find, among all the XPath expressions capable of extracting the same information from a Web page, the one that is less influenced by potential changes in the structure of the page and such an expression identifies the more robust wrapper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65ec5824f6a997df0322827285ee691510b4527a",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generic framework to make wrapper induction algorithms tolerant to noise in the training data. This enables us to learn wrappers in a completely unsupervised manner from automatically and cheaply obtained noisy training data, e.g., using dictionaries and regular expressions. By removing the site-level supervision that wrapper-based techniques require, we are able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques. Our system is used in production at Yahoo! and powers live applications."
            },
            "slug": "Automatic-Wrappers-for-Large-Scale-Web-Extraction-Dalvi-Kumar",
            "title": {
                "fragments": [],
                "text": "Automatic Wrappers for Large Scale Web Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By removing the site-level supervision that wrapper-based techniques require, this work is able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967815"
                        ],
                        "name": "M. E. Califf",
                        "slug": "M.-E.-Califf",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Califf",
                            "middleNames": [
                                "Elaine"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Califf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 112
                            }
                        ],
                        "text": "In the following we shortly describe some Web Data Extraction approaches relying on Machine Learning algorithms [78,16,92,109,46,60,93]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 69
                            }
                        ],
                        "text": "Rapier (Robust Automated Production of Information Extraction Rules) [16,92] is a system designed to learn rules for extracting information from documents and its main advantage is, perhaps, the capability of learning these rules directly from documents without prior parsing or any post-processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 39
                            }
                        ],
                        "text": "Rapier (Robust Automated Production of Information Extraction Rules) [16, 92] is a system designed to learn rules for extracting information from documents and its main advantage is, perhaps, the capability of\nlearning these rules directly from documents without prior parsing or any post-processing."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1646317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3645698144183e6f7dc6d6929d25f2ddfedad3a8",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present an algorithm, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER is a bottom-up learning algorithm that incorporates techniques from several inductive logic programming systems. We have implemented the algorithm in a system that allows patterns to have constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains."
            },
            "slug": "Bottom-Up-Relational-Learning-of-Pattern-Matching-Califf-Mooney",
            "title": {
                "fragments": [],
                "text": "Bottom-Up Relational Learning of Pattern Matching Rules for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An algorithm, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template, and presents encouraging experimental results on two domains."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807004"
                        ],
                        "name": "Shaozhi Ye",
                        "slug": "Shaozhi-Ye",
                        "structuredName": {
                            "firstName": "Shaozhi",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaozhi Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2573527"
                        ],
                        "name": "Juan Lang",
                        "slug": "Juan-Lang",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Lang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Lang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718534"
                        ],
                        "name": "S. F. Wu",
                        "slug": "S.-F.-Wu",
                        "structuredName": {
                            "firstName": "Shyhtsun",
                            "lastName": "Wu",
                            "middleNames": [
                                "Felix"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. F. Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "Due to these reasons, it has been applied in a large number of studies about the topology and structure of Online Social Networks (see, for instance, [20,119,52,123,17])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[123] experimentally studies the impact of black holes by considering both different crawling strategies as well as different OSNs (like YouTube and LiveJournal)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 77
                            }
                        ],
                        "text": "Finally, due to privacy settings, the so-called black hole problem can arise [123]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 58
                            }
                        ],
                        "text": "In case of Social Web applications, many approaches (like [52,17,90,123]) are devoted to extract friendship relationships from a Social Networking Web sites."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5618221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ae975a4bd637d37f0133ce72a1665cc919c3538",
            "isKey": true,
            "numCitedBy": 132,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Extensive research has been conducted on top of online social networks (OSNs), while little attention has been paid to the data collection process. Due to the large scale of OSNs and their privacy control policies, a partial data set is often used for analysis. The data set analyzed is decided by many factors including the choice of seeds, node selection algorithms, and the sample size. These factors may introduce biases and further contaminate or even skew the results. To evaluate the impact of different factors, this paper examines the OSN graph crawling problem, where the nodes are OSN users and the edges are the links (or relationship) among these users. More specifically, by looking at various factors in the crawling process, the following problems are addressed in this paper:* Efficiency: How fast different crawlers discover nodes/links;* Sensitivity: How different OSNs and the number of protected users affect crawlers;* Bias: How major graph properties are skewed.To the best of our knowledge, our simulations on four real world online social graphs provide the first in-depth empirical answers to these questions."
            },
            "slug": "Crawling-Online-Social-Graphs-Ye-Lang",
            "title": {
                "fragments": [],
                "text": "Crawling Online Social Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper examines the OSN graph crawling problem, where the nodes are OSN users and the edges are the links (or relationship) among these users, and looks at various factors in the crawling process."
            },
            "venue": {
                "fragments": [],
                "text": "2010 12th International Asia-Pacific Web Conference"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765521"
                        ],
                        "name": "Daniele Perito",
                        "slug": "Daniele-Perito",
                        "structuredName": {
                            "firstName": "Daniele",
                            "lastName": "Perito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniele Perito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691730"
                        ],
                        "name": "C. Castelluccia",
                        "slug": "C.-Castelluccia",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Castelluccia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Castelluccia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708760"
                        ],
                        "name": "M. K\u00e2afar",
                        "slug": "M.-K\u00e2afar",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "K\u00e2afar",
                            "middleNames": [
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. K\u00e2afar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759333"
                        ],
                        "name": "Pere Manils",
                        "slug": "Pere-Manils",
                        "structuredName": {
                            "firstName": "Pere",
                            "lastName": "Manils",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pere Manils"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2450583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa33c468f69a67a5fb05b565f15743d19cff2a7d",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Usernames are ubiquitously used for identification and authentication purposes on web services and the Internet at large, ranging from the local-part of email addresses to identifiers in social networks. Usernames are generally alphanumerical strings chosen by the users and, by design, are unique within the scope of a single organization or web service. In this paper we investigate the feasibility of using usernames to trace or link multiple profiles across services that belong to the same individual. The intuition is that the probability that two usernames refer to the same physical person strongly depends on the \"entropy\" of the username string itself. Our experiments, based on usernames gathered from real web services, show that a significant portion of the users' profiles can be linked using their usernames. In collecting the data needed for our study, we also show that users tend to choose a small number of related usernames and use them across many services. To the best of our knowledge, this is the first time that usernames are considered as a source of information when profiling users on the Internet."
            },
            "slug": "How-Unique-and-Traceable-Are-Usernames-Perito-Castelluccia",
            "title": {
                "fragments": [],
                "text": "How Unique and Traceable Are Usernames?"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper investigates the feasibility of using usernames to trace or link multiple profiles across services that belong to the same individual, and is the first time that username are considered as a source of information when profiling users on the Internet."
            },
            "venue": {
                "fragments": [],
                "text": "PETS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819393"
                        ],
                        "name": "Fotis Kokkoras",
                        "slug": "Fotis-Kokkoras",
                        "structuredName": {
                            "firstName": "Fotis",
                            "lastName": "Kokkoras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fotis Kokkoras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2280041"
                        ],
                        "name": "Konstantinos Ntonas",
                        "slug": "Konstantinos-Ntonas",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Ntonas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konstantinos Ntonas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2598608"
                        ],
                        "name": "Apostolos Kritikos",
                        "slug": "Apostolos-Kritikos",
                        "structuredName": {
                            "firstName": "Apostolos",
                            "lastName": "Kritikos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Apostolos Kritikos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798238"
                        ],
                        "name": "George Kakarontzas",
                        "slug": "George-Kakarontzas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kakarontzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George Kakarontzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788195"
                        ],
                        "name": "I. Stamelos",
                        "slug": "I.-Stamelos",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Stamelos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Stamelos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "code search engines, we cite OCEAN [73]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "This yields a further level of complexity and create a cognitive overhead to the user [73]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14669903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57ae8d06c8ecc01c60913e578195c316e1a68654",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Source code search engines assist the software development process by providing a way of searching for free source code in code repositories. Although their use is rather straightforward, there exist a few of them and the differences in the way they index and provide access to their assets require considerable time and effort from the programmer to use them. In this paper, we present a federated open source code search engine that simultaneously asks, in real time, existing open source code search engine sites and detail the way we overcome the integration obstacles, by combining provided APIs, browser automation and web content extraction techniques."
            },
            "slug": "Federated-Search-for-Open-Source-Software-Reuse-Kokkoras-Ntonas",
            "title": {
                "fragments": [],
                "text": "Federated Search for Open Source Software Reuse"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a federated open source code search engine that simultaneously asks, in real time, existing open sourcecode search engine sites to answer questions and details the way it overcome the integration obstacles, by combining provided APIs, browser automation and web content extraction techniques."
            },
            "venue": {
                "fragments": [],
                "text": "2012 38th Euromicro Conference on Software Engineering and Advanced Applications"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2323215"
                        ],
                        "name": "Ben Hammersley",
                        "slug": "Ben-Hammersley",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Hammersley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Hammersley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ion [117]: the Web Data Extraction system accesses a Web source and extracts information stored in it. Web sources usually coincide with Web pages, but some approaches consider also as RSS/Atom feeds [57] and Microformats [65]. Some commercial systems, Lixto for rst but also Kapow Mashup Server (described below), include a Graphical User Interface for fully visual and interactive navigation of HTML pa"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 108858849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f508c7516a7f91a0981562a9c76a41dbb8d1bf4",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Perhaps the most explosive technological trend over the past two years has been blogging. As a matter of fact, it's been reported that the number of blogs during that time has grown from 100,000 to 4.8 million-with no end to this growth in sight. What's the technology that makes blogging tick? The answer is RSS--a format that allows bloggers to offer XML-based feeds of their content. It's also the same technology that's incorporated into the websites of media outlets so they can offer material (headlines, links, articles, etc.) syndicated by other sites. As the main technology behind this rapidly growing field of content syndication, RSS is constantly evolving to keep pace with worldwide demand. That's where Developing Feeds with RSS and Atom steps in. It provides bloggers, web developers, and programmers with a thorough explanation of syndication in general and the most popular technologies used to develop feeds. This book not only highlights all the new features of RSS 2.0-the most recent RSS specification-but also offers complete coverage of its close second in the XML-feed arena, Atom. The book has been exhaustively revised to explain: metadata interpretation the different forms of content syndication the increasing use of web services how to use popular RSS news aggregators on the market After an introduction that examines Internet content syndication in general (its purpose, limitations, and traditions), this step-by-step guide tackles various RSS and Atom vocabularies, as well as techniques for applying syndication to problems beyond news feeds. Most importantly, it gives you a firm handle on how to create your own feeds, and consume or combine other feeds. If you're interested in producing your own content feed, Developing Feeds with RSS and Atom is the one book you'll want in hand."
            },
            "slug": "Developing-Feeds-With-RSS-And-Atom-Hammersley",
            "title": {
                "fragments": [],
                "text": "Developing Feeds With RSS And Atom"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Developing Feeds with RSS and Atom provides bloggers, web developers, and programmers with a thorough explanation of syndication in general and the most popular technologies used to develop feeds, as well as techniques for applying syndication to problems beyond news feeds."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913159"
                        ],
                        "name": "Robert B. Doorenbos",
                        "slug": "Robert-B.-Doorenbos",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Doorenbos",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert B. Doorenbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "he HTML documents; alternatively, which is the aim of Web Data Extraction systems, wrappers might be dened and executed by using an inductive approach { a process commonly known as wrapper induction [75]. Web wrapper induction is challenging because it requires high level automation strategies. There exist also hybrid approaches that make possible for users to generate and run wrappers semi-automatic"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5119155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e7402ad740b73cc0bb64178f86df3478c3aaf5",
            "isKey": false,
            "numCitedBy": 1283,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Many Internet information resources present relational data|telephone directories, product catalogs, etc. Because these sites are formatted for people, mechanically extracting their content is di cult. Systems using such resources typically use hand-coded wrappers, procedures to extract data from information resources. We introduce wrapper induction, a method for automatically constructing wrappers, and identify hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources. We use PAC analysis to bound the problem's sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge."
            },
            "slug": "Wrapper-Induction-for-Information-Extraction-Kushmerick-Weld",
            "title": {
                "fragments": [],
                "text": "Wrapper Induction for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work introduces wrapper induction, a method for automatically constructing wrappers, and identifies hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "resented a survey that oers a rigorous taxonomy to classify Web Data Extraction systems. They introduced a set of criteria and a qualitative analysis of various Web Data Extraction tools. Kushmerick [70] tracked a prole of nite-state approaches to the Web Data Extraction problem. The author analyzed both wrapper induction approaches (i.e., approaches capable of automatically generating wrappers by e"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " the problem of automatizing the wrapper maintenance has been faced in recent literature. For example, the rst eort in the direction of automatic wrapper maintenance has been presented by Kushmerick [70], who dened for rst the concept of wrapper verication. The task of wrapper verication arises as a required step during wrapper execution, in which a Web Data Extraction system assess if dened Web "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17197575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "084ef49286a02841b478c740a7364b2554cb3a72",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Information agents are emerging as an important approach to building next- generation value-added information services. An information agent is a distributed system that receives a goal through its user interface, gathers information relevant to this goal from a variety of sources, processes this content as appropriate,and delivers the results to the users. We focus on the second stage in this generic architecture. We survey a variety of information extraction techniques that enable information agents to automatically gather information from heterogeneous sources."
            },
            "slug": "Finite-State-Approaches-to-Web-Information-Kushmerick",
            "title": {
                "fragments": [],
                "text": "Finite-State Approaches to Web Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work surveys a variety of information extraction techniques that enable information agents to automatically gather information from heterogeneous sources and delivers the results to the users."
            },
            "venue": {
                "fragments": [],
                "text": "SCIE"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967815"
                        ],
                        "name": "M. E. Califf",
                        "slug": "M.-E.-Califf",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Califf",
                            "middleNames": [
                                "Elaine"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Califf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 39
                            }
                        ],
                        "text": "Rapier (Robust Automated Production of Information Extraction Rules) [16, 92] is a system designed to learn rules for extracting information from documents and its main advantage is, perhaps, the capability of\nlearning these rules directly from documents without prior parsing or any post-processing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 69
                            }
                        ],
                        "text": "Rapier (Robust Automated Production of Information Extraction Rules) [16, 92] is a system designed to learn rules for extracting information from documents and its main advantage is, perhaps, the capability of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 112
                            }
                        ],
                        "text": "In the following we shortly describe some Web Data Extraction approaches relying on Machine Learning algorithms [76, 16, 92, 109, 46, 60, 93]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 489775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16bd1fbe3694173eda4ad4338a85f8288d19bf02",
            "isKey": false,
            "numCitedBy": 700,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present a system, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and acquires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains."
            },
            "slug": "Relational-Learning-of-Pattern-Match-Rules-for-Califf-Mooney",
            "title": {
                "fragments": [],
                "text": "Relational Learning of Pattern-Match Rules for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "RAPIER employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and acquires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699799"
                        ],
                        "name": "Domenico Amalfitano",
                        "slug": "Domenico-Amalfitano",
                        "structuredName": {
                            "firstName": "Domenico",
                            "lastName": "Amalfitano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Domenico Amalfitano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725488"
                        ],
                        "name": "A. R. Fasolino",
                        "slug": "A.-R.-Fasolino",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Fasolino",
                            "middleNames": [
                                "Rita"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. R. Fasolino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712641"
                        ],
                        "name": "Porfirio Tramontana",
                        "slug": "Porfirio-Tramontana",
                        "structuredName": {
                            "firstName": "Porfirio",
                            "lastName": "Tramontana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Porfirio Tramontana"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] have developed a reverse engineering approach to abstract Finite States Machines representing the client-side behavior offered by RIAs."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29048428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c611c95ab4b7e2d2ed5dc8ae7928f7dd054df10",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last years, rich Internet applications (RIAs) have emerged as a new generation of Web applications offering greater usability and interactivity than traditional ones. At the same time, RIAs introduce new issues and challenges in all the Web application lifecycle activities. As an example, a key problem with RIAs consists of defining suitable software models for representing them and validating reverse engineering techniques for obtaining these models effectively.This paper presents a reverse engineering approach for abstracting finite state machines representing the client-side behaviour offered by RIAs. The approach is based on dynamic analysis of the RIA and employs clustering techniques for solving the problem of state explosion of the state machine. A case study illustrated in the paper shows the results of a preliminary experiment where the proposed process has been executed with success for reverse engineering the behaviour of an existing RIA."
            },
            "slug": "Reverse-Engineering-Finite-State-Machines-from-Rich-Amalfitano-Fasolino",
            "title": {
                "fragments": [],
                "text": "Reverse Engineering Finite State Machines from Rich Internet Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A reverse engineering approach for abstracting finite state machines representing the client-side behaviour offered by RIAs and employs clustering techniques for solving the problem of state explosion of the state machine is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2008 15th Working Conference on Reverse Engineering"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2441542"
                        ],
                        "name": "G. Huck",
                        "slug": "G.-Huck",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Huck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Huck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137629"
                        ],
                        "name": "P. Fankhauser",
                        "slug": "P.-Fankhauser",
                        "structuredName": {
                            "firstName": "P\u00e9ter",
                            "lastName": "Fankhauser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fankhauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751802"
                        ],
                        "name": "K. Aberer",
                        "slug": "K.-Aberer",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Aberer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Aberer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720826"
                        ],
                        "name": "E. Neuhold",
                        "slug": "E.-Neuhold",
                        "structuredName": {
                            "firstName": "Erich",
                            "lastName": "Neuhold",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Neuhold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "g facts from the Web were implemented by means of general purpose languages. Over time libraries (e.g. Ruby Mechanize) and special-purpose query languages evolved on top of this principle (e.g., Jedi [61] and Florid [87]). Wizards that simplify the way to specify queries are the next logical level and for instance have been used in W4F [104] and XWrap [83]. Advanced Web Data Extraction systems oer GU"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2538164,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62c9b4c353afad20f9a1f1d59060066d5aceb708",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Jedi (Java based Extraction and Dissemination of Information) is a lightweight tool for the creation of wrappers and mediators to extract, combine, and reconcile information from several independent information sources. For wrappers it uses attributed grammars, which are evaluated with a fault-tolerant parsing strategy to cope with ambiguous grammars and irregular sources. For mediation it uses a simple generic object-model that can be extended with Java-libraries for specific models such as HTML, XML or the relational model. This paper describes the architecture of Jedi, and then focuses on Jedi's wrapper generator."
            },
            "slug": "Jedi:-extracting-and-synthesizing-information-from-Huck-Fankhauser",
            "title": {
                "fragments": [],
                "text": "Jedi: extracting and synthesizing information from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The architecture of Jedi is described, and then the focus is on Jedi's wrapper generator."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 3rd IFCIS International Conference on Cooperative Information Systems (Cat. No.98EX122)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032526"
                        ],
                        "name": "C. Plake",
                        "slug": "C.-Plake",
                        "structuredName": {
                            "firstName": "Conrad",
                            "lastName": "Plake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Plake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34095393"
                        ],
                        "name": "T. Schiemann",
                        "slug": "T.-Schiemann",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Schiemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Schiemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194378"
                        ],
                        "name": "Marcus Pankalla",
                        "slug": "Marcus-Pankalla",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Pankalla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Pankalla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1851178"
                        ],
                        "name": "J. Hakenberg",
                        "slug": "J.-Hakenberg",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Hakenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hakenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693022"
                        ],
                        "name": "U. Leser",
                        "slug": "U.-Leser",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Leser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Leser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 331,
                                "start": 327
                            }
                        ],
                        "text": "Web Data Extraction systems find extensive use in a wide range of applications including the analysis of text-based documents available to a company (like e-mails, support forums, technical and legal documentation, and so on), Business and Competitive Intelligence [9], crawling of Social Web platforms [17,52], BioInformatics [99] and so on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[99] worked on PubMed(19) \u2013 the biggest repository of medical-scientific works that covers a broad range of topics \u2013 extracting information and relationships to create a graph; this structure could be a good starting point to proceed in extracting data about proteins and genes, for example connections and interactions among them: this information can be usually found, not in Web pages, rather they are available in PDF or Postscript format."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 326,
                                "start": 312
                            }
                        ],
                        "text": "Web Data Extraction systems find extensive use in a wide range of applications including the analysis of textbased documents available to a company (like e-mails, support forums, technical and legal documentation, and so on), Business and Competitive Intelligence [9], crawling of Social Web platforms [17, 52], BioInformatics [99] and so on."
                    },
                    "intents": []
                }
            ],
            "corpusId": 27202003,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "f7a013c3e530404ecf4818ced96a3563972528c9",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "UNLABELLED\nThe biomedical literature contains a wealth of information on associations between many different types of objects, such as protein-protein interactions, gene-disease associations and subcellular locations of proteins. When searching such information using conventional search engines, e.g. PubMed, users see the data only one-abstract at a time and 'hidden' in natural language text. AliBaba is an interactive tool for graphical summarization of search results. It parses the set of abstracts that fit a PubMed query and presents extracted information on biomedical objects and their relationships as a graphical network. AliBaba extracts associations between cells, diseases, drugs, proteins, species and tissues. Several filter options allow for a more focused search. Thus, researchers can grasp complex networks described in various articles at a glance.\n\n\nAVAILABILITY\nhttp://alibaba.informatik.hu-berlin.de/"
            },
            "slug": "ALIBABA:-PubMed-as-a-graph-Plake-Schiemann",
            "title": {
                "fragments": [],
                "text": "ALIBABA: PubMed as a graph"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "AliBaba is an interactive tool for graphical summarization of search results that parses the set of abstracts that fit a PubMed query and presents extracted information on biomedical objects and their relationships as a graphical network so researchers can grasp complex networks described in various articles at a glance."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747414"
                        ],
                        "name": "E. Rahm",
                        "slug": "E.-Rahm",
                        "structuredName": {
                            "firstName": "Erhard",
                            "lastName": "Rahm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rahm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36851470"
                        ],
                        "name": "H. Do",
                        "slug": "H.-Do",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Do",
                            "middleNames": [
                                "Hai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Do"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 43
                            }
                        ],
                        "text": "during these phases, such as data cleaning [101] and conflict resolution [91], users reach the target to obtain homogeneous information under a unique resulting structure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 55665187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fee9b6220e2df26d23f3dc0bfaff3636cadfde1d",
            "isKey": false,
            "numCitedBy": 1632,
            "numCiting": 115,
            "paperAbstract": {
                "fragments": [],
                "text": "We classify data quality problems that are addressed by data cleaning and provide an overview of the main solution approaches. Data cleaning is especially required when integrating heterogeneous data sources and should be addressed together with schema-related data transformations. In data warehouses, data cleaning is a major part of the so-called ETL process. We also discuss current tool support for data cleaning."
            },
            "slug": "Data-Cleaning:-Problems-and-Current-Approaches-Rahm-Do",
            "title": {
                "fragments": [],
                "text": "Data Cleaning: Problems and Current Approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work classifies data quality problems that are addressed by data cleaning and provides an overview of the main solution approaches and discusses current tool support for data cleaning."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Data Eng. Bull."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746914"
                        ],
                        "name": "Sinno Jialin Pan",
                        "slug": "Sinno-Jialin-Pan",
                        "structuredName": {
                            "firstName": "Sinno",
                            "lastName": "Pan",
                            "middleNames": [
                                "Jialin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sinno Jialin Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152290618"
                        ],
                        "name": "Qiang Yang",
                        "slug": "Qiang-Yang",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "Various strategies have been proposed to perform such a transfer of information and some of these strategies take advantages from models and algorithms developed in the field of transfer of learning [96]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 740063,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972",
            "isKey": false,
            "numCitedBy": 13495,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research."
            },
            "slug": "A-Survey-on-Transfer-Learning-Pan-Yang",
            "title": {
                "fragments": [],
                "text": "A Survey on Transfer Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3263907"
                        ],
                        "name": "Norbert Walchhofer",
                        "slug": "Norbert-Walchhofer",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Walchhofer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Norbert Walchhofer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1916046"
                        ],
                        "name": "Milan Hronsky",
                        "slug": "Milan-Hronsky",
                        "structuredName": {
                            "firstName": "Milan",
                            "lastName": "Hronsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Milan Hronsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3162440"
                        ],
                        "name": "Michael P\u00f6ttler",
                        "slug": "Michael-P\u00f6ttler",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "P\u00f6ttler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael P\u00f6ttler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1974369"
                        ],
                        "name": "K. Froeschl",
                        "slug": "K.-Froeschl",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Froeschl",
                            "middleNames": [
                                "Anton"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Froeschl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "[9, 12, 10] deeply analyzed how to apply Web Data Extraction techniques and tools to improve the process of acquiring market information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30689980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "842796fc115181933e77df97d8729112f4daa477",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "SEMAMO (SEmantic MArket MOnitoring) is a research project seeking to make use of the increasingly growing information available at Web-based sales and marketing channels for continuous market research. Assuming that online channels indeed mirror salient market developments faithfully, SEMAMO implements a nearly fully automatic adaptive data capture and analysis process delivering customer-defined market reports on demand. The paper describes the SEMAMO prototype implementation and exemplifies the functionality and utility of the approach in the domain of e-tourism, with a focus on the type of reports and visualisations the software, albeit not completely finished yet, can already deliver based on real-world data. Additionally, the role of formal domain description in SEMAMO is emphasized."
            },
            "slug": "Semantic-Online-Tourism-Market-Monitoring-Walchhofer-Hronsky",
            "title": {
                "fragments": [],
                "text": "Semantic Online Tourism Market Monitoring"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The paper describes the SEMAMO prototype implementation and exemplifies the functionality and utility of the approach in the domain of e-tourism, with a focus on the type of reports and visualisations the software, albeit not completely finished yet, can already deliver based on real-world data."
            },
            "venue": {
                "fragments": [],
                "text": "ENTER"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48898287"
                        ],
                        "name": "Emilio Ferrara",
                        "slug": "Emilio-Ferrara",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ferrara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Ferrara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1946136"
                        ],
                        "name": "Mohsen JafariAsbagh",
                        "slug": "Mohsen-JafariAsbagh",
                        "structuredName": {
                            "firstName": "Mohsen",
                            "lastName": "JafariAsbagh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohsen JafariAsbagh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2307347"
                        ],
                        "name": "Onur Varol",
                        "slug": "Onur-Varol",
                        "structuredName": {
                            "firstName": "Onur",
                            "lastName": "Varol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Onur Varol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705983"
                        ],
                        "name": "Vahed Qazvinian",
                        "slug": "Vahed-Qazvinian",
                        "structuredName": {
                            "firstName": "Vahed",
                            "lastName": "Qazvinian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vahed Qazvinian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653472"
                        ],
                        "name": "F. Menczer",
                        "slug": "F.-Menczer",
                        "structuredName": {
                            "firstName": "Filippo",
                            "lastName": "Menczer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Menczer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769960"
                        ],
                        "name": "A. Flammini",
                        "slug": "A.-Flammini",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Flammini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Flammini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 120
                            }
                        ],
                        "text": "Crawling social media and online social networks data has become the flagship Social application of Web Data Extraction [17,42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3443257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b32a5352528231b2d99bcdb646416fd4dd5269d",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The increasing pervasiveness of social media creates new opportunities to study human social behavior, while challenging our capability to analyze their massive data streams. One of the emerging tasks is to distinguish between different kinds of activities, for example engineered misinformation campaigns versus spontaneous communication. Such detection problems require a formal definition of meme, or unit of information that can spread from person to person through the social network. Once a meme is identified, supervised learning methods can be applied to classify different types of communication. The appropriate granularity of a meme, however, is hardly captured from existing entities such as tags and keywords. Here we present a framework for the novel task of detecting memes by clustering messages from large streams of social data. We evaluate various similarity measures that leverage content, metadata, network features, and their combinations. We also explore the idea of pre-clustering on the basis of existing entities. A systematic evaluation is carried out using a manually curated dataset as ground truth. Our analysis shows that pre-clustering and a combination of heterogeneous features yield the best trade-off between number of clusters and their quality, demonstrating that a simple combination based on pairwise maximization of similarity is as effective as a non-trivial optimization of parameters. Our approach is fully automatic, unsupervised, and scalable for real-time detection of memes in streaming data."
            },
            "slug": "Clustering-memes-in-social-media-Ferrara-JafariAsbagh",
            "title": {
                "fragments": [],
                "text": "Clustering memes in social media"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Analysis shows that pre-clustering and a combination of heterogeneous features yield the best trade-off between number of clusters and their quality, demonstrating that a simple combination based on pairwise maximization of similarity is as effective as a non-trivial optimization of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2915438"
                        ],
                        "name": "T. Anton",
                        "slug": "T.-Anton",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Anton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Anton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 170
                            }
                        ],
                        "text": "These tools generate rule-based wrappers, automatically or semi-automatically: usually they rely on delimiter-based extraction criteria inferred from formatting features [Anton 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9336145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76eb7007bc4aad2f861acbef93ae4c4ce3b1621d",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a wrapper induction algorithm for extracting information from tree-structured documents like HTML or XML. It derives XPath-compatible extraction rules from a set of annotated example documents. The approach builds a minimally generalized tree traversal pattern, and augments it with conditions. Another variant selects a subset of conditions so that (a) the pattern is consistent with the training data, (b) the pat-tern's document coverage is minimized, and (c) conditions that match structures preceding the target nodes are preferred. We discuss the ro-bustness of rules induced by this selection strategy and we illustrate how these rules exhibit knowledge of the target concept."
            },
            "slug": "XPath-Wrapper-Induction-by-generating-tree-patterns-Anton",
            "title": {
                "fragments": [],
                "text": "XPath-Wrapper Induction by generating tree traversal patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A wrapper induction algorithm for extracting information from tree-structured documents like HTML or XML derives XPath-compatible extraction rules from a set of annotated example documents and discusses the ro-bustness of rules induced by this selection strategy."
            },
            "venue": {
                "fragments": [],
                "text": "LWA"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738409"
                        ],
                        "name": "Bernhard Kr\u00fcpl",
                        "slug": "Bernhard-Kr\u00fcpl",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Kr\u00fcpl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Kr\u00fcpl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153481005"
                        ],
                        "name": "M. Herzog",
                        "slug": "M.-Herzog",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Herzog",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Herzog"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2000882"
                        ],
                        "name": "Wolfgang Gatterbauer",
                        "slug": "Wolfgang-Gatterbauer",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Gatterbauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Gatterbauer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 88
                            }
                        ],
                        "text": "In fact, a recent model of data extraction, called Visual Box Model, has been presented [74,51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6574692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38c3a2248eee100437388e4d9f881c5396208d8c",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method to extract tabular data from web pages. Rather than just analyzing the DOM tree, we also exploit visual cues in the rendered version of the document to extract data from tables which are not explicitly marked with an HTML table element. To detect tables, we rely on a variant of the well-known X-Y cut algorithm as used in the OCR community. We implemented the system by directly accessing Mozilla's box model that contains the positional data for all HTML elements of a given web page."
            },
            "slug": "Using-visual-cues-for-extraction-of-tabular-data-Kr\u00fcpl-Herzog",
            "title": {
                "fragments": [],
                "text": "Using visual cues for extraction of tabular data from arbitrary HTML documents"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A method to extract tabular data from web pages by exploiting visual cues in the rendered version of the document to extract data from tables which are not explicitly marked with an HTML table element is described."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2251027"
                        ],
                        "name": "R. Schifanella",
                        "slug": "R.-Schifanella",
                        "structuredName": {
                            "firstName": "Rossano",
                            "lastName": "Schifanella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schifanella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8977167"
                        ],
                        "name": "A. Barrat",
                        "slug": "A.-Barrat",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Barrat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barrat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144300761"
                        ],
                        "name": "C. Cattuto",
                        "slug": "C.-Cattuto",
                        "structuredName": {
                            "firstName": "Ciro",
                            "lastName": "Cattuto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Cattuto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2399519"
                        ],
                        "name": "Benjamin Markines",
                        "slug": "Benjamin-Markines",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Markines",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Markines"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653472"
                        ],
                        "name": "F. Menczer",
                        "slug": "F.-Menczer",
                        "structuredName": {
                            "firstName": "Filippo",
                            "lastName": "Menczer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Menczer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10097662,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0591e5ce18dbb93f254adc6162f48bccb78080ce",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Web 2.0 applications have attracted a considerable amount of attention because their open-ended nature allows users to create lightweight semantic scaffolding to organize and share content. To date, the interplay of the social and semantic components of social media has been only partially explored. Here we focus on Flickr and Last.fm, two social media systems in which we can relate the tagging activity of the users with an explicit representation of their social network. We show that a substantial level of local lexical and topical alignment is observable among users who lie close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local alignment between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar topical interests are more likely to be friends, and therefore semantic similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on the Last.fm data set, confirming that the social network constructed from semantic similarity captures actual friendship more accurately than Last.fm's suggestions based on listening patterns."
            },
            "slug": "Folks-in-Folksonomies:-social-link-prediction-from-Schifanella-Barrat",
            "title": {
                "fragments": [],
                "text": "Folks in Folksonomies: social link prediction from shared metadata"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that a substantial level of local lexical and topical alignment is observable among users who lie close to each other in the social network, and suggests that users with similar topical interests are more likely to be friends, and semantic similarity measures among users based solely on their annotation metadata should be predictive of social links."
            },
            "venue": {
                "fragments": [],
                "text": "WSDM '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729928"
                        ],
                        "name": "A. Mislove",
                        "slug": "A.-Mislove",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Mislove",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mislove"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40279803"
                        ],
                        "name": "M. Marcon",
                        "slug": "M.-Marcon",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Marcon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1958921"
                        ],
                        "name": "K. Gummadi",
                        "slug": "K.-Gummadi",
                        "structuredName": {
                            "firstName": "Krishna",
                            "lastName": "Gummadi",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gummadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736987"
                        ],
                        "name": "P. Druschel",
                        "slug": "P.-Druschel",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Druschel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Druschel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708336"
                        ],
                        "name": "Bobby Bhattacharjee",
                        "slug": "Bobby-Bhattacharjee",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Bhattacharjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Bhattacharjee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "To perform crawling, the approach of [90] suggests to iteratively retrieve the list of friends of a user which have not yet been visited and to add these contact to the list of users to visit."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "As observed by [90], BFS may incur in heavy limitations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 58
                            }
                        ],
                        "text": "In case of Social Web applications, many approaches (like [52,17,90,123]) are devoted to extract friendship relationships from a Social Networking Web sites."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 681127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7631c91b69c6ec58a352bf7c3121282770fdbe20",
            "isKey": false,
            "numCitedBy": 3124,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Online social networking sites like Orkut, YouTube, and Flickr are among the most popular sites on the Internet. Users of these sites form a social network, which provides a powerful means of sharing, organizing, and finding content and contacts. The popularity of these sites provides an opportunity to study the characteristics of online social network graphs at large scale. Understanding these graphs is important, both to improve current systems and to design new applications of online social networks.\n This paper presents a large-scale measurement study and analysis of the structure of multiple online social networks. We examine data gathered from four popular online social networks: Flickr, YouTube, LiveJournal, and Orkut. We crawled the publicly accessible user links on each site, obtaining a large portion of each social network's graph. Our data set contains over 11.3 million users and 328 million links. We believe that this is the first study to examine multiple online social networks at scale.\n Our results confirm the power-law, small-world, and scale-free properties of online social networks. We observe that the indegree of user nodes tends to match the outdegree; that the networks contain a densely connected core of high-degree nodes; and that this core links small groups of strongly clustered, low-degree nodes at the fringes of the network. Finally, we discuss the implications of these structural properties for the design of social network based systems."
            },
            "slug": "Measurement-and-analysis-of-online-social-networks-Mislove-Marcon",
            "title": {
                "fragments": [],
                "text": "Measurement and analysis of online social networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper examines data gathered from four popular online social networks: Flickr, YouTube, LiveJournal, and Orkut, and reports that the indegree of user nodes tends to match the outdegree; the networks contain a densely connected core of high-degree nodes; and that this core links small groups of strongly clustered, low-degree node at the fringes of the network."
            },
            "venue": {
                "fragments": [],
                "text": "IMC '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2592694"
                        ],
                        "name": "Haewoon Kwak",
                        "slug": "Haewoon-Kwak",
                        "structuredName": {
                            "firstName": "Haewoon",
                            "lastName": "Kwak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haewoon Kwak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727835"
                        ],
                        "name": "Changhyun Lee",
                        "slug": "Changhyun-Lee",
                        "structuredName": {
                            "firstName": "Changhyun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changhyun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110626549"
                        ],
                        "name": "Hosung Park",
                        "slug": "Hosung-Park",
                        "structuredName": {
                            "firstName": "Hosung",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hosung Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33772810"
                        ],
                        "name": "S. Moon",
                        "slug": "S.-Moon",
                        "structuredName": {
                            "firstName": "Sue",
                            "lastName": "Moon",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Moon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207178765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2b057987c398ecf332f50f42891161c99e14aa4",
            "isKey": false,
            "numCitedBy": 6461,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Twitter, a microblogging service less than three years old, commands more than 41 million users as of July 2009 and is growing fast. Twitter users tweet about any topic within the 140-character limit and follow others to receive their tweets. The goal of this paper is to study the topological characteristics of Twitter and its power as a new medium of information sharing.\n We have crawled the entire Twitter site and obtained 41.7 million user profiles, 1.47 billion social relations, 4,262 trending topics, and 106 million tweets. In its follower-following topology analysis we have found a non-power-law follower distribution, a short effective diameter, and low reciprocity, which all mark a deviation from known characteristics of human social networks [28]. In order to identify influentials on Twitter, we have ranked users by the number of followers and by PageRank and found two rankings to be similar. Ranking by retweets differs from the previous two rankings, indicating a gap in influence inferred from the number of followers and that from the popularity of one's tweets. We have analyzed the tweets of top trending topics and reported on their temporal behavior and user participation. We have classified the trending topics based on the active period and the tweets and show that the majority (over 85%) of topics are headline news or persistent news in nature. A closer look at retweets reveals that any retweeted tweet is to reach an average of 1,000 users no matter what the number of followers is of the original tweet. Once retweeted, a tweet gets retweeted almost instantly on next hops, signifying fast diffusion of information after the 1st retweet.\n To the best of our knowledge this work is the first quantitative study on the entire Twittersphere and information diffusion on it."
            },
            "slug": "What-is-Twitter,-a-social-network-or-a-news-media-Kwak-Lee",
            "title": {
                "fragments": [],
                "text": "What is Twitter, a social network or a news media?"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work is the first quantitative study on the entire Twittersphere and information diffusion on it and finds a non-power-law follower distribution, a short effective diameter, and low reciprocity, which all mark a deviation from known characteristics of human social networks."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777192"
                        ],
                        "name": "Jan Vosecky",
                        "slug": "Jan-Vosecky",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Vosecky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Vosecky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143840756"
                        ],
                        "name": "Dan Hong",
                        "slug": "Dan-Hong",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Hong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879111"
                        ],
                        "name": "V. Shen",
                        "slug": "V.-Shen",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Shen",
                            "middleNames": [
                                "Yun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Shen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 92
                            }
                        ],
                        "text": "One of the first approaches to dealing with the user identification problem was proposed in [115]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15375214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e2862ebdfe7d3cca379bce83ad3487b107b2787",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Today, more and more people have their virtual identities on the web. It is common that people are users of more than one social network and also their friends may be registered on multiple websites. A facility to aggregate our online friends into a single integrated environment would enable the user to keep up-to-date with their virtual contacts more easily, as well as to provide improved facility to search for people across different websites. In this paper, we propose a method to identify users based on profile matching. We use data from two popular social networks to study the similarity of profile definition. We evaluate the importance of fields in the web profile and develop a profile comparison tool. We demonstrate the effectiveness and efficiency of our tool in identifying and consolidating duplicated users on different websites."
            },
            "slug": "User-identification-across-multiple-social-networks-Vosecky-Hong",
            "title": {
                "fragments": [],
                "text": "User identification across multiple social networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A method to identify users based on profile matching, which uses data from two popular social networks to study the similarity of profile definition and develops and demonstrates the effectiveness and efficiency of the tool in identifying and consolidating duplicated users on different websites."
            },
            "venue": {
                "fragments": [],
                "text": "2009 First International Conference on Networked Digital Technologies"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709372"
                        ],
                        "name": "P. D. Meo",
                        "slug": "P.-D.-Meo",
                        "structuredName": {
                            "firstName": "Pasquale",
                            "lastName": "Meo",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Meo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840213"
                        ],
                        "name": "Antonino Nocera",
                        "slug": "Antonino-Nocera",
                        "structuredName": {
                            "firstName": "Antonino",
                            "lastName": "Nocera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonino Nocera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742046"
                        ],
                        "name": "G. Quattrone",
                        "slug": "G.-Quattrone",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Quattrone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Quattrone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727476"
                        ],
                        "name": "D. Rosaci",
                        "slug": "D.-Rosaci",
                        "structuredName": {
                            "firstName": "Domenico",
                            "lastName": "Rosaci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rosaci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694982"
                        ],
                        "name": "D. Ursino",
                        "slug": "D.-Ursino",
                        "structuredName": {
                            "firstName": "Domenico",
                            "lastName": "Ursino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ursino"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "Finally, Social Web users often create accounts and/or profiles in multiple platforms [112,34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "In the field of Recommender Systems, the approach of [34] show how to merge ratings provided by users in different Social Web platforms with the goal of computing reputation values which are subsequently used to generate recommendations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1615262,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a1cf31876cf3f711454acc5355af5f8e1426582",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Social internetworking systems are a significantly emerging new reality; they group together a set of social networks and allow their users to share resources, to acquire opinions and, more in general, to interact, even if these users belong to different social networks and, therefore, did not previously know each other. In this context the notions of trust and reputation play a very relevant role. These notions have been widely studied in the past in several contexts whereas they have been largely neglected in the social internetworking research; however, since this application field presents several peculiarities, the results found in other application contexts are not automatically valid here. This paper introduces a model to represent and handle trust and reputation in a social internetworking system and proposes an approach that exploits these parameters to provide users with suggestions about the most reliable persons they can contact or social networks they can register to."
            },
            "slug": "Finding-reliable-users-and-social-networks-in-a-Meo-Nocera",
            "title": {
                "fragments": [],
                "text": "Finding reliable users and social networks in a social internetworking system"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper introduces a model to represent and handle trust and reputation in a social internetworking system and proposes an approach that exploits these parameters to provide users with suggestions about the most reliable persons they can contact or social networks they can register to."
            },
            "venue": {
                "fragments": [],
                "text": "IDEAS '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1813573"
                        ],
                        "name": "Marco Balduzzi",
                        "slug": "Marco-Balduzzi",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Balduzzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Balduzzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2278879"
                        ],
                        "name": "Christian Platzer",
                        "slug": "Christian-Platzer",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Platzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Platzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144227650"
                        ],
                        "name": "Thorsten Holz",
                        "slug": "Thorsten-Holz",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Holz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thorsten Holz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707794"
                        ],
                        "name": "E. Kirda",
                        "slug": "E.-Kirda",
                        "structuredName": {
                            "firstName": "Engin",
                            "lastName": "Kirda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kirda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706558"
                        ],
                        "name": "D. Balzarotti",
                        "slug": "D.-Balzarotti",
                        "structuredName": {
                            "firstName": "Davide",
                            "lastName": "Balzarotti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Balzarotti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715189"
                        ],
                        "name": "Christopher Kr\u00fcgel",
                        "slug": "Christopher-Kr\u00fcgel",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Kr\u00fcgel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Kr\u00fcgel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11900159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0da7f044db8e227160497e46abf7cbbad2fbbaa9",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, social networks such as Facebook have experienced a huge surge in popularity. The amount of personal information stored on these sites calls for appropriate security precautions to protect this data. \n \nIn this paper, we describe how we are able to take advantage of a common weakness, namely the fact that an attacker can query popular social networks for registered e-mail addresses on a large scale. Starting with a list of about 10.4 million email addresses, we were able to automatically identify more than 1.2 million user profiles associated with these addresses. By automatically crawling and correlating these profiles, we collect detailed personal information about each user, which we use for automated profiling (i.e., to enrich the information available from each user). Having access to such information would allow an attacker to launch sophisticated, targeted attacks, or to improve the efficiency of spam campaigns. We have contacted the most popular providers, who acknowledged the threat and are currently implementing our proposed countermeasures. Facebook and XING, in particular, have recently fixed the problem."
            },
            "slug": "Abusing-Social-Networks-for-Automated-User-Balduzzi-Platzer",
            "title": {
                "fragments": [],
                "text": "Abusing Social Networks for Automated User Profiling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes how it is able to take advantage of a common weakness, namely the fact that an attacker can query popular social networks for registered e-mail addresses on a large scale, and automatically identify more than 1.2 million user profiles associated with these addresses."
            },
            "venue": {
                "fragments": [],
                "text": "RAID"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144657008"
                        ],
                        "name": "R. Khare",
                        "slug": "R.-Khare",
                        "structuredName": {
                            "firstName": "Rohit",
                            "lastName": "Khare",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Khare"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27353001"
                        ],
                        "name": "Tantek \u00c7elik",
                        "slug": "Tantek-\u00c7elik",
                        "structuredName": {
                            "firstName": "Tantek",
                            "lastName": "\u00c7elik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tantek \u00c7elik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Web sources usually coincide with Web pages, but some approaches consider also as RSS/Atom feeds [57] and Microformats [65]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17484329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e41694542b1c479eb7695ca2eda3f9d180d2eb1a",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Microformats are a clever adaptation of semantic XHTML that makes it easier to publish, index, and extract semi-structured information such as tags, calendar entries, contact information, and reviews on the Web. This makes it a pragmatic path towards achieving the vision set forth for the Semantic Web.Even though it sidesteps the existing \"technology stack\" of RDF, ontologies, and Artificial Intelligence-inspired processing tools, various microformats have emerged that parallel the goals of several well-known Semantic Web projects. This poster compares their prospects to the Semantic Web according to Rogers' Diffusion of Innovation model."
            },
            "slug": "Microformats:-a-pragmatic-path-to-the-semantic-web-Khare-\u00c7elik",
            "title": {
                "fragments": [],
                "text": "Microformats: a pragmatic path to the semantic web"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Microformats are a clever adaptation of semantic XHTML that makes it easier to publish, index, and extract semi-structured information such as tags, calendar entries, contact information, and reviews on the Web."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48898287"
                        ],
                        "name": "Emilio Ferrara",
                        "slug": "Emilio-Ferrara",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ferrara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Ferrara"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 108
                            }
                        ],
                        "text": ") to analyze and understand the dynamics of human behaviors at a planetary scale and in a real time-fashion [38,25,33,43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1470567,
            "fieldsOfStudy": [
                "Computer Science",
                "Sociology"
            ],
            "id": "29b39b3216095bb6e68eff159b56690852793e1c",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding social dynamics that govern human phenomena, such as communications and social relationships is a major problem in current computational social sciences. In particular, given the unprecedented success of online social networks (OSNs), in this paper we are concerned with the analysis of aggregation patterns and social dynamics occurring among users of the largest OSN as the date: Facebook. In detail, we discuss the mesoscopic features of the community structure of this network, considering the perspective of the communities, which has not yet been studied on such a large scale. To this purpose, we acquired a sample of this network containing millions of users and their social relationships; then, we unveiled the communities representing the aggregation units among which users gather and interact; finally, we analyzed the statistical features of such a network of communities, discovering and characterizing some specific organization patterns followed by individuals interacting in online social networks, that emerge considering different sampling techniques and clustering methodologies. This study provides some clues of the tendency of individuals to establish social interactions in online social networks that eventually contribute to building a well-connected social structure, and opens space for further social studies."
            },
            "slug": "A-large-scale-community-structure-analysis-in-Ferrara",
            "title": {
                "fragments": [],
                "text": "A large-scale community structure analysis in Facebook"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This study provides some clues of the tendency of individuals to establish social interactions in online social networks that eventually contribute to building a well-connected social structure, and opens space for further social studies."
            },
            "venue": {
                "fragments": [],
                "text": "EPJ Data Science"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683418"
                        ],
                        "name": "M. Berthold",
                        "slug": "M.-Berthold",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berthold",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Berthold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781982"
                        ],
                        "name": "D. Hand",
                        "slug": "D.-Hand",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hand",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hand"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "RDBMS, a data warehouse, a CMS, etc.). In addition to all the specic elds of application covered later in this work, acquired data can be also generically used for analytical or statistical purposes [14] or simply to republish them under a structured format. 3.2. Layer cake comparisons In this section, we summarize the capability stacks of Web data extraction systems from our understanding, including"
                    },
                    "intents": []
                }
            ],
            "corpusId": 60784336,
            "fieldsOfStudy": [
                "Education",
                "Computer Science"
            ],
            "id": "2de69b9f3a18a5a605c9f599896c84d8ab181ced",
            "isKey": true,
            "numCitedBy": 323,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis monograph is a detailed introductory presentation of the key classes of intelligent data analysis methods. The ten coherently written chapters by leading experts provide complete coverage of the core issues.. \"The book will become a valuable source of reference for professionals concerned with modern data analysis. Students as well as IT professionals interested in learning about intelligent data analysis will appreciate the book as a useful text enhanced by numerous illustrations and examples."
            },
            "slug": "Intelligent-Data-Analysis:-An-Introduction-Berthold-Hand",
            "title": {
                "fragments": [],
                "text": "Intelligent Data Analysis: An Introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This monograph is a detailed introductory presentation of the key classes of intelligent data analysis methods and is a valuable source of reference for professionals concerned with modern data analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709372"
                        ],
                        "name": "P. D. Meo",
                        "slug": "P.-D.-Meo",
                        "structuredName": {
                            "firstName": "Pasquale",
                            "lastName": "Meo",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Meo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48898287"
                        ],
                        "name": "Emilio Ferrara",
                        "slug": "Emilio-Ferrara",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ferrara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Ferrara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714362"
                        ],
                        "name": "F. Abel",
                        "slug": "F.-Abel",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Abel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Abel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745337"
                        ],
                        "name": "Lora Aroyo",
                        "slug": "Lora-Aroyo",
                        "structuredName": {
                            "firstName": "Lora",
                            "lastName": "Aroyo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lora Aroyo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143779489"
                        ],
                        "name": "G. Houben",
                        "slug": "G.-Houben",
                        "structuredName": {
                            "firstName": "Geert-Jan",
                            "lastName": "Houben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Houben"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 108
                            }
                        ],
                        "text": ") to analyze and understand the dynamics of human behaviors at a planetary scale and in a real time-fashion [38,25,33,43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8255760,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c664e76a30a1a5e58bd4b1497f8286d6adaf903",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we present an in-depth analysis of the user behaviors on different Social Sharing systems. We consider three popular platforms, Flickr, Delicious and StumbleUpon, and, by combining techniques from social network analysis with techniques from semantic analysis, we characterize the tagging behavior as well as the tendency to create friendship relationships of the users of these platforms. The aim of our investigation is to see if (and how) the features and goals of a given Social Sharing system reflect on the behavior of its users and, moreover, if there exists a correlation between the social and tagging behavior of the users. We report our findings in terms of the characteristics of user profiles according to three different dimensions: (i) intensity of user activities, (ii) tag-based characteristics of user profiles, and (iii) semantic characteristics of user profiles."
            },
            "slug": "Analyzing-user-behavior-across-social-sharing-Meo-Ferrara",
            "title": {
                "fragments": [],
                "text": "Analyzing user behavior across social sharing environments"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An in-depth analysis of the user behaviors on different Social Sharing systems considers three popular platforms, Flickr, Delicious and StumbleUpon, and, by combining techniques from social network analysis with techniques from semantic analysis, characterize the tagging behavior as well as the tendency to create friendship relationships of the users of these platforms."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Intell. Syst. Technol."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48805475"
                        ],
                        "name": "Zhixuan Zhang",
                        "slug": "Zhixuan-Zhang",
                        "structuredName": {
                            "firstName": "Zhixuan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhixuan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145179409"
                        ],
                        "name": "Chuang Zhang",
                        "slug": "Chuang-Zhang",
                        "structuredName": {
                            "firstName": "Chuang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1869541"
                        ],
                        "name": "Zhiqing Lin",
                        "slug": "Zhiqing-Lin",
                        "structuredName": {
                            "firstName": "Zhiqing",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiqing Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152122717"
                        ],
                        "name": "Bo Xiao",
                        "slug": "Bo-Xiao",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Xiao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 50
                            }
                        ],
                        "text": "In fact, it has been adopted in several scenarios [66, 126, 127, 130, 39, 40, 41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17085160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56e3cbf818bf4d1f1357c16d62e6422c02f42d95",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Rich information is contributed to blogs by millions of users all around the world with the development of blogsphere. However, few work has been done on the study of blog extraction so far. Unlike the traditional template-dependent wrapper, not only blog articles but also blogroll is extracted with template-independent wrapper in this paper. In our method, blog extraction is formalized as a machine learning problem and a template-independent wrapper is learned by using labeled blog pages from a single site. Testing pages are obtained from 10 popular Chinese blog sites. And experimental results on 300 real blog pages indicate that the proposed method can correctly extract data from blogs with the accuracy of 90% or even above."
            },
            "slug": "Blog-extraction-with-template-independent-wrapper-Zhang-Zhang",
            "title": {
                "fragments": [],
                "text": "Blog extraction with template-independent wrapper"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Not only blog articles but also blogroll is extracted with template-independent wrapper in this paper, and experimental results indicate that the proposed method can correctly extract data from blogs with the accuracy of 90% or even above."
            },
            "venue": {
                "fragments": [],
                "text": "2010 2nd IEEE InternationalConference on Network Infrastructure and Digital Content"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143814916"
                        ],
                        "name": "P. Mika",
                        "slug": "P.-Mika",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mika"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "social networks and communities [Mika 2007], bio-informatics [Hu et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1258057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c74d79a646e91a2ceb00a0c1bda7d8dddf63fd4",
            "isKey": false,
            "numCitedBy": 746,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Ontologies-are-us:-A-unified-model-of-social-and-Mika",
            "title": {
                "fragments": [],
                "text": "Ontologies are us: A unified model of social networks and semantics"
            },
            "venue": {
                "fragments": [],
                "text": "J. Web Semant."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3354636"
                        ],
                        "name": "Salvatore Catanese",
                        "slug": "Salvatore-Catanese",
                        "structuredName": {
                            "firstName": "Salvatore",
                            "lastName": "Catanese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Salvatore Catanese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709372"
                        ],
                        "name": "P. D. Meo",
                        "slug": "P.-D.-Meo",
                        "structuredName": {
                            "firstName": "Pasquale",
                            "lastName": "Meo",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Meo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48898287"
                        ],
                        "name": "Emilio Ferrara",
                        "slug": "Emilio-Ferrara",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ferrara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Ferrara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777572"
                        ],
                        "name": "G. Fiumara",
                        "slug": "G.-Fiumara",
                        "structuredName": {
                            "firstName": "Giacomo",
                            "lastName": "Fiumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fiumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785816"
                        ],
                        "name": "A. Provetti",
                        "slug": "A.-Provetti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Provetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Provetti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "The sampling procedure in [17] works as follows: an agent is activated and it queries the Facebook server(s) to obtain the list of Web pages representing the list of friends of a Facebook user."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [17], the authors designed a Web Data Extraction architecture based on Intelligent Agents (see Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 74
                            }
                        ],
                        "text": "Most of them have been discussed and exploited in the context of Facebook [52,17] but, unfortunately, some of them cannot be extended to other platforms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 310,
                                "start": 303
                            }
                        ],
                        "text": "Web Data Extraction systems find extensive use in a wide range of applications including the analysis of text-based documents available to a company (like e-mails, support forums, technical and legal documentation, and so on), Business and Competitive Intelligence [9], crawling of Social Web platforms [17,52], BioInformatics [99] and so on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 120
                            }
                        ],
                        "text": "Crawling social media and online social networks data has become the flagship Social application of Web Data Extraction [17,42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "Due to these reasons, it has been applied in a large number of studies about the topology and structure of Online Social Networks (see, for instance, [20,119,52,123,17])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Therefore, to extract large amounts of data, [17] suggested to send HTTP requests to fetch Web pages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "A nice example of cross-fertilization is in the context of the crawling of Online Social Networks [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 58
                            }
                        ],
                        "text": "In case of Social Web applications, many approaches (like [52,17,90,123]) are devoted to extract friendship relationships from a Social Networking Web sites."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15450009,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40e2338c189c38dd8c7e02a768f5f965c4952547",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe our work in the collection and analysis of massive data describing the connections between participants to online social networks. Alternative approaches to social network data collection are defined and evaluated in practice, against the popular Facebook Web site. Thanks to our ad-hoc, privacy-compliant crawlers, two large samples, comprising millions of connections, have been collected; the data is anonymous and organized as an undirected graph. We describe a set of tools that we developed to analyze specific properties of such social-network graphs, i.e., among others, degree distribution, centrality measures, scaling laws and distribution of friendship."
            },
            "slug": "Crawling-Facebook-for-social-network-analysis-Catanese-Meo",
            "title": {
                "fragments": [],
                "text": "Crawling Facebook for social network analysis purposes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A set of tools that are developed to analyze specific properties of social-network graphs, i.e., among others, degree distribution, centrality measures, scaling laws and distribution of friendship, are described."
            },
            "venue": {
                "fragments": [],
                "text": "WIMS '11"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32980371"
                        ],
                        "name": "Brent J. Hecht",
                        "slug": "Brent-J.-Hecht",
                        "structuredName": {
                            "firstName": "Brent",
                            "lastName": "Hecht",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brent J. Hecht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217278"
                        ],
                        "name": "Lichan Hong",
                        "slug": "Lichan-Hong",
                        "structuredName": {
                            "firstName": "Lichan",
                            "lastName": "Hong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lichan Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3994427"
                        ],
                        "name": "B. Suh",
                        "slug": "B.-Suh",
                        "structuredName": {
                            "firstName": "Bongwon",
                            "lastName": "Suh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Suh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2226805"
                        ],
                        "name": "Ed H. Chi",
                        "slug": "Ed-H.-Chi",
                        "structuredName": {
                            "firstName": "Ed",
                            "lastName": "Chi",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ed H. Chi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "For instance, [59] provided an approach to finding user location on the basis of user tweets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15244950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51ec8194cff157eb712382d589174909e087acf8",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Little research exists on one of the most common, oldest, and most utilized forms of online social geographic information: the 'location' field found in most virtual community user profiles. We performed the first in-depth study of user behavior with regard to the location field in Twitter user profiles. We found that 34% of users did not provide real location information, frequently incorporating fake locations or sarcastic comments that can fool traditional geographic information tools. When users did input their location, they almost never specified it at a scale any more detailed than their city. In order to determine whether or not natural user behaviors have a real effect on the 'locatability' of users, we performed a simple machine learning experiment to determine whether we can identify a user's location by only looking at what that user tweets. We found that a user's country and state can in fact be determined easily with decent accuracy, indicating that users implicitly reveal location information, with or without realizing it. Implications for location-based services and privacy are discussed."
            },
            "slug": "Tweets-from-Justin-Bieber's-heart:-the-dynamics-of-Hecht-Hong",
            "title": {
                "fragments": [],
                "text": "Tweets from Justin Bieber's heart: the dynamics of the location field in user profiles"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The first in-depth study of user behavior with regard to the location field in Twitter user profiles found that a user's country and state can in fact be determined easily with decent accuracy, indicating that users implicitly reveal location information, with or without realizing it."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759801"
                        ],
                        "name": "Ermelinda Oro",
                        "slug": "Ermelinda-Oro",
                        "structuredName": {
                            "firstName": "Ermelinda",
                            "lastName": "Oro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ermelinda Oro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794345"
                        ],
                        "name": "M. Ruffolo",
                        "slug": "M.-Ruffolo",
                        "structuredName": {
                            "firstName": "Massimo",
                            "lastName": "Ruffolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ruffolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752093"
                        ],
                        "name": "Steffen Staab",
                        "slug": "Steffen-Staab",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Staab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Staab"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "frameworks, using methods from document understanding and spatial reasoning such as the approaches of the TamCrow project [75], of the ABBA project [37] spatial XPath extensions [95] and rendition-based extensions in RoadRunner to detect labels [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15925664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "034c29a1a4d91b62b9ef40ab3eb1608db5667bc9",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Querying data from presentation formats like HTML, for purposes such as information extraction, requires the consideration of tree structures as well as the consideration of spatial relationships between laid out elements. The underlying rationale is that frequently the rendering of tree structures is very involved and undergoing more frequent updates than the resulting layout structure. Therefore, in this paper, we present Spatial XPath (SXPath), an extension of XPath 1.0 that allows for inclusion of spatial navigation primitives into the language resulting in conceptually simpler queries on Web documents. The SXPath language is based on a combination of a spatial algebra with formal descriptions of XPath navigation, and maintains polynomial time combined complexity. Practical experiments demonstrate the usability of SXPath."
            },
            "slug": "SXPath-Extending-XPath-towards-Spatial-Querying-on-Oro-Ruffolo",
            "title": {
                "fragments": [],
                "text": "SXPath - Extending XPath towards Spatial Querying on Web Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Spatial XPath (SXPath) is presented, an extension of XPath 1.0 that allows for inclusion of spatial navigation primitives into the language resulting in conceptually simpler queries on Web documents."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803566"
                        ],
                        "name": "Tereza Iofciu",
                        "slug": "Tereza-Iofciu",
                        "structuredName": {
                            "firstName": "Tereza",
                            "lastName": "Iofciu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tereza Iofciu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137629"
                        ],
                        "name": "P. Fankhauser",
                        "slug": "P.-Fankhauser",
                        "structuredName": {
                            "firstName": "P\u00e9ter",
                            "lastName": "Fankhauser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fankhauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714362"
                        ],
                        "name": "F. Abel",
                        "slug": "F.-Abel",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Abel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Abel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39889041"
                        ],
                        "name": "Kerstin Bischoff",
                        "slug": "Kerstin-Bischoff",
                        "structuredName": {
                            "firstName": "Kerstin",
                            "lastName": "Bischoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kerstin Bischoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "nt Social Web systems (like Delicious, Flickr and YouTube) with the goal of nding a mapping involving the dierent user accounts. This mapping can be found by applying a traditional search engine. In [62], the authors suggest to combine prole attributes (like usernames) with an analysis of the user contributed tags to identify users. They suggest various strategies to compare the tag-based proles of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1500898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3f15998c6f18f70bbe8e814e1c29028baf9087e",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "How much do tagging activities tell about a user? Is it possible to identify people in Delicious based on the tags, which they use in Flickr? In this paper we study those questions and investigate whether users can be identified across social tagging systems. We combine two kinds of information: their user ids and their tags. We introduce and compare a variety of approaches to measure the distance between user profiles for identification. With the best performing combination we achieve, depending on the actual settings, accuracies of between 60% and 80% which demonstrates that the traces of Web 2.0 users can reveal quite much about their identity."
            },
            "slug": "Identifying-Users-Across-Social-Tagging-Systems-Iofciu-Fankhauser",
            "title": {
                "fragments": [],
                "text": "Identifying Users Across Social Tagging Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A variety of approaches are introduced and compared to measure the distance between user profiles for identification and it is demonstrated that the traces of Web 2.0 users can reveal quite much about their identity."
            },
            "venue": {
                "fragments": [],
                "text": "ICWSM"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746404"
                        ],
                        "name": "Sheila Kinsella",
                        "slug": "Sheila-Kinsella",
                        "structuredName": {
                            "firstName": "Sheila",
                            "lastName": "Kinsella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheila Kinsella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684660"
                        ],
                        "name": "Vanessa Murdock",
                        "slug": "Vanessa-Murdock",
                        "structuredName": {
                            "firstName": "Vanessa",
                            "lastName": "Murdock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vanessa Murdock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401255262"
                        ],
                        "name": "Neil O'Hare",
                        "slug": "Neil-O'Hare",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "O'Hare",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil O'Hare"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Other studies entwining users geographical locations with the content they posted are provided in [26] and [67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "In an analogous fashion, [67] used geographic coordinates extracted from geotagged Twitter data to model user location."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "Experimental studies in [67] show that the proposed model can predict country, state and city with an accuracy comparable with that achieved by industrial tools for geo-localization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16767795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9400ae54bc004abfd496d905ae33b73ac28b6cc",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Social media such as Twitter generate large quantities of data about what a person is thinking and doing in a particular location. We leverage this data to build models of locations to improve our understanding of a user's geographic context. Understanding the user's geographic context can in turn enable a variety of services that allow us to present information, recommend businesses and services, and place advertisements that are relevant at a hyper-local level.\n In this paper we create language models of locations using coordinates extracted from geotagged Twitter data. We model locations at varying levels of granularity, from the zip code to the country level. We measure the accuracy of these models by the degree to which we can predict the location of an individual tweet, and further by the accuracy with which we can predict the location of a user. We find that we can meet the performance of the industry standard tool for predicting both the tweet and the user at the country, state and city levels, and far exceed its performance at the hyper-local level, achieving a three- to ten-fold increase in accuracy at the zip code level."
            },
            "slug": "\"I'm-eating-a-sandwich-in-Glasgow\":-modeling-with-Kinsella-Murdock",
            "title": {
                "fragments": [],
                "text": "\"I'm eating a sandwich in Glasgow\": modeling locations with tweets"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper creates language models of locations using coordinates extracted from geotagged Twitter data that can meet the performance of the industry standard tool for predicting both the tweet and the user at the country, state and city levels, and far exceed its performance at the hyper-local level."
            },
            "venue": {
                "fragments": [],
                "text": "SMUC '11"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3354636"
                        ],
                        "name": "Salvatore Catanese",
                        "slug": "Salvatore-Catanese",
                        "structuredName": {
                            "firstName": "Salvatore",
                            "lastName": "Catanese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Salvatore Catanese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709372"
                        ],
                        "name": "P. D. Meo",
                        "slug": "P.-D.-Meo",
                        "structuredName": {
                            "firstName": "Pasquale",
                            "lastName": "Meo",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Meo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48898287"
                        ],
                        "name": "Emilio Ferrara",
                        "slug": "Emilio-Ferrara",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ferrara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Ferrara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777572"
                        ],
                        "name": "G. Fiumara",
                        "slug": "G.-Fiumara",
                        "structuredName": {
                            "firstName": "Giacomo",
                            "lastName": "Fiumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fiumara"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "rened sampling techniques. The implementation of these techniques is equivalent to dene new Web Data Extraction procedures. Most of them have been discussed and exploited in the context of Facebook [48, 17, 18] but, unfortunately, some of them can not be extended to other platforms. 30 In particular, Gjoka et al. [48] considered dierent visiting algorithms, like BFS, \\Random Walks&quot; and \\Metropolis-Has"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "interface, which manages the information transfer through the Web. The proposed architecture is able to implement several crawling strategies like BFS or rejection sampling. The sampling procedure in [17, 18] works as follows: an agent is activated and it queries the Facebook server(s) to obtain the list of Web pages representing the list of friends of a Facebook user. Of course, the Facebook account to v"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "om the fact that each Facebook user was uniquely identied by a numerical ID ranging from 0 and 232 1. Of course, such a solution works well for Facebook but it could not work for other platforms. In [17, 18], the authors designed a Web Data Extraction architecture based on Intelligent Agents (see Figure 1 of [18]). Such an architecture consists of three main components: (i) a server running the mining ag"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "f two Web pages may have a devastating impact and thus would require to entirely rewrite the wrapper. A nice example of cross-fertilization is in the context of the crawling of Online Social Networks [18, 17]. In that paper the authors implemented a Web wrapper to crawl Facebook largely exploiting techniques and algorithms which were part of the Lixto suite and that were originally designed to work for Bu"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "graphs which can be modeled as unweighted graphs. Due to these reasons, it has been applied in a large number of studies about the topology and structure of Online Social Networks (see, for instance, [21, 113, 48, 117, 17]). As observed by [84], BFS may incur in heavy limitations. First of all, a crawler can get trapped into a strong connected component of the social graph. In addition, if we would use the BFS sample t"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8303975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "358bef4475f80e5e2a41a90194eaf1c3ad931eba",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Online Social Networks (OSN) during last years acquired a \nhuge and increasing popularity as one of the most important emerging Web phenomena, deeply modifying the behavior of users and contributing to build a solid substrate of connections and relationships among people using the Web. In this preliminary work paper, our purpose is to analyze Facebook, considering a signi\ufffdcant sample of data re \necting relationships among subscribed users. Our goal is to extract, from this platform, relevant information about the distribution of these relations and exploit tools and algorithms provided by the Social Network Analysis (SNA) to discover and, possibly, understand underlying similarities \nbetween the developing of OSN and real-life social networks."
            },
            "slug": "Analyzing-the-Facebook-Friendship-Graph-Catanese-Meo",
            "title": {
                "fragments": [],
                "text": "Analyzing the Facebook Friendship Graph"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The purpose is to analyze Facebook, considering a sample of data about the distribution of relationships among subscribed users to discover and, possibly, understand underlying similarities between the developing of OSN and real-life social networks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51988193"
                        ],
                        "name": "Edward Melomed",
                        "slug": "Edward-Melomed",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Melomed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Melomed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50787181"
                        ],
                        "name": "Irina Gorbach",
                        "slug": "Irina-Gorbach",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Gorbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irina Gorbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056931666"
                        ],
                        "name": "Alexander Berger",
                        "slug": "Alexander-Berger",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38326486"
                        ],
                        "name": "P. Bateman",
                        "slug": "P.-Bateman",
                        "structuredName": {
                            "firstName": "Py",
                            "lastName": "Bateman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bateman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "com) or Microsoft Analysis Services [Melomed et al. 2006]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57673361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3566d7f3a5cbb85213ae687d3ed1836fdbb66d7d",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Microsoft SQL Server 2005 Analysis Services is definitive guide to programming Analysis Services 2005. It will givei\u00be youi\u00be unparalleled insight into the ways in which Analysis Services functions straight from the members of the Analysis Services team at Microsoft . It not only explains ways to use Analysis Services 2005 to design and create multidimensional objects, databases, dimensions, cubes, but it also provides invaluable information about the reasons behind design decision taken by the development team.Chapters include: Introduction to Analysis Services Creating Multidimensional Models using Analysis Services Security High availability Localization and Globalization"
            },
            "slug": "Microsoft-SQL-Server-2005-Analysis-Services-(SQL-Melomed-Gorbach",
            "title": {
                "fragments": [],
                "text": "Microsoft SQL Server 2005 Analysis Services (SQL Server Series)"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This guide explains ways to use Analysis Services 2005 to design and create multidimensional objects, databases, dimensions, cubes, but also provides invaluable information about the reasons behind design decision taken by the development team."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2493222"
                        ],
                        "name": "Christian Kohlsch\u00fctter",
                        "slug": "Christian-Kohlsch\u00fctter",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Kohlsch\u00fctter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Kohlsch\u00fctter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137629"
                        ],
                        "name": "P. Fankhauser",
                        "slug": "P.-Fankhauser",
                        "structuredName": {
                            "firstName": "P\u00e9ter",
                            "lastName": "Fankhauser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fankhauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744808"
                        ],
                        "name": "W. Nejdl",
                        "slug": "W.-Nejdl",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Nejdl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Nejdl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "An approach for boilerplate detection using shallow text features was introduced in [70]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10739339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b11b8299608bd4411f037d4ba60327cd3f105b53",
            "isKey": false,
            "numCitedBy": 532,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In addition to the actual content Web pages consist of navigational elements, templates, and advertisements. This boilerplate text typically is not related to the main content, may deteriorate search precision and thus needs to be detected properly. In this paper, we analyze a small set of shallow text features for classifying the individual text elements in a Web page. We compare the approach to complex, state-of-the-art techniques and show that competitive accuracy can be achieved, at almost no cost. Moreover, we derive a simple and plausible stochastic model for describing the boilerplate creation process. With the help of our model, we also quantify the impact of boilerplate removal to retrieval performance and show significant improvements over the baseline. Finally, we extend the principled approach by straight-forward heuristics, achieving a remarkable detection accuracy."
            },
            "slug": "Boilerplate-detection-using-shallow-text-features-Kohlsch\u00fctter-Fankhauser",
            "title": {
                "fragments": [],
                "text": "Boilerplate detection using shallow text features"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper analyzes a small set of shallow text features for classifying the individual text elements in a Web page and derives a simple and plausible stochastic model for describing the boilerplate creation process."
            },
            "venue": {
                "fragments": [],
                "text": "WSDM '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016692"
                        ],
                        "name": "S. Selkow",
                        "slug": "S.-Selkow",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Selkow",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Selkow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 145
                            }
                        ],
                        "text": "A computationally efficient solution for the problem of the tree edit distance matching is provided by the algorithm called simple tree matching [107], and its variants."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20123824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e2e47f3748368797c9d51b08e938dfb930b97c3",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Tree-to-Tree-Editing-Problem-Selkow",
            "title": {
                "fragments": [],
                "text": "The Tree-to-Tree Editing Problem"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Lett."
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2362490"
                        ],
                        "name": "Oana Goga",
                        "slug": "Oana-Goga",
                        "structuredName": {
                            "firstName": "Oana",
                            "lastName": "Goga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oana Goga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37112758"
                        ],
                        "name": "Howard Lei",
                        "slug": "Howard-Lei",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Lei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Howard Lei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2181961"
                        ],
                        "name": "S. Parthasarathi",
                        "slug": "S.-Parthasarathi",
                        "structuredName": {
                            "firstName": "Sree",
                            "lastName": "Parthasarathi",
                            "middleNames": [
                                "Hari",
                                "Krishnan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Parthasarathi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797144"
                        ],
                        "name": "G. Friedland",
                        "slug": "G.-Friedland",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Friedland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Friedland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690799"
                        ],
                        "name": "Robin Sommer",
                        "slug": "Robin-Sommer",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Sommer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robin Sommer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052907099"
                        ],
                        "name": "Renata Teixeira",
                        "slug": "Renata-Teixeira",
                        "structuredName": {
                            "firstName": "Renata",
                            "lastName": "Teixeira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Renata Teixeira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4811873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dbb7ad4a506134c2fd59d214f89e02d8984045a",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We study how potential attackers can identify accounts on different social network sites that all belong to the same user, exploiting only innocuous activity that inherently comes with posted content. We examine three specific features on Yelp, Flickr, and Twitter: the geo-location attached to a user's posts, the timestamp of posts, and the user's writing style as captured by language models. We show that among these three features the location of posts is the most powerful feature to identify accounts that belong to the same user in different sites. When we combine all three features, the accuracy of identifying Twitter accounts that belong to a set of Flickr users is comparable to that of existing attacks that exploit usernames. Our attack can identify 37% more accounts than using usernames when we instead correlate Yelp and Twitter. Our results have significant privacy implications as they present a novel class of attacks that exploit users' tendency to assume that, if they maintain different personas with different names, the accounts cannot be linked together; whereas we show that the posts themselves can provide enough information to correlate the accounts."
            },
            "slug": "Exploiting-innocuous-activity-for-correlating-users-Goga-Lei",
            "title": {
                "fragments": [],
                "text": "Exploiting innocuous activity for correlating users across sites"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The results have significant privacy implications as they present a novel class of attacks that exploit users' tendency to assume that, if they maintain different personas with different names, the accounts cannot be linked together; whereas it is shown that the posts themselves can provide enough information to correlate the accounts."
            },
            "venue": {
                "fragments": [],
                "text": "WWW"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35497150"
                        ],
                        "name": "Christo Wilson",
                        "slug": "Christo-Wilson",
                        "structuredName": {
                            "firstName": "Christo",
                            "lastName": "Wilson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christo Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2176723"
                        ],
                        "name": "B. Boe",
                        "slug": "B.-Boe",
                        "structuredName": {
                            "firstName": "Bryce",
                            "lastName": "Boe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47194599"
                        ],
                        "name": "A. Sala",
                        "slug": "A.-Sala",
                        "structuredName": {
                            "firstName": "Alessandra",
                            "lastName": "Sala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798598"
                        ],
                        "name": "K. Puttaswamy",
                        "slug": "K.-Puttaswamy",
                        "structuredName": {
                            "firstName": "Krishna",
                            "lastName": "Puttaswamy",
                            "middleNames": [
                                "P.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Puttaswamy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145970007"
                        ],
                        "name": "Ben Y. Zhao",
                        "slug": "Ben-Y.-Zhao",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Zhao",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Y. Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "Due to these reasons, it has been applied in a large number of studies about the topology and structure of Online Social Networks (see, for instance, [20,119,52,123,17])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2552617,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a43ae25752f7b088f1bb92f0d5df1badab74d08",
            "isKey": false,
            "numCitedBy": 968,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Social networks are popular platforms for interaction, communication and collaboration between friends. Researchers have recently proposed an emerging class of applications that leverage relationships from social networks to improve security and performance in applications such as email, web browsing and overlay routing. While these applications often cite social network connectivity statistics to support their designs, researchers in psychology and sociology have repeatedly cast doubt on the practice of inferring meaningful relationships from social network connections alone.\n This leads to the question: Are social links valid indicators of real user interaction? If not, then how can we quantify these factors to form a more accurate model for evaluating socially-enhanced applications? In this paper, we address this question through a detailed study of user interactions in the Facebook social network. We propose the use of interaction graphs to impart meaning to online social links by quantifying user interactions. We analyze interaction graphs derived from Facebook user traces and show that they exhibit significantly lower levels of the \"small-world\" properties shown in their social graph counterparts. This means that these graphs have fewer \"supernodes\" with extremely high degree, and overall network diameter increases significantly as a result. To quantify the impact of our observations, we use both types of graphs to validate two well-known social-based applications (RE and SybilGuard). The results reveal new insights into both systems, and confirm our hypothesis that studies of social applications should use real indicators of user interactions in lieu of social graphs."
            },
            "slug": "User-interactions-in-social-networks-and-their-Wilson-Boe",
            "title": {
                "fragments": [],
                "text": "User interactions in social networks and their implications"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes the use of interaction graphs to impart meaning to online social links by quantifying user interactions, and uses both types of graphs to validate two well-known social-based applications (RE and SybilGuard)."
            },
            "venue": {
                "fragments": [],
                "text": "EuroSys '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49297224"
                        ],
                        "name": "W. Winkler",
                        "slug": "W.-Winkler",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Winkler",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Winkler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To this extent, some techniques, called tree-matching strategies are a good candidate to detect similarities between two tree and they will be discussed in detail in the next sections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7844523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54dc61b6d5154bee538605bae023a1f7c5999c4a",
            "isKey": false,
            "numCitedBy": 992,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides an overview of methods and systems developed for record linkage. Modern record linkage begins with the pioneering work of Newcombe and is especially based on the formal mathematical model of Fellegi and Sunter. In their seminal work, Fellegi and Sunter introduced many powerful ideas for estimating record linkage parameters and other ideas that still influence record linkage today. Record linkage research is characterized by its synergism of statistics, computer science, and operations research. Many difficult algorithms have been developed and put in software systems. Record linkage practice is still very limited. Some limits are due to existing software. Other limits are due to the difficulty in automatically estimating matching parameters and error rates, with current research highlighted by the work of Larsen and Rubin."
            },
            "slug": "The-State-of-Record-Linkage-and-Current-Research-Winkler",
            "title": {
                "fragments": [],
                "text": "The State of Record Linkage and Current Research Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper provides an overview of methods and systems developed for record linkage based on the formal mathematical model of Fellegi and Sunter, and highlights the work of Larsen and Rubin."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730314"
                        ],
                        "name": "S. Flesca",
                        "slug": "S.-Flesca",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Flesca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Flesca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684745"
                        ],
                        "name": "G. Gottlob",
                        "slug": "G.-Gottlob",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Gottlob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gottlob"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To this extent, some techniques, called tree-matching strategies are a good candidate to detect similarities between two tree and they will be discussed in detail in the next sections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41721050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f01f5a819151969f33aafc14e24bb874edef7318",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper illustrates some aspects of the visual wrapper generation tool Lixto and describes its internal declarative logic-based language Elog. In particular, it gives an example scenarioand contains a detailed description of predicates including their input/output behavior and introduces several new conditions. Additionally, entity relationship diagrams of filters and patterns are depicted and some words on the implementation are issued. Finally, some possible ramifications are discussed."
            },
            "slug": "The-Elog-Web-Extraction-Language-Baumgartner-Flesca",
            "title": {
                "fragments": [],
                "text": "The Elog Web Extraction Language"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This paper illustrates some aspects of the visual wrapper generation tool Lixto and describes its internal declarative logic-based language Elog and contains a detailed description of predicates including their input/output behavior and introduces several new conditions."
            },
            "venue": {
                "fragments": [],
                "text": "LPAR"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094210584"
                        ],
                        "name": "Oana",
                        "slug": "Oana",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Oana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104238900"
                        ],
                        "name": "Goga",
                        "slug": "Goga",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Goga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Goga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 45
                            }
                        ],
                        "text": "An interesting study is finally presented in [Goga et al. 2012]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 31
                            }
                        ],
                        "text": "In particular, the analysis of [Goga et al. 2012] focused on three features of online activity like the geo-location of users posts, the timestamp of posts, and the user\u2019s writing style (captured by proper language models)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10459074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4f54a8334a71b63f51a91af5e26348d186187f5",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Online Privacy \"). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. Abstract This paper studies whether it is possible to identify accounts on different social networks that belong to the same user just by using publicly available information in a user's posts. In particular, we explore three features to capture a user's online activity: the geo\u2010location attached to a user's posts, the timestamp of posts, and the user's writing style as captured by language models. Our analysis, based on correlating user accounts across Yelp, Flickr, and Twitter, shows that such otherwise innocuous features can indeed enable attackers to track users across site boundaries. This result has significant privacy implications as users tend to rely on an implicit notion that social networks remain separate realms. Moreover, current privacy controls remain insufficient to contain the risk of cross\u2010site correlation."
            },
            "slug": "Exploiting-Innocuous-User-Activity-for-Correlating-Oana-Goga",
            "title": {
                "fragments": [],
                "text": "Exploiting Innocuous User Activity for Correlating Accounts Across Social Network Sites"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Whether it is possible to identify accounts on different social networks that belong to the same user just by using publicly available information in a user's posts is studied, showing that such otherwise innocuous features can indeed enable attackers to track users across site boundaries."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819393"
                        ],
                        "name": "Fotis Kokkoras",
                        "slug": "Fotis-Kokkoras",
                        "structuredName": {
                            "firstName": "Fotis",
                            "lastName": "Kokkoras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fotis Kokkoras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2908173"
                        ],
                        "name": "Efstratia Lampridou",
                        "slug": "Efstratia-Lampridou",
                        "structuredName": {
                            "firstName": "Efstratia",
                            "lastName": "Lampridou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Efstratia Lampridou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2280041"
                        ],
                        "name": "Konstantinos Ntonas",
                        "slug": "Konstantinos-Ntonas",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Ntonas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konstantinos Ntonas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697941"
                        ],
                        "name": "I. Vlahavas",
                        "slug": "I.-Vlahavas",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Vlahavas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Vlahavas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "As an example, we can cite MOpiS (Multiple Opinion Summarizer) [71], an algorithm that generates summaries of reviews associated with commercial products by taking into account both the review content and some metadata (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18133542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59c0de86e31131e2bb141eeb457a706919c5db15",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Product reviews written by on-line shoppers is a valuable source of information for potential new customers who desire to make an informed purchase decision. Manually processing quite a few dozens, or even hundreds, of reviews for a single product is tedious and time consuming. Although there exist mature and generic text summarization techniques, they are focused primarily on article type content and do not perform well on short and usually repetitive snippets of text found at on-line shops. In this paper, we propose MOpiS, a multiple opinion summarization algorithm that generates improved summaries of product reviews by taking into consideration metadata information that usually accompanies the on-line review text. We demonstrate the effectiveness of our approach with experimental results."
            },
            "slug": "MOpiS:-A-Multiple-Opinion-Summarizer-Kokkoras-Lampridou",
            "title": {
                "fragments": [],
                "text": "MOpiS: A Multiple Opinion Summarizer"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes MOpiS, a multiple opinion summarization algorithm that generates improved summaries of product reviews by taking into consideration metadata information that usually accompanies the on-line review text."
            },
            "venue": {
                "fragments": [],
                "text": "SETN"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29829232"
                        ],
                        "name": "J. Wang",
                        "slug": "J.-Wang",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Wang",
                            "middleNames": [
                                "Tsong-Li"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694805"
                        ],
                        "name": "B. Shapiro",
                        "slug": "B.-Shapiro",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Shapiro",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shapiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695878"
                        ],
                        "name": "D. Shasha",
                        "slug": "D.-Shasha",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Shasha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shasha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698666"
                        ],
                        "name": "Kaizhong Zhang",
                        "slug": "Kaizhong-Zhang",
                        "structuredName": {
                            "firstName": "Kaizhong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaizhong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2712717"
                        ],
                        "name": "K. Currey",
                        "slug": "K.-Currey",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "Currey",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Currey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "A number of techniques to approach this problem have been proposed [116,23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14697648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e9d658a686d1e953803779dcf143ea66f9eeee1",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Ordered, labeled trees are trees in which each node has a label and the left-to-right order of its children (if it has any) is fixed. Such trees have many applications in vision, pattern recognition, molecular biology and natural language processing. We consider a substructure of an ordered labeled tree T to be a connected subgraph of T. Given two ordered labeled trees T/sub 1/ and T/sub 2/ and an integer d, the largest approximately common substructure problem is to find a substructure U/sub 1/ of T/sub 1/ and a substructure U/sub 2/ of T/sub 2/ such that U/sub 1/ is within edit distance d of U/sub 2/ and where there does not exist any other substructure V/sub 1/ of T/sub 1/ and V/sub 2/ of T/sub 2/ such that V/sub 1/ and V/sub 2/ satisfy the distance constraint and the sum of the sizes of V/sub 1/ and V/sub 2/ is greater than the sum of the sizes of U/sub 1/ and U/sub 2/. We present a dynamic programming algorithm to solve this problem, which runs as fast as the fastest known algorithm for computing the edit distance of two trees when the distance allowed in the common substructures is a constant independent of the input trees. To demonstrate the utility of our algorithm, we discuss its application to discovering motifs in multiple RNA secondary structures (which are ordered labeled trees)."
            },
            "slug": "An-Algorithm-for-Finding-the-Largest-Approximately-Wang-Shapiro",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Finding the Largest Approximately Common Substructures of Two Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A dynamic programming algorithm is presented that runs as fast as the fastest known algorithm for computing the edit distance of two trees when the distance allowed in the common substructures is a constant independent of the input trees."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46867608"
                        ],
                        "name": "Yu Zhang",
                        "slug": "Yu-Zhang",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144948990"
                        ],
                        "name": "Bin Cao",
                        "slug": "Bin-Cao",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bin Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739816"
                        ],
                        "name": "D. Yeung",
                        "slug": "D.-Yeung",
                        "structuredName": {
                            "firstName": "Dit-Yan",
                            "lastName": "Yeung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Yeung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2009b] and, finally, other approaches project the user and item space in the various domains in a shared latent space by means of regularization techniques [Zhang et al. 2010]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, it has been adopted in several scenarios [Kim et al. 2007; Zhai and Liu 2005; Zhang et al. 2010; Ferrara and Baumgartner 2011a; 2011b; 2011c]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1025328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93ac8d0782392cd450d25f45539d8a5c5274ada2",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Collaborative filtering is an effective recommendation approach in which the preference of a user on an item is predicted based on the preferences of other users with similar interests. A big challenge in using collaborative filtering methods is the data sparsity problem which often arises because each user typically only rates very few items and hence the rating matrix is extremely sparse. In this paper, we address this problem by considering multiple collaborative filtering tasks in different domains simultaneously and exploiting the relationships between domains. We refer to it as a multi-domain collaborative filtering (MCF) problem. To solve the MCF problem, we propose a probabilistic framework which uses probabilistic matrix factorization to model the rating problem in each domain and allows the knowledge to be adaptively transferred across different domains by automatically learning the correlation between domains. We also introduce the link function for different domains to correct their biases. Experiments conducted on several real-world applications demonstrate the effectiveness of our methods when compared with some representative methods."
            },
            "slug": "Multi-Domain-Collaborative-Filtering-Zhang-Cao",
            "title": {
                "fragments": [],
                "text": "Multi-Domain Collaborative Filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A Probabilistic framework is proposed which uses probabilistic matrix factorization to model the rating problem in each domain and allows the knowledge to be adaptively transferred across different domains by automatically learning the correlation between domains."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144463004"
                        ],
                        "name": "Daniel M. Romero",
                        "slug": "Daniel-M.-Romero",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Romero",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel M. Romero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6794729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "478e23c417b578b8e578ba50419e118684c43796",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "It has often been taken as a working assumption that directed links in information networks are frequently formed by \u201cshort-cutting\u201d a two-step path between the source and the destination \u2014 a kind of implicit \u201clink copying\u201d analogous to the process of triadic closure in social networks. Despite the role of this assumption in theoretical models such as preferential attachment, it has received very little direct empirical investigation. Here we develop a formalization and methodology for studying this type of directed closure process, and we provide evidence for its important role in the formation of links on Twitter. We then analyze a sequence of models designed to capture the structural phenomena related to directed closure that we observe in the Twitter data."
            },
            "slug": "The-Directed-Closure-Process-in-Hybrid-Networks,-an-Romero-Kleinberg",
            "title": {
                "fragments": [],
                "text": "The Directed Closure Process in Hybrid Social-Information Networks, with an Analysis of Link Formation on Twitter"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Here a formalization and methodology for studying this type of directed closure process is developed, and evidence for its important role in the formation of links on Twitter is provided."
            },
            "venue": {
                "fragments": [],
                "text": "ICWSM"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2615186"
                        ],
                        "name": "Chaabane Abdelberi",
                        "slug": "Chaabane-Abdelberi",
                        "structuredName": {
                            "firstName": "Chaabane",
                            "lastName": "Abdelberi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaabane Abdelberi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3265558"
                        ],
                        "name": "G. \u00c1cs",
                        "slug": "G.-\u00c1cs",
                        "structuredName": {
                            "firstName": "Gergely",
                            "lastName": "\u00c1cs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. \u00c1cs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708760"
                        ],
                        "name": "M. K\u00e2afar",
                        "slug": "M.-K\u00e2afar",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "K\u00e2afar",
                            "middleNames": [
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. K\u00e2afar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 3
                            }
                        ],
                        "text": "In [Chaabane et al. 2012] the authors considers a range of user signals expressing user interests (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11490787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfd53f3f786490c28f73f92d74d430f0880a5686",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose that a Facebook user, whose age is hidden or missing, likes Britney Spears. Can you guess his/her age? Knowing that most Britney fans are teenagers, it is fairly easy for humans to answer this question. Interests (or \"likes\") of users is one of the highly-available on-line information. In this paper, we show how these seemingly harmless interests (e.g., music interests) can leak privacy sensitive information about users. In particular, we infer their undisclosed (private) attributes using the public attributes of other users sharing similar interests. In order to compare user-defined interest names, we extract their semantics using an ontologized version of Wikipedia and measure their similarity by applying a statistical learning method. Besides self-declared interests in music, our technique does not rely on any further information about users such as friend relationships or group belongings. Our experiments, based on more than 104K public profiles collected from Facebook and more than 2000 private profiles provided by volunteers, show that our inference technique efficiently predicts attributes that are very often hidden by users. To the best of our knowledge, this is the first time that user interests are used for profiling, and more generally, semantics-driven inference of private data is addressed."
            },
            "slug": "You-are-what-you-like!-Information-leakage-through-Abdelberi-\u00c1cs",
            "title": {
                "fragments": [],
                "text": "You are what you like! Information leakage through users' Interests"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows how seemingly harmless interests of users can leak privacy sensitive information about users, and for the first time that user interests are used for profiling, and semantics-driven inference of private data is addressed."
            },
            "venue": {
                "fragments": [],
                "text": "NDSS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803792"
                        ],
                        "name": "M. Kurant",
                        "slug": "M.-Kurant",
                        "structuredName": {
                            "firstName": "Maciej",
                            "lastName": "Kurant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kurant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706319"
                        ],
                        "name": "A. Markopoulou",
                        "slug": "A.-Markopoulou",
                        "structuredName": {
                            "firstName": "Athina",
                            "lastName": "Markopoulou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Markopoulou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3250409"
                        ],
                        "name": "P. Thiran",
                        "slug": "P.-Thiran",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Thiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Thiran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2434575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a88579f99fd40873f8426b4b4ae122ae9bab075",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Breadth First Search (BFS) and other graph traversal techniques are widely used for measuring large unknown graphs, such as online social networks. It has been empirically observed that incomplete BFS is biased toward high degree nodes. In contrast to more studied sampling techniques, such as random walks, the bias of BFS has not been characterized to date. In this paper, we quantify the degree bias of BFS sampling. In particular, we calculate the node degree distribution expected to be observed by BFS as a function of the fraction of covered nodes, in a random graph RG(pk) with a given (and arbitrary) degree distribution pk. Furthermore, we also show that, for RG(pk), all commonly used graph traversal techniques (BFS, DFS, Forest Fire, and Snowball Sampling) lead to the same bias, and we show how to correct for this bias. To give a broader perspective, we compare this class of exploration techniques to random walks that are well-studied and easier to analyze. Next, we study by simulation the effect of graph properties not captured directly by our model. We find that the bias gets amplified in graphs with strong positive assortativity. Finally, we demonstrate the above results by sampling the Facebook social network, and we provide some practical guidelines for graph sampling in practice."
            },
            "slug": "On-the-bias-of-BFS-(Breadth-First-Search)-Kurant-Markopoulou",
            "title": {
                "fragments": [],
                "text": "On the bias of BFS (Breadth First Search)"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper quantify the degree bias of BFS sampling, and calculates the node degree distribution expected to be observed by BFS as a function of the fraction of covered nodes, in a random graph RG(pk) with a given degree distribution pk."
            },
            "venue": {
                "fragments": [],
                "text": "2010 22nd International Teletraffic Congress (lTC 22)"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2752625"
                        ],
                        "name": "Minas Gjoka",
                        "slug": "Minas-Gjoka",
                        "structuredName": {
                            "firstName": "Minas",
                            "lastName": "Gjoka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minas Gjoka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803792"
                        ],
                        "name": "M. Kurant",
                        "slug": "M.-Kurant",
                        "structuredName": {
                            "firstName": "Maciej",
                            "lastName": "Kurant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kurant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3014984"
                        ],
                        "name": "C. Butts",
                        "slug": "C.-Butts",
                        "structuredName": {
                            "firstName": "Carter",
                            "lastName": "Butts",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Butts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706319"
                        ],
                        "name": "A. Markopoulou",
                        "slug": "A.-Markopoulou",
                        "structuredName": {
                            "firstName": "Athina",
                            "lastName": "Markopoulou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Markopoulou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[52] considered different visiting algorithms, like BFS, \u2018\u2018Random Walks\u2019\u2019 and \u2018\u2018Metropolis\u2013Hastings Random Walks\u2019\u2019."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "For example, collecting digital traces produced by users of Social Web platforms like Facebook, YouTube or Flickr is the key step to understand, model and predict human behavior [68, 94, 3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 74
                            }
                        ],
                        "text": "Most of them have been discussed and exploited in the context of Facebook [52,17] but, unfortunately, some of them cannot be extended to other platforms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 310,
                                "start": 303
                            }
                        ],
                        "text": "Web Data Extraction systems find extensive use in a wide range of applications including the analysis of text-based documents available to a company (like e-mails, support forums, technical and legal documentation, and so on), Business and Competitive Intelligence [9], crawling of Social Web platforms [17,52], BioInformatics [99] and so on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "Due to these reasons, it has been applied in a large number of studies about the topology and structure of Online Social Networks (see, for instance, [20,119,52,123,17])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 189
                            }
                        ],
                        "text": "As for the Social Web domain, extracting data from Social Web sites/Social Media is a relatively new concept that has acquired a big relevance due to the explosive growth of platforms like Facebook, Twitter or Instagram."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 58
                            }
                        ],
                        "text": "In case of Social Web applications, many approaches (like [52,17,90,123]) are devoted to extract friendship relationships from a Social Networking Web sites."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12840167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4903aea23b66bf0680430f52fe4f5fa88379580",
            "isKey": true,
            "numCitedBy": 683,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "With more than 250 million active users, Facebook (FB) is currently one of the most important online social networks. Our goal in this paper is to obtain a representative (unbiased) sample of Facebook users by crawling its social graph. In this quest, we consider and implement several candidate techniques. Two approaches that are found to perform well are the Metropolis-Hasting random walk (MHRW) and a re-weighted random walk (RWRW). Both have pros and cons, which we demonstrate through a comparison to each other as well as to the \"ground-truth\" (UNI - obtained through true uniform sampling of FB userIDs). In contrast, the traditional Breadth-First-Search (BFS) and Random Walk (RW) perform quite poorly, producing substantially biased results. In addition to offline performance assessment, we introduce online formal convergence diagnostics to assess sample quality during the data collection process. We show how these can be used to effectively determine when a random walk sample is of adequate size and quality for subsequent use (i.e., when it is safe to cease sampling). Using these methods, we collect the first, to the best of our knowledge, unbiased sample of Facebook. Finally, we use one of our representative datasets, collected through MHRW, to characterize several key properties of Facebook."
            },
            "slug": "Walking-in-Facebook:-A-Case-Study-of-Unbiased-of-Gjoka-Kurant",
            "title": {
                "fragments": [],
                "text": "Walking in Facebook: A Case Study of Unbiased Sampling of OSNs"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The goal in this paper is to obtain a representative (unbiased) sample of Facebook users by crawling its social graph using several candidate techniques, and introduces online formal convergence diagnostics to assess sample quality during the data collection process."
            },
            "venue": {
                "fragments": [],
                "text": "2010 Proceedings IEEE INFOCOM"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793506"
                        ],
                        "name": "Duen Horng Chau",
                        "slug": "Duen-Horng-Chau",
                        "structuredName": {
                            "firstName": "Duen Horng",
                            "lastName": "Chau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duen Horng Chau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40652387"
                        ],
                        "name": "Shashank Pandit",
                        "slug": "Shashank-Pandit",
                        "structuredName": {
                            "firstName": "Shashank",
                            "lastName": "Pandit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shashank Pandit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108752064"
                        ],
                        "name": "Samuel Wang",
                        "slug": "Samuel-Wang",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702392"
                        ],
                        "name": "C. Faloutsos",
                        "slug": "C.-Faloutsos",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Faloutsos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Faloutsos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "Due to these reasons, it has been applied in a large number of studies about the topology and structure of Online Social Networks (see, for instance, [20,119,52,123,17])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8257722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e22e9b852ac8d7e0c289b8c1409e152ddbd0c8ad",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a huge online social network, how do we retrieve information from it through crawling? Even better, how do we improve the crawling performance by using parallel crawlers that work independently? In this paper, we present the framework of parallel crawlers for online social networks, utilizing a centralized queue. To show how this works in practice, we describe our implementation of the crawlers for an online auction website. The crawlers work independently, therefore the failing of one crawler does not affect the others at all. The framework ensures that no redundant crawling would occur. Using the crawlers that we built, we visited a total of approximately 11 million auction users, about 66,000 of which were completely crawled."
            },
            "slug": "Parallel-crawling-for-online-social-networks-Chau-Pandit",
            "title": {
                "fragments": [],
                "text": "Parallel crawling for online social networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents the framework of parallel crawler for online social networks, utilizing a centralized queue, and describes the implementation of the crawlers for an online auction website."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145170932"
                        ],
                        "name": "Yue Shi",
                        "slug": "Yue-Shi",
                        "structuredName": {
                            "firstName": "Yue",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yue Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145980903"
                        ],
                        "name": "M. Larson",
                        "slug": "M.-Larson",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Larson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Larson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718099"
                        ],
                        "name": "A. Hanjalic",
                        "slug": "A.-Hanjalic",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Hanjalic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanjalic"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10021109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e9d572a7fdbbd0fd414f67fb4f76a1f978822b9",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recommender systems generally face the challenge of making predictions using only the relatively few user ratings available for a given domain. Cross-domain collaborative filtering (CF) aims to alleviate the effects of this data sparseness by transferring knowledge from other domains. We propose a novel algorithm, Tag-induced Cross-Domain Collaborative Filtering (TagCDCF), which exploits user-contributed tags that are common to multiple domains in order to establish the cross-domain links necessary for successful cross-domain CF. TagCDCF extends the state-of-the-art matrix factorization by introducing a constraint involving tag-based similarities between pairs of users and pairs of items across domains. The method requires no common users or items across domains. Using two publicly available CF data sets as different domains, we experimentally demonstrate that TagCDCF substantially outperforms other state-of-the-art single domain CF and cross-domain CF approaches. Additional experiments show that TagCDCF addresses data sparseness and illustrate the influence of the number of tags used by users in both domains."
            },
            "slug": "Tags-as-bridges-between-domains:-improving-with-Shi-Larson",
            "title": {
                "fragments": [],
                "text": "Tags as bridges between domains: improving recommendation with tag-induced cross-domain collaborative filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a novel algorithm, Tag-induced Cross-Domain Collaborative Filtering (TagCDCF), which exploits user-contributed tags that are common to multiple domains in order to establish the cross-domain links necessary for successful cross- domain CF."
            },
            "venue": {
                "fragments": [],
                "text": "UMAP'11"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612372"
                        ],
                        "name": "L. Backstrom",
                        "slug": "L.-Backstrom",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Backstrom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Backstrom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[26] investigated how to organize a large collection of geotagged photos extracted from Flickr."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 98
                            }
                        ],
                        "text": "Other studies entwining users geographical locations with the content they posted are provided in [26,67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2106641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a4dc773c76b016542206375eef7d8fe79fb7cd4",
            "isKey": false,
            "numCitedBy": 858,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate how to organize a large collection of geotagged photos, working with a dataset of about 35 million images collected from Flickr. Our approach combines content analysis based on text tags and image data with structural analysis based on geospatial data. We use the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places. We then study the interplay between this structure and the content, using classification methods for predicting such locations from visual, textual and temporal features of the photos. We find that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features. We illustrate using these techniques to organize a large photo collection, while also revealing various interesting properties about popular cities and landmarks at a global scale."
            },
            "slug": "Mapping-the-world's-photos-Crandall-Backstrom",
            "title": {
                "fragments": [],
                "text": "Mapping the world's photos"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work uses the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places, and finds that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699245"
                        ],
                        "name": "T. Winograd",
                        "slug": "T.-Winograd",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Winograd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Winograd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "that a rich semantic structure emerges from the Web pages. If such an hypothesis holds true, techniques from Information Extraction (IE) and Natural Language Processing (NLP) can be conveniently used [121, 13, 84]. The range of applications beneting from NLP techniques comprises relevant examples both in the enterprise and Social Web scenarios: for instance, relevant applications of NLP/IE techniques are the "
                    },
                    "intents": []
                }
            ],
            "corpusId": 56798209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb20f121c979b535bbeade5ac06676d627d4ad7d",
            "isKey": false,
            "numCitedBy": 2455,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper describes a computer system for understanding English. The system answers questions, executes commands, and accepts information in an interactive English dialog. It is based on the belief that in modeling language understanding, we must deal in an integrated way with all of the aspects of language\u2014syntax, semantics, and inference. The system contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system. We assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing. Therefore, the program is given a detailed model of a particular domain. In addition, the system has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carrying them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, asking for clarification when its heuristic programs cannot understand a sentence through the use of syntactic, semantic, contextual, and physical knowledge. Knowledge in the system is represented in the form of procedures, rather than tables of rules or lists of patterns. By developing special procedural representations for syntax, semantics, and inference, we gain flexibility and power. Since each piece of knowledge can be a procedure, it can call directly on any other piece of knowledge in the system."
            },
            "slug": "Understanding-natural-language-Winograd",
            "title": {
                "fragments": [],
                "text": "Understanding natural language"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A computer system for understanding English that contains a parser, a recognition grammar of English, programs for semantic analysis, and a general problem solving system based on the belief that in modeling language understanding, it must deal in an integrated way with all of the aspects of language\u2014syntax, semantics, and inference."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281410"
                        ],
                        "name": "R. Zafarani",
                        "slug": "R.-Zafarani",
                        "structuredName": {
                            "firstName": "Reza",
                            "lastName": "Zafarani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zafarani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38746648"
                        ],
                        "name": "Huan Liu",
                        "slug": "Huan-Liu",
                        "structuredName": {
                            "firstName": "Huan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huan Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "In [124], the authors studied 12 different Social Web systems (like Delicious, Flickr and YouTube) with the goal of finding a mapping involving the different user accounts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2990641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b57821eeba73b2524b087f75a09ca160a2c07577",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most interesting challenges in the area of social computing and social media analysis is the so-called community analysis. A well known barrier in cross-community (multiple website) analysis is the disconnectedness of these websites. In this paper, our aim is to provide evidence on the existence of a mapping among identities across multiple communities, providing a method for connecting these websites. Our studies have shown that simple, yet effective approaches, which leverage social media\u2019s collective patterns can be utilized to find such a mapping. The employed methods successfully reveal this mapping with 66% accuracy."
            },
            "slug": "Connecting-Corresponding-Identities-across-Zafarani-Liu",
            "title": {
                "fragments": [],
                "text": "Connecting Corresponding Identities across Communities"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper's aim is to provide evidence on the existence of a mapping among identities across multiple communities, providing a method for connecting these websites."
            },
            "venue": {
                "fragments": [],
                "text": "ICWSM"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365265"
                        ],
                        "name": "S. Spiegel",
                        "slug": "S.-Spiegel",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Spiegel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Spiegel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697769"
                        ],
                        "name": "J\u00e9r\u00f4me Kunegis",
                        "slug": "J\u00e9r\u00f4me-Kunegis",
                        "structuredName": {
                            "firstName": "J\u00e9r\u00f4me",
                            "lastName": "Kunegis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J\u00e9r\u00f4me Kunegis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146327490"
                        ],
                        "name": "Fang Li",
                        "slug": "Fang-Li",
                        "structuredName": {
                            "firstName": "Fang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fang Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 32
                            }
                        ],
                        "text": "presented an analogous study in [110]; in that paper the authors combine user ratings (coming from MovieLens database) with movie information provided by IMDB."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16427241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24d0db6802e4a23fba3e8ee518f73a92a1a921db",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses the combination of collaborative and content-based filtering in the context of web-based recommender systems. In particular, we link the well-known MovieLens rating data with supplementary IMDB content information. The resulting network of user-item relations and associated content features is converted into a unified mathematical model, which is applicable to our underlying neighbor-based prediction algorithm. By means of various experiments, we demonstrate the influence of supplementary user as well as item features on the prediction accuracy of Hydra, our proposed hybrid recommender. In order to decrease system runtime and to reveal latent user and item relations, we factorize our hybrid model via singular value decomposition (SVD)."
            },
            "slug": "Hydra:-a-hybrid-recommender-system-[cross-linked-Spiegel-Kunegis",
            "title": {
                "fragments": [],
                "text": "Hydra: a hybrid recommender system [cross-linked rating and content information]"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper links the well-known MovieLens rating data with supplementary IMDB content information and factorizes the hybrid model via singular value decomposition (SVD) in order to decrease system runtime and to reveal latent user and item relations."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM-CNIKM"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48898287"
                        ],
                        "name": "Emilio Ferrara",
                        "slug": "Emilio-Ferrara",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ferrara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Ferrara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2307347"
                        ],
                        "name": "Onur Varol",
                        "slug": "Onur-Varol",
                        "structuredName": {
                            "firstName": "Onur",
                            "lastName": "Varol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Onur Varol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653472"
                        ],
                        "name": "F. Menczer",
                        "slug": "F.-Menczer",
                        "structuredName": {
                            "firstName": "Filippo",
                            "lastName": "Menczer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Menczer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769960"
                        ],
                        "name": "A. Flammini",
                        "slug": "A.-Flammini",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Flammini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Flammini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 108
                            }
                        ],
                        "text": ") to analyze and understand the dynamics of human behaviors at a planetary scale and in a real time-fashion [38,25,33,43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8250562,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "1e3dc1eaace706cb384f12be60ae39fd558e1361",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Trending topics are the online conversations that grab collective attention on social media. They are continually changing and often reflect exogenous events that happen in the real world. Trends are localized in space and time as they are driven by activity in specific geographic areas that act as sources of traffic and information flow. Taken independently, trends and geography have been discussed in recent literature on online social media; although, so far, little has been done to characterize the relation between trends and geography. Here we investigate more than eleven thousand topics that trended on Twitter in 63 main US locations during a period of 50 days in 2013. This data allows us to study the origins and pathways of trends, how they compete for popularity at the local level to emerge as winners at the country level, and what dynamics underlie their production and consumption in different geographic areas. We identify two main classes of trending topics: those that surface locally, coinciding with three different geographic clusters (East coast, Midwest and Southwest); and those that emerge globally from several metropolitan areas, coinciding with the major air traffic hubs of the country. These hubs act as trendsetters, generating topics that eventually trend at the country level, and driving the conversation across the country. This poses an intriguing conjecture, drawing a parallel between the spread of information and diseases: Do trends travel faster by airplane than over the Internet?"
            },
            "slug": "Traveling-trends:-social-butterflies-or-frequent-Ferrara-Varol",
            "title": {
                "fragments": [],
                "text": "Traveling trends: social butterflies or frequent fliers?"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work investigates more than eleven thousand topics that trended on Twitter in 63 main US locations during a period of 50 days in 2013 to study the origins and pathways of trends, how they compete for popularity at the local level to emerge as winners at the country level, and what dynamics underlie their production and consumption in different geographic areas."
            },
            "venue": {
                "fragments": [],
                "text": "COSN '13"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3139587"
                        ],
                        "name": "Marco Descher",
                        "slug": "Marco-Descher",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Descher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Descher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2452555"
                        ],
                        "name": "T. Feilhauer",
                        "slug": "T.-Feilhauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Feilhauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Feilhauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2553074"
                        ],
                        "name": "Thomas Ludescher",
                        "slug": "Thomas-Ludescher",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Ludescher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Ludescher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20474250"
                        ],
                        "name": "P. Masser",
                        "slug": "P.-Masser",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Masser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Masser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145044635"
                        ],
                        "name": "B. Wenzel",
                        "slug": "B.-Wenzel",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Wenzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wenzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701649"
                        ],
                        "name": "P. Brezany",
                        "slug": "P.-Brezany",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brezany",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brezany"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074412175"
                        ],
                        "name": "Ibrahim Elsayed",
                        "slug": "Ibrahim-Elsayed",
                        "structuredName": {
                            "firstName": "Ibrahim",
                            "lastName": "Elsayed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ibrahim Elsayed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35292685"
                        ],
                        "name": "A. W\u00f6hrer",
                        "slug": "A.-W\u00f6hrer",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "W\u00f6hrer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. W\u00f6hrer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145942203"
                        ],
                        "name": "A. Tjoa",
                        "slug": "A.-Tjoa",
                        "structuredName": {
                            "firstName": "A",
                            "lastName": "Tjoa",
                            "middleNames": [
                                "Min"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tjoa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2025814"
                        ],
                        "name": "David Huemer",
                        "slug": "David-Huemer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Huemer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Huemer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "to [Descher et al. 2009] for a selected scientific computing project."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1637306,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "1b0257181d927508db45c09d83fd9d9b28eec9c3",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Within the Austrian Grid project phase 2, three different groups, each allocated to a different work package, join their efforts to implement a grid infrastructure for the european research project \"Breath Gas Analysis for molecular oriented diseases''. This position paper provides background on the task and the resulting requirements, a presentation on solutions developed during related projects in the application domain, identifies problems that have not yet been solved, and finally presents the intended solution to be developed."
            },
            "slug": "Position-Paper:-Secure-Infrastructure-for-Data-Life-Descher-Feilhauer",
            "title": {
                "fragments": [],
                "text": "Position Paper: Secure Infrastructure for Scientific Data Life Cycle Management"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This position paper provides background on the task and the resulting requirements, a presentation on solutions developed during related projects in the application domain, identifies problems that have not yet been solved, and finally presents the intended solution to be developed."
            },
            "venue": {
                "fragments": [],
                "text": "2009 International Conference on Availability, Reliability and Security"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2269127"
                        ],
                        "name": "R. R. Fayzrakhmanov",
                        "slug": "R.-R.-Fayzrakhmanov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Fayzrakhmanov",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. R. Fayzrakhmanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047516905"
                        ],
                        "name": "Max C. G\u00f6bel",
                        "slug": "Max-C.-G\u00f6bel",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "G\u00f6bel",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max C. G\u00f6bel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48047481"
                        ],
                        "name": "Wolfgang Holzinger",
                        "slug": "Wolfgang-Holzinger",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Holzinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Holzinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738409"
                        ],
                        "name": "Bernhard Kr\u00fcpl",
                        "slug": "Bernhard-Kr\u00fcpl",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Kr\u00fcpl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Kr\u00fcpl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46705952"
                        ],
                        "name": "Andreas Mager",
                        "slug": "Andreas-Mager",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Mager",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Mager"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144221846"
                        ],
                        "name": "Robert Baumgartner",
                        "slug": "Robert-Baumgartner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baumgartner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Baumgartner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 207
                            }
                        ],
                        "text": "Hence, screen-scraping made back its way into novel Web Data extraction frameworks, using methods from document understanding and spatial reasoning such as the approaches of the TamCrow project [73], of the ABBA project [37] spatial XPath extensions [95] and rendition-based extensions in RoadRunner to detect labels [29]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "frameworks, using methods from document understanding and spatial reasoning such as the approaches of the TamCrow project [75], of the ABBA project [37] spatial XPath extensions [95] and rendition-based extensions in RoadRunner to detect labels [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "Approaches such as ABBA [37] overcome these limitations."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14380776,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "625e18015628e743118b18ce6bb24107dab36eb4",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this research we introduce the ABBA framework for the generation of advanced screen readers. Current solutions do not enable blind users to participate in the Web in a satisfactory way because they are not time-efficient, cumbersome to use, and do not provide enough overview and orientation within a document. Also, they hide away important layout information from the user. Our approach overcomes these limitations by unifying different semantic views of a document into one multi-axial model and making them accessible to the user in an intuitive interface. The benefit to the user is a higher level of control in different navigation scenarios."
            },
            "slug": "Modelling-web-navigation-with-the-user-in-mind-Fayzrakhmanov-G\u00f6bel",
            "title": {
                "fragments": [],
                "text": "Modelling web navigation with the user in mind"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The ABBA framework for the generation of advanced screen readers is introduced, unifying different semantic views of a document into one multi-axial model and making them accessible to the user in an intuitive interface."
            },
            "venue": {
                "fragments": [],
                "text": "W4A"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "that a rich semantic structure emerges from the Web pages. If such an hypothesis holds true, techniques from Information Extraction (IE) and Natural Language Processing (NLP) can be conveniently used [121, 13, 84]. The range of applications beneting from NLP techniques comprises relevant examples both in the enterprise and Social Web scenarios: for instance, relevant applications of NLP/IE techniques are the "
                    },
                    "intents": []
                }
            ],
            "corpusId": 52800448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "084c55d6432265785e3ff86a2e900a49d501c00a",
            "isKey": false,
            "numCitedBy": 7801,
            "numCiting": 294,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications."
            },
            "slug": "Foundations-of-statistical-natural-language-Manning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Foundations of statistical natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear and provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145804545"
                        ],
                        "name": "L. Bettencourt",
                        "slug": "L.-Bettencourt",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Bettencourt",
                            "middleNames": [
                                "M.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bettencourt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078448903"
                        ],
                        "name": "Ariel Cintr'on-Arias",
                        "slug": "Ariel-Cintr'on-Arias",
                        "structuredName": {
                            "firstName": "Ariel",
                            "lastName": "Cintr'on-Arias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ariel Cintr'on-Arias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37751113"
                        ],
                        "name": "D. Kaiser",
                        "slug": "D.-Kaiser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kaiser",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397423669"
                        ],
                        "name": "Carlos Castillo-Ch'avez",
                        "slug": "Carlos-Castillo-Ch'avez",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Castillo-Ch'avez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Castillo-Ch'avez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8441310,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "87ce25561a59e191e8b66e13a9b21e4a5bde2c45",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-power-of-a-good-idea:-Quantitative-modeling-of-Bettencourt-Cintr'on-Arias",
            "title": {
                "fragments": [],
                "text": "The power of a good idea: Quantitative modeling of the spread of ideas from epidemiological models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710580"
                        ],
                        "name": "A. Berger",
                        "slug": "A.-Berger",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Berger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 141
                            }
                        ],
                        "text": "If such an hypothesis holds true, techniques from Information Extraction (IE) and Natural Language Processing (NLP) can be conveniently used [121,13,84]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1085832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb486e03369a64de2d5b0df86ec0a7b55d3907db",
            "isKey": false,
            "numCitedBy": 3452,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Natural-Language-Berger-Pietra",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A maximum-likelihood approach for automatically constructing maximum entropy models is presented and how to implement this approach efficiently is described, using as examples several problems in natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39820466"
                        ],
                        "name": "Jesse James Garrett",
                        "slug": "Jesse-James-Garrett",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Garrett",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse James Garrett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "stems [98]. Among the most important automation features we cite the possibility to simulate the click stream of the user, lling forms and selecting menus and buttons, the support for AJAX technology [49] to handle the asynchronous updating of the page and the ability of scheduling Web data extraction procedures on a periodical basis. Data transformation. Information could be wrapped from multiple sou"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59711429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c440ae765ff19ddd3deda24a92ac39cef9570f1e",
            "isKey": false,
            "numCitedBy": 1464,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite this, Web interaction designers can\u2019t help but feel a little envious of our colleagues who create desktop software. Desktop applications have a richness and responsiveness that has seemed out of reach on the Web. The same simplicity that enabled the Web\u2019s rapid proliferation also creates a gap between the experiences we can provide and the experiences users can get from a desktop application."
            },
            "slug": "Ajax:-A-New-Approach-to-Web-Applications-Garrett",
            "title": {
                "fragments": [],
                "text": "Ajax: A New Approach to Web Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The same simplicity that enabled the Web\u2019s rapid proliferation also creates a gap between the experiences Web interaction designers can provide and the experiences users can get from a desktop application."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49479696"
                        ],
                        "name": "Michael D. Conover",
                        "slug": "Michael-D.-Conover",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Conover",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael D. Conover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057124"
                        ],
                        "name": "Clayton A. Davis",
                        "slug": "Clayton-A.-Davis",
                        "structuredName": {
                            "firstName": "Clayton",
                            "lastName": "Davis",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clayton A. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48898287"
                        ],
                        "name": "Emilio Ferrara",
                        "slug": "Emilio-Ferrara",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Ferrara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Ferrara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863766"
                        ],
                        "name": "K. McKelvey",
                        "slug": "K.-McKelvey",
                        "structuredName": {
                            "firstName": "Karissa",
                            "lastName": "McKelvey",
                            "middleNames": [
                                "Rae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKelvey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653472"
                        ],
                        "name": "F. Menczer",
                        "slug": "F.-Menczer",
                        "structuredName": {
                            "firstName": "Filippo",
                            "lastName": "Menczer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Menczer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769960"
                        ],
                        "name": "A. Flammini",
                        "slug": "A.-Flammini",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Flammini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Flammini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 108
                            }
                        ],
                        "text": ") to analyze and understand the dynamics of human behaviors at a planetary scale and in a real time-fashion [38,25,33,43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8740560,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "a45f66ee9c927ab4027dd9c9a3a7e05d97a81d68",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Social movements rely in large measure on networked communication technologies to organize and disseminate information relating to the movements\u2019 objectives. In this work we seek to understand how the goals and needs of a protest movement are reflected in the geographic patterns of its communication network, and how these patterns differ from those of stable political communication. To this end, we examine an online communication network reconstructed from over 600,000 tweets from a thirty-six week period covering the birth and maturation of the American anticapitalist movement, Occupy Wall Street. We find that, compared to a network of stable domestic political communication, the Occupy Wall Street network exhibits higher levels of locality and a hub and spoke structure, in which the majority of non-local attention is allocated to high-profile locations such as New York, California, and Washington D.C. Moreover, we observe that information flows across state boundaries are more likely to contain framing language and references to the media, while communication among individuals in the same state is more likely to reference protest action and specific places and times. Tying these results to social movement theory, we propose that these features reflect the movement\u2019s efforts to mobilize resources at the local level and to develop narrative frames that reinforce collective purpose at the national level."
            },
            "slug": "The-Geospatial-Characteristics-of-a-Social-Movement-Conover-Davis",
            "title": {
                "fragments": [],
                "text": "The Geospatial Characteristics of a Social Movement Communication Network"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work examines an online communication network reconstructed from over 600,000 tweets from a thirty-six week period covering the birth and maturation of the American anticapitalist movement, Occupy Wall Street and proposes that these features reflect the movement\u2019s efforts to mobilize resources at the local level and to develop narrative frames that reinforce collective purpose at the national level."
            },
            "venue": {
                "fragments": [],
                "text": "PloS one"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12615602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3764baa7465201f054083d02b58fa75f883c4461",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "slug": "A-New-Statistical-Parser-Based-on-Bigram-Lexical-Collins",
            "title": {
                "fragments": [],
                "text": "A New Statistical Parser Based on Bigram Lexical Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new statistical parser which is based on probabilities of dependencies between head-words in the parse tree, which trains on 40,000 sentences in under 15 minutes and can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108952917"
                        ],
                        "name": "Weimin Chen",
                        "slug": "Weimin-Chen",
                        "structuredName": {
                            "firstName": "Weimin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weimin Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "A number of techniques to approach this problem have been proposed [116,23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26634788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "169ecdf2cea4181c60df305b7f8b5e3c51f084e0",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The ordered tree-to-tree correction problem is to compute the minimum edit cost of transforming one ordered tree to another one. This paper presents a new algorithm for this problem. Given two ordered trees S and T, our algorithm runs in O(|S||T|+min{L2S|T|+L2.5SLT,L2T|S|+L2.5TLS) time, where LS denotes the number of leaves of S and DS denotes the depth of S. The previous best algorithms for this problem run in O(|S||T|min{LS,DS}min{LT,DT}) time (K. Zhang and D. Shasha, SIAM J. Comput.18, No. 6 (1989), 1245?1262) and in O(min{|S|2|T|log2|T|,|T|2|S|log2|S|}) time (P. N. Klein, in \u201cAlgorithms?ESA'98, 6th Annual European Symposium\u201d (G. Bilardi, G. F. Italiano, A. Pietracaprina, and G. Pucci, Eds.), Lecture Notes in Computer Science, Vol. 1461, pp. 91?102, Springer-Verlag, Berlin/New York, 1998). As a comparison, our algorithm is asymptotically faster for certain kind of trees."
            },
            "slug": "New-Algorithm-for-Ordered-Tree-to-Tree-Correction-Chen",
            "title": {
                "fragments": [],
                "text": "New Algorithm for Ordered Tree-to-Tree Correction Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The ordered tree-to-tree correction problem is to compute the minimum edit cost of transforming one ordered tree to another one, and a new algorithm is presented for this problem, which is asymptotically faster for certain kind of trees."
            },
            "venue": {
                "fragments": [],
                "text": "J. Algorithms"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40479470"
                        ],
                        "name": "B. Li",
                        "slug": "B.-Li",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152290618"
                        ],
                        "name": "Qiang Yang",
                        "slug": "Qiang-Yang",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905953"
                        ],
                        "name": "X. Xue",
                        "slug": "X.-Xue",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Xue"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "2009a], other clustering techniques in conjunction with probabilistic models [Li et al. 2009b] and, finally, other approaches project the user and item space in the various domains in a shared latent space by means of regularization techniques [Zhang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5644766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bebfd1e9f14616aeaeae0ab6cc9ff6f8127a7127",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Cross-domain collaborative filtering solves the sparsity problem by transferring rating knowledge across multiple domains. In this paper, we propose a rating-matrix generative model (RMGM) for effective cross-domain collaborative filtering. We first show that the relatedness across multiple rating matrices can be established by finding a shared implicit cluster-level rating matrix, which is next extended to a cluster-level rating model. Consequently, a rating matrix of any related task can be viewed as drawing a set of users and items from a user-item joint mixture model as well as drawing the corresponding ratings from the cluster-level rating model. The combination of these two models gives the RMGM, which can be used to fill the missing ratings for both existing and new users. A major advantage of RMGM is that it can share the knowledge by pooling the rating data from multiple tasks even when the users and items of these tasks do not overlap. We evaluate the RMGM empirically on three real-world collaborative filtering data sets to show that RMGM can outperform the individual models trained separately."
            },
            "slug": "Transfer-learning-for-collaborative-filtering-via-a-Li-Yang",
            "title": {
                "fragments": [],
                "text": "Transfer learning for collaborative filtering via a rating-matrix generative model"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed rating-matrix generative model (RMGM) can share the knowledge by pooling the rating data from multiple tasks even when the users and items of these tasks do not overlap, and can outperform the individual models trained separately."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37062236"
                        ],
                        "name": "Wuu Yang",
                        "slug": "Wuu-Yang",
                        "structuredName": {
                            "firstName": "Wuu",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wuu Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The document tree (henceforth also referred as DOM tree) has been successfully exploited for Web Data Extraction purposes in a number of techniques discussed in the following."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10853673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "445e5f03b05a6febf9630bbb0e3f0c72e02ed403",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Programmers frequently face the need to identify the differences between two programs, usually two different versions of a program. Text\u2010based tools such as the UNIXr\u0300 utility diff often produce unsatisfactory comparisons because they cannot accurately pinpoint the differences and because they sometimes produce irrelevant differences. Since programs have a rigid syntactic structure as described by the grammar of the programming language in which they are written, we develop a comparison algorithm that exploits knowledge of the grammar. The algorithm, which is based on a dynamic programming scheme, can point out the differences between two programs more accurately than previous text comparison tools. Finally, the two programs are pretty\u2010printed \u2018synchronously\u2019 with the differences highlighted so that the differences are easily identified."
            },
            "slug": "Identifying-syntactic-differences-between-two-Yang",
            "title": {
                "fragments": [],
                "text": "Identifying syntactic differences between two programs"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A comparison algorithm is developed that can point out the differences between two programs more accurately than previous text comparison tools and is based on a dynamic programming scheme."
            },
            "venue": {
                "fragments": [],
                "text": "Softw. Pract. Exp."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612372"
                        ],
                        "name": "L. Backstrom",
                        "slug": "L.-Backstrom",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Backstrom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Backstrom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733343"
                        ],
                        "name": "P. Boldi",
                        "slug": "P.-Boldi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Boldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Boldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072638778"
                        ],
                        "name": "M. Rosa",
                        "slug": "M.-Rosa",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Rosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144584024"
                        ],
                        "name": "J. Ugander",
                        "slug": "J.-Ugander",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Ugander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ugander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737624"
                        ],
                        "name": "S. Vigna",
                        "slug": "S.-Vigna",
                        "structuredName": {
                            "firstName": "Sebastiano",
                            "lastName": "Vigna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vigna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 10
                            }
                        ],
                        "text": "Keywords: Web Information Extraction, Web Data Mining, Business Intelligence, Knowledge Engineering, Knowledge-based Systems, Information Retrieval\n\u2217Corresponding author Email addresses: ferrarae@indiana.edu (Emilio Ferrara), pdemeo@unime.it (Pasquale De Meo), gfiumara@unime.it\n(Giacomo Fiumara), robert.baumgartner@lixto.com (Robert Baumgartner)\nPreprint submitted to Knowledge-based systems June 11, 2014\nar X\niv :1\n20 7."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "Keywords: Web Information Extraction, Web Data Mining, Business Intelligence, Knowledge Engineering, Knowledge-based Systems, Information Retrieval\n\u2217Corresponding author Email addresses: ferrarae@indiana.edu (Emilio Ferrara), pdemeo@unime.it (Pasquale De Meo), gfiumara@unime.it\n(Giacomo Fiumara),\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 718955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4b0fb2595ded37509b8f939d054ea5c58a1f2f8",
            "isKey": false,
            "numCitedBy": 518,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Frigyes Karinthy, in his 1929 short story \"L\u00e1ncszemek\" (in English, \"Chains\") suggested that any two persons are distanced by at most six friendship links.1 Stanley Milgram in his famous experiments challenged people to route postcards to a fixed recipient by passing them only through direct acquaintances. Milgram found that the average number of intermediaries on the path of the postcards lay between 4:4 and 5:7, depending on the sample of people chosen. We report the results of the first world-scale social-network graph-distance computations, using the entire Facebook network of active users (\u2248 721 million users, \u2248 69 billion friendship links). The average distance we observe is 4:74, corresponding to 3:74 intermediaries or \"degrees of separation\", prompting the title of this paper. More generally, we study the distance distribution of Facebook and of some interesting geographic subgraphs, looking also at their evolution over time. The networks we are able to explore are almost two orders of magnitude larger than those analysed in the previous literature. We report detailed statistical metadata showing that our measurements (which rely on probabilistic algorithms) are very accurate."
            },
            "slug": "Four-degrees-of-separation-Backstrom-Boldi",
            "title": {
                "fragments": [],
                "text": "Four degrees of separation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The first world-scale social-network graph-distance computations, using the entire Facebook network of active users, and the average distance is 4:74, corresponding to 3:74 intermediaries or \"degrees of separation\", prompting the title of this paper."
            },
            "venue": {
                "fragments": [],
                "text": "WebSci '12"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40479470"
                        ],
                        "name": "B. Li",
                        "slug": "B.-Li",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152290618"
                        ],
                        "name": "Qiang Yang",
                        "slug": "Qiang-Yang",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905953"
                        ],
                        "name": "X. Xue",
                        "slug": "X.-Xue",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Xue"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For instance, some approaches use co-clustering [Li et al. 2009a], other clustering techniques in conjunction with probabilistic models [Li et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 672476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5a55a548fc7cd703c1dd8d867ca1eb6b0c0764c",
            "isKey": false,
            "numCitedBy": 374,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The sparsity problem in collaborative filtering (CF) is a major bottleneck for most CF methods. In this paper, we consider a novel approach for alleviating the sparsity problem in CF by transferring useritem rating patterns from a dense auxiliary rating matrix in other domains (e.g., a popular movie rating website) to a sparse rating matrix in a target domain (e.g., a new book rating website). We do not require that the users and items in the two domains be identical or even overlap. Based on the limited ratings in the target matrix, we establish a bridge between the two rating matrices at a cluster-level of user-item rating patterns in order to transfer more useful knowledge from the auxiliary task domain. We first compress the ratings in the auxiliary rating matrix into an informative and yet compact cluster-level rating pattern representation referred to as a codebook. Then, we propose an efficient algorithm for reconstructing the target rating matrix by expanding the codebook. We perform extensive empirical tests to show that our method is effective in addressing the data sparsity problem by transferring the useful knowledge from the auxiliary tasks, as compared to many state-of-the-art CF methods."
            },
            "slug": "Can-Movies-and-Books-Collaborate-Cross-Domain-for-Li-Yang",
            "title": {
                "fragments": [],
                "text": "Can Movies and Books Collaborate? Cross-Domain Collaborative Filtering for Sparsity Reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes an efficient algorithm for reconstructing the target rating matrix by expanding the codebook, a compact and informative and yet compact cluster-level rating pattern representation referred to as a codebook for transferring useful knowledge from the auxiliary task domain."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152618356"
                        ],
                        "name": "M. Newman",
                        "slug": "M.-Newman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Newman",
                            "middleNames": [
                                "E.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Newman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "eb platforms like Facebook, YouTube or Flickr is the key step to verify sociological theories on a large scale [63, 3] or to check whether mathematical models developed in the eld of Complex Networks [88] are able to correctly explain human behaviors. In the commercial eld, the Web provides a wealth of public domain information which can be retrieved, for instance, from the Web page of a company or fr"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 221278130,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c4925fb114d13a8568f88957c167c928f0c9f1",
            "isKey": false,
            "numCitedBy": 15125,
            "numCiting": 580,
            "paperAbstract": {
                "fragments": [],
                "text": "Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks."
            },
            "slug": "The-Structure-and-Function-of-Complex-Networks-Newman",
            "title": {
                "fragments": [],
                "text": "The Structure and Function of Complex Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Developments in this field are reviewed, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698666"
                        ],
                        "name": "Kaizhong Zhang",
                        "slug": "Kaizhong-Zhang",
                        "structuredName": {
                            "firstName": "Kaizhong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaizhong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821243"
                        ],
                        "name": "R. Statman",
                        "slug": "R.-Statman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Statman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Statman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695878"
                        ],
                        "name": "D. Shasha",
                        "slug": "D.-Shasha",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Shasha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shasha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 82
                            }
                        ],
                        "text": "It has also been proved that the formulation for non ordered trees is NP-complete [128]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16338028,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "909bd42ec0fcee624e0fbea868ceb8325bf6708b",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Editing-Distance-Between-Unordered-Labeled-Zhang-Statman",
            "title": {
                "fragments": [],
                "text": "On the Editing Distance Between Unordered Labeled Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Lett."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093097829"
                        ],
                        "name": "S. Milgram",
                        "slug": "S.-Milgram",
                        "structuredName": {
                            "firstName": "Sharon",
                            "lastName": "Milgram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Milgram"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2019the Human Web\u2019) [Milgram 1967], analyzing statistical data, finding heuristics, inferring information, etc."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60893603,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb06b66f4097107ef698f85c66a3244eddb14653",
            "isKey": false,
            "numCitedBy": 6034,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Small-World-Problem-Milgram",
            "title": {
                "fragments": [],
                "text": "The Small World Problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804028"
                        ],
                        "name": "K. Kaiser",
                        "slug": "K.-Kaiser",
                        "structuredName": {
                            "firstName": "Katharina",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692521"
                        ],
                        "name": "S. Miksch",
                        "slug": "S.-Miksch",
                        "structuredName": {
                            "firstName": "Silvia",
                            "lastName": "Miksch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miksch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "17 3.2 Layer cake comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 88
                            }
                        ],
                        "text": "Kushmerick [77] defined a profile of finite-state approaches to the Web Data Extraction problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29118762,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a54e1212eb46921039c2516905f37b591182751a",
            "isKey": true,
            "numCitedBy": 42,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "1"
            },
            "slug": "Information-Extraction-A-Survey-Kaiser-Miksch",
            "title": {
                "fragments": [],
                "text": "Information Extraction A Survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": ", friendship relationships) are created and evolve over time [69]? How does novel ideas spread and propagate through the web of human contacts [15]? How does the human language evolve through social interactions (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17676138,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "01a2435a90ed22cb01b0493ade550712475a5107",
            "isKey": false,
            "numCitedBy": 374,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Internet-based data on human interaction connects scientific inquiry like never before."
            },
            "slug": "The-convergence-of-social-and-technological-Kleinberg",
            "title": {
                "fragments": [],
                "text": "The convergence of social and technological networks"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Internet-based data on human interaction connects scientific inquiry like never before and helps scientists understand the world around us more fully."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 178
                            }
                        ],
                        "text": "For example, collecting digital traces produced by users of Social Web platforms like Facebook, YouTube or Flickr is the key step to understand, model and predict human behavior [68,94,3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 221559836,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "e2e073433931c4d1a739f548b7d17b6e9b2fa13e",
            "isKey": false,
            "numCitedBy": 2061,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of improving certain characteristics of cadmium mercury telluride single crystal material by heat treating the single crystal material in the presence of both tellurium and mercury."
            },
            "slug": "The-small-world-phenomenon:-an-algorithmic-Kleinberg",
            "title": {
                "fragments": [],
                "text": "The small-world phenomenon: an algorithmic perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A method of improving certain characteristics of cadmium mercury telluride single crystal material by heat treating thesingle crystal material in the presence of both tellurium and mercury."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15181932"
                        ],
                        "name": "Rizal Setya Perdana",
                        "slug": "Rizal-Setya-Perdana",
                        "structuredName": {
                            "firstName": "Rizal",
                            "lastName": "Perdana",
                            "middleNames": [
                                "Setya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rizal Setya Perdana"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [80] the authors used a group of 20 computers, each of them belonging to the Twitter white lists, to perform a real-time monitoring of Twitter."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "The dataset described in [80] consisted of 41."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "We can cite the approach of [80] as a relevant example of how to collect data from a Social Web platform by means of an API."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "Relevant examples of applications capable of extracting data of different type are also present in the context of Social Web applications: for instance, [80] performed in 2009 a crawl of the whole Twitter platform which produced textual data (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 65146187,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "130b32280d64422971c9bf9866b047dc824e2047",
            "isKey": true,
            "numCitedBy": 1256,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-Twitter-Perdana",
            "title": {
                "fragments": [],
                "text": "What is Twitter"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Our contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.4 Organization of the survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 20
                            }
                        ],
                        "text": "4.1 Enterprise Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4.1.1 Context-aware advertising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.1.2 Customer care . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 4
                            }
                        ],
                        "text": "4.1 Enterprise Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4.1.1 Context-aware advertising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.1.2 Customer care . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.1.3 Database building . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.1.4 Software Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Web data extraction system. Encyclopedia of Database Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Web data extraction system. Encyclopedia of Database Systems"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 150
                            }
                        ],
                        "text": "\u2026. . . . . . . . . . . . . . . . . . 26 4.1.12 Web accessibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.1.13 Main content extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.1.14 Web (experience) archiving . . . . . . . .\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 90
                            }
                        ],
                        "text": "In 2007, Fiumara [44] applied these criteria to classify four stateof-the-art Web Data Extraction systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information extraction, Found. Trends Databases"
            },
            "venue": {
                "fragments": [],
                "text": "Information extraction, Found. Trends Databases"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "17 3.2 Layer cake comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 146
                            }
                        ],
                        "text": "The author analyzed both wrapper induction approaches (i.e., approaches capable of automatically generating wrappers by exploiting suitable examples) and maintenance ones (i.e., methods to update a wrapper each time the structure of the Web source changes)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information extraction. Found. Trends databases 1"
            },
            "venue": {
                "fragments": [],
                "text": "Information extraction. Found. Trends databases 1"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "148954033"
                        ],
                        "name": "\u5171\u7acb\u51fa\u7248\u682a\u5f0f\u4f1a\u793e",
                        "slug": "\u5171\u7acb\u51fa\u7248\u682a\u5f0f\u4f1a\u793e",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\u5171\u7acb\u51fa\u7248\u682a\u5f0f\u4f1a\u793e",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u5171\u7acb\u51fa\u7248\u682a\u5f0f\u4f1a\u793e"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 199984869,
            "fieldsOfStudy": [],
            "id": "64cfe81049b1ba5828f81aa0d665cde80b1e6e9d",
            "isKey": false,
            "numCitedBy": 1221,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fb\u30b5\u30a4\u30a8\u30f3\u30b9-:-ACM-computing-surveys-\u5171\u7acb\u51fa\u7248\u682a\u5f0f\u4f1a\u793e",
            "title": {
                "fragments": [],
                "text": "\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fb\u30b5\u30a4\u30a8\u30f3\u30b9 : ACM computing surveys"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108556264"
                        ],
                        "name": "Raleigh North",
                        "slug": "Raleigh-North",
                        "structuredName": {
                            "firstName": "Raleigh",
                            "lastName": "North",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raleigh North"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 130977062,
            "fieldsOfStudy": [
                "Geography"
            ],
            "id": "231ad77fe299baa33f459adcfaefc854f964bbbb",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Haewoon,-Kwak,-Changhyun,-Lee,-Park,-Hosung,-and-.-North",
            "title": {
                "fragments": [],
                "text": "Haewoon, Kwak, Changhyun, Lee, Park, Hosung, and Moon, Sue. . What is Twitter, a Social Network or a News Media?. 19th International World Wide Web (WWW) Conference.April."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738428"
                        ],
                        "name": "Wenke Lee",
                        "slug": "Wenke-Lee",
                        "structuredName": {
                            "firstName": "Wenke",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenke Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2184041"
                        ],
                        "name": "L. M\u00e9",
                        "slug": "L.-M\u00e9",
                        "structuredName": {
                            "firstName": "Ludovic",
                            "lastName": "M\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2481202"
                        ],
                        "name": "A. Wespi",
                        "slug": "A.-Wespi",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Wespi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wespi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 127925452,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "fa676fb9a5fc700d1d9d0fc7a21ce2797ad5ded2",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-4th-International-Symposium-on-Lee-M\u00e9",
            "title": {
                "fragments": [],
                "text": "Proceedings of the 4th International Symposium on Recent Advances in Intrusion Detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5727299"
                        ],
                        "name": "Hyoil Han",
                        "slug": "Hyoil-Han",
                        "structuredName": {
                            "firstName": "Hyoil",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyoil Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687989"
                        ],
                        "name": "R. Elmasri",
                        "slug": "R.-Elmasri",
                        "structuredName": {
                            "firstName": "Ramez",
                            "lastName": "Elmasri",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Elmasri"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some works try to apply the ontological approach to generic domains of Web data extraction [Han 2002] or to tables [Tanaka and Ishida 2006]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62480907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cea29471c4d16508a693a1cba915f1b740fc5a82",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Conceptual-modeling-and-ontology-extraction-for-web-Han-Elmasri",
            "title": {
                "fragments": [],
                "text": "Conceptual modeling and ontology extraction for web information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2199831"
                        ],
                        "name": "J. Masan\u00e8s",
                        "slug": "J.-Masan\u00e8s",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Masan\u00e8s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Masan\u00e8s"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "There are numerous challenges [85] due to the facts that Web pages are ephemeral and due to unpredictable additions, deletions and modifications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60118149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "924501e8811622d25eb19a3f33228e6ddd3a2f9b",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Web-Archiving:-Issues-and-Methods-Masan\u00e8s",
            "title": {
                "fragments": [],
                "text": "Web Archiving: Issues and Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46534634"
                        ],
                        "name": "K. Rose",
                        "slug": "K.-Rose",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Rose",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rose"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 132284496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b043a5ae8100bf60f7b7aa047f2083b2442c0e5b",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-is-Twitter-Rose",
            "title": {
                "fragments": [],
                "text": "What is Twitter"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "standing and spatial reasoning such as the approaches of the TamCrow project [73], of the ABBA project [37] spatial XPath extensions [95] and rendition-based extensions in RoadRunner to detect labels [29].          ! &quot; #    $% &quot;&amp; Figure 5: Deep Web Navigation Capabilities                  ! Figure 6: Web Dat"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27798374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acb7747f6d1c701ed3c21e8077f57e9e791ea33d",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-the-expressiveness-of-ROADRUNNER-Crescenzi-Mecca",
            "title": {
                "fragments": [],
                "text": "Improving the expressiveness of ROADRUNNER"
            },
            "venue": {
                "fragments": [],
                "text": "SEBD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071034516"
                        ],
                        "name": "Alvaro E. Monge",
                        "slug": "Alvaro-E.-Monge",
                        "structuredName": {
                            "firstName": "Alvaro",
                            "lastName": "Monge",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alvaro E. Monge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "during these phases, such as data cleaning [101] and conflict resolution [91], users reach the target to obtain homogeneous information under a unique resulting structure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57297932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cfa878c4619f53ad1bc268855500b76bb655c78",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting database records that are approximate duplicates, but not exact duplicates, is an important task. Databases may contain duplicate records concerning the same real-world entity because of data entry errors, unstandardized abbreviations, or differences in the detailed schemas of records from multiple databases \u2013 such as what happens in data warehousing where records from multiple data sources are integrated into a single source of information \u2013 among other reasons. In this paper we review a system to detect approximate duplicate records in a database and provide properties that a pair-wise record matching algorithm must have in order to have a successful duplicate detection system."
            },
            "slug": "Matching-Algorithms-within-a-Duplicate-Detection-Monge",
            "title": {
                "fragments": [],
                "text": "Matching Algorithms within a Duplicate Detection System"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A system to detect approximate duplicate records in a database is reviewed and properties that a pair-wise record matching algorithm must have in order to have a successful duplicate detection system are provided."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Data Eng. Bull."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1861222864"
                        ],
                        "name": "Miss A.O. Penney",
                        "slug": "Miss-A.O.-Penney",
                        "structuredName": {
                            "firstName": "Miss",
                            "lastName": "Penney",
                            "middleNames": [
                                "A.O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miss A.O. Penney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "Recently, some authors focused on unstructured data management systems (UDMSs) [36], i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[36] provides an overview of Cimple, an UDMS developed at the University of Wisconsin."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 221039574,
            "fieldsOfStudy": [],
            "id": "45fd483402290ad4cae059a4e20cd586c019c3da",
            "isKey": false,
            "numCitedBy": 151803,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "(b)-Penney",
            "title": {
                "fragments": [],
                "text": "(b)"
            },
            "venue": {
                "fragments": [],
                "text": "The New Yale Book of Quotations"
            },
            "year": 2021
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 201836146,
            "fieldsOfStudy": [],
            "id": "4d6b71d56b06ba4d652e0e60f22b927dd072f9af",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Received-(........)-Revised-(........)",
            "title": {
                "fragments": [],
                "text": "Received (........) Revised (........)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 220119561,
            "fieldsOfStudy": [],
            "id": "0a55b22bc98bc997bc31af0244038643e2bae74a",
            "isKey": false,
            "numCitedBy": 6373,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Received",
            "title": {
                "fragments": [],
                "text": "Received"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 188
                            }
                        ],
                        "text": "In addition, if we would use the BFS sample to estimate some structural properties of the social network graph, some properties could be overestimated while others could be underestimated [Kurant et al. 2010]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the bias of breadth first search (bfs) and of other graph sampling techniques"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 22nd International Teletraffic Congress. 1\u20138."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ACM Transactions on Computational Logic"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Transactions on Computational Logic"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "OxPath [48], which is part of the DIADEM project [47] is a declarative formalism that extends XPath to support deep Web navigation and data extraction from interactive Web sites."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "DIADEM: domain-centric"
            },
            "venue": {
                "fragments": [],
                "text": "intelligent, automated data extraction methodology, in: WWW (Companion Volume)"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To this extent, some techniques, called tree-matching strategies are a good candidate to detect similarities between two tree and they will be discussed in detail in the next sections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Logic-based web information extraction, SIGMOD Rec"
            },
            "venue": {
                "fragments": [],
                "text": "Logic-based web information extraction, SIGMOD Rec"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 166
                            }
                        ],
                        "text": ", the tweets and re-tweets produced by users) as well as data indicating the different types of connections among users (\u2018\u2018following\u2019\u2019, \u2018\u2018reply to\u2019\u2019 and \u2018\u2018mention\u2019\u2019) [103]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The directed closure process in hybrid socialinformation networks"
            },
            "venue": {
                "fragments": [],
                "text": "with an analysis of link formation on twitter, in: Proc. 4th International Conference on Weblogs and Social Media"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "The general idea behind the Document Object Model is that HTML Web pages are represented by means of plain text that contains HTML tags, particular keywords defined in the mark-up language, which can be interpreted by the browser to represent the elements specific of a Web page (e.g., hyper-links,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structured data extraction: wrapper generation, Web Data Min"
            },
            "venue": {
                "fragments": [],
                "text": "Structured data extraction: wrapper generation, Web Data Min"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intelligent selfrepairable web wrappers , Lecture Notes in"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Science 6934"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pietra , A maximum entropy approach to natural language processing"
            },
            "venue": {
                "fragments": [],
                "text": "Comput . Linguist ."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cata\u00ec a. Adaptive information extraction"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Comput. Surv"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Web harvesting. Encyclopedia of Database Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Web harvesting. Encyclopedia of Database Systems"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 10
                            }
                        ],
                        "text": "Keywords: Web Information Extraction, Web Data Mining, Business Intelligence, Knowledge Engineering, Knowledge-based Systems, Information Retrieval\n\u2217Corresponding author Email addresses: ferrarae@indiana.edu (Emilio Ferrara), pdemeo@unime.it (Pasquale De Meo), gfiumara@unime.it\n(Giacomo Fiumara), robert.baumgartner@lixto.com (Robert Baumgartner)\nPreprint submitted to Knowledge-based systems June 11, 2014\nar X\niv :1\n20 7."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Keywords: Web Information Extraction, Web Data Mining, Business Intelligence, Knowledge Engineering, Knowledge-based Systems, Information Retrieval\n\u2217Corresponding author Email addresses: ferrarae@indiana.edu (Emilio Ferrara), pdemeo@unime.it (Pasquale De Meo), gfiumara@unime.it\n(Giacomo Fiumara),\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The structure and function of complex networks, SIAM review"
            },
            "venue": {
                "fragments": [],
                "text": "The structure and function of complex networks, SIAM review"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 8
                            }
                        ],
                        "text": "Boronat [Boronat 2008], in his Master Thesis, analyzed and compared performances of common Web data extraction tools."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparison of html-aware tools for web data extraction"
            },
            "venue": {
                "fragments": [],
                "text": "M.S. thesis, Universit\u00e4t Leipzig, Fakult\u00e4t f\u00fcr Mathematik und Informatik. Abteilung Datenbanken."
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A unified framework for wrapping"
            },
            "venue": {
                "fragments": [],
                "text": "mediating and restructuring information from the Web, in: Advances in Conceptual Modeling, Sprg. LNCS 1727"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kushmerick . Finite - state approaches to web information extraction"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . of 3 rd Summer Convention on Information Extraction"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 180
                            }
                        ],
                        "text": "\u2026that a large amount of semistructured information is present in non-HTML formats (think of e-mail messages, software code and related documentations, system logs and so on) but the research approaches targeting at extracting information from this type of sources are out of the scope of this work."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structured data extraction: Wrapper generation. Web Data Mining"
            },
            "venue": {
                "fragments": [],
                "text": "Structured data extraction: Wrapper generation. Web Data Mining"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "OxPath [48], which is part of the DIADEM project [47] is a declarative formalism that extends XPath to support deep Web navigation and data extraction from interactive Web sites."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OXPath: a language for scalable"
            },
            "venue": {
                "fragments": [],
                "text": "memory-efficient data extraction from web applications, Proc. VLDB Endowment 4 (11) "
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Web data extraction system , Encycl"
            },
            "venue": {
                "fragments": [],
                "text": "Database Syst ."
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 125
                            }
                        ],
                        "text": "Wizards that simplify the way to specify queries are the next logical level and for instance have been used in W4F [104] and XWrap [83]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 153
                            }
                        ],
                        "text": "Wizards that simplify the way to specify queries are the next logical level and for instance have been used in W4F [Sahuguet and Azavant 1999] and XWrap [Liu et al. 2000]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "XWrap: An extensible wrapper construction system for internet information"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 16th International Conference on Data Engineering."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic online tourism market monitoring. Proc. of the 17th ENTER eTourism International Conference"
            },
            "venue": {
                "fragments": [],
                "text": "Semantic online tourism market monitoring. Proc. of the 17th ENTER eTourism International Conference"
            },
            "year": 2010
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 79,
            "methodology": 68
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 179,
        "totalPages": 18
    },
    "page_url": "https://www.semanticscholar.org/paper/Web-Data-Extraction,-Applications-and-Techniques:-A-Ferrara-Meo/419147578d24f03fd879716e2409bfe84a02df9f?sort=total-citations"
}