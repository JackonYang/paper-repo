{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143818235"
                        ],
                        "name": "Andrew Carlson",
                        "slug": "Andrew-Carlson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Carlson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Carlson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143796414"
                        ],
                        "name": "C. Schafer",
                        "slug": "C.-Schafer",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Schafer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schafer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "Firstly, some attributes(e.g., book-title)correspond to numerous and diversevalueswhichappear \nindi.erent sitesunevenlyand can hardly be fully covered by a seed site."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9040930,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "725bd456f1439a71cb3c59ab5efa574545cb0153",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of extracting structured records from semi-structured web pages with no human supervision required for each target web site. Previous work on this problem has either required significant human effort for each target site or used brittle heuristics to identify semantic data types. Our method only requires annotation for a few pages from a few sites in the target domain. Thus, after a tiny investment of human effort, our method allows automatic extraction from potentially thousands of other sites within the same domain. Our approach extends previous methods for detecting data fields in semi-structured web pages by matching those fields to domain schema columns using robust models of data values and contexts. Annotating 2---5 pages for 4---6 web sites yields an extraction accuracy of 83.8% on job offer sites and 91.1% on vacation rental sites. These results significantly outperform a baseline approach."
            },
            "slug": "Bootstrapping-Information-Extraction-from-Web-Pages-Carlson-Schafer",
            "title": {
                "fragments": [],
                "text": "Bootstrapping Information Extraction from Semi-structured Web Pages"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work considers the problem of extracting structured records from semi-structured web pages with no human supervision required for each target web site, and extends previous methods for detecting data fields in semi- structural web pages by matching those fields to domain schema columns using robust models of data values and contexts."
            },
            "venue": {
                "fragments": [],
                "text": "ECML/PKDD"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1907018"
                        ],
                        "name": "Jiang-Ming Yang",
                        "slug": "Jiang-Ming-Yang",
                        "structuredName": {
                            "firstName": "Jiang-Ming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang-Ming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145625690"
                        ],
                        "name": "Rui Cai",
                        "slug": "Rui-Cai",
                        "structuredName": {
                            "firstName": "Rui",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rui Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46394915"
                        ],
                        "name": "Yida Wang",
                        "slug": "Yida-Wang",
                        "structuredName": {
                            "firstName": "Yida",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yida Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145254043"
                        ],
                        "name": "Jun Zhu",
                        "slug": "Jun-Zhu",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39089563"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712167"
                        ],
                        "name": "Wei-Ying Ma",
                        "slug": "Wei-Ying-Ma",
                        "structuredName": {
                            "firstName": "Wei-Ying",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Ying Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2858616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03b60b4dac3abcb31841aa8fab2bf45f73d623b0",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Web forums have become an important data resource for many web applications, but extracting structured data from unstructured web forum pages is still a challenging task due to both complex page layout designs and unrestricted user created posts. In this paper, we study the problem of structured data extraction from various web forum sites. Our target is to find a solution as general as possible to extract structured data, such as post title, post author, post time, and post content from any forum site. In contrast to most existing information extraction methods, which only leverage the knowledge inside an individual page, we incorporate both page-level and site-level knowledge and employ Markov logic networks (MLNs) to effectively integrate all useful evidence by learning their importance automatically. Site-level knowledge includes (1) the linkages among different object pages, such as list pages and post pages, and (2) the interrelationships of pages belonging to the same object. The experimental results on 20 forums show a very encouraging information extraction performance, and demonstrate the ability of the proposed approach on various forums. We also show that the performance is limited if only page-level knowledge is used, while when incorporating the site-level knowledge both precision and recall can be significantly improved."
            },
            "slug": "Incorporating-site-level-knowledge-to-extract-data-Yang-Cai",
            "title": {
                "fragments": [],
                "text": "Incorporating site-level knowledge to extract structured data from web forums"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The target is to find a solution as general as possible to extract structured data, such as post title, post author, post time, and post content from any forum site, by incorporating both page-level and site-level knowledge and employing Markov logic networks."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 3514097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cc263c84b85027164bd39db169f5d5959ef6822",
            "isKey": false,
            "numCitedBy": 464,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks. We introduce an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that STALKER does significantly better then other approaches; on one hand, STALKER requires up to two orders of magnitude fewer examples than other algorithms, while on the other hand it can handle information sources that could not be wrapped by existing techniques."
            },
            "slug": "A-hierarchical-approach-to-wrapper-induction-Muslea-Minton",
            "title": {
                "fragments": [],
                "text": "A hierarchical approach to wrapper induction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples that can handle information sources that could not be wrapped by existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "AGENTS '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40343421"
                        ],
                        "name": "Yanhong Zhai",
                        "slug": "Yanhong-Zhai",
                        "structuredName": {
                            "firstName": "Yanhong",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanhong Zhai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145321667"
                        ],
                        "name": "B. Liu",
                        "slug": "B.-Liu",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "That is, to build a system \nwhich (1) is general enough to handle any vertical without re-implementation and (2) requires only one \nlabeled example site from each vertical for training to automatically deal with other sites in the same \nvertical."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12750207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f49e35b6a85b8d81d2c9d9e26e8bf19dd94fad3a",
            "isKey": false,
            "numCitedBy": 600,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the problem of extracting data from a Web page that contains several structured data records. The objective is to segment these data records, extract data items/fields from them and put the data in a database table. This problem has been studied by several researchers. However, existing methods still have some serious limitations. The first class of methods is based on machine learning, which requires human labeling of many examples from each Web site that one is interested in extracting data from. The process is time consuming due to the large number of sites and pages on the Web. The second class of algorithms is based on automatic pattern discovery. These methods are either inaccurate or make many assumptions. This paper proposes a new method to perform the task automatically. It consists of two steps, (1) identifying individual data records in a page, and (2) aligning and extracting data items from the identified data records. For step 1, we propose a method based on visual information to segment data records, which is more accurate than existing methods. For step 2, we propose a novel partial alignment technique based on tree matching. Partial alignment means that we align only those data fields in a pair of data records that can be aligned (or matched) with certainty, and make no commitment on the rest of the data fields. This approach enables very accurate alignment of multiple data records. Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately."
            },
            "slug": "Web-data-extraction-based-on-partial-tree-alignment-Zhai-Liu",
            "title": {
                "fragments": [],
                "text": "Web data extraction based on partial tree alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145254043"
                        ],
                        "name": "Jun Zhu",
                        "slug": "Jun-Zhu",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38301933"
                        ],
                        "name": "Zaiqing Nie",
                        "slug": "Zaiqing-Nie",
                        "structuredName": {
                            "firstName": "Zaiqing",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zaiqing Nie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259699"
                        ],
                        "name": "Ji-Rong Wen",
                        "slug": "Ji-Rong-Wen",
                        "structuredName": {
                            "firstName": "Ji-Rong",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji-Rong Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49846744"
                        ],
                        "name": "Bo Zhang",
                        "slug": "Bo-Zhang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712167"
                        ],
                        "name": "Wei-Ying Ma",
                        "slug": "Wei-Ying-Ma",
                        "structuredName": {
                            "firstName": "Wei-Ying",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Ying Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5916894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d496e3a5edf49a616c53bb80046b132a90934f51",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown the feasibility and promise of template-independent Web data extraction. However, existing approaches use decoupled strategies - attempting to do data record detection and attribute labeling in two separate phases. In this paper, we show that separately extracting data records and attributes is highly ineffective and propose a probabilistic model to perform these two tasks simultaneously. In our approach, record detection can benefit from the availability of semantics required in attribute labeling and, at the same time, the accuracy of attribute labeling can be improved when data records are labeled in a collective manner. The proposed model is called Hierarchical Conditional Random Fields. It can efficiently integrate all useful features by learning their importance, and it can also incorporate hierarchical interactions which are very important for Web data extraction. We empirically compare the proposed model with existing decoupled approaches for product information extraction, and the results show significant improvements in both record detection and attribute labeling."
            },
            "slug": "Simultaneous-record-detection-and-attribute-in-web-Zhu-Nie",
            "title": {
                "fragments": [],
                "text": "Simultaneous record detection and attribute labeling in web data extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that separately extracting data records and attributes is highly ineffective and a probabilistic model to perform these two tasks simultaneously is proposed and it can also incorporate hierarchical interactions which are very important for Web data extraction."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049801"
                        ],
                        "name": "Shuyi Zheng",
                        "slug": "Shuyi-Zheng",
                        "structuredName": {
                            "firstName": "Shuyi",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuyi Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35119829"
                        ],
                        "name": "Ruihua Song",
                        "slug": "Ruihua-Song",
                        "structuredName": {
                            "firstName": "Ruihua",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruihua Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153693432"
                        ],
                        "name": "Ji-rong Wen",
                        "slug": "Ji-rong-Wen",
                        "structuredName": {
                            "firstName": "Ji-rong",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji-rong Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145200192"
                        ],
                        "name": "Di Wu",
                        "slug": "Di-Wu",
                        "structuredName": {
                            "firstName": "Di",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Di Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 833565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61db194fc4693b002d507c6f027beeefef6ae3e7",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Many websites have large collections of pages generated dynamically from an underlying structured source like a database. The data of a category are typically encoded into similar pages by a common script or template. In recent years, some value-added services, such as comparison shopping and vertical search in a specific domain, have motivated the research of extraction technologies with high accuracy. Almost all previous works assume that input pages of a wrapper induction system conform to a common template and they can be easily identified in terms of a common schema of URL. However, we observed that it is hard to distinguish different templates using dynamic URLs today. Moreover, since extraction accuracy heavily depends on how consistent input pages are, we argue that it is risky to determine whether pages share a common template solely based on URLs. Instead, we propose a new approach that utilizes similarity between pages to detect templates. Our approach separates pages with notable inner differences and then generates wrappers, respectively. Experimental results show that our proposed approach is feasible and effective for improving extraction accuracy."
            },
            "slug": "Joint-optimization-of-wrapper-generation-and-Zheng-Song",
            "title": {
                "fragments": [],
                "text": "Joint optimization of wrapper generation and template detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work argues that it is risky to determine whether pages share a common template solely based on URLs and proposes a new approach that utilizes similarity between pages to detect templates, which is feasible and effective for improving extraction accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39756936"
                        ],
                        "name": "Tak-Lam Wong",
                        "slug": "Tak-Lam-Wong",
                        "structuredName": {
                            "firstName": "Tak-Lam",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tak-Lam Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144594306"
                        ],
                        "name": "Wai Lam",
                        "slug": "Wai-Lam",
                        "structuredName": {
                            "firstName": "Wai",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wai Lam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15287551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64ad134fa13221ed251d3d651298c636144201db",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites. Our approach aims at automatically adapting the information extraction knowledge previously learned from a source Web site to a new unseen site, at the same time, discovering previously unseen attributes. Two kinds of text-related clues from the source Web site are considered. The first kind of clue is obtained from the extraction pattern contained in the previously learned wrapper. The second kind of clue is derived from the previously extracted or collected items. A generative model for the generation of the site-independent content information and the site-dependent layout format of the text fragments related to attribute values contained in a Web page is designed to harness the uncertainty involved. Bayesian learning and expectation-maximization (EM) techniques are developed under the proposed generative model for identifying new training data for learning the new wrapper for new unseen sites. Previously unseen attributes together with their semantic labels can also be discovered via another EM-based Bayesian learning based on the generative model. We have conducted extensive experiments from more than 30 real-world Web sites in three different domains to demonstrate the effectiveness of our framework."
            },
            "slug": "Learning-to-Adapt-Web-Information-Extraction-and-a-Wong-Lam",
            "title": {
                "fragments": [],
                "text": "Learning to Adapt Web Information Extraction Knowledge and Discovering New Attributes via a Bayesian Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720956"
                        ],
                        "name": "Chia-Hui Chang",
                        "slug": "Chia-Hui-Chang",
                        "structuredName": {
                            "firstName": "Chia-Hui",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chia-Hui Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687115"
                        ],
                        "name": "Shao-Chen Lui",
                        "slug": "Shao-Chen-Lui",
                        "structuredName": {
                            "firstName": "Shao-Chen",
                            "lastName": "Lui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shao-Chen Lui"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "That is, to build a system \nwhich (1) is general enough to handle any vertical without re-implementation and (2) requires only one \nlabeled example site from each vertical for training to automatically deal with other sites in the same \nvertical."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207646361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c22e82083b78e4022d5b9d2cd4c2a2f42f73151a",
            "isKey": false,
            "numCitedBy": 527,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "research in information extraction (IE) regards the generation of wrappers that can extract particular information from semi- structured Web documents. Similar to compiler generation, the extractor is actually a driver program, which is accompanied with the generated extraction rule. Previous work in this field aims to learn extraction rules from users' training example. In this paper, we propose IEPAD, a system that automatically discovers extraction rules from Web pages. The system can automatically identify record boundary by repeated pattern mining and multiple sequence alignment. The discovery of repeated patterns are realized through a data structure call PAT trees. Additionally, repeated patterns are further extended by pattern alignment to comprehend all record instances. This new track to IE involves no human effort and content-dependent heuristics. Experimental results show that the constructed extraction rules can achieve 97 percent extraction over fourteen popular search engines."
            },
            "slug": "IEPAD:-information-extraction-based-on-pattern-Chang-Lui",
            "title": {
                "fragments": [],
                "text": "IEPAD: information extraction based on pattern discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "IEPAD is proposed, a system that automatically discovers extraction rules from Web pages that can automatically identify record boundary by repeated pattern mining and multiple sequence alignment and can achieve 97 percent extraction over fourteen popular search engines."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145254043"
                        ],
                        "name": "Jun Zhu",
                        "slug": "Jun-Zhu",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38301933"
                        ],
                        "name": "Zaiqing Nie",
                        "slug": "Zaiqing-Nie",
                        "structuredName": {
                            "firstName": "Zaiqing",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zaiqing Nie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259699"
                        ],
                        "name": "Ji-Rong Wen",
                        "slug": "Ji-Rong-Wen",
                        "structuredName": {
                            "firstName": "Ji-Rong",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji-Rong Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49846744"
                        ],
                        "name": "Bo Zhang",
                        "slug": "Bo-Zhang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712167"
                        ],
                        "name": "Wei-Ying Ma",
                        "slug": "Wei-Ying-Ma",
                        "structuredName": {
                            "firstName": "Wei-Ying",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Ying Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12517841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1926ba0277ef84942683a07d50f61ed3582b76f",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The Web contains an abundance of useful semistructured information about real world objects, and our empirical study shows that strong sequence characteristics exist for Web information about objects of the same type across different Web sites. Conditional Random Fields (CRFs) are the state of the art approaches taking the sequence characteristics to do better labeling. However, as the information on a Web page is two-dimensionally laid out, previous linear-chain CRFs have their limitations for Web information extraction. To better incorporate the two-dimensional neighborhood interactions, this paper presents a two-dimensional CRF model to automatically extract object information from the Web. We empirically compare the proposed model with existing linear-chain CRF models for product information extraction, and the results show the effectiveness of our model."
            },
            "slug": "2D-Conditional-Random-Fields-for-Web-information-Zhu-Nie",
            "title": {
                "fragments": [],
                "text": "2D Conditional Random Fields for Web information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a two-dimensional CRF model to automatically extract object information from the Web, and empirically compare the proposed model with existing linear-chain CRF models for product information extraction, and the results show the effectiveness of the model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144911391"
                        ],
                        "name": "M\u00e1rcio L. A. Vidal",
                        "slug": "M\u00e1rcio-L.-A.-Vidal",
                        "structuredName": {
                            "firstName": "M\u00e1rcio",
                            "lastName": "Vidal",
                            "middleNames": [
                                "L.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M\u00e1rcio L. A. Vidal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690426"
                        ],
                        "name": "A. D. Silva",
                        "slug": "A.-D.-Silva",
                        "structuredName": {
                            "firstName": "Altigran",
                            "lastName": "Silva",
                            "middleNames": [
                                "Soares",
                                "da"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. D. Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733769"
                        ],
                        "name": "E. Moura",
                        "slug": "E.-Moura",
                        "structuredName": {
                            "firstName": "Edleno",
                            "lastName": "Moura",
                            "middleNames": [
                                "Silva",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Moura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2458052"
                        ],
                        "name": "J. Cavalcanti",
                        "slug": "J.-Cavalcanti",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Cavalcanti",
                            "middleNames": [
                                "M.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cavalcanti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9869954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d21ec68c55d66b3bf2a9039d62a5ea9615d8366",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Many Web IR and Digital Library applications require a crawling process to collect pages with the ultimate goal of taking advantage of useful information available on Web sites. For some of these applications the criteria to determine when a page is to be present in a collection are related to the page content. However, there are situations in which the inner structure of the pages provides a better criteria to guide the crawling process than their content. In this paper, we present a structure-driven approach for generating Web crawlers that requires a minimum effort from users. The idea is to take as input a sample page and an entry point to a Web site and generate a structure-driven crawler based on navigation patterns, sequences of patterns for the links a crawler has to follow to reach the pages structurally similar to the sample page. In the experiments we have carried out, structure-driven crawlers generated by our new approach were able to collect all pages that match the samples given, including those pages added after their generation."
            },
            "slug": "Structure-driven-crawler-generation-by-example-Vidal-Silva",
            "title": {
                "fragments": [],
                "text": "Structure-driven crawler generation by example"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A structure-driven approach for generating Web crawlers that requires a minimum effort from users based on navigation patterns, sequences of patterns for the links a crawler has to follow to reach the pages structurally similar to the sample page is presented."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913159"
                        ],
                        "name": "Robert B. Doorenbos",
                        "slug": "Robert-B.-Doorenbos",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Doorenbos",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert B. Doorenbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5119155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e7402ad740b73cc0bb64178f86df3478c3aaf5",
            "isKey": false,
            "numCitedBy": 1283,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Many Internet information resources present relational data|telephone directories, product catalogs, etc. Because these sites are formatted for people, mechanically extracting their content is di cult. Systems using such resources typically use hand-coded wrappers, procedures to extract data from information resources. We introduce wrapper induction, a method for automatically constructing wrappers, and identify hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources. We use PAC analysis to bound the problem's sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge."
            },
            "slug": "Wrapper-Induction-for-Information-Extraction-Kushmerick-Weld",
            "title": {
                "fragments": [],
                "text": "Wrapper Induction for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work introduces wrapper induction, a method for automatically constructing wrappers, and identifies hlrt, a wrapper class that is e ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8359747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22fb3b3b2bdf768dd435eedfc5ef5155d3e56b1a",
            "isKey": false,
            "numCitedBy": 1071,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically.WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semi-structured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories."
            },
            "slug": "Learning-Information-Extraction-Rules-for-and-Free-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning Information Extraction Rules for Semi-Structured and Free Text"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences, and can also handle extraction from free text such as news stories."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145321667"
                        ],
                        "name": "B. Liu",
                        "slug": "B.-Liu",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743874"
                        ],
                        "name": "R. Grossman",
                        "slug": "R.-Grossman",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Grossman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grossman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40343421"
                        ],
                        "name": "Yanhong Zhai",
                        "slug": "Yanhong-Zhai",
                        "structuredName": {
                            "firstName": "Yanhong",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanhong Zhai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 167
                            }
                        ],
                        "text": "That is, to build a system \nwhich (1) is general enough to handle any vertical without re-implementation and (2) requires only one \nlabeled example site from each vertical for training to automatically deal with other sites in the same \nvertical."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11383614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8cdce10fc4c9505ca22796920d1cd4b0f82dc76",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A large amount of information on the Web is contained in regularly structured objects, which we call data records. Such data records are important because they often present the essential information of their host pages, e.g., lists of products or services. It is useful to mine such data records in order to extract information from them to provide value-added services. Existing automatic techniques are not satisfactory because of their poor accuracies. In this paper, we propose a more effective technique to perform the task. The technique is based on two observations about data records on the Web and a string matching algorithm. The proposed technique is able to mine both contiguous and non-contiguous data records. Our experimental results show that the proposed technique outperforms existing techniques substantially."
            },
            "slug": "Mining-data-records-in-Web-pages-Liu-Grossman",
            "title": {
                "fragments": [],
                "text": "Mining data records in Web pages"
            },
            "tldr": {
                "abstractSimilarityScore": 34,
                "text": "The experimental results show that the proposed technique outperforms existing techniques substantially, and is able to mine both contiguous and non-contiguous data records."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35707712"
                        ],
                        "name": "Steve Dill",
                        "slug": "Steve-Dill",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Dill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steve Dill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098146"
                        ],
                        "name": "Nadav Eiron",
                        "slug": "Nadav-Eiron",
                        "structuredName": {
                            "firstName": "Nadav",
                            "lastName": "Eiron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nadav Eiron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060104135"
                        ],
                        "name": "David Gibson",
                        "slug": "David-Gibson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Gibson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2890298"
                        ],
                        "name": "D. Gruhl",
                        "slug": "D.-Gruhl",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gruhl",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gruhl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145615125"
                        ],
                        "name": "R. Guha",
                        "slug": "R.-Guha",
                        "structuredName": {
                            "firstName": "Ramanathan",
                            "lastName": "Guha",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Guha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779853"
                        ],
                        "name": "A. Jhingran",
                        "slug": "A.-Jhingran",
                        "structuredName": {
                            "firstName": "Anant",
                            "lastName": "Jhingran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jhingran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626870"
                        ],
                        "name": "T. Kanungo",
                        "slug": "T.-Kanungo",
                        "structuredName": {
                            "firstName": "Tapas",
                            "lastName": "Kanungo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanungo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145941159"
                        ],
                        "name": "S. Rajagopalan",
                        "slug": "S.-Rajagopalan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Rajagopalan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rajagopalan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49365095"
                        ],
                        "name": "A. Tomkins",
                        "slug": "A.-Tomkins",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Tomkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tomkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717630"
                        ],
                        "name": "J. Tomlin",
                        "slug": "J.-Tomlin",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tomlin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tomlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118829"
                        ],
                        "name": "J. Zien",
                        "slug": "J.-Zien",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Zien",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17741338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f60ff1e240be1b1e74c164d0ef4fb2d67f0cbdc3",
            "isKey": false,
            "numCitedBy": 582,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes Seeker, a platform for large-scale text analytics, and SemTag, an application written on the platform to perform automated semantic tagging of large corpora. We apply SemTag to a collection of approximately 264 million web pages, and generate approximately 434 million automatically disambiguated semantic tags, published to the web as a label bureau providing metadata regarding the 434 million annotations. To our knowledge, this is the largest scale semantic tagging effort to date.We describe the Seeker platform, discuss the architecture of the SemTag application, describe a new disambiguation algorithm specialized to support ontological disambiguation of large-scale data, evaluate the algorithm, and present our final results with information about acquiring and making use of the semantic tags. We argue that automated large scale semantic tagging of ambiguous content can bootstrap and accelerate the creation of the semantic web."
            },
            "slug": "SemTag-and-seeker:-bootstrapping-the-semantic-web-Dill-Eiron",
            "title": {
                "fragments": [],
                "text": "SemTag and seeker: bootstrapping the semantic web via automated semantic annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is argued that automated large scale semantic tagging of ambiguous content can bootstrap and accelerate the creation of the semantic web."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39756936"
                        ],
                        "name": "Tak-Lam Wong",
                        "slug": "Tak-Lam-Wong",
                        "structuredName": {
                            "firstName": "Tak-Lam",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tak-Lam Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144594306"
                        ],
                        "name": "Wai Lam",
                        "slug": "Wai-Lam",
                        "structuredName": {
                            "firstName": "Wai",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wai Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Bo Chen",
                        "slug": "Bo-Chen",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13138053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0e5534ba8181b70d20fc70f7d30f2c5329a07e1",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed an approach for analyzing online job advertisements in different domains (industries) from different regions worldwide. Our approach is able to extract precise information from the text content supporting useful employment market analysis locally and globally. A major component in our approach is an information extraction framework which is composed of two challenging tasks. The first task is to detect unformatted text blocks automatically based on an unsupervised learning model. Identifying these useful text blocks through this learning model allows the generation of highly effective features for the next task which is text fragment extraction learning. The task of text fragment extraction learning is formulated as a domain adaptation model for text fragment classification. One advantage of our approach is that it can easily adapt to a large number of online job advertisements in different and new domains. Extensive experiments have been conducted to demonstrate the effectiveness and flexibility of our approach."
            },
            "slug": "Mining-employment-market-via-text-block-detection-Wong-Lam",
            "title": {
                "fragments": [],
                "text": "Mining employment market via text block detection and adaptive cross-domain information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach is able to extract precise information from the text content supporting useful employment market analysis locally and globally and can easily adapt to a large number of online job advertisements in different and new domains."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15075203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dd1f9f7795b31493d98d9f260d37aad07550f6e",
            "isKey": false,
            "numCitedBy": 1157,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper investigates techniques for extracting data from HTML sites through the use of automatically generated wrappers. To automate the wrapper generation and the data extraction process, the paper develops a novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach."
            },
            "slug": "RoadRunner:-Towards-Automatic-Data-Extraction-from-Crescenzi-Mecca",
            "title": {
                "fragments": [],
                "text": "RoadRunner: Towards Automatic Data Extraction from Large Web Sites"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences is developed, which confirms the feasibility of the approach on real-life data-intensive Web sites."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079828"
                        ],
                        "name": "Tobias D\u00f6nz",
                        "slug": "Tobias-D\u00f6nz",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "D\u00f6nz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tobias D\u00f6nz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13550720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4fb16e487560e82aacc9cea9cb662c9eb324aa80",
            "isKey": false,
            "numCitedBy": 417,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extracting-Structured-Data-from-Web-Pages-D\u00f6nz",
            "title": {
                "fragments": [],
                "text": "Extracting Structured Data from Web Pages"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "PL (page-level semantic prediction) 3. PL + IP (inter-page aggregation) <Full solution>"
            },
            "venue": {
                "fragments": [],
                "text": "PL (page-level semantic prediction) 3. PL + IP (inter-page aggregation) <Full solution>"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stacked Skews Model) Carlson et al"
            },
            "venue": {
                "fragments": [],
                "text": "SSM"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "inter-attribute re-ranking) @BULLET One seed site (by turns), test on other sites @BULLET Performance metrics: precision & recall"
            },
            "venue": {
                "fragments": [],
                "text": "inter-attribute re-ranking) @BULLET One seed site (by turns), test on other sites @BULLET Performance metrics: precision & recall"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Various categories of entities, \ncalled verticals, havebeeninvolvedinavarietyof applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document object model. http://en.wikipedia.org/wiki/Document Object Model"
            },
            "venue": {
                "fragments": [],
                "text": "Document object model. http://en.wikipedia.org/wiki/Document Object Model"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 21,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/From-one-tree-to-a-forest:-a-unified-solution-for-Hao-Cai/5da5952da2fceac1823393ee2e0bdde0e0a02d2b?sort=total-citations"
}