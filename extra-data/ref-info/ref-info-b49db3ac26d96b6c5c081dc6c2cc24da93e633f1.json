{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57083455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3121f48d51ba29c979e65567414ec984f15e7872",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Lexical ambiguity resolution is a pervasive problem in natural language processing. An important example is target-word choice in machine translation, such as deciding whether the English word sentence should be translated into French as peine (legal sentence) or phrase (grammatical sentence) depending upon analysis of surrounding context. The same problem arises in text-to-speech synthesis, where pronunciations such as lead role and lead mine must be resolved through context. Similar problems include capitalization and accent restoration, proper-name classification, and general word-sense disambiguation for many applications. \nThis dissertation describes three original algorithms for solving this class of problems. The first is a Bayesian discriminator for semantic word classes. It uses statistical models of context to identify the most likely thesaurus category at each position in a document. Sense and translation differences are resolved through these class models. Applications of this work to discourse analysis and language modelling are explored. \nThe second algorithm is a supervised statistical decision procedure using a variant of decision lists. It offers an efficient mechanism for utilizing diverse, non-independent sources of evidence in a very large parameter space. The dissertation includes empirical studies in language polysemy on which this algorithm and its smoothing procedures are based. The algorithm is evaluated on a wide range of homographs, include ambiguities in text-to-speech synthesis and accent restoration in Spanish and French. \nThe third algorithm is an essentially unsupervised decision procedure that bootstraps from a small number of seed words automatically extracted from machine-readable dictionaries. The algorithm is driven by the joint exploitation of two empirically studied properties--that words tend to exhibit only one sense in a given collocation and in a given discourse. Accuracy exceeds 96% on diverse test sets. This performance rivals that of previous fully supervised methods while eliminating the need for costly hand-tagged training data, the lack of which has been a severe bottleneck for progress in this area."
            },
            "slug": "Three-machine-learning-algorithms-for-lexical-Yarowsky-Marcus",
            "title": {
                "fragments": [],
                "text": "Three machine learning algorithms for lexical ambiguity resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This dissertation describes three original algorithms for solvingLexical ambiguity resolution in natural language processing, including a Bayesian discriminator for semantic word classes and an essentially unsupervised decision procedure that bootstraps from a small number of seed words automatically extracted from machine-readable dictionaries."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046487"
                        ],
                        "name": "Jeffrey C. Reynar",
                        "slug": "Jeffrey-C.-Reynar",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Reynar",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey C. Reynar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6204420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6edceaf0fada3588ee5f036e944c1a00661df77a",
            "isKey": false,
            "numCitedBy": 477,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Identifying-Sentence-Reynar-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Identifying Sentence Boundaries"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A trainable model for identifying sentence boundaries in raw text that can be trained easily on any genre of English, and should be trainable on any other Romanalphabet language."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227843"
                        ],
                        "name": "M. Meteer",
                        "slug": "M.-Meteer",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Meteer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meteer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2452408"
                        ],
                        "name": "Jeff Palmucci",
                        "slug": "Jeff-Palmucci",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Palmucci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Palmucci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6838726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a94da952fb8ffc77881028081e90efb494f1c5d",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models. This paper reports our experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints, and learning caseframe informationfor verbsfrom example uses.From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical informationfrom a corpus, by supplementing knowledge-based techniques.Based on the results of those experiments, we have constructed a new natural language system (PLUM) for extracting data from text, e.g., newswire text."
            },
            "slug": "Coping-with-Ambiguity-and-Unknown-Words-through-Weischedel-Meteer",
            "title": {
                "fragments": [],
                "text": "Coping with Ambiguity and Unknown Words through Probabilistic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A new natural language system (PLUM) is constructed for extracting data from text, e.g., newswire text, based on results of experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33611948"
                        ],
                        "name": "D. Palmer",
                        "slug": "D.-Palmer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Palmer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Palmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9701154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b14be12cecba09db74a901bf25a33a67ac784a6",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German."
            },
            "slug": "Adaptive-Multilingual-Sentence-Boundary-Palmer-Hearst",
            "title": {
                "fragments": [],
                "text": "Adaptive Multilingual Sentence Boundary Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article presents an efficient, trainable system for sentence boundary disambiguation, called Satz, which makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuated mark."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5598810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
            },
            "slug": "Towards-History-based-Grammars:-Using-Richer-Models-Black-Jelinek",
            "title": {
                "fragments": [],
                "text": "Towards History-based Grammars: Using Richer Models for Probabilistic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way and significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686820"
                        ],
                        "name": "B. M\u00e9rialdo",
                        "slug": "B.-M\u00e9rialdo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "M\u00e9rialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00e9rialdo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2727455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4614650c3bb3e835c80612d3bca9586f81db95a3",
            "isKey": false,
            "numCitedBy": 629,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined:using text that has been tagged by hand and computing relative frequency counts,using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.Experminents show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available."
            },
            "slug": "Tagging-English-Text-with-a-Probabilistic-Model-M\u00e9rialdo",
            "title": {
                "fragments": [],
                "text": "Tagging English Text with a Probabilistic Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experminents show that the best training is obtained by using as much tagged text as possible, and show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036",
            "isKey": false,
            "numCitedBy": 717,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length."
            },
            "slug": "Statistical-Decision-Tree-Models-for-Parsing-Magerman",
            "title": {
                "fragments": [],
                "text": "Statistical Decision-Tree Models for Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "SPATTER is described, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17567112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214",
            "isKey": false,
            "numCitedBy": 657,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources. The availability of this testing and training material has enabled us to develop quantitative disambiguation methods that achieve 92% accuracy in discriminating between two very distinct senses of a noun. In the training phase, we collect a number of instances of each sense of the polysemous noun. Then in the testing phase, we are given a new instance of the noun, and are asked to assign the instance to one of the senses. We attempt to answer this question by comparing the context of the unknown instance with contexts of known instances using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval. The proposed method is probably most appropriate for those aspects of sense disambiguation that are closest to the information retrieval task. In particular, the proposed method was designed to disambiguate senses that are usually associated with different topics."
            },
            "slug": "A-method-for-disambiguating-word-senses-in-a-large-Gale-Church",
            "title": {
                "fragments": [],
                "text": "A method for disambiguating word senses in a large corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The proposed method was designed to disambiguate senses that are usually associated with different topics using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Humanit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3271427"
                        ],
                        "name": "M. Kayaalp",
                        "slug": "M.-Kayaalp",
                        "structuredName": {
                            "firstName": "Mehmet",
                            "lastName": "Kayaalp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kayaalp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2614094"
                        ],
                        "name": "Rebecca F. Bruce",
                        "slug": "Rebecca-F.-Bruce",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Bruce",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rebecca F. Bruce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6575020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d1a912d1e5e4f147929cef0b2b5ed8dc7c485a0",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical classification methods usually rely on a single best model to make accurate predictions. Such a model aims to maximize accuracy by balancing precision and recall. The Model Switching method as presented in this paper performs with higher predictive accuracy and 100% recall by using a set of decomposable models instead of a single one. The implemented system, MS1, is tested on a case study, predicting Prepositional Phrase Attachment (PPA). The results show that iV is more accurate than other statistical techniques that select single models for classification and competitive with other successful NLP approaches in PPA disambiguation. The Model Switching method may be preferable to other methods because of its generality (i.e., wide range of applicability), and its competitive accuracy in prediction. It may also be used as an analytical tool to investigate the nature of the domain and the characteristics of the data with the help of generated models."
            },
            "slug": "A-Statistical-Decision-Making-Method:-A-Case-Study-Kayaalp-Pedersen",
            "title": {
                "fragments": [],
                "text": "A Statistical Decision Making Method: A Case Study on Prepositional Phrase Attachment"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The results show that iV is more accurate than other statistical techniques that select single models for classification and competitive with other successful NLP approaches in PPA disambiguation."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2220955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ad8e98574a275930bf04a477ce3532fd13c503c",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques. The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis. We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars. The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser. The latter is constructed by associating probabilities with the LR parse table directly. This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism. We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser. We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions. Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic) frequency of occurrence."
            },
            "slug": "Generalized-Probabilistic-LR-Parsing-of-Natural-Briscoe-Carroll",
            "title": {
                "fragments": [],
                "text": "Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "The construction of a very wide-coverage probabilistic parsing system for natural language (NL) based on LR parsing techniques, intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5939680,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f11e1b9d2656d8738329458ef3d325e6c6194a5",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 144,
            "paperAbstract": {
                "fragments": [],
                "text": "A CORPUS-BASED APPROACH TO LANGUAGE LEARNING Eric Brill Supervisor: Mitchell Marcus One goal of computational linguistics is to discover a method for assigning a rich structural annotation to sentences that are presented as simple linear strings of words; meaning can be much more readily extracted from a structurally annotated sentence than from a sentence with no structural information. Also, structure allows for a more in-depth check of the well-formedness of a sentence. There are two phases to assigning these structural annotations: rst, a knowledge base is created and second, an algorithm is used to generate a structural annotation for a sentence based upon the facts provided in the knowledge base. Until recently, most knowledge bases were created manually by language experts. These knowledge bases are expensive to create and have not been used e ectively in structurally parsing sentences from other than highly restricted domains. The goal of this dissertation is to make signi cant progress toward designing automata that are able to learn some structural aspects of human language with little human guidance. In particular, we describe a learning algorithm that takes a small structurally annotated corpus of text and a larger unannotated corpus as input, and automatically learns how to assign accurate structural descriptions to sentences not in the training corpus. The main tool we use to automatically discover structural information about language from corpora is transformation-based error-driven learning. The distribution of errors produced by an imperfect annotator is examined to learn an ordered list of transformations that can be applied to provide an accurate structural annotation. We demonstrate the application of this learning algorithm iv to part of speech tagging and parsing. Successfully applying this technique to create systems that learn could lead to robust, trainable and accurate natural language processing systems. v"
            },
            "slug": "A-corpus-based-approach-to-language-learning-Brill",
            "title": {
                "fragments": [],
                "text": "A corpus-based approach to language learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A learning algorithm is described that takes a small structurally annotated corpus of text and a larger unannotated corpus as input, and automatically learns how to assign accurate structural descriptions to sentences not in the training corpus."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145272844"
                        ],
                        "name": "C. Apt\u00e9",
                        "slug": "C.-Apt\u00e9",
                        "structuredName": {
                            "firstName": "Chidanand",
                            "lastName": "Apt\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Apt\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "68982679"
                        ],
                        "name": "Fred J. Damerau",
                        "slug": "Fred-J.-Damerau",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Damerau",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred J. Damerau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145700185"
                        ],
                        "name": "S. Weiss",
                        "slug": "S.-Weiss",
                        "structuredName": {
                            "firstName": "Sholom",
                            "lastName": "Weiss",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10826654,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9257779eed46107bcdce9f4dc86298572ff466ce",
            "isKey": false,
            "numCitedBy": 945,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to \u201cread\u201d documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67% recall/precision breakeven point to 80.5%. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features."
            },
            "slug": "Automated-learning-of-decision-rules-for-text-Apt\u00e9-Damerau",
            "title": {
                "fragments": [],
                "text": "Automated learning of decision rules for text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation, and compared with other machine-learning techniques."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2614094"
                        ],
                        "name": "Rebecca F. Bruce",
                        "slug": "Rebecca-F.-Bruce",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Bruce",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rebecca F. Bruce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144120827"
                        ],
                        "name": "J. Wiebe",
                        "slug": "J.-Wiebe",
                        "structuredName": {
                            "firstName": "Janyce",
                            "lastName": "Wiebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wiebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 942975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98c5a252475fce36934e6c3d4710af1aa08a382b",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features. In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun interest. We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model. Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data."
            },
            "slug": "Word-Sense-Disambiguation-Using-Decomposable-Models-Bruce-Wiebe",
            "title": {
                "fragments": [],
                "text": "Word-Sense Disambiguation Using Decomposable Models"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35371521"
                        ],
                        "name": "Kristina Toutanvoa",
                        "slug": "Kristina-Toutanvoa",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanvoa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanvoa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10807721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1504a9d5829033a8cb4cf37b8bb13dfd4baddc7b",
            "isKey": false,
            "numCitedBy": 1574,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging. In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words."
            },
            "slug": "Enriching-the-Knowledge-Sources-Used-in-a-Maximum-Toutanvoa-Manning",
            "title": {
                "fragments": [],
                "text": "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging by incorporating these features: more extensive treatment of capitalization for unknown words, and features for the disambiguation of the tense forms of verbs."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054787633"
                        ],
                        "name": "Raymond Lau",
                        "slug": "Raymond-Lau",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Lau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond Lau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7536384,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49b2862ab73be40bf69ac3f457039f18d12df0ae",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe our ongoing efforts at adaptive statistical language modeling. Central to our approach is the Maximum Entropy (ME) Principle, allowing us to combine evidence from multiple sources, such as long-distance triggers and conventional short-distance trigrams. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. Among the advantages of this approach are its simplicity, its generality, and its incremental nature. Among its disadvantages are its computational requirements. We describe a succession of ME models, culminating in our current Maximum Likelihood/Maximum Entropy (ML/ME) model. Preliminary results with the latter show a 27% perplexity reduction as compared to a conventional trigram model."
            },
            "slug": "Adaptive-Language-Modeling-Using-the-Maximum-Lau-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Adaptive Language Modeling Using the Maximum Entropy Principle"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A succession of ME models are described, culminating in the current Maximum Likelihood/Maximum Entropy (ML/ME) model, which shows a 27% perplexity reduction as compared to a conventional trigram model."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12309040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "733234e097dceb9011baa8914930861996eb0b5e",
            "isKey": false,
            "numCitedBy": 624,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In (Brill 1992), a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger-can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty."
            },
            "slug": "Some-Advances-in-Transformation-Based-Part-of-Brill",
            "title": {
                "fragments": [],
                "text": "Some Advances in Transformation-Based Part of Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method for expressing lexical relations in tagging that stochastic taggers are currently unable to express is described and how the tagger-can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2918948"
                        ],
                        "name": "R. Baayen",
                        "slug": "R.-Baayen",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Baayen",
                            "middleNames": [
                                "Harald"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baayen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145421878"
                        ],
                        "name": "R. Sproat",
                        "slug": "R.-Sproat",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sproat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sproat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8592595,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "2301608d82ff3f85e2ee1a81866dac69bacb3d55",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a form that is previously unseen in a sufficiently large training corpus, and that is morphologically n-ways ambiguous (serves n different lexical functions) what is the best estimator for the lexical prior probabilities for the various functions of the form? We argue that the best estimator is provided by computing the relative frequencies of the various functions among the hapax legomena---the forms that occur exactly once in a corpus; in particular, a hapax-based estimator is better than one based on the proportion of the various functions among words of all frequency ranges. As we shall argue, this is because when one computes an overall measure, one is including high-frequency words, and high-frequency words tend to have idiosyncratic properties that are not at all representative of the much larger mass of (productively formed) low-frequency words. This result has potential importance for various kinds of applications requiring lexical disambiguation, including, in particular, stochastic taggers. This is especially true when some initial hand-tagging of a corpus is required: for predicting lexical priors for very low-frequency morphologically ambiguous types (most of which would not occur in any given corpus), one should concentrate on tagging a good representative sample of the hapax legomena, rather than extensively tagging words of all frequency ranges."
            },
            "slug": "Estimating-Lexical-Priors-for-Low-Frequency-Forms-Baayen-Sproat",
            "title": {
                "fragments": [],
                "text": "Estimating Lexical Priors for Low-Frequency Morphologically Ambiguous Forms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that a hapax-based estimator is better than one based on the proportion of the various functions among words of all frequency ranges because when one computes an overall measure, one is including high-frequency words, and high- frequencies words tend to have idiosyncratic properties that are not at all representative of the much larger mass of (productively formed) low- frequency words."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067634566"
                        ],
                        "name": "James Brooks",
                        "slug": "James-Brooks",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Brooks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Brooks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc9e5bf851dc95369e26f1869c2637b1d8919e6c",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events \u2014 ignoring events which occur less than 5 times in training data reduces performance to 81.6%."
            },
            "slug": "Prepositional-Phrase-Attachment-through-a-Model-Collins-Brooks",
            "title": {
                "fragments": [],
                "text": "Prepositional Phrase Attachment through a Backed-off Model"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This paper shows that the problem of prepositional phrase attachment ambiguity is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145043214"
                        ],
                        "name": "Jason Eisner",
                        "slug": "Jason-Eisner",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Eisner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Eisner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3262717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "adfef97814b292a09520d8c78a141e7a4baf8726",
            "isKey": false,
            "numCitedBy": 728,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "After presenting a novel O(n3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank). In these results, the generative model performs significantly better than the others, and does about equally well at assigning part-of-speech tags."
            },
            "slug": "Three-New-Probabilistic-Models-for-Dependency-An-Eisner",
            "title": {
                "fragments": [],
                "text": "Three New Probabilistic Models for Dependency Parsing: An Exploration"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank) suggest the generative model performs significantly better than the others, and does about equally well at assigning part-of-speech tags."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2165139"
                        ],
                        "name": "H. V. Halteren",
                        "slug": "H.-V.-Halteren",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Halteren",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. V. Halteren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316623"
                        ],
                        "name": "Jakub Zavrel",
                        "slug": "Jakub-Zavrel",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Zavrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Zavrel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 852013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b863f180d6588222663cd6e2c434f50a7f06fff3",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generator (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best indvidual tagger."
            },
            "slug": "Improving-Data-Driven-Wordclass-Tagging-by-System-Halteren-Zavrel",
            "title": {
                "fragments": [],
                "text": "Improving Data Driven Wordclass Tagging by System Combination"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "How the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system is examined."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710580"
                        ],
                        "name": "A. Berger",
                        "slug": "A.-Berger",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Berger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1085832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb486e03369a64de2d5b0df86ec0a7b55d3907db",
            "isKey": false,
            "numCitedBy": 3452,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Natural-Language-Berger-Pietra",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A maximum-likelihood approach for automatically constructing maximum entropy models is presented and how to implement this approach efficiently is described, using as examples several problems in natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714612"
                        ],
                        "name": "S. Sekine",
                        "slug": "S.-Sekine",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Sekine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sekine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 470934,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f75ff8694578b82308c397e5f18252a5b3ed39a9",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A major concern in corpus based approaches is that the applicability of the acquired knowledge may be limited by some feature of the corpus, in particular, the notion of text 'domain'. In order to examine the domain dependence of parsing, in this paper, we report 1) Comparison of structure distributions across domains; 2) Examples of domain specific structures; and 3) Parsing experiment using some domain dependent grammars. The observations using the Brown corpus demonstrate domain dependence and idiosyncrasy of syntactic structure. The parsing results show that the best accuracy is obtained using the grammar acquired from the same domain or the same class (fiction or nonfiction). We will also discuss the relationship between parsing accuracy and the size of training corpus."
            },
            "slug": "The-Domain-Dependence-of-Parsing-Sekine",
            "title": {
                "fragments": [],
                "text": "The Domain Dependence of Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Comparison of structure distributions across domains; examples of domain specific structures; and Parsing experiment using some domain dependent grammars demonstrate domain dependence and idiosyncrasy of syntactic structure."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12615602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3764baa7465201f054083d02b58fa75f883c4461",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "slug": "A-New-Statistical-Parser-Based-on-Bigram-Lexical-Collins",
            "title": {
                "fragments": [],
                "text": "A New Statistical Parser Based on Bigram Lexical Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new statistical parser which is based on probabilities of dependencies between head-words in the parse tree, which trains on 40,000 sentences in under 15 minutes and can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316623"
                        ],
                        "name": "Jakub Zavrel",
                        "slug": "Jakub-Zavrel",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Zavrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Zavrel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1138221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41c00f7fbec0ab603836357d020cef7843ef7405",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyses the relation between the use of similarity in Memory-Based Learning and the notion of backed-off smoothing in statistical language modeling. We show that the two approaches are closely related, and we argue that feature weighting methods in the Memory-Based paradigm can offer the advantage of automatically specifying a suitable domain-specific hierarchy between most specific and most general conditioning information without the need for a large number of parameters. We report two applications of this approach: PP-attachment and POS-tagging. Our method achieves state-of-the-art performance in both domains, and allows the easy integration of diverse information sources, such as rich lexical representations."
            },
            "slug": "Memory-Based-Learning:-Using-Similarity-for-Zavrel-Daelemans",
            "title": {
                "fragments": [],
                "text": "Memory-Based Learning: Using Similarity for Smoothing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is argued that feature weighting methods in the Memory-Based paradigm can offer the advantage of automatically specifying a suitable domain-specific hierarchy between most specific and most general conditioning information without the need for a large number of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2614094"
                        ],
                        "name": "Rebecca F. Bruce",
                        "slug": "Rebecca-F.-Bruce",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Bruce",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rebecca F. Bruce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144120827"
                        ],
                        "name": "J. Wiebe",
                        "slug": "J.-Wiebe",
                        "structuredName": {
                            "firstName": "Janyce",
                            "lastName": "Wiebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wiebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 81321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28e11a77bf6670984c6f1753ff41ec5046fc040b",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical models of word-sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features. Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features. This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word-sense disambiguation."
            },
            "slug": "Sequential-Model-Selection-for-Word-Sense-Pedersen-Bruce",
            "title": {
                "fragments": [],
                "text": "Sequential Model Selection for Word Sense Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word-sense disambiguation."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60458454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69859be3ea6cb8eb38434c80fef5d4997eaec2dc",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation introduces a new theoretical model for text classification systems, including systems for document retrieval, automated indexing, electronic mail filtering, and similar tasks. The Concept Learning model emphasizes the role of manual and automated feature selection and classifier formation in text classification. It enables drawing on results from statistics and machine learning in explaining the effectiveness of alternate representations of text, and specifies desirable characteristics of text representations. \nThe use of syntactic parsing to produce indexing phrases has been widely investigated as a possible route to better text representations. Experiments with syntactic phrase indexing, however, have never yielded significant improvements in text retrieval performance. The Concept Learning model suggests that the poor statistical characteristics of a syntactic indexing phrase representation negate its desirable semantic characteristics. The application of term clustering to this representation to improve its statistical properties while retaining its desirable meaning properties is proposed. \nStandard term clustering strategies from information retrieval (IR), based on cooccurrence of indexing terms in documents or groups of documents, were tested on a syntactic indexing phrase representation. In experiments using a standard text retrieval test collection, small effectiveness improvements were obtained. \nAs a means of evaluating representation quality, a text retrieval test collection introduces a number of confounding factors. In contrast, the text categorization task allows much cleaner determination of text representation properties. In preparation for the use of text categorization to study text representation, a more effective and theoretically well-founded probabilistic text categorization algorithm was developed, building on work by Maron, Fuhr, and others. \nText categorization experiments supported a number of predictions of the Concept Learning model about properties of phrasal representations, including dimensionality properties not previously measured for text representations. However, in carefully controlled experiments using syntactic phrases produced by Church's stochastic bracketer, in conjunction with reciprocal nearest neighbor clustering, term clustering was found to produce essentially no improvement in the properties of the phrasal representation. New cluster analysis approaches are proposed to remedy the problems found in traditional term clustering methods."
            },
            "slug": "Representation-and-Learning-in-Information-Lewis",
            "title": {
                "fragments": [],
                "text": "Representation and Learning in Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new theoretical model for text classification systems, including systems for document retrieval, automated indexing, electronic mail filtering, and similar tasks, is introduced, suggesting that the poor statistical characteristics of a syntactic indexing phrase representation negate its desirable semantic characteristics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 725590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9c71db75046473f0e3d3229950d7c84c09afd5e",
            "isKey": false,
            "numCitedBy": 1530,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach."
            },
            "slug": "Text-Chunking-using-Transformation-Based-Learning-Ramshaw-Marcus",
            "title": {
                "fragments": [],
                "text": "Text Chunking using Transformation-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has shown that the transformation-based learning approach can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277328"
                        ],
                        "name": "D. Flickenger",
                        "slug": "D.-Flickenger",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Flickenger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Flickenger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1929125"
                        ],
                        "name": "C. Gdaniec",
                        "slug": "C.-Gdaniec",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Gdaniec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gdaniec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143878270"
                        ],
                        "name": "P. Harrison",
                        "slug": "P.-Harrison",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Harrison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Harrison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21169546"
                        ],
                        "name": "Donald Hindle",
                        "slug": "Donald-Hindle",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hindle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Hindle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319612"
                        ],
                        "name": "R. Ingria",
                        "slug": "R.-Ingria",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Ingria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ingria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761739"
                        ],
                        "name": "Judith L. Klavans",
                        "slug": "Judith-L.-Klavans",
                        "structuredName": {
                            "firstName": "Judith",
                            "lastName": "Klavans",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Judith L. Klavans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144173823"
                        ],
                        "name": "M. Liberman",
                        "slug": "M.-Liberman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Liberman",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liberman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791072"
                        ],
                        "name": "T. Strzalkowski",
                        "slug": "T.-Strzalkowski",
                        "structuredName": {
                            "firstName": "Tomek",
                            "lastName": "Strzalkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Strzalkowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17643319,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "7689778171dc100bb636fc0e4e2ce4063967d3c9",
            "isKey": false,
            "numCitedBy": 554,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even the simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (Hewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boeing), Don Hindle (AT&T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in common only the following constituents, when each grammarian provides the single parse which he/she would ideally want his/her grammar to specify for three sample Brown Corpus sentences:The famed Yankee Clipper, now retired, has been assisting (as (a batting coach)).One of those capital-gains ventures, in fact, has saddled him (with Gore Court).He said this constituted a (very serious) misuse (of the (Criminal court) processes)."
            },
            "slug": "A-Procedure-for-Quantitatively-Comparing-the-of-Black-Abney",
            "title": {
                "fragments": [],
                "text": "A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046487"
                        ],
                        "name": "Jeffrey C. Reynar",
                        "slug": "Jeffrey-C.-Reynar",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Reynar",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey C. Reynar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 129886,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "00f20179b9087fbf24b6656008a9380c590d9ec9",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A parser for natural language must often choose between two or more equally grammatical parses for the same sentence. Often the correct parse can be determined from the lexical properties of certain key words or from the context in which the sentence occurs. For example in the sentence."
            },
            "slug": "A-Maximum-Entropy-Model-for-Prepositional-Phrase-Ratnaparkhi-Reynar",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Model for Prepositional Phrase Attachment"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A parser for natural language must often choose between two or more equally grammatical parses for the same sentence."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145428168"
                        ],
                        "name": "M. Riley",
                        "slug": "M.-Riley",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Riley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19840120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ae53db00b936dfc948a644dc19b53eaa8039180",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Several applications of statistical tree-based modelling are described here to problems in speech and language. Classification and regression trees are well suited to many of the pattern recognition problems encountered in this area since they (1) statistically select the most significant features involved (2) provide \"honest\" estimates of their performance, (3) permit both categorical and continuous features to be considered, and (4) allow human interpretation and exploration of their result. First the method is summarized, then its application to automatic stop classification, segment duration prediction for synthesis, phoneme-to-phone classification, and end-of-sentence detection in text are described. For other applications to speech and language, see [Lucassen 1984], [Bahl, et al 1987]."
            },
            "slug": "Some-Applications-of-Tree-based-Modelling-to-Speech-Riley",
            "title": {
                "fragments": [],
                "text": "Some Applications of Tree-based Modelling to Speech and Language"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Several applications of statistical tree-based modelling to problems in speech and language, including automatic stop classification, segment duration prediction for synthesis, phoneme-to-phone classification, and end-of-sentence detection in text are described."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16695022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17beaab6b27faef07f988325503b7c30a6377753",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a new technique for parsing free text: a transformational grammar is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce the number of errors. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction."
            },
            "slug": "Transformation-Based-Error-Driven-Parsing-Brill",
            "title": {
                "fragments": [],
                "text": "Transformation-Based Error-Driven Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A transformational grammar is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees, and a set of simple structural transformations are applied to reduce the number of errors."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853032"
                        ],
                        "name": "J. Lai",
                        "slug": "J.-Lai",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Lai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10986188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3de5d40b60742e3dfa86b19e7f660962298492af",
            "isKey": false,
            "numCitedBy": 3318,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics."
            },
            "slug": "Class-Based-n-gram-Models-of-Natural-Language-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "Class-Based n-gram Models of Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work addresses the problem of predicting a word from previous words in a sample of text and discusses n-gram models based on classes of words, finding that these models are able to extract classes that have the flavor of either syntactically based groupings or semanticallybased groupings, depending on the nature of the underlying statistics."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3166885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.<<ETX>>"
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written and performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2697855"
                        ],
                        "name": "M. Ringuette",
                        "slug": "M.-Ringuette",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Ringuette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ringuette"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16894634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9fd1a7ae0322d417ab2d32017e373dd50efc063",
            "isKey": false,
            "numCitedBy": 745,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the use of inductive learning to categorize natural language documents into predeened content categories. Categorization of text is of increasing importance in information retrieval and natural language processing systems. Previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it diicult to draw conclusions about the performance of particular methods. In this paper we present empirical results on the performance of a Bayesian classiier and a decision tree learning algorithm on two text categorization data sets. We nd that both algorithms achieve reasonable performance and allow controlled tradeoos between false positives and false negatives. The stepwise feature selection in the decision tree algorithm is particularly eeective in dealing with the large feature sets common in text categorization. However, even this algorithm is aided by an initial preeltering of features, connrming the results found by Almuallim and Dietterich on artiicial data sets. We also demonstrate the impact of the time-varying nature of category deenitions."
            },
            "slug": "A-comparison-of-two-learning-algorithms-for-text-Lewis-Ringuette",
            "title": {
                "fragments": [],
                "text": "A comparison of two learning algorithms for text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that both algorithms achieve reasonable performance and allow controlled tradeoos between false positives and false negatives, and the stepwise feature selection in the decision tree algorithm is particularly eeective in dealing with the large feature sets common in text categorization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9880507,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a5e619f2c5f4220438b1357e596db5b1578398d",
            "isKey": false,
            "numCitedBy": 643,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence. This model is used in a parsing system by finding the parse for the sentence with the highest probability. This system outperforms previous schemes. As this is the third in a series of parsers by different authors that are similar enough to invite detailed comparisons but different enough to give rise to different levels of performance, we also report on some experiments designed to identify what aspects of these systems best explain their relative performance."
            },
            "slug": "Statistical-Parsing-with-a-Context-Free-Grammar-and-Charniak",
            "title": {
                "fragments": [],
                "text": "Statistical Parsing with a Context-Free Grammar and Word Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence that outperforms previous schemes is described."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17446277"
                        ],
                        "name": "J. Goodman",
                        "slug": "J.-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18774233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2588593c42126e059fb8aad7673fa1736755f1e1",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate features one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words."
            },
            "slug": "Probabilistic-Feature-Grammars-Goodman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Feature Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Probabilistic feature grammar combines most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098924318"
                        ],
                        "name": "Jiri Stetina",
                        "slug": "Jiri-Stetina",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Stetina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Stetina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2162080"
                        ],
                        "name": "M. Nagao",
                        "slug": "M.-Nagao",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Nagao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nagao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6493988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31ea50a9a54050decad35d775788afa7a4615ced",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity. We propose a new supervised learning method for PPattachment based on a semantically tagged corpus. Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags. We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods."
            },
            "slug": "Corpus-Based-PP-Attachment-Ambiguity-Resolution-a-Stetina-Nagao",
            "title": {
                "fragments": [],
                "text": "Corpus Based PP Attachment Ambiguity Resolution with a Semantic Dictionary"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A new supervised learning method for PP attachment based on a semantically tagged corpus and a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "VLC"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9716882,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "56d7826f3afaa374077f87ca3529709b1ca7e044",
            "isKey": false,
            "numCitedBy": 992,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this: \n \n(1) \n \n[I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time] \n \n \n \n \n \n \nThese chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurrence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them; and the order in which chunks occur is much more flexible than the order of words within chunks."
            },
            "slug": "Parsing-By-Chunks-Abney",
            "title": {
                "fragments": [],
                "text": "Parsing By Chunks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template, and the relationships between chunks are mediated more by lexical selection than by rigid templates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21898,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ffa423a5283396c88ff3d4033d541796bd039cc",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96)."
            },
            "slug": "Three-Generative,-Lexicalised-Models-for-Parsing-Collins",
            "title": {
                "fragments": [],
                "text": "Three Generative, Lexicalised Models for Statistical Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new statistical parsing model is proposed, which is a generative model of lexicalised context-free grammar and extended to include a probabilistic treatment of both subcategorisation and wh-movement."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118715495"
                        ],
                        "name": "Michael White",
                        "slug": "Michael-White",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "White",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 129
                            }
                        ],
                        "text": "Siteswhich logically should be marked with multiple punctuation marks will often only haveone ([Nunberg, 1990] as summarized in [White, 1995])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 25016096,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "19e70e4aa29995ca77a58c467630c91b7fd3b9cf",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Until recently, punctuation has received very little attention in the linguistics and computational inguistics literature, even though punctuation marks are evidently among the most important structural elements in written language. Since the publication of Nunberg\u2019s (1990) monograph The Linguistics of Punctuation, however, punctuation has seen its stock begin to rise: spurred in part by Nunberg\u2019s ground-breaking work, a number of valuable inquiries have been subsequently undertaken, including Hovy and Arens (1991), Dale (1991), Pascual (1993), Jones (1994), and Briscoe (1994). Of these recent works, those on the synthesis side have focused on ways of formalizing the discourse functions of punctuation, whereas those on the analysis side have emphasized how punctuation can improve robust parsing."
            },
            "slug": "Presenting-Punctuation-White",
            "title": {
                "fragments": [],
                "text": "Presenting Punctuation"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Until recently, punctuation has received very little attention in the linguistics and computational inguistics literature, but since the publication of Nunberg\u2019s monograph The Linguistics of Punctuation, its stock has seen its stock begin to rise."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1606448536"
                        ],
                        "name": "Daniel Karp",
                        "slug": "Daniel-Karp",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Karp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Karp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50365696"
                        ],
                        "name": "Martin Zaidel",
                        "slug": "Martin-Zaidel",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Zaidel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Zaidel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2898298"
                        ],
                        "name": "Dania Egedi",
                        "slug": "Dania-Egedi",
                        "structuredName": {
                            "firstName": "Dania",
                            "lastName": "Egedi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dania Egedi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5480885,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "d5bd9ad9a3b49ce936679a3c2bf5ec7277bcb5f5",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a morphological lexicon for English that handle more than 317000 inflected forms derived from over 90000 stems. The lexicon is available in two formats. The first can be used by an implementation of a two-level processor for morphological analysis (Karttunen and Wittenburg, 1983; Antworth, 1990). The second, derived from the first one for efficiency reasons, consists of a disk-based database using a UNIX hash table facility (Seltzer and Yigit, 1991). We also built an X Window tool to facilitate the maintenance and browsing of the lexicon. The package is ready to be integrated into an natural language application such as a parser through hooks written in Lisp and C.To our knowledge, this package is the only available free English morphological analyzer with very wide coverage."
            },
            "slug": "A-Freely-Available-Wide-Coverage-Morphological-for-Karp-Schabes",
            "title": {
                "fragments": [],
                "text": "A Freely Available Wide Coverage Morphological Analyzer for English"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A morphological lexicon for English that handles more than 317000 inflected forms derived from over 90000 stems and is ready to be integrated into an natural language application such as a parser through hooks written in Lisp and C."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 936966,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "5bfa91e7ec19c6401a763c73f2a2007c04836609",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Parser development is generally viewed as a primarily linguistic enterprise. A grammarian examines sentences, skillfully extracts the linguistic generalizations evident in the data, and writes grammar rules which cover the language. The grammarian then evaluates the performance of the grammar, and upon analysis of the errors made by the grammar-based parser, carefully refines the rules, repeating this process, typically over a period of several years."
            },
            "slug": "Decision-Tree-Parsing-using-a-Hidden-Derivation-Jelinek-Lafferty",
            "title": {
                "fragments": [],
                "text": "Decision Tree Parsing using a Hidden Derivation Model"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The grammarian then evaluates the performance of the grammar, and upon analysis of the errors made by the grammar-based parser, carefully refines the rules, repeating this process, typically over a period of several years."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21169546"
                        ],
                        "name": "Donald Hindle",
                        "slug": "Donald-Hindle",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hindle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Hindle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416786"
                        ],
                        "name": "Mats Rooth",
                        "slug": "Mats-Rooth",
                        "structuredName": {
                            "firstName": "Mats",
                            "lastName": "Rooth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mats Rooth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5410054,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "bdaf232c561f1f50e88b1d24097e214890b37e8b",
            "isKey": false,
            "numCitedBy": 638,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning."
            },
            "slug": "Structural-Ambiguity-and-Lexical-Relations-Hindle-Rooth",
            "title": {
                "fragments": [],
                "text": "Structural Ambiguity and Lexical Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "It is proposed that many ambiguous prepositional phrase attachments can be resolved on the based of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6616065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd459cc59b09e612eeec5327d0690d1508ffe362",
            "isKey": false,
            "numCitedBy": 876,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Assume that the syntax of natural language can be parsed by a left-to-right deterministic mechanism without facilities for parallelism or backup. It will be shown that this 'determinism' hypothesis, explored within the context of the grammar of English, leads to a simple mechanism, a grammar interpreter. (Author)"
            },
            "slug": "A-theory-of-syntactic-recognition-for-natural-Marcus",
            "title": {
                "fragments": [],
                "text": "A theory of syntactic recognition for natural language"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It will be shown that this 'determinism' hypothesis, explored within the context of the grammar of English, leads to a simple mechanism, a grammar interpreter."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756798"
                        ],
                        "name": "I. Csisz\u00e1r",
                        "slug": "I.-Csisz\u00e1r",
                        "structuredName": {
                            "firstName": "Imre",
                            "lastName": "Csisz\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Csisz\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18053591,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "72b2aeeb76dbff312321ccbcc58e85009e0b57ae",
            "isKey": false,
            "numCitedBy": 1453,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve and extend access to The Annals of Probability. Some geometric properties of PD's are established, Kullback's I-divergence playing the role of squared Euclidean distance. The minimum discrimination information problem is viewed as that of projecting a PD onto a convex set of PD's and useful existence theorems for and characterizations of the minimizing PD are arrived at. A natural generalization of known iterative algorithms converging to the minimizing PD in special situations is given; even for those special cases, our convergence proof is more generally valid than those previously published. As corollaries of independent interest, generalizations of known results on the existence of PD's or nonnegative matrices of a certain form are obtained. The Lagrange multiplier technique is not used."
            },
            "slug": "$I$-Divergence-Geometry-of-Probability-and-Problems-Csisz\u00e1r",
            "title": {
                "fragments": [],
                "text": "$I$-Divergence Geometry of Probability Distributions and Minimization Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1886381"
                        ],
                        "name": "Erika F. de Lima",
                        "slug": "Erika-F.-de-Lima",
                        "structuredName": {
                            "firstName": "Erika",
                            "lastName": "Lima",
                            "middleNames": [
                                "F.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erika F. de Lima"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 425946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97c3d6bba043d8ddedaa7e063691de5684c29f42",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a corpus-based method to assign grammatical subject/object relations to ambiguous German constructs. It makes use of an unsupervised learning procedure to collect training and test data, and the back-off model to make assignment decisions."
            },
            "slug": "Assigning-Grammatical-Relations-with-a-Back-off-Lima",
            "title": {
                "fragments": [],
                "text": "Assigning Grammatical Relations with a Back-off Model"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper presents a corpus-based method to assign grammatical subject/object relations to ambiguous German constructs that makes use of an unsupervised learning procedure to collect training and test data, and the back-off model to make assignment decisions."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2216687"
                        ],
                        "name": "D. Hosmer",
                        "slug": "D.-Hosmer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hosmer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hosmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2671578"
                        ],
                        "name": "S. Lemeshow",
                        "slug": "S.-Lemeshow",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Lemeshow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lemeshow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119351629,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8f451c7adaea6a65cea674e0326237e29be7dc70",
            "isKey": false,
            "numCitedBy": 33801,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "\"A new edition of the definitive guide to logistic regression modeling for health science and other applicationsThis thoroughly expanded Third Edition provides an easily accessible introduction to the logistic regression (LR) model and highlights the power of this model by examining the relationship between a dichotomous outcome and a set of covariables. Applied Logistic Regression, Third Edition emphasizes applications in the health sciences and handpicks topics that best suit the use of modern statistical software. The book provides readers with state-of-the-art techniques for building, interpreting, and assessing the performance of LR models. New and updated features include: A chapter on the analysis of correlated outcome data. A wealth of additional material for topics ranging from Bayesian methods to assessing model fit Rich data sets from real-world studies that demonstrate each method under discussion. Detailed examples and interpretation of the presented results as well as exercises throughout Applied Logistic Regression, Third Edition is a must-have guide for professionals and researchers who need to model nominal or ordinal scaled outcome variables in public health, medicine, and the social sciences as well as a wide range of other fields and disciplines\"--"
            },
            "slug": "Applied-Logistic-Regression-Hosmer-Lemeshow",
            "title": {
                "fragments": [],
                "text": "Applied Logistic Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Applied Logistic Regression, Third Edition provides an easily accessible introduction to the logistic regression model and highlights the power of this model by examining the relationship between a dichotomous outcome and a set of covariables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4683457,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "ea8f1c73422b087738827a665b2aaf9b93d5c543",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a new corpus-based approach to prepositional phrase attachment disambiguation, and present results comparing performance of this algorithm with other corpus-based approaches to this problem."
            },
            "slug": "A-Rule-Based-Approach-to-Prepositional-Phrase-Brill-Resnik",
            "title": {
                "fragments": [],
                "text": "A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A new corpus- based approach to prepositional phrase attachment disambiguation is described, and results comparing performance of this algorithm with other corpus-based approaches to this problem are presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069628131"
                        ],
                        "name": "Alexander Franz",
                        "slug": "Alexander-Franz",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Franz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Franz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16478155,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "1ee2e86360ee6ab31669a1cdd577c562dd4cc166",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Many current approaches to statistical language modeling rely on independence assumptions between the different explanatory variables. This results in models which are computationally simple, but which only model the main effects of the explanatory variables on the response variable. This paper presents an argument in favor of a statistical approach that also models the interactions between the explanatory variables. The argument rests on empirical evidence from two series of experimetns concerning automatic ambiguity resolution."
            },
            "slug": "Independence-Assumptions-Considered-Harmful-Franz",
            "title": {
                "fragments": [],
                "text": "Independence Assumptions Considered Harmful"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An argument in favor of a statistical approach that also models the interactions between the explanatory variables is presented, based on empirical evidence from two series of experimetns concerning automatic ambiguity resolution."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42795,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48892185"
                        ],
                        "name": "J. Teahan",
                        "slug": "J.-Teahan",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Teahan",
                            "middleNames": [
                                "K"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Teahan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6633939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d36910319d11359b995ff5413696aa9e9995e163",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "\\A new data structure for cumulative probability tables\". Soft-\\The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression\"."
            },
            "slug": "\\self-organized-Language-Modeling-for-Speech-In-Teahan-Cleary",
            "title": {
                "fragments": [],
                "text": "\\self-organized Language Modeling for Speech Recognition\". In"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression and a new data structure for cumulative probability tables are studied."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21603225"
                        ],
                        "name": "Fernando S\u00e1nchez Le\u00f3n",
                        "slug": "Fernando-S\u00e1nchez-Le\u00f3n",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Le\u00f3n",
                            "middleNames": [
                                "S\u00e1nchez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando S\u00e1nchez Le\u00f3n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 223
                            }
                        ],
                        "text": "\u2026inconsistencies exist in thecorpus.5.6 Experiments on other CorporaThe tagger has also been evaluated on the LOB corpus[Johansson, 1986], which containssamples of British English, and also on the CRATER corpus[S anchez-Le on, 1994], whichcontains samples of Spanish in the telecommunications domain."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41746218,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1e657187d5c2afdd00eae89aa5ef79c5b18fab7e",
            "isKey": true,
            "numCitedBy": 12,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This working paper describes the Spanish tagset to be used in the context of CRATER, a CEC funded project aiming at the creation of a multilingual (English, French, Spanish) aligned corpus using the International Telecommunications Union corpus. In this respect, each version of the corpus will be (or is currently) tagged. Xerox PARC tagger will be adapted to Spanish in order to perform the tagging of the Spanish version. This tagset has been devised as the ideal one for Spanish, and has been posted to several lists in order to get feedback to it."
            },
            "slug": "A-Spanish-Tagset-for-the-CRATER-Project-Le\u00f3n",
            "title": {
                "fragments": [],
                "text": "A Spanish Tagset for the CRATER Project"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This working paper describes the Spanish tagset to be used in the context of CRATER, a CEC funded project aiming at the creation of a multilingual (English, French, Spanish) aligned corpus using the International Telecommunications Union corpus."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715952"
                        ],
                        "name": "A. Aho",
                        "slug": "A.-Aho",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Aho",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144281449"
                        ],
                        "name": "R. Sethi",
                        "slug": "R.-Sethi",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Sethi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sethi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42981739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7f33d55d94e75a554251fe7dc07f1d7b4db8e1a",
            "isKey": false,
            "numCitedBy": 9130,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction 1.1 Language Processors 1.2 The Structure of a Compiler 1.3 The Evolution of Programming Languages 1.4 The Science of Building a Compiler 1.5 Applications of Compiler Technology 1.6 Programming Language Basics 1.7 Summary of Chapter 1 1.8 References for Chapter 1 2 A Simple Syntax-Directed Translator 2.1 Introduction 2.2 Syntax Definition 2.3 Syntax-Directed Translation 2.4 Parsing 2.5 A Translator for Simple Expressions 2.6 Lexical Analysis 2.7 Symbol Tables 2.8 Intermediate Code Generation 2.9 Summary of Chapter 2 3 Lexical Analysis 3.1 The Role of the Lexical Analyzer 3.2 Input Buffering 3.3 Specification of Tokens 3.4 Recognition of Tokens 3.5 The Lexical-Analyzer Generator Lex 3.6 Finite Automata 3.7 From Regular Expressions to Automata 3.8 Design of a Lexical-Analyzer Generator 3.9 Optimization of DFA-Based Pattern Matchers 3.10 Summary of Chapter 3 3.11 References for Chapter 3 4 Syntax Analysis 4.1 Introduction 4.2 Context-Free Grammars 4.3 Writing a Grammar 4.4 Top-Down Parsing 4.5 Bottom-Up Parsing 4.6 Introduction to LR Parsing: Simple LR 4.7 More Powerful LR Parsers 4.8 Using Ambiguous Grammars 4.9 Parser Generators 4.10 Summary of Chapter 4 4.11 References for Chapter 4 5 Syntax-Directed Translation 5.1 Syntax-Directed Definitions 5.2 Evaluation Orders for SDD's 5.3 Applications of Syntax-Directed Translation 5.4 Syntax-Directed Translation Schemes 5.5 Implementing L-Attributed SDD's 5.6 Summary of Chapter 5 5.7 References for Chapter 5 6 Intermediate-Code Generation 6.1 Variants of Syntax Trees 6.2 Three-Address Code 6.3 Types and Declarations 6.4 Translation of Expressions 6.5 Type Checking 6.6 Control Flow 6.7 Backpatching 6.8 Switch-Statements 6.9 Intermediate Code for Procedures 6.10 Summary of Chapter 6 6.11 References for Chapter 6 7 Run-Time Environments 7.1 Storage Organization 7.2 Stack Allocation of Space 7.3 Access to Nonlocal Data on the Stack 7.4 Heap Management 7.5 Introduction to Garbage Collection 7.6 Introduction to Trace-Based Collection 7.7 Short-Pause Garbage Collection 7.8 Advanced Topics in Garbage Collection 7.9 Summary of Chapter 7 7.10 References for Chapter 7 8 Code Generation 8.1 Issues in the Design of a Code Generator 8.2 The Target Language 8.3 Addresses in the Target Code 8.4 Basic Blocks and Flow Graphs 8.5 Optimization of Basic Blocks 8.6 A Simple Code Generator 8.7 Peephole Optimization 8.8 Register Allocation and Assignment 8.9 Instruction Selection by Tree Rewriting 8.10 Optimal Code Generation for Expressions 8.11 Dynamic Programming Code-Generation 8.12 Summary of Chapter 8 8.13 References for Chapter 8 9 Machine-Independent Optimizations 9.1 The Principal Sources of Optimization 9.2 Introduction to Data-Flow Analysis 9.3 Foundations of Data-Flow Analysis 9.4 Constant Propagation 9.5 Partial-Redundancy Elimination 9.6 Loops in Flow Graphs 9.7 Region-Based Analysis 9.8 Symbolic Analysis 9.9 Summary of Chapter 9 9.10 References for Chapter 9 10 Instruction-Level Parallelism 10.1 Processor Architectures 10.2 Code-Scheduling Constraints 10.3 Basic-Block Scheduling 10.4 Global Code Scheduling 10.5 Software Pipelining 10.6 Summary of Chapter 10 10.7 References for Chapter 10 11 Optimizing for Parallelism and Locality 11.1 Basic Concepts 11.2 Matrix Multiply: An In-Depth Example 11.3 Iteration Spaces 11.4 Affine Array Indexes 11.5 Data Reuse 11.6 Array Data-Dependence Analysis 11.7 Finding Synchronization-Free Parallelism 11.8 Synchronization Between Parallel Loops 11.9 Pipelining 11.10 Locality Optimizations 11.11 Other Uses of Affine Transforms 11.12 Summary of Chapter 11 11.13 References for Chapter 11 12 Interprocedural Analysis 12.1 Basic Concepts 12.2 Why Interprocedural Analysis? 12.3 A Logical Representation of Data Flow 12.4 A Simple Pointer-Analysis Algorithm 12.5 Context-Insensitive Interprocedural Analysis 12.6 Context-Sensitive Pointer Analysis 12.7 Datalog Implementation by BDD's 12.8 Summary of Chapter 12 12.9 References for Chapter 12 A A Complete Front End A.1 The Source Language A.2 Main A.3 Lexical Analyzer A.4 Symbol Tables and Types A.5 Intermediate Code for Expressions A.6 Jumping Code for Boolean Expressions A.7 Intermediate Code for Statements A.8 Parser A.9 Creating the Front End B Finding Linearly Independent Solutions Index"
            },
            "slug": "Compilers:-Principles,-Techniques,-and-Tools-Aho-Sethi",
            "title": {
                "fragments": [],
                "text": "Compilers: Principles, Techniques, and Tools"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This book discusses the design of a Code Generator, the role of the Lexical Analyzer, and other topics related to code generation and optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Addison-Wesley series in computer science / World student series edition"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644031891"
                        ],
                        "name": "WeischedelRalph",
                        "slug": "WeischedelRalph",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "WeischedelRalph",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "WeischedelRalph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644053665"
                        ],
                        "name": "SchwartzRichard",
                        "slug": "SchwartzRichard",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "SchwartzRichard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "SchwartzRichard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644053565"
                        ],
                        "name": "PalmucciJeff",
                        "slug": "PalmucciJeff",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "PalmucciJeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "PalmucciJeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643823873"
                        ],
                        "name": "MeteerMarie",
                        "slug": "MeteerMarie",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "MeteerMarie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "MeteerMarie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644053939"
                        ],
                        "name": "RamshawLance",
                        "slug": "RamshawLance",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "RamshawLance",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "RamshawLance"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 215902625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec70ef24e627bc93c30b94a7dcd5f6e957983770",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models. This paper reports o..."
            },
            "slug": "Coping-with-ambiguity-and-unknown-words-through-WeischedelRalph-SchwartzRichard",
            "title": {
                "fragments": [],
                "text": "Coping with ambiguity and unknown words through probabilistic models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "From spring 1990 through fall 1991, a battery of small experiments were performed to test the effectiveness of supplementing knowledge-based techniques with probabilistic models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17870175,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "08b67692bc037eada8d3d7ce76cc70994e7c8116",
            "isKey": false,
            "numCitedBy": 10876,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Treatment of the predictive aspect of statistical mechanics as a form of statistical inference is extended to the density-matrix formalism and applied to a discussion of the relation between irreversibility and information loss. A principle of \"statistical complementarity\" is pointed out, according to which the empirically verifiable probabilities of statistical mechanics necessarily correspond to incomplete predictions. A preliminary discussion is given of the second law of thermodynamics and of a certain class of irreversible processes, in an approximation equivalent to that of the semiclassical theory of radiation."
            },
            "slug": "Information-Theory-and-Statistical-Mechanics-Jaynes",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistical Mechanics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49580787"
                        ],
                        "name": "B. Shalit",
                        "slug": "B.-Shalit",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Shalit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shalit"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33044865,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "de6e09032fd99e01707ebe62077681dc9da968f0",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "How well a person can cope with any situation is determined, among other factors, by his ability to resolve the ambiguity of that situation, which in turn depends on its structural complexity. We attempted to analyze a whole range of situations in terms of three dimesions: differentiations, the number of alternatives perceived; articulation, the differentiation and rankability of these alternatives; and loading, the emotional loading (positive or negative) associated with the situation. The efficiency of mapping with these dimensions was investigated by analyzing the effects of 75 situations on coping, as reported in the literature. These were divided into three groups, 25 situations in each, associated with good coping, reduced coping, and failure to come (in terms of performance relative to a baseline or to the population's norm). The interaction between the demands imposed by the complexity of the situation and the success of coping was analyzed by the Multidimensional Scalogram Analysis (MSA). Results show effectiveness of coping to be inversely related to the structural complexity of a situation, mapped in terms of articulation, loading and differentiation. The relative importance of these dimensions is as ordered above; this offers a possible way to construct an \"ambiguity score\" which may be the primary factor determining the coping limit set by any situation."
            },
            "slug": "Structural-ambiguity-and-limits-to-coping.-Shalit",
            "title": {
                "fragments": [],
                "text": "Structural ambiguity and limits to coping."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Results show effectiveness of coping to be inversely related to the structural complexity of a situation, mapped in terms of articulation, loading and differentiation."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of human stress"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140072"
                        ],
                        "name": "R. Larsen",
                        "slug": "R.-Larsen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Larsen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Larsen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 208849637,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e6289fea26b64688d2de7ad43fac98b794728582",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction 1.1 An Overview 1.2 Some Examples 1.3 A Brief History 1.4 A Chapter Summary 2. Probability 2.1 Introduction 2.2 Sample Spaces and the Algebra of Sets 2.3 The Probability Function 2.4 Conditional Probability 2.5 Independence 2.6 Combinatorics 2.7 Combinatorial Probability 2.8 Taking a Second Look at Statistics (Monte Carlo Techniques) 3. Random Variables 3.1 Introduction 3.2 Binomial and Hypergeometric Probabilities 3.3 Discrete Random Variables 3.4 Continuous Random Variables 3.5 Expected Values 3.6 The Variance 3.7 Joint Densities 3.8 Transforming and Combining Random Variables 3.9 Further Properties of the Mean and Variance 3.10 Order Statistics 3.11 Conditional Densities 3.12 Moment-Generating Functions 3.13 Taking a Second Look at Statistics (Interpreting Means) Appendix 3.A.1 MINITAB Applications 4. Special Distributions 4.1 Introduction 4.2 The Poisson Distribution 4.3 The Normal Distribution 4.4 The Geometric Distribution 4.5 The Negative Binomial Distribution 4.6 The Gamma Distribution 4.7 Taking a Second Look at Statistics (Monte Carlo Simulations) Appendix 4.A.1 MINITAB Applications Appendix 4.A.2 A Proof of the Central Limit Theorem 5. Estimation 5.1 Introduction 5.2 Estimating Parameters: The Method of Maximum Likelihood and the Method of Moments 5.3 Interval Estimation 5.4 Properties of Estimators 5.5 Minimum-Variance Estimators: The Crami?\u00bdr-Rao Lower Bound 5.6 Sufficient Estimators 5.7 Consistency 5.8 Bayesian Estimation 5.9 Taking A Second Look at Statistics (Beyond Classical Estimation) Appendix 5.A.1 MINITAB Applications 6. Hypothesis Testing 6.1 Introduction 6.2 The Decision Rule 6.3 Testing Binomial Data\u00e2 H0: p = po 6.4 Type I and Type II Errors 6.5 A Notion of Optimality: The Generalized Likelihood Ratio 6.6 Taking a Second Look at Statistics (Statistical Significance versus \u00e2 Practical\u00e2 Significance) 7. Inferences Based on the Normal Distribution 7.1 Introduction 7.2 Comparing Y-i?\u00bd s/ vn and Y-i?\u00bd S/ vn 7.3 Deriving the Distribution of Y-i?\u00bd S/ vn 7.4 Drawing Inferences About i?\u00bd 7.5 Drawing Inferences About s2 7.6 Taking a Second Look at Statistics (Type II Error) Appendix 7.A.1 MINITAB Applications Appendix 7.A.2 Some Distribution Results for Y and S2 Appendix 7.A.3 A Proof that the One-Sample t Test is a GLRT Appendix 7.A.4 A Proof of Theorem 7.5.2 8. Types of Data: A Brief Overview 8.1 Introduction 8.2 Classifying Data 8.3 Taking a Second Look at Statistics (Samples Are Not \u00e2 Valid\u00e2 !) 9. Two-Sample Inferences 9.1 Introduction 9.2 Testing H0: i?\u00bdX =i?\u00bdY 9.3 Testing H0: s2X=s2Y\u00e2 The F Test 9.4 Binomial Data: Testing H0: pX = pY 9.5 Confidence Intervals for the Two-Sample Problem 9.6 Taking a Second Look at Statistics (Choosing Samples) Appendix 9.A.1 A Derivation of the Two-Sample t Test (A Proof of Theorem 9.2.2) Appendix 9.A.2 MINITAB Applications 10. Goodness-of-Fit Tests 10.1 Introduction 10.2 The Multinomial Distribution 10.3 Goodness-of-Fit Tests: All Parameters Known 10.4 Goodness-of-Fit Tests: Parameters Unknown 10.5 Contingency Tables 10.6 Taking a Second Look at Statistics (Outliers) Appendix 10.A.1 MINITAB Applications 11. Regression 11.1 Introduction 11.2 The Method of Least Squares 11.3 The Linear Model 11.4 Covariance and Correlation 11.5 The Bivariate Normal Distribution 11.6 Taking a Second Look at Statistics (How Not to Interpret the Sample Correlation Coefficient) Appendix 11.A.1 MINITAB Applications Appendix 11.A.2 A Proof of Theorem 11.3.3 12. The Analysis of Variance 12.1 Introduction 12.2 The F Test 12.3 Multiple Comparisons: Tukey\u00e2 s Method 12.4 Testing Subhypotheses with Contrasts 12.5 Data Transformations 12.6 Taking a Second Look at Statistics (Putting the Subject of Statistics together\u00e2 the Contributions of Ronald A. Fisher) Appendix 12.A.1 MINITAB Applications Appendix 12.A.2 A Proof of Theorem 12.2.2 Appendix 12.A.3 The Distribution of SSTR/(k\u00e2 1) SSE/(n\u00e2 k)When H1 is True 13. Randomized Block Designs 13.1 Introduction 13.2 The F Test for a Randomized Block Design 13.3 The Paired t Test 13.4 Taking a Second Look at Statistics (Choosing between a Two-Sample t Test and a Paired t Test) Appendix 13.A.1 MINITAB Applications 14. Nonparametric Statistics 14.1 Introduction 14.2 The Sign Test 14.3 Wilcoxon Tests 14.4 The Kruskal-Wallis Test 14.5 The Friedman Test 14.6 Testing for Randomness 14.7 Taking a Second Look at Statistics (Comparing Parametric and Nonparametric Procedures) Appendix 14.A.1 MINITAB Applications Appendix: Statistical Tables Answers to Selected Odd-Numbered Questions Bibliography Index"
            },
            "slug": "An-introduction-to-mathematical-statistics-and-its-Larsen",
            "title": {
                "fragments": [],
                "text": "An introduction to mathematical statistics and its applications / Richard J. Larsen, Morris L. Marx"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 223
                            }
                        ],
                        "text": "\u2026inconsistencies exist in thecorpus.5.6 Experiments on other CorporaThe tagger has also been evaluated on the LOB corpus[Johansson, 1986], which containssamples of British English, and also on the CRATER corpus[S anchez-Le on, 1994], whichcontains samples of Spanish in the telecommunications domain."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spanish tagset for the CRATER project Also available on cmp-lg archive a s http:::xxx.lanl"
            },
            "venue": {
                "fragments": [],
                "text": "Laboratorio de Ling u istica Inform atica"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179124"
                        ],
                        "name": "I. Good",
                        "slug": "I.-Good",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Good",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Good"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 123298174,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "602084417015618f112c796828786a6af72bf7d9",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Maximum-Entropy-for-Hypothesis-Formulation,-for-Good",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy for Hypothesis Formulation, Especially for Multidimensional Contingency Tables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49223598"
                        ],
                        "name": "J. Darroch",
                        "slug": "J.-Darroch",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Darroch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Darroch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12360582"
                        ],
                        "name": "D. Ratcliff",
                        "slug": "D.-Ratcliff",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Ratcliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ratcliff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120862597,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37c931cbaa9217b829596dd196520a838562a109",
            "isKey": false,
            "numCitedBy": 1329,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalized-Iterative-Scaling-for-Log-Linear-Models-Darroch-Ratcliff",
            "title": {
                "fragments": [],
                "text": "Generalized Iterative Scaling for Log-Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756798"
                        ],
                        "name": "I. Csisz\u00e1r",
                        "slug": "I.-Csisz\u00e1r",
                        "structuredName": {
                            "firstName": "Imre",
                            "lastName": "Csisz\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Csisz\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 119460543,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2a64dbd2fa289013b56d04621c5be72f14a44cf7",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-geometric-interpretation-of-Darroch-and-iterative-Csisz\u00e1r",
            "title": {
                "fragments": [],
                "text": "A geometric interpretation of Darroch and Ratcliff's generalized iterative scaling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35097577"
                        ],
                        "name": "W. Francis",
                        "slug": "W.-Francis",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Francis",
                            "middleNames": [
                                "Nelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Francis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16318570"
                        ],
                        "name": "H. Kucera",
                        "slug": "H.-Kucera",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kucera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kucera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053677731"
                        ],
                        "name": "Andrew Mackie",
                        "slug": "Andrew-Mackie",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Mackie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Mackie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60972899,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "729316fbded86763104f3412cadf98f00a9a3993",
            "isKey": false,
            "numCitedBy": 2159,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "FREQUENCY-ANALYSIS-OF-ENGLISH-USAGE:-LEXICON-AND-Francis-Kucera",
            "title": {
                "fragments": [],
                "text": "FREQUENCY ANALYSIS OF ENGLISH USAGE: LEXICON AND GRAMMAR"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144480181"
                        ],
                        "name": "S. Johansson",
                        "slug": "S.-Johansson",
                        "structuredName": {
                            "firstName": "Stig",
                            "lastName": "Johansson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Johansson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144214753"
                        ],
                        "name": "E. Atwell",
                        "slug": "E.-Atwell",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Atwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Atwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153733790"
                        ],
                        "name": "R. Garside",
                        "slug": "R.-Garside",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Garside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Garside"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143998726"
                        ],
                        "name": "G. Leech",
                        "slug": "G.-Leech",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Leech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Leech"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53754601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c2dff1c9b04da49f50fd85ba010d2a041fc5c39",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-tagged-LOB-Corpus-:-user's-manual-Johansson-Atwell",
            "title": {
                "fragments": [],
                "text": "The tagged LOB Corpus : user's manual"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145065299"
                        ],
                        "name": "H. K\u00fcnkel",
                        "slug": "H.-K\u00fcnkel",
                        "structuredName": {
                            "firstName": "Helmut",
                            "lastName": "K\u00fcnkel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. K\u00fcnkel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 39702276,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "0e2c87c162a7d078b0103fe487fd977890059f9f",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Frequency-analysis.-K\u00fcnkel",
            "title": {
                "fragments": [],
                "text": "Frequency analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Electroencephalography and clinical neurophysiology. Supplement"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15061443,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f374db77b414bfdd142616434162a0aa88d5a551",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-maximum-entropy-model-for-parsing-Ratnaparkhi-Roukos",
            "title": {
                "fragments": [],
                "text": "A maximum entropy model for parsing"
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143939590"
                        ],
                        "name": "Paola Merlo",
                        "slug": "Paola-Merlo",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Merlo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paola Merlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823416"
                        ],
                        "name": "M. Crocker",
                        "slug": "M.-Crocker",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Crocker",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Crocker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3091378"
                        ],
                        "name": "Cathy Berthouzoz",
                        "slug": "Cathy-Berthouzoz",
                        "structuredName": {
                            "firstName": "Cathy",
                            "lastName": "Berthouzoz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cathy Berthouzoz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3265262,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19d378af9ecd95f86a43ae768bd5a31f5d628dab",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "There has recently been considerable interest in the use of lexically-based statistical techniques to resolve prepositional phrase attachments. To our knowledge, however, these investigations have only considered the problem of attaching the first PP, i.e., in a [V NP PP] configuration. In this paper, we consider one technique which has been successfully applied to this problem, backed-off estimation, and demonstrate how it can be extended to deal with the problem of multiple PP attachment. The multiple PP attachment introduces two related problems: sparser data (since multiple PPs are naturally rarer), and greater syntactic ambiguity (more attachment configurations which must be distinguished). We present and algorithm which solves this problem through re-use of the relatively rich data obtained from first PP training, in resolving subsequent PP attachments."
            },
            "slug": "Attaching-Multiple-Prepositional-Phrases:-Merlo-Crocker",
            "title": {
                "fragments": [],
                "text": "Attaching Multiple Prepositional Phrases: Generalized Backed-off Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An algorithm is presented which solves the problem of multiple PP attachment through re-use of the relatively rich data obtained from first PP training, in resolving subsequent PP attachments."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Computer, Speech, and Language"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive statistical language modelling. Master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive statistical language modelling. Master's thesis"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parsing By Chunks Principle-Based Parsing"
            },
            "venue": {
                "fragments": [],
                "text": "Parsing By Chunks Principle-Based Parsing"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimating lexical priors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Probabilistic LR Parsing of Natural Language Corpora with Uniication-Based Grammars. Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": "Generalized Probabilistic LR Parsing of Natural Language Corpora with Uniication-Based Grammars. Computational Linguistics"
            },
            "year": 191
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spanish tagset for the CRATER project Also available on cmp-lg archive a s http"
            },
            "venue": {
                "fragments": [],
                "text": "Laboratorio de Ling u istica Informm atica"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 84
                            }
                        ],
                        "text": "See [Csiszar, 1989] for a proof of GIS under the I-divergence geometry framework of[Csiszar, 1975]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Csiszar, I. 1975. I-Divergence Geometry of Probability Distributions and Minimization Problems. The Annals of Probability"
            },
            "venue": {
                "fragments": [],
                "text": "Csiszar, I. 1975. I-Divergence Geometry of Probability Distributions and Minimization Problems. The Annals of Probability"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Rule Based Approach to Prepo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Presenting punctuation The Nether- lands"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth European Workshop on Natural Language Generation"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive s t a t i s tical language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Computer, Speech, and Language"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A method for disam"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive statistical language modelling. Master's thesis, Massachusetts Institute of Technology"
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive statistical language modelling. Master's thesis, Massachusetts Institute of Technology"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for language model component o f a s p e e c h recognizer"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Acoustics, Speech, and Signal Processing"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive statistical language modelling. Master's thesis, Massachusetts Institute of Technology, C a m bridge"
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive statistical language modelling. Master's thesis, Massachusetts Institute of Technology, C a m bridge"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CopingWith Ambiguity and UnknownWords through Probabilistic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Procedure for Quantitatively"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spanish tagset for the CRATER project Also available on cmp-lg archive as http"
            },
            "venue": {
                "fragments": [],
                "text": "Spanish tagset for the CRATER project Also available on cmp-lg archive as http"
            },
            "year": 1994
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 1,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 89,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/Maximum-entropy-models-for-natural-language-Ratnaparkhi-Marcus/b49db3ac26d96b6c5c081dc6c2cc24da93e633f1?sort=total-citations"
}