{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522004"
                        ],
                        "name": "J. Gllavata",
                        "slug": "J.-Gllavata",
                        "structuredName": {
                            "firstName": "Julinda",
                            "lastName": "Gllavata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gllavata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738703"
                        ],
                        "name": "R. Ewerth",
                        "slug": "R.-Ewerth",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Ewerth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ewerth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685922"
                        ],
                        "name": "B. Freisleben",
                        "slug": "B.-Freisleben",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Freisleben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Freisleben"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "'s method [29]: it is based on wavelet coefficients classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31975917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "593e78f18ba5f5577b34ed81663db3e5d7d569cd",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Text localization and recognition in images is important for searching information in digital photo archives, video databases and Web sites. However, since text is often printed against a complex background, it is often difficult to detect. In this paper, a robust text localization approach is presented, which can automatically detect horizontally aligned text with different sizes, fonts, colors and languages. First, a wavelet transform is applied to the image and the distribution of high-frequency wavelet coefficients is considered to statistically characterize text and non-text areas. Then, the k-means algorithm is used to classify text areas in the image. The detected text areas undergo a projection analysis in order to refine their localization. Finally, a binary segmented text image is generated, to be used as input to an OCR engine. The detection performance of our approach is demonstrated by presenting experimental results for a set of video frames taken from the MPEG-7 video test set."
            },
            "slug": "Text-detection-in-images-based-on-unsupervised-of-Gllavata-Ewerth",
            "title": {
                "fragments": [],
                "text": "Text detection in images based on unsupervised classification of high-frequency wavelet coefficients"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A robust text localization approach is presented, which can automatically detect horizontally aligned text with different sizes, fonts, colors and languages and is demonstrated by presenting experimental results for a set of video frames taken from the MPEG-7 video test set."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694936"
                        ],
                        "name": "Qixiang Ye",
                        "slug": "Qixiang-Ye",
                        "structuredName": {
                            "firstName": "Qixiang",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qixiang Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689702"
                        ],
                        "name": "Qingming Huang",
                        "slug": "Qingming-Huang",
                        "structuredName": {
                            "firstName": "Qingming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153706048"
                        ],
                        "name": "W. Gao",
                        "slug": "W.-Gao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725937"
                        ],
                        "name": "Debin Zhao",
                        "slug": "Debin-Zhao",
                        "structuredName": {
                            "firstName": "Debin",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Debin Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8]: a feature selection algorithm which uses a SVM classifier to verify multi-scale wavelet features in the text/nontext classification tasks; (2) Mancas-Thillou et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 127
                            }
                        ],
                        "text": "Recently, there is a lot of interest on using pattern classification techniques (such as AdaBoost [7], support vector machines [5,8,10], belief propagation [11] and neural networks [9]) for text localization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17956059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcb8cd892adbfded8373716a53787f55da89180a",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-and-robust-text-detection-in-images-and-video-Ye-Huang",
            "title": {
                "fragments": [],
                "text": "Fast and robust text detection in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719610"
                        ],
                        "name": "J. Odobez",
                        "slug": "J.-Odobez",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Odobez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Odobez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "Recently, there is a lot of interest on using pattern classification techniques (such as AdaBoost [7], support vector machines [5,8,10], belief propagation [11] and neural networks [9]) for text localization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11796155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42a8ff86566538103c6116f9047a4c3128e1542c",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-detection,-recognition-in-images-and-video-Chen-Odobez",
            "title": {
                "fragments": [],
                "text": "Text detection, recognition in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785083"
                        ],
                        "name": "Michael R. Lyu",
                        "slug": "Michael-R.-Lyu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lyu",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael R. Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332642"
                        ],
                        "name": "Jiqiang Song",
                        "slug": "Jiqiang-Song",
                        "structuredName": {
                            "firstName": "Jiqiang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiqiang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052052370"
                        ],
                        "name": "Min Cai",
                        "slug": "Min-Cai",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Cai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "The second category is based on edges [2], which assume highcontrast differences between the text and background."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18648576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e14cf92ecb1589d21324d934b2009451e602d1be",
            "isKey": false,
            "numCitedBy": 371,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Text in video is a very compact and accurate clue for video indexing and summarization. Most video text detection and extraction methods hold assumptions on text color, background contrast, and font style. Moreover, few methods can handle multilingual text well since different languages may have quite different appearances. This paper performs a detailed analysis of multilingual text characteristics, including English and Chinese. Based on the analysis, we propose a comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing. The proposed method is also robust to various background complexities and text appearances. The text detection is carried out by edge detection, local thresholding, and hysteresis edge recovery. The coarse-to-fine localization scheme is then performed to identify text regions accurately. The text extraction consists of adaptive thresholding, dam point labeling, and inward filling. Experimental results on a large number of video images and comparisons with other methods are reported in detail."
            },
            "slug": "A-comprehensive-method-for-multilingual-video-text-Lyu-Song",
            "title": {
                "fragments": [],
                "text": "A comprehensive method for multilingual video text detection, localization, and extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing, and is also robust to various background complexities and text appearances."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109862994"
                        ],
                        "name": "Sunil Kumar",
                        "slug": "Sunil-Kumar",
                        "structuredName": {
                            "firstName": "Sunil",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunil Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110344379"
                        ],
                        "name": "Rajat Gupta",
                        "slug": "Rajat-Gupta",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajat Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48676526"
                        ],
                        "name": "N. Khanna",
                        "slug": "N.-Khanna",
                        "structuredName": {
                            "firstName": "Nitin",
                            "lastName": "Khanna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Khanna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144725842"
                        ],
                        "name": "S. Chaudhury",
                        "slug": "S.-Chaudhury",
                        "structuredName": {
                            "firstName": "Santanu",
                            "lastName": "Chaudhury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chaudhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705669"
                        ],
                        "name": "S. Joshi",
                        "slug": "S.-Joshi",
                        "structuredName": {
                            "firstName": "Shiv",
                            "lastName": "Joshi",
                            "middleNames": [
                                "Dutt"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Joshi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "The fourth category is based on frequencies [4], in which the text is extracted from the background in the frequency (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1223283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "909f2c6dec43e702d425b6e5166043d878e42996",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we have proposed a novel scheme for the extraction of textual areas of an image using globally matched wavelet filters. A clustering-based technique has been devised for estimating globally matched wavelet filters using a collection of groundtruth images. We have extended our text extraction scheme for the segmentation of document images into text, background, and picture components (which include graphics and continuous tone images). Multiple, two-class Fisher classifiers have been used for this purpose. We also exploit contextual information by using a Markov random field formulation-based pixel labeling scheme for refinement of the segmentation results. Experimental results have established effectiveness of our approach."
            },
            "slug": "Text-Extraction-and-Document-Image-Segmentation-and-Kumar-Gupta",
            "title": {
                "fragments": [],
                "text": "Text Extraction and Document Image Segmentation Using Matched Wavelets and MRF Model"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A clustering-based technique has been devised for estimating globally matched wavelet filters using a collection of groundtruth images and a text extraction scheme for the segmentation of document images into text, background, and picture components is extended."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681758"
                        ],
                        "name": "Wumo Pan",
                        "slug": "Wumo-Pan",
                        "structuredName": {
                            "firstName": "Wumo",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wumo Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144957197"
                        ],
                        "name": "T. Bui",
                        "slug": "T.-Bui",
                        "structuredName": {
                            "firstName": "Tien",
                            "lastName": "Bui",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[17] proposed the use of a sparse representation for text detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "'s method [17] (\u201cCanny+K-SVD\u201d); 4) The use of a support vector machine (SVM) classifier operating on the wavelet features directly (\u201cwavelet+SVM\u201d)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 230672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "443bf555d3ac1b78d90715ef09228833622e3530",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A sparse representation based method is proposed for text detection from scene images. We start with edge information extracted using Canny operator and then group these edge points into connected components. Each connected component is labeled as text or non-text by a two-level labeling process: pixel level labeling and connected component labeling. The core of the labeling process is a sparsity test using an over-complete dictionary, which is learned from edge segments of isolated character images. Layout analysis is further applied to verify these text candidates. Experimental results show that improvements in both recall rate and detection accuracy in text detection have been achieved."
            },
            "slug": "Text-detection-from-scene-images-using-sparse-Pan-Bui",
            "title": {
                "fragments": [],
                "text": "Text detection from scene images using sparse representation"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A sparse representation based method is proposed for text detection from scene images that starts with edge information extracted using Canny operator and then group these edge points into connected components that are labeled as text or non-text by a two-level labeling process."
            },
            "venue": {
                "fragments": [],
                "text": "2008 19th International Conference on Pattern Recognition"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[27]: it detects texts by using a complex-valued multilayer feed-forward network trained to detect text at a fixed scale and position;"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "applying the adaptive run-length smoothing algorithm [24] and projection profile analysis [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "'s method [27]: it is based on candidate text block selection and segmentation of text portion of the image; (5) Gllavata et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 143774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8387d4998f810cd2b60bd81545cb993087bc8788",
            "isKey": false,
            "numCitedBy": 467,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Many images, especially those used for page design on Web pages, as well as videos contain visible text. If these text occurrences could be detected, segmented, and recognized automatically, they would be a valuable source of high-level semantics for indexing and retrieval. We propose a novel method for localizing and segmenting text in complex images and videos. Text lines are identified by using a complex-valued multilayer feed-forward network trained to detect text at a fixed scale and position. The network's output at all scales and positions is integrated into a single text-saliency map, serving as a starting point for candidate text lines. In the case of video, these candidate text lines are refined by exploiting the temporal redundancy of text in video. Localized text lines are then scaled to a fixed height of 100 pixels and segmented into a binary image with black characters on white background. For videos, temporal redundancy is exploited to improve segmentation performance. Input images and videos can be of any size due to a true multiresolution approach. Moreover, the system is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video, so that one text bitmap is created for all instances of that text line. Therefore, our text segmentation results can also be used for object-based video encoding such as that enabled by MPEG-4."
            },
            "slug": "Localizing-and-segmenting-text-in-images-and-videos-Lienhart-Wernicke",
            "title": {
                "fragments": [],
                "text": "Localizing and segmenting text in images and videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel method for localizing and segmenting text in complex images and videos that is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits Syst. Video Technol."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704629"
                        ],
                        "name": "M. Anthimopoulos",
                        "slug": "M.-Anthimopoulos",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Anthimopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anthimopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7232446"
                        ],
                        "name": "B. Gatos",
                        "slug": "B.-Gatos",
                        "structuredName": {
                            "firstName": "Basilios",
                            "lastName": "Gatos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gatos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748249"
                        ],
                        "name": "I. Pratikakis",
                        "slug": "I.-Pratikakis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pratikakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pratikakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 127
                            }
                        ],
                        "text": "Recently, there is a lot of interest on using pattern classification techniques (such as AdaBoost [7], support vector machines [5,8,10], belief propagation [11] and neural networks [9]) for text localization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18523324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "980087708326d4c4411ca2b76a9f2afb3f68238b",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-two-stage-scheme-for-text-detection-in-video-Anthimopoulos-Gatos",
            "title": {
                "fragments": [],
                "text": "A two-stage scheme for text detection in video images"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3186240"
                        ],
                        "name": "Weihua Huang",
                        "slug": "Weihua-Huang",
                        "structuredName": {
                            "firstName": "Weihua",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weihua Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1509248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f557c05cea306115fe825a7bfc772d3ba5aad24c",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we explore new edge features such as straightness for the elimination of non significant edges from the segmented text portion of a video frame to detect accurate boundary of the text lines in video images. To segment the complete text portions, the method introduces candidate text block selection from a given image. Heuristic rules are formed based on combination of filters and edge analysis for identifying a candidate text block in the image. Furthermore, the same rules are extended to grow boundary of candidate text block in order to segment complete text portions in the image. The experimental results of the proposed method show that the method outperforms an existing method in terms of a number of metrics."
            },
            "slug": "Efficient-video-text-detection-using-edge-features-Shivakumara-Huang",
            "title": {
                "fragments": [],
                "text": "Efficient video text detection using edge features"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "New edge features such as straightness for the elimination of non significant edges from the segmented text portion of a video frame to detect accurate boundary of the text lines in video images are explored."
            },
            "venue": {
                "fragments": [],
                "text": "2008 19th International Conference on Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403970934"
                        ],
                        "name": "C. Mancas-Thillou",
                        "slug": "C.-Mancas-Thillou",
                        "structuredName": {
                            "firstName": "C\u00e9line",
                            "lastName": "Mancas-Thillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mancas-Thillou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50276543"
                        ],
                        "name": "B. Gosselin",
                        "slug": "B.-Gosselin",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Gosselin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gosselin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[26]: it uses two metrics to merge similar colors together for text-driven segmentation in the RGB color space; (3) Lienhart et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42539643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0aaca7527d703a6945ba73ce15e7e7353258fc8a",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Color-text-extraction-with-selective-metric-based-Mancas-Thillou-Gosselin",
            "title": {
                "fragments": [],
                "text": "Color text extraction with selective metric-based clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95055206"
                        ],
                        "name": "Cheolkon Jung",
                        "slug": "Cheolkon-Jung",
                        "structuredName": {
                            "firstName": "Cheolkon",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheolkon Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48873890"
                        ],
                        "name": "Qifeng Liu",
                        "slug": "Qifeng-Liu",
                        "structuredName": {
                            "firstName": "Qifeng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qifeng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722566"
                        ],
                        "name": "Joongkyu Kim",
                        "slug": "Joongkyu-Kim",
                        "structuredName": {
                            "firstName": "Joongkyu",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joongkyu Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 127
                            }
                        ],
                        "text": "Recently, there is a lot of interest on using pattern classification techniques (such as AdaBoost [7], support vector machines [5,8,10], belief propagation [11] and neural networks [9]) for text localization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11685721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "714e8702e9583ad74861522bbc892fea1163f00c",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Accurate-text-localization-in-images-based-on-SVM-Jung-Liu",
            "title": {
                "fragments": [],
                "text": "Accurate text localization in images based on SVM output scores"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2565323"
                        ],
                        "name": "C. Strouthopoulos",
                        "slug": "C.-Strouthopoulos",
                        "structuredName": {
                            "firstName": "Charalambos",
                            "lastName": "Strouthopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Strouthopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368634"
                        ],
                        "name": "N. Papamarkos",
                        "slug": "N.-Papamarkos",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Papamarkos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Papamarkos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 181
                            }
                        ],
                        "text": "Recently, there is a lot of interest on using pattern classification techniques (such as AdaBoost [7], support vector machines [5,8,10], belief propagation [11] and neural networks [9]) for text localization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1755656,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce1e7aa9fd5ba2f54b342b7cac2625835771daf2",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-identification-for-document-image-analysis-a-Strouthopoulos-Papamarkos",
            "title": {
                "fragments": [],
                "text": "Text identification for document image analysis using a neural network"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152675651"
                        ],
                        "name": "J. Kim",
                        "slug": "J.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "The third category is based on textures [3], and assumes that texts have specific texture patterns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17901853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14bbcdc1744cc5982ffb64ea4755a72921d98d08",
            "isKey": false,
            "numCitedBy": 503,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The current paper presents a novel texture-based method for detecting texts in images. A support vector machine (SVM) is used to analyze the textural properties of texts. No external texture feature extraction module is used, but rather the intensities of the raw pixels that make up the textural pattern are fed directly to the SVM, which works well even in high-dimensional spaces. Next, text regions are identified by applying a continuously adaptive mean shift algorithm (CAMSHIFT) to the results of the texture analysis. The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "slug": "Texture-Based-Approach-for-Text-Detection-in-Images-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Texture-Based Approach for Text Detection in Images Using Support Vector Machines and Continuously Adaptive Mean Shift Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752239"
                        ],
                        "name": "Xiabi Liu",
                        "slug": "Xiabi-Liu",
                        "structuredName": {
                            "firstName": "Xiabi",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiabi Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113504611"
                        ],
                        "name": "Hui Fu",
                        "slug": "Hui-Fu",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7415267"
                        ],
                        "name": "Yunde Jia",
                        "slug": "Yunde-Jia",
                        "structuredName": {
                            "firstName": "Yunde",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunde Jia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45832072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fb8de25351c543676c269421e05c0efc47f4c40",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gaussian-mixture-modeling-and-learning-of-for-text-Liu-Fu",
            "title": {
                "fragments": [],
                "text": "Gaussian mixture modeling and learning of neighboring characters for multilingual text extraction in images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "The first one is the Microsoft common test set for video text detection [25] with 45 video frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15798803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c44fc2f6d748f54badcaf86feef8eb347d0b1c2",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Text presented in videos provides important supplemental information for video indexing and retrieval. Many efforts have been made for text detection in videos. However, there is still a lack of performance evaluation protocols for video text detection. In this paper, we propose an objective and comprehensive performance evaluation protocol for video text detection algorithms. The protocol includes a positive set and a negative set of indices at the textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes. In the protocol, we assign a detection difficulty (DD) level to each ground truth textbox. The performance indices can then be normalized with respect to the textbox DD level and are therefore tolerant to different ground-truth difficulties to a certain degree. We also assign a detectability index (DI) value to each ground-truth textbox. The overall detection rate is the DI-weighted average of the detection qualities of all ground-truth textboxes, which makes the detection rate more accurate to reveal the real performance. The automatic performance evaluation scheme has been applied to performance evaluation of a text detection approach to determine the best thresholds that can yield the best detection results. The protocol has also been employed to compare the performances of several text detection systems. Hence, we believe that the proposed protocol can be used to compare the performance of different video/image text detection algorithms/systems and can even help improve, select, and design new text detection methods."
            },
            "slug": "An-automatic-performance-evaluation-protocol-for-Hua-Liu",
            "title": {
                "fragments": [],
                "text": "An automatic performance evaluation protocol for video text detection algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An objective and comprehensive performance evaluation protocol for video text detection algorithms, which includes a positive set and a negative set of indices at the textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109837388"
                        ],
                        "name": "J. Lim",
                        "slug": "J.-Lim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Lim",
                            "middleNames": [
                                "Young"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109191421"
                        ],
                        "name": "Jonghyun Park",
                        "slug": "Jonghyun-Park",
                        "structuredName": {
                            "firstName": "Jonghyun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonghyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3463966"
                        ],
                        "name": "G. Medioni",
                        "slug": "G.-Medioni",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Medioni",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Medioni"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "CCA is fast."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "The first category uses connected component analysis (CCA) [1], in which pixels with similar colors are grouped into connected components, and then into text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7869656,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3698e529a4e968a486fda530d1c711e25504ed92",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-segmentation-in-color-images-using-tensor-Lim-Park",
            "title": {
                "fragments": [],
                "text": "Text segmentation in color images using tensor voting"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47874686"
                        ],
                        "name": "A. Chaudhuri",
                        "slug": "A.-Chaudhuri",
                        "structuredName": {
                            "firstName": "Avinanda",
                            "lastName": "Chaudhuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Chaudhuri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29730478"
                        ],
                        "name": "S. Chaudhuri",
                        "slug": "S.-Chaudhuri",
                        "structuredName": {
                            "firstName": "Sujoy",
                            "lastName": "Chaudhuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chaudhuri"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 60
                            }
                        ],
                        "text": "There are many skewed text line detection methods up to now [32,33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23236077,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "f1c37652e4327b6c93937aca275ed768aae94143",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a robust yet fast algorithm for skew detection in binary document images. The method is based on interline cross-correlation in the scanned image. Instead of finding the correlation for the entire image, it is calculated over small regions selected randomly. The proposed method does not require prior segmentation of the document into text and graphics regions. The maximum median of cross-correlation is used as the criterion to obtain the skew, and a Monte Carlo sampling technique is chosen to determine the number of regions over which the correlations have to be calculated. Experimental results on detecting skews in various types of documents containing different linguistic scripts are presented here."
            },
            "slug": "Robust-detection-of-skew-in-document-images-Chaudhuri-Chaudhuri",
            "title": {
                "fragments": [],
                "text": "Robust detection of skew in document images"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A robust yet fast algorithm for skew detection in binary document images based on interline cross-correlation in the scanned image, which does not require prior segmentation of the document into text and graphics regions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116066317"
                        ],
                        "name": "Shutao Li",
                        "slug": "Shutao-Li",
                        "structuredName": {
                            "firstName": "Shutao",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shutao Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069020982"
                        ],
                        "name": "Qinghua Shen",
                        "slug": "Qinghua-Shen",
                        "structuredName": {
                            "firstName": "Qinghua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qinghua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144291081"
                        ],
                        "name": "Jun Sun",
                        "slug": "Jun-Sun",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Sun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 60
                            }
                        ],
                        "text": "There are many skewed text line detection methods up to now [32,33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207327243,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "032958b6d8f95439d592a78b1d179aa666d3c019",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Skew-detection-using-wavelet-decomposition-and-Li-Shen",
            "title": {
                "fragments": [],
                "text": "Skew detection using wavelet decomposition and projection profile analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145470231"
                        ],
                        "name": "Kwan Y. Wong",
                        "slug": "Kwan-Y.-Wong",
                        "structuredName": {
                            "firstName": "Kwan",
                            "lastName": "Wong",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kwan Y. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34896449"
                        ],
                        "name": "R. Casey",
                        "slug": "R.-Casey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Casey",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Casey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880661"
                        ],
                        "name": "F. Wahl",
                        "slug": "F.-Wahl",
                        "structuredName": {
                            "firstName": "Friedrich",
                            "lastName": "Wahl",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Wahl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "Next, we perform the vertical ARLSA and vertical projection profile analysis in those text lines which have been extracted from the horizontal projection analysis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Finally, the text regions are located by performing ARLSA and projection profile analysis on the edge classification results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Empirically, ARLSA can also handle images containing characters with variable font sizes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "In each direction, RLSA eliminates white runs whose lengths are smaller than a threshold smoothing value."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "In our algorithm, ARLSA is first performed in the horizontal direction (Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "However, RLSA may sometimes group inhomogeneous connected components or different slanted lines together."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Sample results from the vertical ARLSA and projection analysis are shown in Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "The ARLSA is an extension which overcomes these drawbacks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "The classical run-length smoothing algorithm (RLSA) [30] is applicable to binary images, which takes advantage of the white runs existing in the horizontal and vertical directions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15921038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abca302c74d2f5adfd323a28e26d40b019df2b5",
            "isKey": true,
            "numCitedBy": 594,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper outlines the requirements and components for a proposed Document Analysis System, which assists a user in encoding printed documents for computer processing. Several critical functions have been investigated and the technical approaches are discussed. The first is the segmentation and classification of digitized printed documents into regions of text and images. A nonlinear, run-length smoothing algorithm has been used for this purpose. By using the regular features of text lines, a linear adaptive classification scheme discriminates text regions from others. The second technique studied is an adaptive approach to the recognition of the hundreds of font styles and sizes that can occur on printed documents. A preclassifier is constructed during the input process and used to speed up a well-known pattern-matching method for clustering characters from an arbitrary print source into a small sample of prototypes. Experimental results are included."
            },
            "slug": "Document-Analysis-System-Wong-Casey",
            "title": {
                "fragments": [],
                "text": "Document Analysis System"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The requirements and components for a proposed Document Analysis System, which assists a user in encoding printed documents for computer processing, are outlined and several critical functions have been investigated and the technical approaches are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 487757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20cef0f008ae5c68795220389b2d83b38850dc23",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised classification algorithm is derived by modeling observed data as a mixture of several mutually exclusive classes that are each described by linear combinations of independent, non-Gaussian densities. The algorithm estimates the data density in each class by using parametric nonlinear functions that fit to the non-Gaussian structure of the data. This improves classification accuracy compared with standard Gaussian mixture models. When applied to images, the algorithm can learn efficient codes (basis functions) for images that capture the statistically significant structure intrinsic in the images. We apply this technique to the problem of unsupervised classification, segmentation, and denoising of images. We demonstrate that this method was effective in classifying complex image textures such as natural scenes and text. It was also useful for denoising and filling in missing pixels in images with complex structures. The advantage of this model is that image codes can be learned with increasing numbers of classes thus providing greater flexibility in modeling structure and in finding more image features than in either Gaussian mixture models or standard independent component analysis (ICA) algorithms."
            },
            "slug": "Unsupervised-image-classification,-segmentation,-Lee-Lewicki",
            "title": {
                "fragments": [],
                "text": "Unsupervised image classification, segmentation, and enhancement using ICA mixture models"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper demonstrates that the unsupervised classification method, derived by modeling observed data as a mixture of several mutually exclusive classes that are each described by linear combinations of independent, non-Gaussian densities, was effective in classifying complex image textures such as natural scenes and text."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2599292"
                        ],
                        "name": "J. Mairal",
                        "slug": "J.-Mairal",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Mairal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mairal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699339"
                        ],
                        "name": "G. Sapiro",
                        "slug": "G.-Sapiro",
                        "structuredName": {
                            "firstName": "Guillermo",
                            "lastName": "Sapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sapiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "At each iteration, we prune the two sets by keeping the \u201cbest 90% classified patches\u201d [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "It is shown in [21] that each such iteration of updating dictionary Dj is equivalent to solving a problem of the form:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "We then use the discriminative dictionary training algorithm in [21] to construct two dictionaries, D1 for the text and D2 for the background."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "proposed an algorithm that learns multiple discriminative dictionaries [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 428083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a072cbdee54b83c8df43a431065f009d2cd2e70",
            "isKey": true,
            "numCitedBy": 818,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse signal models have been the focus of much recent research, leading to (or improving upon) state-of-the-art results in signal, image, and video restoration. This article extends this line of research into a novel framework for local image discrimination tasks, proposing an energy formulation with both sparse reconstruction and class discrimination components, jointly optimized during dictionary learning. This approach improves over the state of the art in texture segmentation experiments using the Brodatz database, and it paves the way for a novel scene analysis and recognition framework based on simultaneously learning discriminative and reconstructive dictionaries. Preliminary results in this direction using examples from the Pascal VOC06 and Graz02 datasets are presented as well."
            },
            "slug": "Discriminative-learned-dictionaries-for-local-image-Mairal-Bach",
            "title": {
                "fragments": [],
                "text": "Discriminative learned dictionaries for local image analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This article proposes an energy formulation with both sparse reconstruction and class discrimination components, jointly optimized during dictionary learning, for local image discrimination tasks, and paves the way for a novel scene analysis and recognition framework based on simultaneously learning discriminative and reconstructive dictionaries."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093164350"
                        ],
                        "name": "Nikos A. Nikolaou",
                        "slug": "Nikos-A.-Nikolaou",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Nikolaou",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikos A. Nikolaou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2355097"
                        ],
                        "name": "M. Makridis",
                        "slug": "M.-Makridis",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Makridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Makridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7232446"
                        ],
                        "name": "B. Gatos",
                        "slug": "B.-Gatos",
                        "structuredName": {
                            "firstName": "Basilios",
                            "lastName": "Gatos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gatos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700033"
                        ],
                        "name": "N. Stamatopoulos",
                        "slug": "N.-Stamatopoulos",
                        "structuredName": {
                            "firstName": "Nikolaos",
                            "lastName": "Stamatopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Stamatopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368634"
                        ],
                        "name": "N. Papamarkos",
                        "slug": "N.-Papamarkos",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Papamarkos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Papamarkos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "applying the adaptive run-length smoothing algorithm [24] and projection profile analysis [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "adaptive run-length smoothing algorithm (ARSLA) [24] to separate the true texts from the candidate ones."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 169
                            }
                        ],
                        "text": "Edge detection results for the letter \u201cR\u201d in a complex background: (a) original image (d) edges obtained by the wavelet detector.\nadaptive run-length smoothing algorithm (ARSLA) [24] to separate the true texts from the candidate ones."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8568424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "402e0a3b7500ec527c1173316b677bd85df66a52",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Segmentation-of-historical-machine-printed-using-Nikolaou-Makridis",
            "title": {
                "fragments": [],
                "text": "Segmentation of historical machine-printed documents using Adaptive Run Length Smoothing and skeleton segmentation paths"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7969151"
                        ],
                        "name": "H. Shen",
                        "slug": "H.-Shen",
                        "structuredName": {
                            "firstName": "Huiying",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8436115"
                        ],
                        "name": "J. Coughlan",
                        "slug": "J.-Coughlan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Coughlan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Coughlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061455070"
                        ],
                        "name": "V. Ivanchenko",
                        "slug": "V.-Ivanchenko",
                        "structuredName": {
                            "firstName": "Volodymyr",
                            "lastName": "Ivanchenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ivanchenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "Recently, there is a lot of interest on using pattern classification techniques (such as AdaBoost [7], support vector machines [5,8,10], belief propagation [11] and neural networks [9]) for text localization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3191268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcbf6638be4c9a7c25a92ba311ef5adf1f9f0933",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Figure-ground-segmentation-using-factor-graphs-Shen-Coughlan",
            "title": {
                "fragments": [],
                "text": "Figure-ground segmentation using factor graphs"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143607492"
                        ],
                        "name": "John Wright",
                        "slug": "John-Wright",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Wright",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Wright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40984420"
                        ],
                        "name": "A. Yang",
                        "slug": "A.-Yang",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Yang",
                            "middleNames": [
                                "Yuqing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701028"
                        ],
                        "name": "Arvind Ganesh",
                        "slug": "Arvind-Ganesh",
                        "structuredName": {
                            "firstName": "Arvind",
                            "lastName": "Ganesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arvind Ganesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144797536"
                        ],
                        "name": "S. Sastry",
                        "slug": "S.-Sastry",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sastry",
                            "middleNames": [
                                "Shankar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sastry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50032052"
                        ],
                        "name": "Yi Ma",
                        "slug": "Yi-Ma",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "These include face recognition [12], signal classification [13] and texture classification [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7677466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5eec41043d91964879c4c745c7165f823967f29",
            "isKey": false,
            "numCitedBy": 9015,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by C1-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims."
            },
            "slug": "Robust-Face-Recognition-via-Sparse-Representation-Wright-Yang",
            "title": {
                "fragments": [],
                "text": "Robust Face Recognition via Sparse Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This work considers the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise, and proposes a general classification algorithm for (image-based) object recognition based on a sparse representation computed by C1-minimization."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40622051"
                        ],
                        "name": "M. Aharon",
                        "slug": "M.-Aharon",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aharon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 129
                            }
                        ],
                        "text": "Recent works showed that these non-parametric dictionaries can significantly outperform the standard, off-the-shelf dictionaries [15,16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6888534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e07416eabd4ba6c69fa473756bb04ae7161177be",
            "isKey": false,
            "numCitedBy": 4979,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods"
            },
            "slug": "Image-Denoising-Via-Sparse-and-Redundant-Over-Elad-Aharon",
            "title": {
                "fragments": [],
                "text": "Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This work addresses the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image, and uses the K-SVD algorithm to obtain a dictionary that describes the image content effectively."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40622051"
                        ],
                        "name": "M. Aharon",
                        "slug": "M.-Aharon",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610924"
                        ],
                        "name": "A. Bruckstein",
                        "slug": "A.-Bruckstein",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Bruckstein",
                            "middleNames": [
                                "Marcel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bruckstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 38
                            }
                        ],
                        "text": "One of the most popular algorithms is K-SVD [20], in which an overcomplete dictionary is obtained by solving the following optimization problem:\nmin \u03b1"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "While K-SVD can obtain a good reconstructive representation, it may not perform well in a classification setting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 316
                            }
                        ],
                        "text": "In this section, the following methods will be compared:\n1) The proposed method, which will be denoted as \u201cwavelet+ discriminative dictionary\u201d; 2) A variant of the proposed method, which uses the Canny edge detector instead of the wavelet detector (\u201cCanny+discriminative dictionary\u201d); 3) Pan et al.'s method [17] (\u201cCanny+K-SVD\u201d); 4) The use of a support vector machine (SVM) classifier operating on\nthe wavelet features directly (\u201cwavelet+SVM\u201d)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 16
                            }
                        ],
                        "text": "However, as the K-SVD dictionary is designed for coding and denoising, it can be confused by complex backgrounds with text-like areas."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "It extracts text-like edges from an image by using a dictionary obtained by K-SVD [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "One of the most popular algorithms is K-SVD [20], in which an overcomplete dictionary is obtained by solving the following optimization problem:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7477309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83b522f4bfa5db7f7d34f839475af7d078107634",
            "isKey": true,
            "numCitedBy": 7336,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data"
            },
            "slug": "$rm-K$-SVD:-An-Algorithm-for-Designing-Overcomplete-Aharon-Elad",
            "title": {
                "fragments": [],
                "text": "$rm K$-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A novel algorithm for adapting dictionaries in order to achieve sparse signal representations, the K-SVD algorithm, an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148558"
                        ],
                        "name": "S. Zhong",
                        "slug": "S.-Zhong",
                        "structuredName": {
                            "firstName": "Sifen",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zhong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Mallat and Zhong [23] pointed out that finding the local maxima of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Mallat and Zhong [23] pointed out that finding the local maxima of\na): text dictionary; (b) background dictionary.\na wavelet transform is equivalent to the multi-scale Canny edge detection [22]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5698299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f42d06ff2160dae7412c299619c04f63395b784b",
            "isKey": false,
            "numCitedBy": 3302,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "A multiscale Canny edge detection is equivalent to finding the local maxima of a wavelet transform. The authors study the properties of multiscale edges through the wavelet theory. For pattern recognition, one often needs to discriminate different types of edges. They show that the evolution of wavelet local maxima across scales characterize the local shape of irregular structures. Numerical descriptors of edge types are derived. The completeness of a multiscale edge representation is also studied. The authors describe an algorithm that reconstructs a close approximation of 1-D and 2-D signals from their multiscale edges. For images, the reconstruction errors are below visual sensitivity. As an application, a compact image coding algorithm that selects important edges and compresses the image data by factors over 30 has been implemented. >"
            },
            "slug": "Characterization-of-Signals-from-Multiscale-Edges-Mallat-Zhong",
            "title": {
                "fragments": [],
                "text": "Characterization of Signals from Multiscale Edges"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The authors describe an algorithm that reconstructs a close approximation of 1-D and 2-D signals from their multiscale edges and shows that the evolution of wavelet local maxima across scales characterize the local shape of irregular structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729041"
                        ],
                        "name": "J. Canny",
                        "slug": "J.-Canny",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Canny",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Canny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Afterwards, edges are extracted by the Canny edge detector [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "a wavelet transform is equivalent to the multi-scale Canny edge detection [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13284142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec",
            "isKey": false,
            "numCitedBy": 27659,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge."
            },
            "slug": "A-Computational-Approach-to-Edge-Detection-Canny",
            "title": {
                "fragments": [],
                "text": "A Computational Approach to Edge Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "There is a natural uncertainty principle between detection and localization performance, which are the two main goals, and with this principle a single operator shape is derived which is optimal at any scale."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143935332"
                        ],
                        "name": "Songpo Yang",
                        "slug": "Songpo-Yang",
                        "structuredName": {
                            "firstName": "Songpo",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Songpo Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152442779"
                        ],
                        "name": "Jing Wang",
                        "slug": "Jing-Wang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785977"
                        ],
                        "name": "Chein-I. Chang",
                        "slug": "Chein-I.-Chang",
                        "structuredName": {
                            "firstName": "Chein-I.",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chein-I. Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120917"
                        ],
                        "name": "J. Jensen",
                        "slug": "J.-Jensen",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Jensen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144082674"
                        ],
                        "name": "J. Jensen",
                        "slug": "J.-Jensen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Jensen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jensen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "These include face recognition [12], signal classification [13] and texture classification [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 129440223,
            "fieldsOfStudy": [
                "Environmental Science",
                "Mathematics"
            ],
            "id": "ab9728dc8df952cab8990b5cd6e1de96c020ec00",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised image classification for remotely sensed imagery is very challenging due to the fact that the unknown image background generally varies with a wide range of spectral deviations. Additionally, spectral similarity among subtle small calsses also causes tremendous difficulty in classification. This paper investigates three major issues, (1) image background removal, (2) generation of training sample data, (3) determination of the number of classes to be classified, p which are encountered in unsupervised image classification. The study on these three issues is conducted via a well-known Airborne Visible InfraRed Imaging Spectrometer (AVIRIS) image scene, Indiana Pine test site available online at Purdue University's website. Since image background varies with different applications, it is generally difficult to perform background removal without prior knowledge. In order for unsupervised classification to be effective, a good set of training data is also necessary. These training samples must be generated directly from the image data in an unsupervised manner. This paper develops an unsupervised training sample generation algorithm (UTSGA) that can generate a good sample pool of training data for supervised classification. In determining p, a newly developed concept, called virtual dimensionality (VD) is used to estimate the p where a Neyman-Pearson-based eigen-analysis approach developed by Harsanyi, Farrand and Chang, called noise-whitened HFC (NWHFC)'s method, is implemented to find the VD to be used for the p. Finally, an unsupervised image classification algorithm can be derived by implementing a supervised classifier in conjunction with teh UTSGA algorithm and NWHFC's method."
            },
            "slug": "Unsupervised-image-classification-for-remotely-Yang-Wang",
            "title": {
                "fragments": [],
                "text": "Unsupervised image classification for remotely sensed imagery"
            },
            "venue": {
                "fragments": [],
                "text": "SPIE Optics + Photonics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456485"
                        ],
                        "name": "L. Shang",
                        "slug": "L.-Shang",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Shang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 133
                            }
                        ],
                        "text": "Sparse representation models have recently been used in image understanding tasks such as texture segmentation and feature selection [18,19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205397064,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8b8ffd1ede77d4aab2e6cd0eaf0c5e94d17126f",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Non-negative-sparse-coding-shrinkage-for-image-Shang",
            "title": {
                "fragments": [],
                "text": "Non-negative sparse coding shrinkage for image denoising using normal inverse Gaussian density model"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090622"
                        ],
                        "name": "W. V. Aerschot",
                        "slug": "W.-V.-Aerschot",
                        "structuredName": {
                            "firstName": "Ward",
                            "lastName": "Aerschot",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. V. Aerschot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143851999"
                        ],
                        "name": "M. Jansen",
                        "slug": "M.-Jansen",
                        "structuredName": {
                            "firstName": "Maarten",
                            "lastName": "Jansen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Jansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2123691"
                        ],
                        "name": "A. Bultheel",
                        "slug": "A.-Bultheel",
                        "structuredName": {
                            "firstName": "Adhemar",
                            "lastName": "Bultheel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bultheel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "These include face recognition [12], signal classification [13] and texture classification [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11862074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c04d89daa306409b602c1b2c8e3a2cd6ee4d39b7",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Normal-mesh-based-geometrical-image-compression-Aerschot-Jansen",
            "title": {
                "fragments": [],
                "text": "Normal mesh based geometrical image compression"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 129
                            }
                        ],
                        "text": "Recent works showed that these non-parametric dictionaries can significantly outperform the standard, off-the-shelf dictionaries [15,16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13058320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfcae80f4d34ac09ba8063c5cfb5be954d0bf5f1",
            "isKey": false,
            "numCitedBy": 704,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach provides a practical method for learning high-order Markov random field (MRF) models with potential functions that extend over large pixel neighborhoods. These clique potentials are modeled using the Product-of-Experts framework that uses non-linear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field-of-Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with specialized techniques."
            },
            "slug": "Fields-of-Experts-Roth-Black",
            "title": {
                "fragments": [],
                "text": "Fields of Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach provides a practical method for learning high-order Markov random field models with potential functions that extend over large pixel neighborhoods with non-linear functions of many linear filter responses."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153304766"
                        ],
                        "name": "Fan Zhang",
                        "slug": "Fan-Zhang",
                        "structuredName": {
                            "firstName": "Fan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114145131"
                        ],
                        "name": "Xiaoqiong Ye",
                        "slug": "Xiaoqiong-Ye",
                        "structuredName": {
                            "firstName": "Xiaoqiong",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoqiong Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 133
                            }
                        ],
                        "text": "Sparse representation models have recently been used in image understanding tasks such as texture segmentation and feature selection [18,19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14663320,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "9c81ee71df8a8c52a4af4bc775a36fc2f31753ad",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Decomposing an image into a texture part and a non-texture (cartoon) part, as well as grouping the texture part into several homogeneous subparts, is studied in this letter. The so-called texture part is composed of both the self-similar structure and the oscillatory noise. The self-similar structure of each homogenous subtexture is captured in its principal subspace. Both the segmentation and the decomposition are essentially related to sparse representation and are united to a framework."
            },
            "slug": "Image-Decomposition-and-Texture-Segmentation-via-Zhang-Ye",
            "title": {
                "fragments": [],
                "text": "Image Decomposition and Texture Segmentation via Sparse Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Decomposing an image into a texture part and a non-texture (cartoon) part, as well as grouping the texture part into several homogeneous subparts, is studied in this letter."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2843211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2077d0f30507d51a0d3bbec4957d55e817d66a59",
            "isKey": false,
            "numCitedBy": 1051,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques."
            },
            "slug": "Fields-of-Experts:-a-framework-for-learning-image-Roth-Black",
            "title": {
                "fragments": [],
                "text": "Fields of Experts: a framework for learning image priors"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks, developed using a Products-of-Experts framework."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144543582"
                        ],
                        "name": "B. Gorte",
                        "slug": "B.-Gorte",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Gorte",
                            "middleNames": [
                                "G.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gorte"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62343229,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "5fefa98cff601b1f206b6fcd5894d8f37a16df47",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Supervised-image-classification-Gorte",
            "title": {
                "fragments": [],
                "text": "Supervised image classification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The ICDAR 2003 competitions"
            },
            "venue": {
                "fragments": [],
                "text": "The ICDAR 2003 competitions"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 14
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 36,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Text-detection-in-images-using-sparse-with-Zhao-Li/5f5f802bf01c8632dda4586b05bc86fc4878326f?sort=total-citations"
}