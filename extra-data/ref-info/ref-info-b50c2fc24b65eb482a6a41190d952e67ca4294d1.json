{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48569250"
                        ],
                        "name": "Xiaojun Li",
                        "slug": "Xiaojun-Li",
                        "structuredName": {
                            "firstName": "Xiaojun",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojun Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109080279"
                        ],
                        "name": "Weiqiang Wang",
                        "slug": "Weiqiang-Wang",
                        "structuredName": {
                            "firstName": "Weiqiang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiqiang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689702"
                        ],
                        "name": "Qingming Huang",
                        "slug": "Qingming-Huang",
                        "structuredName": {
                            "firstName": "Qingming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101001846"
                        ],
                        "name": "Wen Gao",
                        "slug": "Wen-Gao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2343895"
                        ],
                        "name": "Laiyun Qing",
                        "slug": "Laiyun-Qing",
                        "structuredName": {
                            "firstName": "Laiyun",
                            "lastName": "Qing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laiyun Qing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15531004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91517254eb6356b7e26cec7384bf9cc0ec047784",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a hybrid text segmentation approach for embedded text in images, aiming to combining the advantages of the difference-based methods and the similarity-based methods together. First a new stroke edge filter is applied to obtain stroke edge map. Then a twothreshold method based on the improved Niblack thresholding technique is utilized to identify stroke edges. Those pixels between the edge pairs above the high threshold are collected to estimate the representative of stroke color, so that stroke pixels are further extracted by computing the color similarity. Finally some heuristic rules are devised to integrate stroke edge and stroke region information to obtain better segmentation results. The experimental results show that our approach can effectively segment text from background."
            },
            "slug": "A-hybrid-text-segmentation-approach-Li-Wang",
            "title": {
                "fragments": [],
                "text": "A hybrid text segmentation approach"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A hybrid text segmentation approach for embedded text in images, aiming to combining the advantages of the difference- based methods and the similarity-based methods together, and shows that the approach can effectively segment text from background."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Conference on Multimedia and Expo"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403970934"
                        ],
                        "name": "C. Mancas-Thillou",
                        "slug": "C.-Mancas-Thillou",
                        "structuredName": {
                            "firstName": "C\u00e9line",
                            "lastName": "Mancas-Thillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mancas-Thillou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50276543"
                        ],
                        "name": "B. Gosselin",
                        "slug": "B.-Gosselin",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Gosselin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gosselin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42539643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0aaca7527d703a6945ba73ce15e7e7353258fc8a",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Color-text-extraction-with-selective-metric-based-Mancas-Thillou-Gosselin",
            "title": {
                "fragments": [],
                "text": "Color text extraction with selective metric-based clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403970934"
                        ],
                        "name": "C. Mancas-Thillou",
                        "slug": "C.-Mancas-Thillou",
                        "structuredName": {
                            "firstName": "C\u00e9line",
                            "lastName": "Mancas-Thillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mancas-Thillou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50276543"
                        ],
                        "name": "B. Gosselin",
                        "slug": "B.-Gosselin",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Gosselin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gosselin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11571951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c78b93a9a4fcf27945cd982c59f29b59f4a53a53",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Character recognition has a continuous importance for several years and recently, new challenges appeared with camera-based pictures. This paper deals with text extraction for color natural scenes images. Many papers try to combine several color spaces or to choose the best one for a particular database. We show that the main problem is not in the choice of color spaces for generic text extraction but in the choice of clustering distances to handle alt degradations present in this kind of images. Comparative results are given using a public database."
            },
            "slug": "Color-text-extraction-from-camera-based-images:-the-Mancas-Thillou-Gosselin",
            "title": {
                "fragments": [],
                "text": "Color text extraction from camera-based images: the impact of the choice of the clustering distance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the main problem is not in the choice of color spaces for generic text extraction but in the decision of clustering distances to handle alt degradations present in this kind of images."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9401177"
                        ],
                        "name": "Zhou Zhiwei",
                        "slug": "Zhou-Zhiwei",
                        "structuredName": {
                            "firstName": "Zhou",
                            "lastName": "Zhiwei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhou Zhiwei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48297280"
                        ],
                        "name": "Liu Linlin",
                        "slug": "Liu-Linlin",
                        "structuredName": {
                            "firstName": "Liu",
                            "lastName": "Linlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liu Linlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72329598"
                        ],
                        "name": "T. C. Lim",
                        "slug": "T.-C.-Lim",
                        "structuredName": {
                            "firstName": "Tan",
                            "lastName": "Lim",
                            "middleNames": [
                                "Chew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. C. Lim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195908748,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9df0d8ad5b944b394b8c61d2470bff0a66802144",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a binarization method based on edge for video text images, especially for images with complex background or low contrast. The binarization method first detects the contour of the text, and utilizes a local thresholding method to decide the inner side of the contour, and then fills up the contour to form characters that are recognizable to OCR software. Experiment results show that our method is especially effective on complex background and low contrast images."
            },
            "slug": "Edge-Based-Binarization-for-Video-Text-Images-Zhiwei-Linlin",
            "title": {
                "fragments": [],
                "text": "Edge Based Binarization for Video Text Images"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A binarization method based on edge for video text images, especially for images with complex background or low contrast, that utilizes a local thresholding method and fills up the contour to form characters that are recognizable to OCR software."
            },
            "venue": {
                "fragments": [],
                "text": "ICPR 2010"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114144904"
                        ],
                        "name": "Xiangyun Ye",
                        "slug": "Xiangyun-Ye",
                        "structuredName": {
                            "firstName": "Xiangyun",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangyun Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145541445"
                        ],
                        "name": "M. Cheriet",
                        "slug": "M.-Cheriet",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Cheriet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cheriet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8698232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7d91ab0b52cd0b1c90f1120ab2cf7fa89054210",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Global gray-level thresholding techniques such as Otsu's method, and local gray-level thresholding techniques such as edge-based segmentation or the adaptive thresholding method are powerful in extracting character objects from simple or slowly varying backgrounds. However, they are found to be insufficient when the backgrounds include sharply varying contours or fonts in different sizes. A stroke-model is proposed to depict the local features of character objects as double-edges in a predefined size. This model enables us to detect thin connected components selectively, while ignoring relatively large backgrounds that appear complex. Meanwhile, since the stroke width restriction is fully factored in, the proposed technique can be used to extract characters in predefined font sizes. To process large volumes of documents efficiently, a hybrid method is proposed for character extraction from various backgrounds. Using the measurement of class separability to differentiate images with simple backgrounds from those with complex backgrounds, the hybrid method can process documents with different backgrounds by applying the appropriate methods. Experiments on extracting handwriting from a check image, as well as machine-printed characters from scene images demonstrate the effectiveness of the proposed model."
            },
            "slug": "Stroke-model-based-character-extraction-from-images-Ye-Cheriet",
            "title": {
                "fragments": [],
                "text": "Stroke-model-based character extraction from gray-level document images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A stroke-model is proposed to depict the local features of character objects as double-edges in a predefined size, which enables us to detect thin connected components selectively, while ignoring relatively large backgrounds that appear complex."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": false,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2591506"
                        ],
                        "name": "Minoru Yokobayashi",
                        "slug": "Minoru-Yokobayashi",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Yokobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minoru Yokobayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069541"
                        ],
                        "name": "T. Wakahara",
                        "slug": "T.-Wakahara",
                        "structuredName": {
                            "firstName": "Toru",
                            "lastName": "Wakahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wakahara"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3198280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd568cc328d34c43af90e10f6b133c03994bee09",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 135,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new technique of segmentation and recognition of characters with a wide variety of image degradations and complex backgrounds in natural scenes. The key ideas are twofold. One is segmentation of character and background by local/adaptive binarization of one of Cyan/Magenta/Yellow (CMY) color planes with the maximum breadth of histogram. The other is affine-invariant grayscale character recognition using global affine transformation (GAT) correlation. In experiments, we use a total of 698 test images extracted from the public ICDAR 2003 robust OCR dataset containing images of single characters in natural scenes. In advance, we classify those images into seven groups according to the degree of image degradations and/or background complexity. On the other hand, we prepare a single-font set of 62 alphanumerics for templates. Experimental results show an average recognition rate of 70.3%, ranging from 95.5% for clear images to 24.3% for little-contrast images."
            },
            "slug": "Segmentation-and-recognition-of-characters-in-scene-Yokobayashi-Wakahara",
            "title": {
                "fragments": [],
                "text": "Segmentation and recognition of characters in scene images using selective binarization in color space and GAT correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new technique of segmentation and recognition of characters with a wide variety of image degradations and complex backgrounds in natural scenes using affine-invariant grayscale character recognition using global affine transformation (GAT) correlation is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151482419"
                        ],
                        "name": "Jing Yu",
                        "slug": "Jing-Yu",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109137618"
                        ],
                        "name": "Lei Huang",
                        "slug": "Lei-Huang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800844"
                        ],
                        "name": "Chang-ping Liu",
                        "slug": "Chang-ping-Liu",
                        "structuredName": {
                            "firstName": "Chang-ping",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chang-ping Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 348958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0b681c869970457d3e91a0d2d3cb66c3d3f1e8d",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Global gray-level thresholding techniques such as Otsupsilas method, and local gray-level thresholding techniques such as adaptive thresholding method are powerful in extracting character objects from simple or slowly varying backgrounds. However, they are found to be insufficient when the backgrounds include sharply varying contours or fonts in different sizes. In this paper, we propose a double-edge model insensitive to stroke width to extract character strokes with an unknown stroke width from complex or sharply varying backgrounds. Also, we propose a novel postprocessing method combining 2-level global thresholding and Canny edge detection to keep the character object in integrality and remove the background simultaneously. Experiment results show that the proposed method can extract character objects from complex backgrounds with satisfactory quality."
            },
            "slug": "Double-edge-model-based-character-stroke-extraction-Yu-Huang",
            "title": {
                "fragments": [],
                "text": "Double-edge-model based character stroke extraction from complex backgrounds"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A double-edge model insensitive to stroke width is proposed to extract character strokes with an unknown stroke width from complex or sharply varying backgrounds and a novel postprocessing method combining 2-level global thresholding and Canny edge detection to keep the character object in integrality and remove the background simultaneously is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 19th International Conference on Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2591506"
                        ],
                        "name": "Minoru Yokobayashi",
                        "slug": "Minoru-Yokobayashi",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Yokobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minoru Yokobayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069541"
                        ],
                        "name": "T. Wakahara",
                        "slug": "T.-Wakahara",
                        "structuredName": {
                            "firstName": "Toru",
                            "lastName": "Wakahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wakahara"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10678544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b8e87e9d7f1edebc8cb6992ed4eac17b4a8682c",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new technique of binarization and recognition of characters in color with a wide variety of image degradations and complex backgrounds. The key ideas are twofold. One is to automatically select one axis in the RGB color space that maximizes the between-class separability by a suitably chosen threshold for segmentation of character and background or binarization. The other is affine-invariant or distortion-tolerant grayscale character recognition using global affine transformation (GAT) correlation that yields the maximum correlation value between input and template images. In experiments, we use a total of 698 test images extracted from the public ICDAR 2003 robust OCR dataset containing a variety of single-character images in natural scenes. In advance, we classify those images into seven groups according to the degree of image degradations and/or background complexity. On the other hand, we only prepare a single-font set of 62 alphanumerics for templates. Experimental results show an average recognition rate of 81.4%, ranging from 94.5% for clear images to 39.3% for seriously distorted images"
            },
            "slug": "Binarization-and-Recognition-of-Degraded-Characters-Yokobayashi-Wakahara",
            "title": {
                "fragments": [],
                "text": "Binarization and Recognition of Degraded Characters Using a Maximum Separability Axis in Color Space and GAT Correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new technique of binarization and recognition of characters in color with a wide variety of image degradations and complex backgrounds using global affine transformation (GAT) correlation that yields the maximum correlation value between input and template images is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "18th International Conference on Pattern Recognition (ICPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727969"
                        ],
                        "name": "P. Correia",
                        "slug": "P.-Correia",
                        "structuredName": {
                            "firstName": "Paulo",
                            "lastName": "Correia",
                            "middleNames": [
                                "Lobato"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Correia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152639054"
                        ],
                        "name": "F. Pereira",
                        "slug": "F.-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3067600,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "4fc5655e22bdca7fb8ae3ca2cdb43936a2ff4534",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Video segmentation assumes a major role in the context of object-based coding and description applications. Evaluating the adequacy of a segmentation result for a given application is a requisite both to allow the appropriate selection of segmentation algorithms as well as to adjust their parameters for optimal performance. Subjective testing, the current practice for the evaluation of video segmentation quality, is an expensive and time-consuming process. Objective segmentation quality evaluation techniques can alternatively be used; however, it is recognized that, so far, much less research effort has been devoted to this subject than to the development of segmentation algorithms. This paper discusses the problem of video segmentation quality evaluation, proposing evaluation methodologies and objective segmentation quality metrics for individual objects as well as for complete segmentation partitions. Both stand alone and relative evaluation metrics are developed to cover the cases for which a reference segmentation is missing or available for comparison."
            },
            "slug": "Objective-evaluation-of-video-segmentation-quality-Correia-Pereira",
            "title": {
                "fragments": [],
                "text": "Objective evaluation of video segmentation quality"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Both stand alone and relative evaluation metrics are developed to cover the cases for which a reference segmentation is missing or available for comparison, as well as for complete segmentation partitions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809629"
                        ],
                        "name": "N. Otsu",
                        "slug": "N.-Otsu",
                        "structuredName": {
                            "firstName": "Nobuyuki",
                            "lastName": "Otsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Otsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15326934,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "1d4816c612e38dac86f2149af667a5581686cdef",
            "isKey": false,
            "numCitedBy": 32882,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A nonparametric and unsupervised method ofautomatic threshold selection for picture segmentation is presented. An optimal threshold is selected by the discriminant criterion, namely, so as to maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zerothand the first-order cumulative moments of the gray-level histogram. It is straightforward to extend the method to multithreshold problems. Several experimental results are also presented to support the validity of the method."
            },
            "slug": "A-threshold-selection-method-from-gray-level-Otsu",
            "title": {
                "fragments": [],
                "text": "A threshold selection method from gray level histograms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 11,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/An-Algorithm-for-Colour-Based-Natural-Scene-Text-Zeng-Jia/b50c2fc24b65eb482a6a41190d952e67ca4294d1?sort=total-citations"
}