{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "Buntine and Weigend (1991) and MacKay (1992) approach this problem by approximating the posterior distribution by a Gaussian."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 59
                            }
                        ],
                        "text": "RESULTS ON A TEST PROBLEM I use the \"robot arm\" problem of MacKay (1992) for testing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 201
                            }
                        ],
                        "text": "A Bayesian approach to handling this is to assign 0\" a vague prior distribution and then \u00b7.ntcgrating it away, giving the following probability for the training set (see (Buntine and Weigend, 1991) or (Neal, 1992) for details):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 97
                            }
                        ],
                        "text": "Numerous other variations on these methods are possible as well, some of which are discussed in (Neal, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15366323,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc053fd3feade79df85fd0612d7f817f5ae3cd44",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the \\Hybrid Monte Carlo\" method. This approach allows the true predictive distribution for a test case given a set of training cases to be approximated arbitrarily closely, in contrast to previous approaches which approximate the posterior weight distribution by a Gaussian. In this work, the Hybrid Monte Carlo method is implemented in conjunction with simulated annealing, in order to speed relaxation to a good region of parameter space. The method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions. Appropriate weight scaling factors are found automatically. By applying known techniques for calculation of \\free energy\" diierences, it should also be possible to compare the merits of diierent network architectures. The work described here should also be applicable to a wide variety of statistical models other than neural networks."
            },
            "slug": "Bayesian-training-of-backpropagation-networks-by-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian training of backpropagation networks by the hybrid Monte-Carlo method"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the Hybrid Monte Carlo method, and the method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 170
                            }
                        ],
                        "text": "A Bayesian approach to handling this is to assign 0\" a vague prior distribution and then \u00b7.ntcgrating it away, giving the following probability for the training set (see (Buntine and Weigend, 1991) or (Neal, 1992) for details):"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Buntine and Weigend (1991) and MacKay (1992) approach this problem by approximating the posterior distribution by a Gaussian."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14814125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c83684f6207697c12850db423fd9747572cf1784",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist feed-forward networks, t rained with backpropagat ion, can be used both for nonlinear regression and for (discrete one-of-C ) classification. This paper presents approximate Bayesian meth ods to statistical components of back-propagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior probability), pruning insignifican t weights, est imat ing the uncertainty of weights, predict ing for new pat terns (\"out -of-sample\") , est imating the uncertainty in the choice of this predict ion (\"erro r bars\" ), estimating the generalizat ion erro r, comparing different network st ructures, and handling missing values in the t raining patterns. These methods extend some heurist ic techniques suggested in the literature, and in most cases require a small addit ional facto r in comput at ion during back-propagat ion, or computation once back-pro pagat ion has finished."
            },
            "slug": "Bayesian-Back-Propagation-Buntine-Weigend",
            "title": {
                "fragments": [],
                "text": "Bayesian Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401345"
                        ],
                        "name": "A. Kennedy",
                        "slug": "A.-Kennedy",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Kennedy",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kennedy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121101759,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "22ea20339015130099017185e7f36e87933c6a43",
            "isKey": false,
            "numCitedBy": 2584,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hybrid-Monte-Carlo-Kennedy",
            "title": {
                "fragments": [],
                "text": "Hybrid Monte Carlo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32922277"
                        ],
                        "name": "N. Metropolis",
                        "slug": "N.-Metropolis",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Metropolis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Metropolis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "91743329"
                        ],
                        "name": "A. W. Rosenbluth",
                        "slug": "A.-W.-Rosenbluth",
                        "structuredName": {
                            "firstName": "Arianna",
                            "lastName": "Rosenbluth",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. W. Rosenbluth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2991661"
                        ],
                        "name": "M. Rosenbluth",
                        "slug": "M.-Rosenbluth",
                        "structuredName": {
                            "firstName": "Marshall",
                            "lastName": "Rosenbluth",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rosenbluth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46516796"
                        ],
                        "name": "A. H. Teller",
                        "slug": "A.-H.-Teller",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Teller",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. H. Teller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3840350"
                        ],
                        "name": "E. Teller",
                        "slug": "E.-Teller",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Teller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Teller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 47
                            }
                        ],
                        "text": "This method is a variation on the algorithm of Metropolis, et al (1953), which generates a Markov chain by considering randomly-selected changes to the state."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1046577,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f6a13f116e270dde9d67848495f801cdb8efa25d",
            "isKey": false,
            "numCitedBy": 32417,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two\u2010dimensional rigid\u2010sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four\u2010term virial coefficient expansion."
            },
            "slug": "Equation-of-state-calculations-by-fast-computing-Metropolis-Rosenbluth",
            "title": {
                "fragments": [],
                "text": "Equation of state calculations by fast computing machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100550660"
                        ],
                        "name": "H. C. Andersen",
                        "slug": "H.-C.-Andersen",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Andersen",
                            "middleNames": [
                                "Christian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. C. Andersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34820304,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9b2c7cfba54c9dc301e51d950ebe7b517b53a5a0",
            "isKey": false,
            "numCitedBy": 4301,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In the molecular dynamics simulation method for fluids, the equations of motion for a collection of particles in a fixed volume are solved numerically. The energy, volume, and number of particles are constant for a particular simulation, and it is assumed that time averages of properties of the simulated fluid are equal to microcanonical ensemble averages of the same properties. In some situations, it is desirable to perform simulations of a fluid for particular values of temperature and/or pressure or under conditions in which the energy and volume of the fluid can fluctuate. This paper proposes and discusses three methods for performing molecular dynamics simulations under conditions of constant temperature and/or pressure, rather than constant energy and volume. For these three methods, it is shown that time averages of properties of the simulated fluid are equal to averages over the isoenthalpic\u2013isobaric, canonical, and isothermal\u2013isobaric ensembles. Each method is a way of describing the dynamics of a certain number of particles in a volume element of a fluid while taking into account the influence of surrounding particles in changing the energy and/or density of the simulated volume element. The influence of the surroundings is taken into account without introducing unwanted surface effects. Examples of situations where these methods may be useful are discussed."
            },
            "slug": "Molecular-dynamics-simulations-at-constant-pressure-Andersen",
            "title": {
                "fragments": [],
                "text": "Molecular dynamics simulations at constant pressure and/or temperature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 6,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Bayesian-Learning-via-Stochastic-Dynamics-Neal/d275cf94e620bf5b3776bba8a88acccdcfcd9a19?sort=total-citations"
}