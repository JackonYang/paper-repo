{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2243473"
                        ],
                        "name": "Athanasios Katsamanis",
                        "slug": "Athanasios-Katsamanis",
                        "structuredName": {
                            "firstName": "Athanasios",
                            "lastName": "Katsamanis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Athanasios Katsamanis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738119"
                        ],
                        "name": "Vassilis Pitsikalis",
                        "slug": "Vassilis-Pitsikalis",
                        "structuredName": {
                            "firstName": "Vassilis",
                            "lastName": "Pitsikalis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vassilis Pitsikalis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750686"
                        ],
                        "name": "P. Maragos",
                        "slug": "P.-Maragos",
                        "structuredName": {
                            "firstName": "Petros",
                            "lastName": "Maragos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Maragos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 212
                            }
                        ],
                        "text": "Although using audio information alone performs reasonably well for speech recognition, fusing audio and visual information can substantially improve performance, especially when the audio is degraded with noise [19, 20, 21, 23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "7% Discrete Cosine Transform [19] 64% \u2020\u00a7 Active Appearence Model [20] 75."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10103768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30aabb8981195f0f608824622ba56800c1776677",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the effect of uncertain feature measurements and show how classification and learning rules should be adjusted to compensate for it. Our approach is particularly fruitful in multimodal fusion scenarios, such as audio-visual speech recognition, where multiple streams of complementary features whose reliability is time-varying are integrated. For such applications, by taking the measurement noise uncertainty of each feature stream into account, the proposed framework leads to highly adaptive multimodal fusion rules for classification and learning which are widely applicable and easy to implement. We further show that previous multimodal fusion methods relying on stream weights fall under our scheme under certain assumptions; this provides novel insights into their applicability for various tasks and suggests new practical ways for estimating the stream weights adaptively. The potential of our approach is demonstrated in audio-visual speech recognition experiments."
            },
            "slug": "Multimodal-Fusion-and-Learning-with-Uncertain-to-Papandreou-Katsamanis",
            "title": {
                "fragments": [],
                "text": "Multimodal Fusion and Learning with Uncertain Features Applied to Audiovisual Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work studies the effect of uncertain feature measurements and shows how classification and learning rules should be adjusted to compensate for it, and shows that previous multimodal fusion methods relying on stream weights fall under this scheme under certain assumptions."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 9th Workshop on Multimedia Signal Processing"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2243473"
                        ],
                        "name": "Athanasios Katsamanis",
                        "slug": "Athanasios-Katsamanis",
                        "structuredName": {
                            "firstName": "Athanasios",
                            "lastName": "Katsamanis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Athanasios Katsamanis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738119"
                        ],
                        "name": "Vassilis Pitsikalis",
                        "slug": "Vassilis-Pitsikalis",
                        "structuredName": {
                            "firstName": "Vassilis",
                            "lastName": "Pitsikalis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vassilis Pitsikalis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750686"
                        ],
                        "name": "P. Maragos",
                        "slug": "P.-Maragos",
                        "structuredName": {
                            "firstName": "Petros",
                            "lastName": "Maragos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Maragos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "1% \u2020 Visemic AAM[23] 83% \u2020\u00a7 (b) CUAVE Video Table 1: Classification performance for visual speech classification on (a) AVLetters and (b) CUAVE."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 212
                            }
                        ],
                        "text": "Although using audio information alone performs reasonably well for speech recognition, fusing audio and visual information can substantially improve performance, especially when the audio is degraded with noise [19, 20, 21, 23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15030650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa9816564cdb09ee7a581ec962e8a348421802a0",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "While the accuracy of feature measurements heavily depends on changing environmental conditions, studying the consequences of this fact in pattern recognition tasks has received relatively little attention to date. In this paper, we explicitly take feature measurement uncertainty into account and show how multimodal classification and learning rules should be adjusted to compensate for its effects. Our approach is particularly fruitful in multimodal fusion scenarios, such as audiovisual speech recognition, where multiple streams of complementary time-evolving features are integrated. For such applications, provided that the measurement noise uncertainty for each feature stream can be estimated, the proposed framework leads to highly adaptive multimodal fusion rules which are easy and efficient to implement. Our technique is widely applicable and can be transparently integrated with either synchronous or asynchronous multimodal sequence integration architectures. We further show that multimodal fusion methods relying on stream weights can naturally emerge from our scheme under certain assumptions; this connection provides valuable insights into the adaptivity properties of our multimodal uncertainty compensation approach. We show how these ideas can be practically applied for audiovisual speech recognition. In this context, we propose improved techniques for person-independent visual feature extraction and uncertainty estimation with active appearance models, and also discuss how enhanced audio features along with their uncertainty estimates can be effectively computed. We demonstrate the efficacy of our approach in audiovisual speech recognition experiments on the CUAVE database using either synchronous or asynchronous multimodal integration models."
            },
            "slug": "Adaptive-Multimodal-Fusion-by-Uncertainty-With-to-Papandreou-Katsamanis",
            "title": {
                "fragments": [],
                "text": "Adaptive Multimodal Fusion by Uncertainty Compensation With Application to Audiovisual Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper explicitly takes feature measurement uncertainty into account and shows how multimodal classification and learning rules should be adjusted to compensate for its effects, and proposes improved techniques for person-independent visual feature extraction and uncertainty estimation with active appearance models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409971380"
                        ],
                        "name": "Ben Packer",
                        "slug": "Ben-Packer",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Packer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Packer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "The multimodal learning settings we consider can be viewed as a special case of self-taught learning [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6692382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "isKey": false,
            "numCitedBy": 1611,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation."
            },
            "slug": "Self-taught-learning:-transfer-learning-from-data-Raina-Battle",
            "title": {
                "fragments": [],
                "text": "Self-taught learning: transfer learning from unlabeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data to form a succinct input representation and significantly improve classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713496"
                        ],
                        "name": "P. Lucey",
                        "slug": "P.-Lucey",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Lucey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lucey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729760"
                        ],
                        "name": "S. Sridharan",
                        "slug": "S.-Sridharan",
                        "structuredName": {
                            "firstName": "Sridha",
                            "lastName": "Sridharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sridharan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 65244996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9408ff9e3e826c22461f2e3b488dbfc760e9981e",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual information from a speaker's mouth region is known to \nimprove automatic speech recognition robustness, especially in the presence of acoustic noise. To date, the vast majority of work in this field has viewed these visual features in a holistic manner, which may not take into account the various changes that occur within articulation (process of changing the shape of the vocal tract using the articulators, i.e lips and jaw). Motivated by the work being conducted in fields of audio-visual automatic speech \nrecognition (AVASR) and face recognition using articulatory \nfeatures (AFs) and patches respectively, we present a \nproof of concept paper which represents the mouth region as a ensemble of image patches. Our experiments show that by dealing with the mouth region in this manner, we are able to extract more speech information from the visual domain. For the task of visual-only speaker-independent isolated digit recognition, we were able to improve the relative word error rate by more than 23\\% on the CUAVE audio-visual corpus."
            },
            "slug": "Patch-Based-Representation-of-Visual-Speech-Lucey-Sridharan",
            "title": {
                "fragments": [],
                "text": "Patch-Based Representation of Visual Speech"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "By dealing with the mouth region in this manner, it is shown that by extracting more speech information from the visual domain, the relative word error rate on the CUAVE audio-visual corpus is improved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798462"
                        ],
                        "name": "Pierre-Antoine Manzagol",
                        "slug": "Pierre-Antoine-Manzagol",
                        "structuredName": {
                            "firstName": "Pierre-Antoine",
                            "lastName": "Manzagol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Antoine Manzagol"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Instead, we propose a training method inspired by denoising autoencoders [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207168299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843959ffdccf31c6694d135fad07425924f785b1",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite."
            },
            "slug": "Extracting-and-composing-robust-features-with-Vincent-Larochelle",
            "title": {
                "fragments": [],
                "text": "Extracting and composing robust features with denoising autoencoders"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788454"
                        ],
                        "name": "Mihai Gurban",
                        "slug": "Mihai-Gurban",
                        "structuredName": {
                            "firstName": "Mihai",
                            "lastName": "Gurban",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mihai Gurban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710257"
                        ],
                        "name": "J. Thiran",
                        "slug": "J.-Thiran",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Thiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Thiran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 212
                            }
                        ],
                        "text": "Although using audio information alone performs reasonably well for speech recognition, fusing audio and visual information can substantially improve performance, especially when the audio is degraded with noise [19, 20, 21, 23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "7% Discrete Cosine Transform [19] 64% \u2020\u00a7 Active Appearence Model [20] 75."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9403350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48c6b1d7fd3d9b5e170481c255aaab8bd66d40ab",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of feature selection has been thoroughly analyzed in the context of pattern classification, with the purpose of avoiding the curse of dimensionality. However, in the context of multimodal signal processing, this problem has been studied less. Our approach to feature extraction is based on information theory, with an application on multimodal classification, in particular audio-visual speech recognition. Contrary to previous work in information theoretic feature selection applied to multimodal signals, our proposed methods penalize features for their redundancy, achieving more compact feature sets and better performance. We propose two greedy selection algorithms, one that penalizes a proportion of feature redundancy, while the other uses conditional mutual information as an evaluation measure, for the selection of visual features for audio-visual speech recognition. Our features perform better than linear discriminant analysis, the most usual transform for dimensionality reduction in the field, across a wide range of dimensionality values and combined with audio at different quality levels."
            },
            "slug": "Information-Theoretic-Feature-Extraction-for-Speech-Gurban-Thiran",
            "title": {
                "fragments": [],
                "text": "Information Theoretic Feature Extraction for Audio-Visual Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes two greedy selection algorithms, one that penalizes a proportion of feature redundancy, while the other uses conditional mutual information as an evaluation measure, for the selection of visual features for audio-visual speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122924999"
                        ],
                        "name": "E. Patterson",
                        "slug": "E.-Patterson",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Patterson",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Patterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3097743"
                        ],
                        "name": "S. Gurbuz",
                        "slug": "S.-Gurbuz",
                        "structuredName": {
                            "firstName": "Sabri",
                            "lastName": "Gurbuz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gurbuz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2348364"
                        ],
                        "name": "Z. Tufekci",
                        "slug": "Z.-Tufekci",
                        "structuredName": {
                            "firstName": "Zekeriya",
                            "lastName": "Tufekci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tufekci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39960224"
                        ],
                        "name": "J. Gowdy",
                        "slug": "J.-Gowdy",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Gowdy",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gowdy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206738073,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd70cd5c716b7c484f4a9833cf61453e1e70d387",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Multimodal signal processing has become an important topic of research for overcoming certain problems of audio-only speech processing. Audio-visual speech recognition is one area with great potential. Difficulties due to background noise and multiple speakers are significantly reduced by the additional information provided by extra visual features. Despite a few efforts to create databases in this area, none has emerged as a standard for comparison for several possible reasons. This paper seeks to introduce a new audiovisual database that is flexible and fairly comprehensive, yet easily available to researchers on one DVD. The CUAVE database is a speaker-independent corpus of over 7,000 utterances of both connected and isolated digits. It is designed to meet several goals that are discussed in this paper. The most notable are availability of the database, flexibility for use of the audio-visual data, and realistic considerations in the recordings (such as speaker movement). Another important focus of the database is the inclusion of pairs of simultaneous speakers, the first documented database of this kind. The overall goal of this project is to facilitate more widespread audio-visual research through an easily available database. For information on obtaining CUAVE, please visit our webpage (http://ece.clemson.edu/speech)."
            },
            "slug": "CUAVE:-A-new-audio-visual-database-for-multimodal-Patterson-Gurbuz",
            "title": {
                "fragments": [],
                "text": "CUAVE: A new audio-visual database for multimodal human-computer interface research"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new audiovisual database that is flexible and fairly comprehensive, yet easily available to researchers on one DVD, and the inclusion of pairs of simultaneous speakers, the first documented database of this kind are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779306"
                        ],
                        "name": "Chaitanya Ekanadham",
                        "slug": "Chaitanya-Ekanadham",
                        "structuredName": {
                            "firstName": "Chaitanya",
                            "lastName": "Ekanadham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaitanya Ekanadham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "We use an extension of RBMs with sparsity [8], which have been shown to be able to learn meaningful features for digits and natural images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "We first describe the restricted Boltzmann machine (RBM) [5, 6] followed by the sparsity regularization method [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12589862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "202cbbf671743aefd380d2f23987bd46b9caaf97",
            "isKey": false,
            "numCitedBy": 1028,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or \"deep,\" structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Specifically, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge filters, similar to the Gabor functions known to model V1 cell receptive fields. Further, the second layer in our model encodes correlations of the first layer responses in the data. Specifically, it picks up both colinear (\"contour\") features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex \"corner\" features matches well with the results from the Ito & Komatsu's study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features."
            },
            "slug": "Sparse-deep-belief-net-model-for-visual-area-V2-Lee-Ekanadham",
            "title": {
                "fragments": [],
                "text": "Sparse deep belief net model for visual area V2"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An unsupervised learning model is presented that faithfully mimics certain properties of visual area V2 and the encoding of these more complex \"corner\" features matches well with the results from the Ito & Komatsu's study of biological V2 responses, suggesting that this sparse variant of deep belief networks holds promise for modeling more higher-order features."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738119"
                        ],
                        "name": "Vassilis Pitsikalis",
                        "slug": "Vassilis-Pitsikalis",
                        "structuredName": {
                            "firstName": "Vassilis",
                            "lastName": "Pitsikalis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vassilis Pitsikalis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2243473"
                        ],
                        "name": "Athanasios Katsamanis",
                        "slug": "Athanasios-Katsamanis",
                        "structuredName": {
                            "firstName": "Athanasios",
                            "lastName": "Katsamanis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Athanasios Katsamanis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750686"
                        ],
                        "name": "P. Maragos",
                        "slug": "P.-Maragos",
                        "structuredName": {
                            "firstName": "Petros",
                            "lastName": "Maragos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Maragos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 212
                            }
                        ],
                        "text": "Although using audio information alone performs reasonably well for speech recognition, fusing audio and visual information can substantially improve performance, especially when the audio is degraded with noise [19, 20, 21, 23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6648031,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b12bcbfd482bd935603c2f0e5f6bd5e85c25997",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "While the accuracy of feature measurements heavily depends on changing environmental conditions, studying the consequences of this fact in pattern recognition tasks has received relatively little attention to date. In this work we explicitly take into account feature measurement uncertainty and we show how classification rules should be adjusted to compensate for its effects. Our approach is particularly fruitful in multimodal fusion scenarios, such as audiovisual speech recognition, where multiple streams of complementary time-evolving features are integrated. For such applications, provided that the measurement noise uncertainty for each feature stream can be estimated, the proposed framework leads to highly adaptive multimodal fusion rules which are widely applicable and easy to implement. We further show that previous multimodal fusion methods relying on stream weights fall under our scheme under certain assumptions; this provides novel insights into their applicability for various tasks and suggests new practical ways for estimating the stream weights adaptively. The potential of our approach is demonstrated in audio-visual speech recognition using either synchronous or asynchronous models. Index Terms: multimodal fusion, audiovisual speech recognition, uncertainty compensation, Active Appearance Models, product HMMs, stream weights"
            },
            "slug": "Adaptive-multimodal-fusion-by-uncertainty-Pitsikalis-Katsamanis",
            "title": {
                "fragments": [],
                "text": "Adaptive multimodal fusion by uncertainty compensation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work explicitly takes into account feature measurement uncertainty and shows how classification rules should be adjusted to compensate for its effects, which leads to highly adaptive multimodal fusion rules which are widely applicable and easy to implement."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2510464"
                        ],
                        "name": "P. Duchnowski",
                        "slug": "P.-Duchnowski",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Duchnowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Duchnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145352356"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17187892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "704d9fbe2b36b5187abc6982f666558bdd4cf386",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present recent work on integration of visual information (automatic lip-reading) with acoustic speech for better overall speech recognition. A Multi-State Time Delay Neural Network performs the recognition of spelled letter sequences taking advantage of lip images from a standard camera. The problems addressed include e cient but e ective representation of the visual information and optimum manner of combining the two modalities when rendering a decision. We show results for several alternatives to direct gray level image as the visual evidence. These are: Principal Components, Linear Discriminants, and DFT coe cients. Dimensionality of the input is decreased by a factor of 12 while maintaining recognition rates. Combination of the visual and acoustic information is performed at three different levels of abstraction. Results suggest that integration of higher order input features works best. On a continuous spelling task, visual-alone recognition of 45-55%, when combined with acoustic data, lowers audio-alone error rates by 30-40%."
            },
            "slug": "See-me,-hear-me:-integrating-automatic-speech-and-Duchnowski-Meier",
            "title": {
                "fragments": [],
                "text": "See me, hear me: integrating automatic speech recognition and lip-reading"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Recent work on integration of visual information (automatic lip-reading) with acoustic speech for better overall speech recognition suggests that integration of higher order input features works best."
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 57
                            }
                        ],
                        "text": "We first describe the restricted Boltzmann machine (RBM) [5, 6] followed by the sparsity regularization method [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 96
                            }
                        ],
                        "text": "While self-taught learning was first motivated with sparse coding, recent work on deep learning [5, 6, 7] have examined how deep sigmoidal networks can be trained to produce useful representations for handwritten digits and text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13411,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132323"
                        ],
                        "name": "S. Cox",
                        "slug": "S.-Cox",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Cox",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053633522"
                        ],
                        "name": "R. Harvey",
                        "slug": "R.-Harvey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harvey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harvey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1934275"
                        ],
                        "name": "Yuxuan Lan",
                        "slug": "Yuxuan-Lan",
                        "structuredName": {
                            "firstName": "Yuxuan",
                            "lastName": "Lan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuxuan Lan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2537639"
                        ],
                        "name": "Jacob L. Newman",
                        "slug": "Jacob-L.-Newman",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Newman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob L. Newman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2785748"
                        ],
                        "name": "B. Theobald",
                        "slug": "B.-Theobald",
                        "structuredName": {
                            "firstName": "Barry-John",
                            "lastName": "Theobald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Theobald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8556936,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "3ccf2122a3890b062798790acc8d1d1c084a7b0f",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In speech recognition, the problem of speaker variability has been well studied. Common approaches to dealing with it include normalising for a speaker's vocal tract length and learning a linear transform that moves the speaker-independent models closer to to a new speaker. In pure lip-reading (no audio) the problem has been less well studied. Results are often presented that are based on speaker-dependent (single speaker) or multispeaker (speakers in the test-set are also in the training-set) data, situations that are of limited use in real applications. This paper shows the danger of not using different speakers in the trainingand test-sets. Firstly, we present classification results on a new single-word database AVletters 2 which is a high-definition version of the well known AVletters database. By careful choice of features, we show that it is possible for the performance of visual-only lip-reading to be very close to that of audio-only recognition for the single speaker and multi-speaker configurations. However, in the speaker independent configuration, the performance of the visual-only channel degrades dramatically. By applying multidimensional scaling (MDS) to both the audio features and visual features, we demonstrate that lip-reading visual features, when compared with the MFCCs commonly used for audio speech recognition, have inherently small variation within a single speaker across all classes spoken. However, visual features are highly sensitive to the identity of the speaker, whereas audio features are relatively invariant."
            },
            "slug": "The-challenge-of-multispeaker-lip-reading-Cox-Harvey",
            "title": {
                "fragments": [],
                "text": "The challenge of multispeaker lip-reading"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper shows the danger of not using different speakers in the trainingand test-sets and demonstrates that lip-reading visual features, when compared with the MFCCs commonly used for audio speech recognition, have inherently small variation within a single speaker across all classes spoken."
            },
            "venue": {
                "fragments": [],
                "text": "AVSP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757287"
                        ],
                        "name": "Guoying Zhao",
                        "slug": "Guoying-Zhao",
                        "structuredName": {
                            "firstName": "Guoying",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guoying Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144302675"
                        ],
                        "name": "M. Barnard",
                        "slug": "M.-Barnard",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 156
                            }
                        ],
                        "text": "We also note that adding temporal derivatives to the representations has been widely used in the literature as it helps to model dynamic speech information [3, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 53
                            }
                        ],
                        "text": "We report results on the third-test settings used by [14, 16] for comparisons."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8049969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c4a6a915a7fbb9af5beeff55bf7d8cef18bf93a",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual speech information plays an important role in lipreading under noisy conditions or for listeners with a hearing impairment. In this paper, we present local spatiotemporal descriptors to represent and recognize spoken isolated phrases based solely on visual input. Spatiotemporal local binary patterns extracted from mouth regions are used for describing isolated phrase sequences. In our experiments with 817 sequences from ten phrases and 20 speakers, promising accuracies of 62% and 70% were obtained in speaker-independent and speaker-dependent recognition, respectively. In comparison with other methods on AVLetters database, the accuracy, 62.8%, of our method clearly outperforms the others. Analysis of the confusion matrix for 26 English letters shows the good clustering characteristics of visemes for the proposed descriptors. The advantages of our approach include local processing and robustness to monotonic gray-scale changes. Moreover, no error prone segmentation of moving lips is needed."
            },
            "slug": "Lipreading-With-Local-Spatiotemporal-Descriptors-Zhao-Barnard",
            "title": {
                "fragments": [],
                "text": "Lipreading With Local Spatiotemporal Descriptors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Local spatiotemporal descriptors are presented to represent and recognize spoken isolated phrases based solely on visual input to include local processing and robustness to monotonic gray-scale changes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711695"
                        ],
                        "name": "I. Matthews",
                        "slug": "I.-Matthews",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Matthews",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Matthews"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144427091"
                        ],
                        "name": "J. Bangham",
                        "slug": "J.-Bangham",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bangham",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bangham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132323"
                        ],
                        "name": "S. Cox",
                        "slug": "S.-Cox",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Cox",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144439756"
                        ],
                        "name": "Richard Harvey",
                        "slug": "Richard-Harvey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harvey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Harvey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "8% Multiscale Spatial Analysis [16] 44."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 53
                            }
                        ],
                        "text": "We report results on the third-test settings used by [14, 16] for comparisons."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 599027,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f78867834f7f6797ca6396f98edb10aad2a864fb",
            "isKey": false,
            "numCitedBy": 512,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": "The multimodal nature of speech is often ignored in human-computer interaction, but lip deformations and other body motion, such as those of the head, convey additional information. We integrate speech cues from many sources and this improves intelligibility, especially when the acoustic signal is degraded. The paper shows how this additional, often complementary, visual speech information can be used for speech recognition. Three methods for parameterizing lip image sequences for recognition using hidden Markov models are compared. Two of these are top-down approaches that fit a model of the inner and outer lip contours and derive lipreading features from a principal component analysis of shape or shape and appearance, respectively. The third, bottom-up, method uses a nonlinear scale-space analysis to form features directly from the pixel intensity. All methods are compared on a multitalker visual speech recognition task of isolated letters."
            },
            "slug": "Extraction-of-Visual-Features-for-Lipreading-Matthews-Cootes",
            "title": {
                "fragments": [],
                "text": "Extraction of Visual Features for Lipreading"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Three methods for parameterizing lip image sequences for recognition using hidden Markov models are compared and two are top-down approaches that fit a model of the inner and outer lip contours and derive lipreading features from a principal component analysis of shape or shape and appearance, respectively."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688852"
                        ],
                        "name": "G. Potamianos",
                        "slug": "G.-Potamianos",
                        "structuredName": {
                            "firstName": "Gerasimos",
                            "lastName": "Potamianos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Potamianos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2264160"
                        ],
                        "name": "C. Neti",
                        "slug": "C.-Neti",
                        "structuredName": {
                            "firstName": "Chalapathy",
                            "lastName": "Neti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Neti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678373"
                        ],
                        "name": "J. Luettin",
                        "slug": "J.-Luettin",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Luettin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Luettin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711695"
                        ],
                        "name": "I. Matthews",
                        "slug": "I.-Matthews",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Matthews",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Matthews"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 189
                            }
                        ],
                        "text": "For the multimodal fusion setting, data from all modalities is available at all phases; this represents the typical setting considered in most prior work in audio-visual speech recognition [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "For both modalities, we also performed feature mean normalization over time [3], akin to removing the DC component from each example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 156
                            }
                        ],
                        "text": "We also note that adding temporal derivatives to the representations has been widely used in the literature as it helps to model dynamic speech information [3, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59850647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afb50fe3d6490ad5cd0b624ac72e569fbf33f619",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "We have made significant progress in automatic speech recognition (ASR) for well-defined applications like dictation and medium vocabulary transaction processing tasks in relatively controlled environments. However, ASR performance has yet to reach the level required for speech to become a truly pervasive user interface. Indeed, even in \u201cclean\u201d acoustic environments, and for a variety of tasks, state of the art ASR system performance lags human speech perception by up to an order of magnitude (Lippmann, 1997). In addition, current systems are quite sensitive to channel, environment, and style of speech variations. A number of techniques for improving ASR robustness have met limited success in severely degraded environments, mismatched to system training (Ghitza, 1986; Nadas et al., 1989; Juang, 1991; Liu et al., 1993; Hermansky and Morgan, 1994; Neti, 1994; Gales, 1997; Jiang et al., 2001). Clearly, novel, non-traditional approaches, that use orthogonal sources of information to the acoustic input, are needed to achieve ASR performance closer to the human speech perception level, and robust enough to be deployable in field applications. Visual speech is the most promising source of additional speech information, and it is obviously not affected by the acoustic environment and noise."
            },
            "slug": "Audio-Visual-Automatic-Speech-Recognition:-An-Potamianos-Neti",
            "title": {
                "fragments": [],
                "text": "Audio-Visual Automatic Speech Recognition: An Overview"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Novel, non-traditional approaches, that use orthogonal sources of information to the acoustic input, are needed to achieve ASR performance closer to the human speech perception level, and robust enough to be deployable in field applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745906"
                        ],
                        "name": "Y. Konig",
                        "slug": "Y.-Konig",
                        "structuredName": {
                            "firstName": "Yochai",
                            "lastName": "Konig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Konig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2367883,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "423a5882a9d04273d3d56ccaa06cd2e92cdd7d4a",
            "isKey": false,
            "numCitedBy": 362,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We improve the performance of a hybrid connectionist speech recognition system by incorporating visual information about the corresponding lip movements. Specifically, we investigate the benefits of adding visual features in the presence of additive noise and crosstalk (cocktail party effect). Our study extends our previous experiments by using a new visual front end, and an alternative architecture for combining the visual and acoustic information. Furthermore, we have extended our recognizer to a multi-speaker, connected letters recognizer. Our results show a significant improvement for the combined architecture (acoustic and visual information) over just the acoustic system in the presence of additive noise and crosstalk.<<ETX>>"
            },
            "slug": "\"Eigenlips\"-for-robust-speech-recognition-Bregler-Konig",
            "title": {
                "fragments": [],
                "text": "\"Eigenlips\" for robust speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This study improves the performance of a hybrid connectionist speech recognition system by incorporating visual information about the corresponding lip movements by using a new visual front end, and an alternative architecture for combining the visual and acoustic information."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145352356"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733304"
                        ],
                        "name": "Wolfgang H\u00fcrst",
                        "slug": "Wolfgang-H\u00fcrst",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "H\u00fcrst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang H\u00fcrst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2510464"
                        ],
                        "name": "P. Duchnowski",
                        "slug": "P.-Duchnowski",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Duchnowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Duchnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[13, 25] trained separate networks to model phonemes and visemes and combined the predictions at a phonetic layer to predict the spoken phoneme."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 139
                            }
                        ],
                        "text": "While we present special cases of neural networks here for multimodal learning, we note that prior work on audio-visual speech recognition [13, 24, 25] has also explored the use of neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17583077,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4ce7832ea382432d8fb6286c834e0a4cde79145",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present work on improving the performance of automated speech recognizers by using additional visual information: (lip-/speechreading); achieving error reduction of up to 50%. This paper focuses on different methods of combining the visual and acoustic data to improve the recognition performance. We show this on an extension of an existing state-of-the-art speech recognition system, a modular MS-TDNN. We have developed adaptive combination methods at several levels of the recognition network. Additional information such as estimated signal-to-noise ratio (SNR) is used in some cases. The results of the different combination methods are shown for clean speech and data with artificial noise (white, music, motor). The new combination methods adapt automatically to varying noise conditions making hand-tuned parameters unnecessary."
            },
            "slug": "Adaptive-bimodal-sensor-fusion-for-automatic-Meier-H\u00fcrst",
            "title": {
                "fragments": [],
                "text": "Adaptive bimodal sensor fusion for automatic speechreading"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "Different methods of combining the visual and acoustic data to improve the recognition performance of automated speech recognizers by using additional visual information are presented, achieving error reduction of up to 50%."
            },
            "venue": {
                "fragments": [],
                "text": "1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2588939"
                        ],
                        "name": "B. Yuhas",
                        "slug": "B.-Yuhas",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Yuhas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yuhas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32165157"
                        ],
                        "name": "M. Goldstein",
                        "slug": "M.-Goldstein",
                        "structuredName": {
                            "firstName": "Moise",
                            "lastName": "Goldstein",
                            "middleNames": [
                                "H."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goldstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[24] trained a neural network to predict the auditory signal given the visual input."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 139
                            }
                        ],
                        "text": "While we present special cases of neural networks here for multimodal learning, we note that prior work on audio-visual speech recognition [13, 24, 25] has also explored the use of neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18498061,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "f8bc7b83a70f4e1854c23a950ed168cbc3f98fec",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Results from a series of experiments that use neural networks to process the visual speech signals of a male talker are presented. In these preliminary experiments, the results are limited to static images of vowels. It is demonstrated that these networks are able to extract speech information from the visual images and that this information can be used to improve automatic vowel recognition. The structure of speech and its corresponding acoustic and visual signals are reviewed. The specific data that was used in the experiments along with the network architectures and algorithms are described. The results of integrating the visual and auditory signals for vowel recognition in the presence of acoustic noise are presented.<<ETX>>"
            },
            "slug": "Integration-of-acoustic-and-visual-speech-signals-Yuhas-Goldstein",
            "title": {
                "fragments": [],
                "text": "Integration of acoustic and visual speech signals using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated that neural networks are able to extract speech information from the visual images and that this information can be used to improve automatic vowel recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Communications Magazine"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 96
                            }
                        ],
                        "text": "While self-taught learning was first motivated with sparse coding, recent work on deep learning [5, 6, 7] have examined how deep sigmoidal networks can be trained to produce useful representations for handwritten digits and text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1501682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a",
            "isKey": false,
            "numCitedBy": 1260,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Semantic-hashing-Salakhutdinov-Hinton",
            "title": {
                "fragments": [],
                "text": "Semantic hashing"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Approx. Reason."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "To motivate our deep autoencoder [5] model, we first describe several simple models and their drawbacks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 57
                            }
                        ],
                        "text": "We first describe the restricted Boltzmann machine (RBM) [5, 6] followed by the sparsity regularization method [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 96
                            }
                        ],
                        "text": "While self-taught learning was first motivated with sparse coding, recent work on deep learning [5, 6, 7] have examined how deep sigmoidal networks can be trained to produce useful representations for handwritten digits and text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": false,
            "numCitedBy": 14645,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "We used an off-the-shelf object detector [12] with median filtering over time to extract the mouth regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29263,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054241"
                        ],
                        "name": "H. McGurk",
                        "slug": "H.-McGurk",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "McGurk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. McGurk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144239757"
                        ],
                        "name": "J. MacDonald",
                        "slug": "J.-MacDonald",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "MacDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. MacDonald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "The McGurk effect [1] refers to an audio-visual perception phenomenon where a visual /ga/ with a audio /ba/ is perceived as /da/ by most subjects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "This was first exemplified in the McGurk effect [1] where a visual /ga/ with a voiced /ba/ is perceived as /da/ by most subjects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4171157,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "eef41ae597a20ea377461d522fd5100da6a7a9b7",
            "isKey": false,
            "numCitedBy": 5484,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "MOST verbal communication occurs in contexts where the listener can see the speaker as well as hear him. However, speech perception is normally regarded as a purely auditory process. The study reported here demonstrates a previously unrecognised influence of vision upon speech perception. It stems from an observation that, on being shown a film of a young woman's talking head, in which repeated utterances of the syllable [ba] had been dubbed on to lip movements for [ga], normal adults reported hearing [da]. With the reverse dubbing process, a majority reported hearing [bagba] or [gaba]. When these subjects listened to the soundtrack from the film, without visual input, or when they watched untreated film, they reported the syllables accurately as repetitions of [ba] or [ga]. Subsequent replications confirm the reliability of these findings; they have important implications for the understanding of speech perception."
            },
            "slug": "Hearing-lips-and-seeing-voices-McGurk-MacDonald",
            "title": {
                "fragments": [],
                "text": "Hearing lips and seeing voices"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The study reported here demonstrates a previously unrecognised influence of vision upon speech perception, on being shown a film of a young woman's talking head in which repeated utterances of the syllable [ba] had been dubbed on to lip movements for [ga]."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4571,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8539297"
                        ],
                        "name": "Q. Summerfield",
                        "slug": "Q.-Summerfield",
                        "structuredName": {
                            "firstName": "Quentin",
                            "lastName": "Summerfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Summerfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "In particular, the visual modality provides information on the place of articulation [2] and muscle movements which can often help to disambiguate between speech with similar acoustics (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8459772,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "6da7738f4ab6789196f8980c9eec56e99e861c2e",
            "isKey": false,
            "numCitedBy": 384,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reviews progress in understanding the psychology of lipreading and audio-visual speech perception. It considers four questions. What distinguishes better from poorer lipreaders? What are the effects of introducing a delay between the acoustical and optical speech signals? What have attempts to produce computer animations of talking faces contributed to our understanding of the visual cues that distinguish consonants and vowels? Finally, how should the process of audio-visual integration in speech perception be described; that is, how are the sights and sounds of talking faces represented at their conflux?"
            },
            "slug": "Lipreading-and-audio-visual-speech-perception.-Summerfield",
            "title": {
                "fragments": [],
                "text": "Lipreading and audio-visual speech perception."
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Progress in understanding the psychology of lipreading and audio-visual speech perception is reviewed and how are the sights and sounds of talking faces represented at their conflux is reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763894"
                        ],
                        "name": "D. Hardoon",
                        "slug": "D.-Hardoon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hardoon",
                            "middleNames": [
                                "Roi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hardoon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540580"
                        ],
                        "name": "S. Szedm\u00e1k",
                        "slug": "S.-Szedm\u00e1k",
                        "structuredName": {
                            "firstName": "S\u00e1ndor",
                            "lastName": "Szedm\u00e1k",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Szedm\u00e1k"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, we suggest using canonical correlation analysis (CCA) (Hardoon et al., 2004), which finds linear transformations of audio and video data, to form a shared representation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 202473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6b5b20151c752beb74508f813699fa5216dedfa",
            "isKey": false,
            "numCitedBy": 2670,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general method using kernel canonical correlation analysis to learn a semantic representation to web images and their associated text. The semantic space provides a common representation and enables a comparison between the text and images. In the experiments, we look at two approaches of retrieving images based on only their content from a text query. We compare orthogonalization approaches against a standard cross-representation retrieval technique known as the generalized vector space model."
            },
            "slug": "Canonical-Correlation-Analysis:-An-Overview-with-to-Hardoon-Szedm\u00e1k",
            "title": {
                "fragments": [],
                "text": "Canonical Correlation Analysis: An Overview with Application to Learning Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A general method using kernel canonical correlation analysis to learn a semantic representation to web images and their associated text and compares orthogonalization approaches against a standard cross-representation retrieval technique known as the generalized vector space model is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "We used the TIMIT [18] dataset for unsupervised audio feature pre-training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 86
                            }
                        ],
                        "text": "23 volunteers spoke the digits 0 to 9, letters A to Z and selected sentences from the TIMIT dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "TIMIT."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Goudie Marshall"
            },
            "venue": {
                "fragments": [],
                "text": "The DARPA speech recognition research database: Specification and status. In DARPA Speech Recognition Workshop"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "This model can be viewed as an instance of multitask learning [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 155100635,
            "fieldsOfStudy": [],
            "id": "4d031e39474f2b622e87316314cb6c33eeda0786",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multitask Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Learning to Learn"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The DARPA speech recognition research database : Specification and status"
            },
            "venue": {
                "fragments": [],
                "text": "DARPA Speech Recognition Workshop"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The DARPA speech recognition research database: Specification and status"
            },
            "venue": {
                "fragments": [],
                "text": "DARPA Speech Recognition Workshop"
            },
            "year": 1986
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 14,
            "methodology": 14
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Multimodal-Deep-Learning-Ngiam-Khosla/80e9e3fc3670482c1fee16b2542061b779f47c4f?sort=total-citations"
}