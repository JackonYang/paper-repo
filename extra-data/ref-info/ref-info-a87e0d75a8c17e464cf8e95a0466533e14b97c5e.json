{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 231
                            }
                        ],
                        "text": "Under certain forms of the model, this is equivalent to the methods of redundancy reduction and maximizing the mutual information between the input and the representation [37, 38, 39], and has been advocated by several researchers [24, 40, 25, 1, 41, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 110
                            }
                        ],
                        "text": "Typically, bases that are chosen for their low-entropy coding properties, such as Gabor functions or wavelets [1, 2, 3], are handdesigned rather than being adapted to the data so as to optimize coding e ciency."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20021288,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "da9cee8647743711c1f8b4f61185bfbece0ad284",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In biological visual systems, it is not obvious whether coding efficiency as measured by mutual information among the neurons is a factor that explains any of their properties. The center/surround receptive field profiles of neurons in the retina and geniculate are far from an orthogonal set, but a given neuron can still be regarded as a decorrelator of the incoming signal in the sense that it responds primarily to changes in the image. At the level of the brain's visual cortex, the introduction of the new variable of orientation selectivity can be regarded not only as a means for providing orientation labels for image structure, but also more basically as an effective decorrelator of the neural representation. The present image coding simulations, based on quantitative neurobiological data about the code primitives, provide measures of the bit-rate efficiency of such oriented, quadrature, neural codes. Demonstrations of data compression to below 1 bit/pixel in cortically-based, quadrature self-similar wavelet image codes are also provided.<<ETX>>"
            },
            "slug": "Entropy-reduction-and-decorrelation-in-visual-by-Daugman",
            "title": {
                "fragments": [],
                "text": "Entropy reduction and decorrelation in visual coding by oriented neural receptive fields"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The present image coding simulations, based on quantitative neurobiological data about the code primitives, provide measures of the bit-rate efficiency of such oriented, quadrature, neural codes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Biomedical Engineering"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615848"
                        ],
                        "name": "Rajesh P. N. Rao",
                        "slug": "Rajesh-P.-N.-Rao",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Rao",
                            "middleNames": [
                                "P.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajesh P. N. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12785609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6aad54a507ee88a3cc93219c2a2356243e758c4",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "Neurons in the mammalian primary visual cortex are known to possess spatially localized, oriented receptive fields. It has previously been suggested that these distinctive properties may reflect an efficient image encoding strategy based on maximizing the sparseness of the distribution of output neuronal activities or alternately, extracting the independent components of natural image ensembles. Here, we show that a strategy for transformation-invariant coding of images based on a first-order Taylor series expansion of an image also causes localized, oriented receptive fields to be learned from natural image inputs. These receptive fields, which approximate localized first-order differential operators at various orientations, allow a pair of cooperating neural networks, one estimating object identity ('what') and the other estimating object transformations ('where'), to simultaneously recognize an object and estimate its pose by jointly maximizing the a posteriori probability of generating the observed visual data. We provide experimental results demonstrating the ability of such networks to factor retinal stimuli into object-centred features and object-invariant transformation estimates."
            },
            "slug": "Development-of-localized-oriented-receptive-fields-Rao-Ballard",
            "title": {
                "fragments": [],
                "text": "Development of localized oriented receptive fields by learning a translation-invariant code for natural images."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that a strategy for transformation-invariant coding of images based on a first-order Taylor series expansion of an image also causes localized, oriented receptive fields to be learned from natural image inputs."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "Here, we present an alternate derivation that demonstrates more directly the connections to the previous learning rule of Olshausen and Field [10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 115
                            }
                        ],
                        "text": "The basis function characteristics found here are generally consistent with previous results using related methods [10, 21, 22]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 53
                            }
                        ],
                        "text": "This result is consistent with previous observations [14, 10, 21, 22], and in fact the learning algorithm used by Olshausen and Field [14, 10] and the ICA learning rule used by Bell and Sejnowski [21] can both be derived from this framework [4]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 222
                            }
                        ],
                        "text": "Overcomplete bases have been advocated because they allow certain advantages in terms of interpolation [8], in achieving a tight frame with nonorthogonal basis functions [3], or in achieving sparsity in the representation [9, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "In order to ascertain the receptive elds of the model they would have to be mapped with spots and gratings, and previous experience with this [10] has shown that the non-linearity tends to make units more selective than one would predict from a simple linear input-output relationship."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 130
                            }
                        ],
                        "text": "Such a distribution re ects the notion that natural images should be described in terms of a small number of descriptive elements [2, 10]; thus, any given coe cient will rarely be active, and when it does become active it takes on a value along a continuum."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "This characterizes in part the approach taken by Olshausen and Field [10], although that algorithm samples the posterior only at its maximum, ignoring the volume and thus requiring an additional adaptive step to scale the basis functions so that the coe cients have the same variance as dictated by the prior."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 139
                            }
                        ],
                        "text": "An intriguing aspect of codes adapted to natural images is their resemblance to the receptive elds of neurons in the primary visual cortex [14, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14208692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2805537bec87a6177037b18f9a3a9d3f1038867b",
            "isKey": false,
            "numCitedBy": 3574,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "The approach presented by Lewicki and Sejnowski [4] generalizes ICA in two ways that are relevant to learning e cient image codes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 241
                            }
                        ],
                        "text": "This result is consistent with previous observations [14, 10, 21, 22], and in fact the learning algorithm used by Olshausen and Field [14, 10] and the ICA learning rule used by Bell and Sejnowski [21] can both be derived from this framework [4]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "This rule can be written in the form derived by Lewicki and Sejnowski [4] by premultiplying by AAT AAT A / hAATesT AATAH 1i (52) = A(zsT +ATAH 1) ; (53) where the last step is obtained using ATe = z."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "By applying the methods of Lewicki and Sejnowski [4], we show how a linear basis function expansion can be used to model the probability distribution of natural images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "To further speed convergence, we used the modi cations of the basic gradient ascent procedure described previously [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 48
                            }
                        ],
                        "text": "This rule can be written in the form derived by Lewicki and Sejnowski6 by premultiplying by AAT\nAATDA } l~AATesT 2 AATAH21#, (A21)\n5 2A~zsT 1 ATAH21!, (A22)\nwhere the last step is obtained by using lATe 5 2z.\n2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "A second generalization given by Lewicki and Sejnowski [4], was that the technique also applies to the case where the number of basis functions is greater than the dimensionality of the inputs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 57
                            }
                        ],
                        "text": "Premultiplying this rule by AAT yields the form given by Lewicki and Sejnowski6:\nDA } 2A~zsT 1 ATAH21!, (20)\nwhere z 5 d log P(s)/ds (see Appendix A for details)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "Premultiplying this rule by AAT yields the form given by Lewicki and Sejnowski [4]: A / A(zsT +ATAH 1) ; (20) where z = d logP (s)=ds (see appendix for details)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "Appendix Derivation of learning rule A derivation of the learning rule has been presented previously by Lewicki and Sejnowski [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 67
                            }
                        ],
                        "text": "A derivation of the learning rule has been presented previously by Lewicki and Sejnowski.6 Here we present an alternate derivation that demonstrates more directly the connections to the previous learning rule of Olshausen and Field.10 The log-probability of the data has the form\nlog P~xuA!"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "Overcomplete codes have been shown to yield greater coding e ciency on some test data sets [4], but for the natural image data used here, overcomplete codes did not yield and improvement in coding e ciency."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "To estimate how well a particular basis represented a given set of data, we followed two methods described in detail in [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6254191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42d906c733f273109c0ed716a5ef6e2a379beb26",
            "isKey": false,
            "numCitedBy": 1255,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "In an overcomplete basis, the number of basis vectors is greater than the dimensionality of the input, and the representation of an input is not a unique combination of basis vectors. Overcomplete representations have been advocated because they have greater robustness in the presence of noise, can be sparser, and can have greater flexibility in matching structure in the data. Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary visual cortex. Previous work has focused on finding the best representation of a signal using a fixed overcomplete basis (or dictionary). We present an algorithm for learning an overcomplete basis by viewing it as probabilistic model of the observed data. We show that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency. This can be viewed as a generalization of the technique of independent component analysis and provides a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
            },
            "slug": "Learning-Overcomplete-Representations-Lewicki-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning Overcomplete Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency and provide a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 343,
                                "start": 325
                            }
                        ],
                        "text": "The basis function characteristics found here are generally consistent with previous results obtained with related methods.10,21,22 The differences are that Olshausen and Field10 show a somewhat more multimodal distribution of spatial-frequency tuning clustered at either low, medium, or high frequencies, while the bases of Bell and Sejnowski,21 as well as those of van Hateren and van der Schaaf,22 appear more highly skewed toward the highest spatial frequencies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 167
                            }
                        ],
                        "text": "This result is consistent with previous observations,10,16,21,22 and in fact the learning algorithm used by Olshausen and Field10,16 and the ICA learning rule used by Bell and Sejnowski21 can both be derived from this framework.6 The notion of efficient coding can be defined only with respect to a model, and this represents perhaps the simplest non-Gaussian model that produces Gabor-like receptive fields."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 38
                            }
                        ],
                        "text": "The authors thank Tony Bell and Terry Sejnowski for stimulating discussions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 61
                            }
                        ],
                        "text": "Olshausen and Field used a generalized Cauchy prior, whereas Bell and Sejnowski use the prior P(sm) } sech(sm) (corresponding to the hyperbolic tangent output nonlinearity), which is less peaked at zero (approximately Gaussian for sm between 21 and 1, or approximately 50% of the total probability) and as a result is less sparse than the Laplacian that we employ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 115
                            }
                        ],
                        "text": "The basis function characteristics found here are generally consistent with previous results using related methods [10, 21, 22]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 53
                            }
                        ],
                        "text": "This result is consistent with previous observations [14, 10, 21, 22], and in fact the learning algorithm used by Olshausen and Field [14, 10] and the ICA learning rule used by Bell and Sejnowski [21] can both be derived from this framework [4]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "The results obtained here, as well as similar results obtained using related methods [14, 21, 22], suggest that the localized, oriented, and bandpass structure of V1 receptive elds can be accounted for in terms of a rather general coding principle | i."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 140
                            }
                        ],
                        "text": "Olshausen and Field assumed a relatively high noise level compared with that assumed here (l ' 100 versus l 5 3000), whereas the methods of Bell and Sejnowski and of van Hateren and van der Schaaf assume zero noise."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 60
                            }
                        ],
                        "text": "This rule can be written in the form derived by Lewicki and Sejnowski6 by premultiplying by AAT\nAATDA } l~AATesT 2 AATAH21#, (A21)\n5 2A~zsT 1 ATAH21!, (A22)\nwhere the last step is obtained by using lATe 5 2z.\n2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 99
                            }
                        ],
                        "text": "Nearly all of the learned basis functions show a Gabor-like structure as has been found previously [14, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 69
                            }
                        ],
                        "text": "Premultiplying this rule by AAT yields the form given by Lewicki and Sejnowski6:\nDA } 2A~zsT 1 ATAH21!, (20)\nwhere z 5 d log P(s)/ds (see Appendix A for details)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 79
                            }
                        ],
                        "text": "A derivation of the learning rule has been presented previously by Lewicki and Sejnowski.6 Here we present an alternate derivation that demonstrates more directly the connections to the previous learning rule of Olshausen and Field.10 The log-probability of the data has the form\nlog P~xuA!"
                    },
                    "intents": []
                }
            ],
            "corpusId": 6219133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca1d23be869380ac9e900578c601c2d1febcc0c9",
            "isKey": true,
            "numCitedBy": 2373,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-\u201cindependent-components\u201d-of-natural-scenes-are-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "The \u201cindependent components\u201d of natural scenes are edge filters"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110711666"
                        ],
                        "name": "T. Lee",
                        "slug": "T.-Lee",
                        "structuredName": {
                            "firstName": "Tai",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "(5), given only information about the image x, is ill-posed for two reasons."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7230571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2adac8e7ad1818d6a1fbc19ecf858581d9a5df9f",
            "isKey": false,
            "numCitedBy": 1733,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper extends to two dimensions the frame criterion developed by Daubechies for one-dimensional wavelets, and it computes the frame bounds for the particular case of 2D Gabor wavelets. Completeness criteria for 2D Gabor image representations are important because of their increasing role in many computer vision applications and also in modeling biological vision, since recent neurophysiological evidence from the visual cortex of mammalian brains suggests that the filter response profiles of the main class of linearly-responding cortical neurons (called simple cells) are best modeled as a family of self-similar 2D Gabor wavelets. We therefore derive the conditions under which a set of continuous 2D Gabor wavelets will provide a complete representation of any image, and we also find self-similar wavelet parametrization which allow stable reconstruction by summation as though the wavelets formed an orthonormal basis. Approximating a \"tight frame\" generates redundancy which allows low-resolution neural responses to represent high-resolution images."
            },
            "slug": "Image-Representation-Using-2D-Gabor-Wavelets-Lee",
            "title": {
                "fragments": [],
                "text": "Image Representation Using 2D Gabor Wavelets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The conditions under which a set of continuous 2D Gabor wavelets will provide a complete representation of any image are derived, and self-similar wavelet parametrization is found which allow stable reconstruction by summation as though the wavelets formed an orthonormal basis."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1650980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff1152582155acaa0e9d0ccbc900a4641504256d",
            "isKey": false,
            "numCitedBy": 1344,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of recent attempts have been made to describe early sensory coding in terms of a general information processing strategy. In this paper, two strategies are contrasted. Both strategies take advantage of the redundancy in the environment to produce more effective representations. The first is described as a compact coding scheme. A compact code performs a transform that allows the input to be represented with a reduced number of vectors (cells) with minimal RMS error. This approach has recently become popular in the neural network literature and is related to a process called Principal Components Analysis (PCA). A number of recent papers have suggested that the optimal compact code for representing natural scenes will have units with receptive field profiles much like those found in the retina and primary visual cortex. However, in this paper, it is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway. In contrast, it is proposed that the visual system is near to optimal in representing natural scenes only if optimality is defined in terms of sparse distributed coding. In a sparse distributed code, all cells in the code have an equal response probability across the class of images but have a low response probability for any single image. In such a code, the dimensionality is not reduced. Rather, the redundancy of the input is transformed into the redundancy of the firing pattern of cells. It is proposed that the signature for a sparse code is found in the fourth moment of the response distribution (i.e., the kurtosis). In measurements with 55 calibrated natural scenes, the kurtosis was found to peak when the bandwidths of the visual code matched those of cells in the mammalian visual cortex. Codes resembling wavelet transforms are proposed to be effective because the response histograms of such codes are sparse (i.e., show high kurtosis) when presented with natural scenes. It is proposed that the structure of the image that allows sparse coding is found in the phase spectrum of the image. It is suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet). Possible reasons for why sensory systems would evolve toward sparse coding are presented."
            },
            "slug": "What-Is-the-Goal-of-Sensory-Coding-Field",
            "title": {
                "fragments": [],
                "text": "What Is the Goal of Sensory Coding?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway and suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32052472"
                        ],
                        "name": "R. Buccigrossi",
                        "slug": "R.-Buccigrossi",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Buccigrossi",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Buccigrossi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 151
                            }
                        ],
                        "text": "The coefficient values in the approach presented here are computed iteratively by maximizing the posterior distribution over the coefficients, whereas Simoncelli and Adelson compute the coefficients by simply projecting the image onto the basis functions and then computing the mean of the posterior distribution given these coefficients (no iteration required)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 114
                            }
                        ],
                        "text": "Note that all of the histograms for the non-adapted bases are non-Gaussian, consistent with previous observations [2, 30]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 109
                            }
                        ],
                        "text": "This method of denoising has many elements in common with the method of Bayesian wavelet \u2018\u2018coring\u2019\u2019 developed by Simoncelli and Adelson.34 Both utilize a transformation through a set of basis functions to reveal sparse, non-Gaussian histograms on the coefficients, and both utilize Bayesian inference based on these histograms to estimate the underlying signal."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 86
                            }
                        ],
                        "text": "The Daubechies wavelet basis used in the image code comparison was obtained from Eero Simoncelli\u2019s image pyramid toolkit in MATLAB (http:// www.cis.upenn.edu/;eero/steerpyr.html)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Buccigrossi and Simoncelli33 have observed that coefficients of wavelet representations of images are more sparse than predicted by the Laplace distribution and can be well modeled with a generalized Laplace distribution (log P(s) } 2 uu sup)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Buccigrossi and Simoncelli [30] have observed that coe cients of wavelet representations of images are more sparse than predicted by the Laplace distribution and can be well modeled with a generalized Laplace distribution (logP (s) / j sjp)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1887438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "156ed6cd90506cd1111dffcd8e80e33ff62ca5ae",
            "isKey": true,
            "numCitedBy": 620,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a probability model for natural images, based on empirical observation of their statistics in the wavelet transform domain. Pairs of wavelet coefficients, corresponding to basis functions at adjacent spatial locations, orientations, and scales, are found to be non-Gaussian in both their marginal and joint statistical properties. Specifically, their marginals are heavy-tailed, and although they are typically decorrelated, their magnitudes are highly correlated. We propose a Markov model that explains these dependencies using a linear predictor for magnitude coupled with both multiplicative and additive uncertainties, and show that it accounts for the statistics of a wide variety of images including photographic images, graphical images, and medical images. In order to directly demonstrate the power of the model, we construct an image coder called EPWIC (embedded predictive wavelet image coder), in which subband coefficients are encoded one bitplane at a time using a nonadaptive arithmetic encoder that utilizes conditional probabilities calculated from the model. Bitplanes are ordered using a greedy algorithm that considers the MSE reduction per encoded bit. The decoder uses the statistical model to predict coefficient values based on the bits it has received. Despite the simplicity of the model, the rate-distortion performance of the coder is roughly comparable to the best image coders in the literature."
            },
            "slug": "Image-compression-via-joint-statistical-in-the-Buccigrossi-Simoncelli",
            "title": {
                "fragments": [],
                "text": "Image compression via joint statistical characterization in the wavelet domain"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A Markov model is proposed that explains dependencies using a linear predictor for magnitude coupled with both multiplicative and additive uncertainties, and it is shown that it accounts for the statistics of a wide variety of images including photographic images, graphical images, and medical images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742997"
                        ],
                        "name": "C. Fyfe",
                        "slug": "C.-Fyfe",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Fyfe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fyfe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34931109"
                        ],
                        "name": "R. Baddeley",
                        "slug": "R.-Baddeley",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Baddeley",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baddeley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2 K l 2 ux 2 As \u02c6u2 1 log P~s \u02c6! 2 1 2 log det HL , ( 18 )"
                    },
                    "intents": []
                }
            ],
            "corpusId": 8604471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5ae37805bc9e0291c4c2c64cb2435f91849de74",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Some recent work has investigated the dichotomy between compact coding using dimensionality reduction and sparse-distributed coding in the context of understanding biological information processing. We introduce an artificial neural network which self-organizes on the basis of simple Hebbian learning and negative feedback of activation and show that it is capable both of forming compact codings of data distributions and of identifying filters most sensitive to sparse-distributed codes. The network is extremely simple and its biological relevance is investigated via its response to a set of images which are typical of everyday life. However, an analysis of the network's identification of the filter for sparse coding reveals that this coding may not be globally optimal and that there exists an innate limiting factor which cannot be transcended."
            },
            "slug": "Finding-compact-and-sparse-distributed-of-visual-Fyfe-Baddeley",
            "title": {
                "fragments": [],
                "text": "Finding compact and sparse-distributed representations of visual images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An artificial neural network which self-organizes on the basis of simple Hebbian learning and negative feedback of activation is introduced and it is shown that it is capable both of forming compact codings of data distributions and of identifying filters most sensitive to sparse-distributed codes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5624877"
                        ],
                        "name": "S. Mar\u010delja",
                        "slug": "S.-Mar\u010delja",
                        "structuredName": {
                            "firstName": "Stjepan",
                            "lastName": "Mar\u010delja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mar\u010delja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 200
                            }
                        ],
                        "text": "The learned basis functions also resemble the spatial receptive eld pro les of simple cells found in the primary visual cortex of mammals, which numerous investigators have likened to Gabor functions [17, 18, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 250
                            }
                        ],
                        "text": "One of the reasons the Gabor basis has been advocated as a model of V1 image coding, as well as for e cient image coding in general, is that it possesses the attractive property of maximum localization in both the space and spatial-frequency domains [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 316,
                                "start": 305
                            }
                        ],
                        "text": "Previous attempts to account for the structure of V1 receptive elds in terms of quantitative principles have been based either on principal components analysis [15, 16], or on the fact that Gabor functions provide an optimal tradeo in achieving localization in both the space and spatialfrequency domains [17, 18, 3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 23364863,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "85af5a0b7102919e96eebe9143c082fe9e453e46",
            "isKey": false,
            "numCitedBy": 1317,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "On the basis of measured receptive field profiles and spatial frequency tuning characteristics of simple cortical cells, it can be concluded that the representation of an image in the visual cortex must involve both spatial and spatial frequency variables. In a scheme due to Gabor, an image is represented in terms of localized symmetrical and antisymmetrical elementary signals. Both measured receptive fields and measured spatial frequency tuning curves conform closely to the functional form of Gabor elementary signals. It is argued that the visual cortex representation corresponds closely to the Gabor scheme owing to its advantages in treating the subsequent problem of pattern recognition."
            },
            "slug": "Mathematical-description-of-the-responses-of-simple-Mar\u010delja",
            "title": {
                "fragments": [],
                "text": "Mathematical description of the responses of simple cortical cells."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that the visual cortex representation corresponds closely to the Gabor scheme owing to its advantages in treating the subsequent problem of pattern recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615848"
                        ],
                        "name": "Rajesh P. N. Rao",
                        "slug": "Rajesh-P.-N.-Rao",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Rao",
                            "middleNames": [
                                "P.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajesh P. N. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "DA }l ^esT 2 AH21&. ( 19 ) Note that the first term is precisely Olshausen and Field\u2019s16 learning rule, while the second term results from"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7136784,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e3a83c2ed3af29a23ab342212d1ae9650a0c64a1",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": "The responses of visual cortical neurons during fixation tasks can be significantly modulated by stimuli from beyond the classical receptive field. Modulatory effects in neural responses have also been recently reported in a task where a monkey freely views a natural scene. In this article, we describe a hierarchical network model of visual recognition that explains these experimental observations by using a form of the extended Kalman filter as given by the minimum description length (MDL) principle. The model dynamically combines input-driven bottom-up signals with expectation-driven top-down signals to predict current recognition state. Synaptic weights in the model are adapted in a Hebbian manner according to a learning rule also derived from the MDL principle. The resulting prediction-learning scheme can be viewed as implementing a form of the expectation-maximization (EM) algorithm. The architecture of the model posits an active computational role for the reciprocal connections between adjoining visual cortical areas in determining neural response properties. In particular, the model demonstrates the possible role of feedback from higher cortical areas in mediating neurophysiological effects due to stimuli from beyond the classical receptive field. Simulations of the model are provided that help explain the experimental observations regarding neural responses in both free viewing and fixating conditions."
            },
            "slug": "Dynamic-Model-of-Visual-Recognition-Predicts-Neural-Rao-Ballard",
            "title": {
                "fragments": [],
                "text": "Dynamic Model of Visual Recognition Predicts Neural Response Properties in the Visual Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A hierarchical network model of visual recognition that explains experimental observations regarding neural responses in both free viewing and fixating conditions by using a form of the extended Kalman filter as given by the minimum description length (MDL) principle is described."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Shannon\u2019s source coding theorem states that the lower bound on code-word length is determined by the entropy of the data: L > H~ p! 52 ( p ~ x !log p~x!. ( 1 )"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our goals are twofold: ( 1 ) to find a good matrix A for coding natural images and (2) to infer for each image the proper state of the coefficients s. The first problem is one of adaptation and is analogous to the process of learning (through either development or evolution) in the visual system, while the second problem is one of image representation and is most analogous to perception."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9271650,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "02f89cd1fd6f013a1a301a292936ff8fb06aff25",
            "isKey": false,
            "numCitedBy": 3420,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Two-dimensional spatial linear filters are constrained by general uncertainty relations that limit their attainable information resolution for orientation, spatial frequency, and two-dimensional (2D) spatial position. The theoretical lower limit for the joint entropy, or uncertainty, of these variables is achieved by an optimal 2D filter family whose spatial weighting functions are generated by exponentiated bivariate second-order polynomials with complex coefficients, the elliptic generalization of the one-dimensional elementary functions proposed in Gabor's famous theory of communication [J. Inst. Electr. Eng. 93, 429 (1946)]. The set includes filters with various orientation bandwidths, spatial-frequency bandwidths, and spatial dimensions, favoring the extraction of various kinds of information from an image. Each such filter occupies an irreducible quantal volume (corresponding to an independent datum) in a four-dimensional information hyperspace whose axes are interpretable as 2D visual space, orientation, and spatial frequency, and thus such a filter set could subserve an optimally efficient sampling of these variables. Evidence is presented that the 2D receptive-field profiles of simple cells in mammalian visual cortex are well described by members of this optimal 2D filter family, and thus such visual neurons could be said to optimize the general uncertainty relations for joint 2D-spatial-2D-spectral information resolution. The variety of their receptive-field dimensions and orientation and spatial-frequency bandwidths, and the correlations among these, reveal several underlying constraints, particularly in width/length aspect ratio and principal axis organization, suggesting a polar division of labor in occupying the quantal volumes of information hyperspace.(ABSTRACT TRUNCATED AT 250 WORDS)"
            },
            "slug": "Uncertainty-relation-for-resolution-in-space,-and-Daugman",
            "title": {
                "fragments": [],
                "text": "Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Evidence is presented that the 2D receptive-field profiles of simple cells in mammalian visual cortex are well described by members of this optimal 2D filter family, and thus such visual neurons could be said to optimize the general uncertainty relations for joint 2D-spatial-2D-spectral information resolution."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474176"
                        ],
                        "name": "J. H. Hateren",
                        "slug": "J.-H.-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hateren",
                            "middleNames": [
                                "H.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46498936"
                        ],
                        "name": "A. Schaaf",
                        "slug": "A.-Schaaf",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "Schaaf",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schaaf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "u~x, y ! 5 ~x 2 x0!cos~ u! 1 ~ y 2 y0!sin~ u!, (22)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15666050,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "31b0e2c5bec857e497ab545ff808ce9ccba9f3d1",
            "isKey": false,
            "numCitedBy": 802,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed."
            },
            "slug": "Independent-component-filters-of-natural-images-in-Hateren-Schaaf",
            "title": {
                "fragments": [],
                "text": "Independent component filters of natural images compared with simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis on a large set of natural images: there is no match, however, in calculated and measured distributions for the peak of the spatial frequency response."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3741160"
                        ],
                        "name": "J. V. van Hateren",
                        "slug": "J.-V.-van-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "van Hateren",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. van Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51922762"
                        ],
                        "name": "A. van der Schaaf",
                        "slug": "A.-van-der-Schaaf",
                        "structuredName": {
                            "firstName": "Arjen",
                            "lastName": "van der Schaaf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. van der Schaaf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1789554,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "117175a54263cc2ae693fe82c9b3fe0553931cd8",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed."
            },
            "slug": "Independent-component-filters-of-natural-images-in-Hateren-Schaaf",
            "title": {
                "fragments": [],
                "text": "Independent component filters of natural images compared with simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis on a large set of natural images: there is no match, however, in calculated and measured distributions for the peak of the spatial frequency response."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Royal Society of London. Series B: Biological Sciences"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780112"
                        ],
                        "name": "R. Coifman",
                        "slug": "R.-Coifman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Coifman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Coifman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709398"
                        ],
                        "name": "M. Wickerhauser",
                        "slug": "M.-Wickerhauser",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Wickerhauser",
                            "middleNames": [
                                "Victor"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wickerhauser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Figure 7 shows an example where the complete (13) learned bases were used to infer the image structure when 70% of the pixel values had been removed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 546882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5478a91c183c3a460bd4098acb8927bfc671367c",
            "isKey": false,
            "numCitedBy": 3339,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Adapted waveform analysis uses a library of orthonormal bases and an efficiency functional to match a basis to a given signal or family of signals. It permits efficient compression of a variety of signals, such as sound and images. The predefined libraries of modulated waveforms include orthogonal wavelet-packets and localized trigonometric functions, and have reasonably well-controlled time-frequency localization properties. The idea is to build out of the library functions an orthonormal basis relative to which the given signal or collection of signals has the lowest information cost. The method relies heavily on the remarkable orthogonality properties of the new libraries: all expansions in a given library conserve energy and are thus comparable. Several cost functionals are useful; one of the most attractive is Shannon entropy, which has a geometric interpretation in this context. >"
            },
            "slug": "Entropy-based-algorithms-for-best-basis-selection-Coifman-Wickerhauser",
            "title": {
                "fragments": [],
                "text": "Entropy-based algorithms for best basis selection"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Adapted waveform analysis uses a library of orthonormal bases and an efficiency functional to match a basis to a given signal or family of signals, and relies heavily on the remarkable orthogonality properties of the new libraries."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39092098"
                        ],
                        "name": "Y. Wu",
                        "slug": "Y.-Wu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Wu",
                            "middleNames": [
                                "Nian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "5 K 1 P~xuA! E lesTP~xus, A!P~s!dsL , (15)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15926,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd9ed76e8b9fa8b69257d3fc61fbc38bee973016",
            "isKey": false,
            "numCitedBy": 492,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "This article proposes a general theory and methodology, called the minimax entropy principle, for building statistical models for images (or signals) in a variety of applications. This principle consists of two parts. The first is the maximum entropy principle for feature binding (or fusion): for a given set of observed feature statistics, a distribution can be built to bind these feature statistics together by maximizing the entropy over all distributions that reproduce them. The second part is the minimum entropy principle for feature selection: among all plausible sets of feature statistics, we choose the set whose maximum entropy distribution has the minimum entropy. Computational and inferential issues in both parts are addressed; in particular, a feature pursuit procedure is proposed for approximately selecting the optimal set of features. The minimax entropy principle is then corrected by considering the sample variation in the observed feature statistics, and an information criterion for feature pursuit is derived. The minimax entropy principle is applied to texture modeling, where a novel Markov random field (MRF) model, called FRAME (filter, random field, and minimax entropy), is derived, and encouraging results are obtained in experiments on a variety of texture images. The relationship between our theory and the mechanisms of neural computation is also discussed."
            },
            "slug": "Minimax-Entropy-Principle-and-Its-Application-to-Zhu-Wu",
            "title": {
                "fragments": [],
                "text": "Minimax Entropy Principle and Its Application to Texture Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The minimax entropy principle is applied to texture modeling, where a novel Markov random field model, called FRAME, is derived, and encouraging results are obtained in experiments on a variety of texture images."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039472"
                        ],
                        "name": "N. Parga",
                        "slug": "N.-Parga",
                        "structuredName": {
                            "firstName": "N\u00e9stor",
                            "lastName": "Parga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Parga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 171
                            }
                        ],
                        "text": "Under certain forms of the model, this is equivalent to the methods of redundancy reduction and maximizing the mutual information between the input and the representation [37, 38, 39], and has been advocated by several researchers [24, 40, 25, 1, 41, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2344498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4547e5e6ac2e1fcb0ba51edbbe5d65b5765b4ca",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "In the context of both sensory coding and signal processing, building factorized codes has been shown to be an efficient strategy. In a wide variety of situations, the signal to be processed is a linear mixture of statistically independent sources. Building a factorized code is then equivalent to performing blind source separation. Thanks to the linear structure of the data, this can be done, in the language of signal processing, by finding an appropriate linear filter, or equivalently, in the language of neural modeling, by using a simple feedforward neural network. In this article, we discuss several aspects of the source separation problem. We give simple conditions on the network output that, if satisfied, guarantee that source separation has been obtained. Then we study adaptive approaches, in particular those based on redundancy reduction and maximization of mutual information. We show how the resulting updating rules are related to the BCM theory of synaptic plasticity. Eventually we briefly discuss extensions to the case of nonlinear mixtures. Through out this article, we take care to put into perspective our work with other studies on source separation and redundancy reduction. In particular we review algebraic solutions, pointing out their simplicity but also their drawbacks."
            },
            "slug": "Redundancy-Reduction-and-Independent-Component-on-Nadal-Parga",
            "title": {
                "fragments": [],
                "text": "Redundancy Reduction and Independent Component Analysis: Conditions on Cumulants and Adaptive Approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This article gives simple conditions on the network output that guarantee that source separation has been obtained and shows how the resulting updating rules are related to the BCM theory of synaptic plasticity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39409429"
                        ],
                        "name": "R. L. Valois",
                        "slug": "R.-L.-Valois",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Valois",
                            "middleNames": [
                                "L.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. L. Valois"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2129244"
                        ],
                        "name": "D. G. Albrecht",
                        "slug": "D.-G.-Albrecht",
                        "structuredName": {
                            "firstName": "Duane",
                            "lastName": "Albrecht",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. G. Albrecht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40471674"
                        ],
                        "name": "Lisa G. Thorell",
                        "slug": "Lisa-G.-Thorell",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Thorell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lisa G. Thorell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2 D , ( 29 ) then the procedure for finding the most probable values of the coefficients,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16496844,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e2f74bec30cc4e471919de4dfd27c45dbc7b4b9d",
            "isKey": false,
            "numCitedBy": 1118,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spatial-frequency-selectivity-of-cells-in-macaque-Valois-Albrecht",
            "title": {
                "fragments": [],
                "text": "Spatial frequency selectivity of cells in macaque visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829021"
                        ],
                        "name": "D. Ruderman",
                        "slug": "D.-Ruderman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ruderman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruderman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "number of bits > 2log2 P~xuA! 2 L log2~ sx!, (24)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2793971,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "13206b6ba3711a14a56cf1599ecb08c16f49061e",
            "isKey": false,
            "numCitedBy": 912,
            "numCiting": 144,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently there has been a resurgence of interest in the properties of natural images. Their statistics are important not only in image compression but also for the study of sensory processing in biology, which can be viewed as satisfying certain \u2018design criteria\u2019. This review summarizes previous work on image statistics and presents our own data. Perhaps the most notable property of natural images is an invariance to scale. We present data to support this claim as well as evidence for a hierarchical invariance in natural scenes. These symmetries provide a powerful description of natural images as they greatly restrict the class of allowed distributions."
            },
            "slug": "The-statistics-of-natural-images-Ruderman",
            "title": {
                "fragments": [],
                "text": "The statistics of natural images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "Our goals are twofold: (1) to find a good matrix A for coding natural images and (2) to infer for each image the proper state of the coefficients s."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1984348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6513888c5ef473bdbb3167c7b52f0985be071f7a",
            "isKey": false,
            "numCitedBy": 1899,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "A three-layered neural network is described for transforming two-dimensional discrete signals into generalized nonorthogonal 2-D Gabor representations for image analysis, segmentation, and compression. These transforms are conjoint spatial/spectral representations, which provide a complete image description in terms of locally windowed 2-D spectral coordinates embedded within global 2-D spatial coordinates. In the present neural network approach, based on interlaminar interactions involving two layers with fixed weights and one layer with adjustable weights, the network finds coefficients for complete conjoint 2-D Gabor transforms without restrictive conditions. In wavelet expansions based on a biologically inspired log-polar ensemble of dilations, rotations, and translations of a single underlying 2-D Gabor wavelet template, image compression is illustrated with ratios up to 20:1. Also demonstrated is image segmentation based on the clustering of coefficients in the complete 2-D Gabor transform. >"
            },
            "slug": "Complete-discrete-2-D-Gabor-transforms-by-neural-Daugman",
            "title": {
                "fragments": [],
                "text": "Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A three-layered neural network based on interlaminar interactions involving two layers with fixed weights and one layer with adjustable weights finds coefficients for complete conjoint 2-D Gabor transforms without restrictive conditions for image analysis, segmentation, and compression."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727797"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Chen",
                            "middleNames": [
                                "Saobing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "(12)], which minimizes the Kullback\u2013Leibler divergence between the model density and the distribution of the data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Equation (12) makes clear that the form of the model distribution depends not only on the choice of basis functions A but also on the choice of prior P(s)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2429822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "isKey": false,
            "numCitedBy": 9740,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver."
            },
            "slug": "Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Atomic Decomposition by Basis Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Basis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187506"
                        ],
                        "name": "J. Nadal",
                        "slug": "J.-Nadal",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "Nadal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039472"
                        ],
                        "name": "N. Parga",
                        "slug": "N.-Parga",
                        "structuredName": {
                            "firstName": "N\u00e9stor",
                            "lastName": "Parga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Parga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115302789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "698aedd44c51da829228e2c7d243960345efeb94",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the consequences of maximizing information transfer in a simple neural network (one input layer, one output layer), focusing on the case of nonlinear transfer functions. We assume that both receptive fields (synaptic efficacies) and transfer functions can be adapted to the environment. The main result is that, for bounded and invertible transfer functions, in the case of a vanishing additive output noise, and no input noise, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow. We also show that this result is valid for linear and, more generally, unbounded, transfer functions, provided optimization is performed under an additive constraint, i.e. which can be written as a sum of terms, each one being specific to one output neuron. Finally, we study the effect of a non-zero input noise. We find that, to first order in the input noise, assumed to be small in comparison with th..."
            },
            "slug": "Nonlinear-neurons-in-the-low-noise-limit:-a-code-5-Nadal-Parga",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low-noise limit: a factorial code maximizes information transfer Network 5"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The main result is that, for bounded and invertible transfer functions, maximization of information (Linsker's infomax principle) leads to a factorial code-hence to the same solution as required by the redundancy-reduction principle of Barlow."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109019649"
                        ],
                        "name": "Zhifeng Zhang",
                        "slug": "Zhifeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhifeng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "5 K 1 P~xuA! E ] ]A P~xus, A!P~s!dsL , (14)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14427335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2210a7157565422261b03cf2cdf4e91b583df5a0",
            "isKey": false,
            "numCitedBy": 8852,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). >"
            },
            "slug": "Matching-pursuits-with-time-frequency-dictionaries-Mallat-Zhang",
            "title": {
                "fragments": [],
                "text": "Matching pursuits with time-frequency dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions, chosen in order to best match the signal structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, it should be remembered that the basis functions of the model are not generally equivalent to receptive fields because of the nonlinear mapping from the image space x to the coefficient space s [see Eq. ( 9 )]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We have shown in this paper how the framework of probabilistic inference can be employed both for learning efficient image codes [Eq. (20)] and for inferring the most probable representation for a given image [Eq. ( 9 )]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This relation assumes that changes in s \u02c6 k are smooth with respect to changes in A ij , which may not be true at a small number of critical points because the mapping from x to s \u02c6 is nonlinear [Eq. ( 9 )]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": true,
            "numCitedBy": 8758,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3741160"
                        ],
                        "name": "J. V. van Hateren",
                        "slug": "J.-V.-van-Hateren",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "van Hateren",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. van Hateren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829021"
                        ],
                        "name": "D. Ruderman",
                        "slug": "D.-Ruderman",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Ruderman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ruderman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "will result in an image, x \u02c6 5 As \u02c6, ( 31 ) that interpolates the values for these pixels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "u~x, y! 5 ~x 2 x 0!cos~ u! 1 ~ y 2 y 0!sin~ u!, ( 22 )"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14810959,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "b5b27b9b7eb35fa913715a404c3a3d06f4899530",
            "isKey": false,
            "numCitedBy": 345,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Simple cells in the primary visual cortex process incoming visual information with receptive fields localized in space and time, bandpass in spatial and temporal frequency, tuned in orientation, and commonly selective for the direction of movement. It is shown that performing independent component analysis (ICA) on video sequences of natural scenes produces results with qualitatively similar spatio-temporal properties. Whereas the independent components of video resemble moving edges or bars, the independent component filters, i.e. the analogues of receptive fields, resemble moving sinusoids windowed by steady Gaussian envelopes. Contrary to earlier ICA results on static images, which gave only filters at the finest possible spatial scale, the spatio\u2013temporal analysis yields filters at a range of spatial and temporal scales. Filters centred at low spatial frequencies are generally tuned to faster movement than those at high spatial frequencies."
            },
            "slug": "Independent-component-analysis-of-natural-image-to-Hateren-Ruderman",
            "title": {
                "fragments": [],
                "text": "Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that performing independent component analysis (ICA) on video sequences of natural scenes produces results with qualitatively similar spatio-temporal properties, contrary to earlier ICA results on static images, which gave only filters at the finest possible spatial scale."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Royal Society of London. Series B: Biological Sciences"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 231
                            }
                        ],
                        "text": "Under certain forms of the model, this is equivalent to the methods of redundancy reduction and maximizing the mutual information between the input and the representation [37, 38, 39], and has been advocated by several researchers [24, 40, 25, 1, 41, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1527671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16d70e8af45ca0ae2c1bb73f3be6628518d40b8f",
            "isKey": false,
            "numCitedBy": 1417,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The emergence of a feature-analyzing function from the development rules of simple, multilayered networks is explored. It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory. The network studied is based on the visual system. These results are used to infer an information-theoretic principle that can be applied to the network as a whole, rather than a single cell. The organizing principle proposed is that the network connections develop in such a way as to maximize the amount of information that is preserved when signals are transformed at each processing stage, subject to certain constraints. The operation of this principle is illustrated for some simple cases.<<ETX>>"
            },
            "slug": "Self-organization-in-a-perceptual-network-Linsker",
            "title": {
                "fragments": [],
                "text": "Self-organization in a perceptual network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876925"
                        ],
                        "name": "B. Rao",
                        "slug": "B.-Rao",
                        "structuredName": {
                            "firstName": "Bhaskar",
                            "lastName": "Rao",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395421758"
                        ],
                        "name": "K. Kreutz-Delgado",
                        "slug": "K.-Kreutz-Delgado",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Kreutz-Delgado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kreutz-Delgado"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8035693,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88855bc679d6b94e42b297f37b3decf451cd908e",
            "isKey": false,
            "numCitedBy": 538,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "A methodology is developed to derive algorithms for optimal basis selection by minimizing diversity measures proposed by Wickerhauser (1994) and Donoho (1994). These measures include the p-norm-like (l/sub (p/spl les/1)/) diversity measures and the Gaussian and Shannon entropies. The algorithm development methodology uses a factored representation for the gradient and involves successive relaxation of the Lagrangian necessary condition. This yields algorithms that are intimately related to the affine scaling transformation (AST) based methods commonly employed by the interior point approach to nonlinear optimization. The algorithms minimizing the (l/sub (p/spl les/1)/) diversity measures are equivalent to a previously developed class of algorithms called focal underdetermined system solver (FOCUSS). The general nature of the methodology provides a systematic approach for deriving this class of algorithms and a natural mechanism for extending them. It also facilitates a better understanding of the convergence behavior and a strengthening of the convergence results. The Gaussian entropy minimization algorithm is shown to be equivalent to a well-behaved p=0 norm-like optimization algorithm. Computer experiments demonstrate that the p-norm-like and the Gaussian entropy algorithms perform well, converging to sparse solutions. The Shannon entropy algorithm produces solutions that are concentrated but are shown to not converge to a fully sparse solution."
            },
            "slug": "An-affine-scaling-methodology-for-best-basis-Rao-Kreutz-Delgado",
            "title": {
                "fragments": [],
                "text": "An affine scaling methodology for best basis selection"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A methodology is developed to derive algorithms for optimal basis selection by minimizing diversity measures proposed by Wickerhauser (1994) and Donoho (1994), which include the p-norm-like (l/sub (p/spl les/1)/) diversity measures and the Gaussian and Shannon entropies."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735689"
                        ],
                        "name": "A. Parker",
                        "slug": "A.-Parker",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Parker",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Parker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2480424"
                        ],
                        "name": "M. Hawken",
                        "slug": "M.-Hawken",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Hawken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hawken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 139
                            }
                        ],
                        "text": "The vast majority of recorded cells appear to reside in the mid to low spatial-frequency range when scaled to the retinal sampling lattice [27, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20372548,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "11a437b768ff54b6af01b5568087b38d97942f00",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Measurements of the spatial contrast sensitivity function and orientation selectivity of visual neurons in the foveal striate cortex (V1) of primates were interpreted within the context of a model of the two-dimensional spatial structure of their receptive fields. Estimates of the spatial dimensions of the receptive fields along the axis of preferred orientation were derived from the application of the model and were compared with estimates of the smallest spatial subunit in the dimension orthogonal to the preferred orientation. Some measure of agreement was found with corresponding estimates of parameters for psychophysical channels in human foveal vision."
            },
            "slug": "Two-dimensional-spatial-structure-of-receptive-in-Parker-Hawken",
            "title": {
                "fragments": [],
                "text": "Two-dimensional spatial structure of receptive fields in monkey striate cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Measurements of the spatial contrast sensitivity function and orientation selectivity of visual neurons in the foveal striate cortex (V1) of primates were interpreted within the context of a model of the two-dimensional spatial structure of their receptive fields."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 151
                            }
                        ],
                        "text": "The coefficient values in the approach presented here are computed iteratively by maximizing the posterior distribution over the coefficients, whereas Simoncelli and Adelson compute the coefficients by simply projecting the image onto the basis functions and then computing the mean of the posterior distribution given these coefficients (no iteration required)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "This method of denoising shares many elements in common with the method of Bayesian wavelet `coring' developed by Simoncelli and Adelson [31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 109
                            }
                        ],
                        "text": "This method of denoising has many elements in common with the method of Bayesian wavelet \u2018\u2018coring\u2019\u2019 developed by Simoncelli and Adelson.34 Both utilize a transformation through a set of basis functions to reveal sparse, non-Gaussian histograms on the coefficients, and both utilize Bayesian inference based on these histograms to estimate the underlying signal."
                    },
                    "intents": []
                }
            ],
            "corpusId": 235072,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "85a1725bfd3b4a2d3fe9a7272d66ebf03c016fed",
            "isKey": false,
            "numCitedBy": 766,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The classical solution to the noise removal problem is the Wiener filter, which utilizes the second-order statistics of the Fourier decomposition. Subband decompositions of natural images have significantly non-Gaussian higher-order point statistics; these statistics capture image properties that elude Fourier-based techniques. We develop a Bayesian estimator that is a natural extension of the Wiener solution, and that exploits these higher-order statistics. The resulting nonlinear estimator performs a \"coring\" operation. We provide a simple model for the subband statistics, and use it to develop a semi-blind noise removal algorithm based on a steerable wavelet pyramid."
            },
            "slug": "Noise-removal-via-Bayesian-wavelet-coring-Simoncelli-Adelson",
            "title": {
                "fragments": [],
                "text": "Noise removal via Bayesian wavelet coring"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A Bayesian estimator is developed that is a natural extension of the Wiener solution, and that exploits higher-order statistics of the Fourier decomposition to develop a semi-blind noise removal algorithm based on a steerable wavelet pyramid."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd IEEE International Conference on Image Processing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783831"
                        ],
                        "name": "P. Comon",
                        "slug": "P.-Comon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Comon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "s \u02c6 5 max s P~sux, A!, (7) 5 max s @log P~xuA, s! 1 log P~s!#, ( 8 )"
                    },
                    "intents": []
                }
            ],
            "corpusId": 18340548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96a1effa4be3f8caa88270d6d258de418993d2e7",
            "isKey": false,
            "numCitedBy": 8327,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Independent-component-analysis,-A-new-concept-Comon",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, A new concept?"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360881"
                        ],
                        "name": "D. Heeger",
                        "slug": "D.-Heeger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heeger",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 103
                            }
                        ],
                        "text": "Overcomplete bases have been advocated because they allow certain advantages in terms of interpolation [8], in achieving a tight frame with nonorthogonal basis functions [3], or in achieving sparsity in the representation [9, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43701174,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8515604037444b3f079a9d328b0c560f33da0a19",
            "isKey": false,
            "numCitedBy": 1428,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. >"
            },
            "slug": "Shiftable-multiscale-transforms-Simoncelli-Freeman",
            "title": {
                "fragments": [],
                "text": "Shiftable multiscale transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two examples of jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored and the usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786990"
                        ],
                        "name": "H. Attias",
                        "slug": "H.-Attias",
                        "structuredName": {
                            "firstName": "Hagai",
                            "lastName": "Attias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Attias"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 746481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2307fd6058ab4f7554a0b1f188507150ddb5b9a2",
            "isKey": false,
            "numCitedBy": 596,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the independent factor analysis (IFA) method for recovering independent hidden sources from their observed mixtures. IFA generalizes and unifies ordinary factor analysis (FA), principal component analysis (PCA), and independent component analysis (ICA), and can handle not only square noiseless mixing but also the general case where the number of mixtures differs from the number of sources and the data are noisy. IFA is a two-step procedure. In the first step, the source densities, mixing matrix, and noise covariance are estimated from the observed data by maximum likelihood. For this purpose we present an expectation-maximization (EM) algorithm, which performs unsupervised learning of an associated probabilistic model of the mixing situation. Each source in our model is described by a mixture of gaussians; thus, all the probabilistic calculations can be performed analytically. In the second step, the sources are reconstructed from the observed data by an optimal nonlinear estimator. A variational approximation of this algorithm is derived for cases with a large number of sources, where the exact algorithm becomes intractable. Our IFA algorithm reduces to the one for ordinary FA when the sources become gaussian, and to an EM algorithm for PCA in the zero-noise limit. We derive an additional EM algorithm specifically for noiseless IFA. This algorithm is shown to be superior to ICA since it can learn arbitrary source densities from the data. Beyond blind separation, IFA can be used for modeling multidimensional data by a highly constrained mixture of gaussians and as a tool for nonlinear signal encoding."
            },
            "slug": "Independent-Factor-Analysis-Attias",
            "title": {
                "fragments": [],
                "text": "Independent Factor Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An expectation-maximization (EM) algorithm is presented, which performs unsupervised learning of an associated probabilistic model of the mixing situation and is shown to be superior to ICA since it can learn arbitrary source densities from the data."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583773"
                        ],
                        "name": "L. Parra",
                        "slug": "L.-Parra",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Parra",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Parra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 93
                            }
                        ],
                        "text": "One promising approach for more accurate estimates is to model P (s) with a Gaussian mixture [34, 35] and again use a Gaussian approximation at the (maximum) posterior mode."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9704838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cea9d59691410447bde0f39a028ffb3e21181a3",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In the square linear blind source separation problem, one must find a linear unmixing operator which can detangle the result xi(t) of mixing n unknown independent sources si(t) through an unknown n \u00d7 n mixing matrix A(t) of causal linear filters: xi = \u03a3j aij * sj. We cast the problem as one of maximum likelihood density estimation, and in that framework introduce an algorithm that searches for independent components using both temporal and spatial cues. We call the resulting algorithm \"Contextual ICA,\" after the (Bell and Sejnowski 1995) Infomax algorithm, which we show to be a special case of cICA. Because cICA can make use of the temporal structure of its input, it is able separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms."
            },
            "slug": "Maximum-Likelihood-Blind-Source-Separation:-A-of-Pearlmutter-Parra",
            "title": {
                "fragments": [],
                "text": "Maximum Likelihood Blind Source Separation: A Context-Sensitive Generalization of ICA"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The resulting algorithm is called cICA, after the (Bell and Sejnowski 1995) Infomax algorithm, which is able to separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160279"
                        ],
                        "name": "P. Hancock",
                        "slug": "P.-Hancock",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hancock",
                            "middleNames": [
                                "J.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hancock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34931109"
                        ],
                        "name": "R. Baddeley",
                        "slug": "R.-Baddeley",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Baddeley",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baddeley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708497"
                        ],
                        "name": "Leslie S. Smith",
                        "slug": "Leslie-S.-Smith",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Smith",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leslie S. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9967699,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "7dcfa42cfe3b59becb441844b72558b361693608",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural net was used to analyse samples of natural images and text. For the natural images, components resemble derivatives of Gaussian operators, similar to those found in visual cortex and inferred from psychophysics. While the results from natural images do not depend on scale, those from text images are highly scale dependent. Convolution of one of the text components with an original image shows that it is sensitive to inter-word gaps."
            },
            "slug": "The-principal-components-of-natural-images-Hancock-Baddeley",
            "title": {
                "fragments": [],
                "text": "The principal components of natural images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": false,
            "numCitedBy": 3642,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155551370"
                        ],
                        "name": "Shaobing Chen",
                        "slug": "Shaobing-Chen",
                        "structuredName": {
                            "firstName": "Shaobing",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaobing Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122514140"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Donoho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "The method described here is virtually identical to the `Basis Pursuit Denoising' method [32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 96447294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30f3567eeb13079a1a02ac1342f610cbd95df3bc",
            "isKey": false,
            "numCitedBy": 655,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed an enormous number of over-complete signal dictionaries, wavelets, wavelet packets, cosine packets, Wilson bases, chirplets, warped bases, and hyperbolic cross bases being a few examples. Basis pursuit is a technique for decomposing a signal into an \"optimal\" superposition of dictionary elements. The optimization criterion is the l/sup 1/ norm of coefficients. The method has several advantages over matching pursuit and best ortho basis, including super-resolution and stability.<<ETX>>"
            },
            "slug": "Basis-pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Basis pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Basis pursuit is a technique for decomposing a signal into an \"optimal\" superposition of dictionary elements, which has several advantages over matching pursuit and best ortho basis, including super-resolution and stability."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696508"
                        ],
                        "name": "C. Jutten",
                        "slug": "C.-Jutten",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Jutten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jutten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798563"
                        ],
                        "name": "J. H\u00e9rault",
                        "slug": "J.-H\u00e9rault",
                        "structuredName": {
                            "firstName": "Jeanny",
                            "lastName": "H\u00e9rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e9rault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 152
                            }
                        ],
                        "text": "A recently developed generalization of PCA, called independent component analysis (ICA), allows for non-Gaussian distributions and non-orthogonal bases [5, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33162734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e73081ed096c62c073b3faa1b3b80aab89998c5",
            "isKey": false,
            "numCitedBy": 2689,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Blind-separation-of-sources,-part-I:-An-adaptive-on-Jutten-H\u00e9rault",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2837738"
                        ],
                        "name": "D. Whitteridge",
                        "slug": "D.-Whitteridge",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Whitteridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Whitteridge"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1117453,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "d4f903d5d6be6b4d9852c906d72271c183ad786c",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-and-Relearning-Whitteridge",
            "title": {
                "fragments": [],
                "text": "Learning and relearning."
            },
            "venue": {
                "fragments": [],
                "text": "Lancet"
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737063"
                        ],
                        "name": "I. Daubechies",
                        "slug": "I.-Daubechies",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "Daubechies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Daubechies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2727398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d08a2ba82c351add7caf7a69d5567b2fddb025d",
            "isKey": false,
            "numCitedBy": 8724,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We construct orthonormal bases of compactly supported wavelets, with arbitrarily high regularity. The order of regularity increases linearly with the support width. We start by reviewing the concept of multiresolution analysis as well as several algorithms in vision decomposition and reconstruction. The construction then follows from a synthesis of these different approaches."
            },
            "slug": "Orthonormal-bases-of-compactly-supported-wavelets-Daubechies",
            "title": {
                "fragments": [],
                "text": "Orthonormal bases of compactly supported wavelets"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work construct orthonormal bases of compactly supported wavelets, with arbitrarily high regularity, by reviewing the concept of multiresolution analysis as well as several algorithms in vision decomposition and reconstruction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695079"
                        ],
                        "name": "R. Everson",
                        "slug": "R.-Everson",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Everson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Everson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49555086"
                        ],
                        "name": "L. Sirovich",
                        "slug": "L.-Sirovich",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Sirovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sirovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Everson and Sirovich [33] describe a similar method for lling-in missing pixels by using the Karhunen-Loeve transform applied to a speci c image class, i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Everson and Sirovich36 describe a similar method for filling in missing pixels by using the Karhunen\u2013Loe\u0300ve transform applied to a specific image class, i.e., faces."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15219006,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "61f0033343913a41cc958f7c061727cb4058841d",
            "isKey": false,
            "numCitedBy": 633,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of using the Karhunen\u2013Loeve transform with partial data is addressed. Given a set of empirical eigenfunctions, we show how to recover the modal coefficients for each gappy snapshot by a least-squares procedure. This method gives an unbiased estimate of the data that lie in the gaps and permits gaps to be filled in a reasonable manner. In addition, a scheme is advanced for finding empirical eigenfunctions from gappy data. It is shown numerically that this procedure obtains spectra and eigenfunctions that are close to those obtained from unmarred data."
            },
            "slug": "Karhunen\u2013Lo\u00e8ve-procedure-for-gappy-data-Everson-Sirovich",
            "title": {
                "fragments": [],
                "text": "Karhunen\u2013Lo\u00e8ve procedure for gappy data"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Given a set of empirical eigenfunctions, it is shown how to recover the modal coefficients for each gappy snapshot by a least-squares procedure that gives an unbiased estimate of the data that lie in the gaps and permits gaps to be filled in a reasonable manner."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 171
                            }
                        ],
                        "text": "Under certain forms of the model, this is equivalent to the methods of redundancy reduction and maximizing the mutual information between the input and the representation [37, 38, 39], and has been advocated by several researchers [24, 40, 25, 1, 41, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14149261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aaf352dc0b8c02e22bf0e11dc7bbcbed90e4f16f",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for the blind separation of sources can be derived from several different principles. This article shows that the infomax (information-maximization) principle is equivalent to the maximum likelihood. The application of the infomax principle to source separation consists of maximizing an output entropy."
            },
            "slug": "Infomax-and-maximum-likelihood-for-blind-source-Cardoso",
            "title": {
                "fragments": [],
                "text": "Infomax and maximum likelihood for blind source separation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article shows that the infomax (information-maximization) principle is equivalent to the maximum likelihood."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2243366"
                        ],
                        "name": "B. Radich",
                        "slug": "B.-Radich",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Radich",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Radich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144801582"
                        ],
                        "name": "K. Buckley",
                        "slug": "K.-Buckley",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Buckley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14936749,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "3e5c7898c35aca46461b478b5b23d1e71b1e606e",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a Bayesian solution to the problem of multiple direction of arrival (DOA) detection/estimation from a single temporal observation of array data, where the array can be of general configuration. The proposed method consists of multiple-step likelihood marginalizations using noninformative priors selected specifically to allow valid comparison of alternative model orders. This yields a novel DOA estimation cost function and a new detection algorithm useful for the single-snapshot case."
            },
            "slug": "Single-snapshot-DOA-estimation-and-source-number-Radich-Buckley",
            "title": {
                "fragments": [],
                "text": "Single-snapshot DOA estimation and source number detection"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A Bayesian solution to the problem of multiple direction of arrival detection/estimation from a single temporal observation of array data, where the array can be of general configuration, yields a novel DOA estimation cost function and a new detection algorithm useful for the single-snapshot case."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 231
                            }
                        ],
                        "text": "Under certain forms of the model, this is equivalent to the methods of redundancy reduction and maximizing the mutual information between the input and the representation [37, 38, 39], and has been advocated by several researchers [24, 40, 25, 1, 41, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022238"
                        ],
                        "name": "L. Ling",
                        "slug": "L.-Ling",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Ling",
                            "middleNames": [
                                "Luan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068747992"
                        ],
                        "name": "T. Berger",
                        "slug": "T.-Berger",
                        "structuredName": {
                            "firstName": "Toby",
                            "lastName": "Berger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976952"
                        ],
                        "name": "Erez Aviczer",
                        "slug": "Erez-Aviczer",
                        "structuredName": {
                            "firstName": "Erez",
                            "lastName": "Aviczer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erez Aviczer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6210233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbdbe5fd7f18debc8dfc7f37a864740f649ee1c1",
            "isKey": false,
            "numCitedBy": 389,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Online dynamic signature verification systems were designed and tested. A database of more than 10,000 signatures in (x(t), y(t))-form was acquired using a graphics tablet. We extracted a 42-parameter feature set at first, and advanced to a set of 49 normalized features that tolerate inconsistencies in genuine signatures while retaining the power to discriminate against forgeries. We studied algorithms for selecting and perhaps orthogonalizing features in accordance with the availability of training data and the level of system complexity. For decision making we studied several classifiers types. A modified version of our majority classifier yielded 2.5% equal error rate and, more importantly, an asymptotic performance of 7% false acceptance rate at zero false rejection rate, was robust to the speed of genuine signatures, and used only 15 parameter features."
            },
            "slug": "Reliable-On-Line-Human-Signature-Verification-Ling-Berger",
            "title": {
                "fragments": [],
                "text": "Reliable On-Line Human Signature Verification Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Online dynamic signature verification systems were designed and tested, and algorithms for selecting and perhaps orthogonalizing features in accordance with the availability of training data and the level of system complexity were studied."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "3 Learning codes for natural scenes Here we learn complete and two-times overcomplete representations of natural scenes using the same data set used by Olshausen and Field [14] (whitened images of Alaska nature scenes)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 53
                            }
                        ],
                        "text": "This result is consistent with previous observations [14, 10, 21, 22], and in fact the learning algorithm used by Olshausen and Field [14, 10] and the ICA learning rule used by Bell and Sejnowski [21] can both be derived from this framework [4]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "The results obtained here, as well as similar results obtained using related methods [14, 21, 22], suggest that the localized, oriented, and bandpass structure of V1 receptive elds can be accounted for in terms of a rather general coding principle | i."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 92
                            }
                        ],
                        "text": "Because it is well established that images are not well described by Gaussian distributions [2, 23, 14], we are thus obligated to choose a non-Gaussian prior."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "The training data consisted of 12 12 image patches randomly sampled from the ten 512 512 images in the dataset of Olshausen and Field [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 99
                            }
                        ],
                        "text": "Nearly all of the learned basis functions show a Gabor-like structure as has been found previously [14, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "We choose this distribution to be factorial and Laplacian, P (sm) / exp( mjsmj), which assumes that A decomposes the images into sparse, statistically independent components [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 192
                            }
                        ],
                        "text": "Performing gradient ascent on this expression yields the following learning rule (see appendix for derivation), A / hesT AH 1i : (19) Note that the rst term is precisely Olshausen and Field's [14] learning rule, while the second term results from approximating the volume under the posterior, and thus does away with the need for the additional rescaling step used in Olshausen and Field's algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 139
                            }
                        ],
                        "text": "An intriguing aspect of codes adapted to natural images is their resemblance to the receptive elds of neurons in the primary visual cortex [14, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive- eld properties by  learning a sparse code for natural"
            },
            "venue": {
                "fragments": [],
                "text": "images. Nature,"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815314"
                        ],
                        "name": "J. Cardoso",
                        "slug": "J.-Cardoso",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Cardoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cardoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716124"
                        ],
                        "name": "Beate H. Laheld",
                        "slug": "Beate-H.-Laheld",
                        "structuredName": {
                            "firstName": "Beate",
                            "lastName": "Laheld",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beate H. Laheld"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 152
                            }
                        ],
                        "text": "A recently developed generalization of PCA, called independent component analysis (ICA), allows for non-Gaussian distributions and non-orthogonal bases [5, 6, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60478593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46909441dd40badf6f2f1a815dcdfb295bb194ef",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-information-maximization-approach-to-blind-and-Cardoso-Laheld",
            "title": {
                "fragments": [],
                "text": "An information-maximization approach to blind separation and blind deconvolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2102692030"
                        ],
                        "name": "RussLL L. Ds Vnlos",
                        "slug": "RussLL-L.-Ds-Vnlos",
                        "structuredName": {
                            "firstName": "RussLL",
                            "lastName": "Vnlos",
                            "middleNames": [
                                "L.",
                                "Ds"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "RussLL L. Ds Vnlos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081557442"
                        ],
                        "name": "Duaxs G. ALSREcHT",
                        "slug": "Duaxs-G.-ALSREcHT",
                        "structuredName": {
                            "firstName": "Duaxs",
                            "lastName": "ALSREcHT",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duaxs G. ALSREcHT"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103028334"
                        ],
                        "name": "Lrse G. Tsonrll",
                        "slug": "Lrse-G.-Tsonrll",
                        "structuredName": {
                            "firstName": "Lrse",
                            "lastName": "Tsonrll",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lrse G. Tsonrll"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9306470,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d54f243c31a5797b059c945fec65502e47d5e879",
            "isKey": false,
            "numCitedBy": 701,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SPATIAL-FREQUENCY-SELECTIVITY-OF-CELLS-IN-MACAQUE-Vnlos-ALSREcHT",
            "title": {
                "fragments": [],
                "text": "SPATIAL FREQUENCY SELECTIVITY OF CELLS IN MACAQUE VISUAL CORTEX"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Field , \u2018 \u2018 Sparse coding with an overcomplete basis set : a strategy employed by V 1 ?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Field . Sparse coding with an overcomplete basis set : Astrategy employed by V 1 ?"
            },
            "venue": {
                "fragments": [],
                "text": "Atomic decomposition by basis pursuit . Technical report"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 173
                            }
                        ],
                        "text": "However in the former case the basis functions learn only from the pairwise statistics in the images and so do not become localized unless arti cially constrained (see also [19, 20]), and in the latter case it is unclear why joint localization in the space/spatial-frequency domains is desirable in the rst place, or that it is a principle that could be generalized to higher stages of processing and other modalities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 281
                            }
                        ],
                        "text": "For the cases of positive noise ( > 0) and overcomplete representations, computing the most probable coe cients is not straightforward, although recent work has made progress in nding the most probable coe cients for overcomplete representations with a generalized Laplacian prior [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An a ne scaling methodology for best basis se-  lection"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, Center for Information Engineering, Univ. California San  Deigo,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis, a new concept ,'' Signal Process"
            },
            "venue": {
                "fragments": [],
                "text": "Independent component analysis, a new concept ,'' Signal Process"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 171
                            }
                        ],
                        "text": "Under certain forms of the model, this is equivalent to the methods of redundancy reduction and maximizing the mutual information between the input and the representation [37, 38, 39], and has been advocated by several researchers [24, 40, 25, 1, 41, 42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low-noise limit: A a factorial code  maximizes information"
            },
            "venue": {
                "fragments": [],
                "text": "transfer. Network,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 93
                            }
                        ],
                        "text": "One promising approach for more accurate estimates is to model P (s) with a Gaussian mixture [34, 35] and again use a Gaussian approximation at the (maximum) posterior mode."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of noisy mixtures: An EM algorithm for indepentent factor  analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "A more general approach, suggested by equation 17, is to use Monte Carlo methods [36] to estimate P (xjA) by sampling the coe cient posterior."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 173
                            }
                        ],
                        "text": "However in the former case the basis functions learn only from the pairwise statistics in the images and so do not become localized unless arti cially constrained (see also [19, 20]), and in the latter case it is unclear why joint localization in the space/spatial-frequency domains is desirable in the rst place, or that it is a principle that could be generalized to higher stages of processing and other modalities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Development of localized oriented receptive- elds  by learning a translation-invariant code for natural images. Network Computation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Systems,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Could informationtheory provide an ecological theory of sensory processing , \u2019 \u2019 Network Comput"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Syst ."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Numerical Recipies in C: The Art of Scientiic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Numerical Recipies in C: The Art of Scientiic Programming"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Opt. Soc. Am. A"
            },
            "venue": {
                "fragments": [],
                "text": "J. Opt. Soc. Am. A"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 53
                            }
                        ],
                        "text": "This result is consistent with previous observations [14, 10, 21, 22], and in fact the learning algorithm used by Olshausen and Field [14, 10] and the ICA learning rule used by Bell and Sejnowski [21] can both be derived from this framework [4]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "The results obtained here, as well as similar results obtained using related methods [14, 21, 22], suggest that the localized, oriented, and bandpass structure of V1 receptive elds can be accounted for in terms of a rather general coding principle | i."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 115
                            }
                        ],
                        "text": "The basis function characteristics found here are generally consistent with previous results using related methods [10, 21, 22]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component lters of natural  images campared with simple cells in primary visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Royal Soc. Lond.  B,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blind separation of sources. 1. An adaptive algorithm based on neuromimetic architecture ,'' Signal Process"
            },
            "venue": {
                "fragments": [],
                "text": "Blind separation of sources. 1. An adaptive algorithm based on neuromimetic architecture ,'' Signal Process"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low - noise limit : A a factorial codemaximizes information transfer"
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Alternatively, it has been suggested this discrepancy might be resolved by considering the time domain of the visual signal [29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis of natural  images sequences yield spatiotemporal lters similar to simple cells in primary visual  cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Royal Soc. Lond. B,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Numerical Recipies in C: The Art of Scientific Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Numerical Recipies in C: The Art of Scientific Programming"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis of natural images sequences yield spatiotemporal filters similar to simple cells in primary visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. R. Soc. London Ser. B"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component lters of natural images campared with simple cells in primary visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Independent component lters of natural images campared with simple cells in primary visual cortex"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the lownoise limit: a factorial code maximizes information transfer"
            },
            "venue": {
                "fragments": [],
                "text": "Nonlinear neurons in the lownoise limit: a factorial code maximizes information transfer"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "a strategy employed by V1?\u2019\u2019 Vision Res"
            },
            "venue": {
                "fragments": [],
                "text": "37, 3311\u20133325"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Could informationtheory provide an ecological theory of sensory process"
            },
            "venue": {
                "fragments": [],
                "text": "Network - Computation in Neural Systems"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks (Springer-Verlag"
            },
            "venue": {
                "fragments": [],
                "text": "New York,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis of natural images sequences yield spatiotemporal lters similar to simple cells in primary visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Royal Soc. Lond. B"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Independent component analysis of natural images sequences yield spatiotemporal filters similar to simple cells in primary visual cortex,\u2019"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. R. Soc. London Ser. B 265,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Opt. Soc. Am. A"
            },
            "venue": {
                "fragments": [],
                "text": "J. Opt. Soc. Am. A"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear neurons in the low-noise limit: A a factorial code maximizes information transfer"
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "The most probable basis function coe cients, \u015d, were obtained using a modi ed conjugate gradient routine [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Numerical Recip-  ies in C: The Art of Scienti c Programming (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 200
                            }
                        ],
                        "text": "The learned basis functions also resemble the spatial receptive eld pro les of simple cells found in the primary visual cortex of mammals, which numerous investigators have likened to Gabor functions [17, 18, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 250
                            }
                        ],
                        "text": "One of the reasons the Gabor basis has been advocated as a model of V1 image coding, as well as for e cient image coding in general, is that it possesses the attractive property of maximum localization in both the space and spatial-frequency domains [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 316,
                                "start": 305
                            }
                        ],
                        "text": "Previous attempts to account for the structure of V1 receptive elds in terms of quantitative principles have been based either on principal components analysis [15, 16], or on the fact that Gabor functions provide an optimal tradeo in achieving localization in both the space and spatialfrequency domains [17, 18, 3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image-analysis by local 2-D spectral signatures"
            },
            "venue": {
                "fragments": [],
                "text": "J. Optical Soc. of  Am. a Optics and Image Science,"
            },
            "year": 1985
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 22,
            "result": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 76,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/PROBABILISTIC-FRAMEWORK-FOR-THE-ADAPTATION-AND-OF-Lewicki-Olshausen/a87e0d75a8c17e464cf8e95a0466533e14b97c5e?sort=total-citations"
}