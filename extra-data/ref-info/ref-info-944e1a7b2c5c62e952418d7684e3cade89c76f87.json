{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135485"
                        ],
                        "name": "Tom. Mitchell",
                        "slug": "Tom.-Mitchell",
                        "structuredName": {
                            "firstName": "Tom.",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 72
                            }
                        ],
                        "text": "Co-training implementation Our implementation follows the original work (Blum and Mitchell, 1998), with the same feature splits as used in our auxiliary problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 64
                            }
                        ],
                        "text": "Another popular semi-supervised learning method is co-training (Blum and Mitchell, 1998), which is related to the bootstrap method used in some NLP applications (Yarowsky, 1995) and to EM (Nigam et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 63
                            }
                        ],
                        "text": "Another popular semi-supervised learning method is co-training (Blum and Mitchell, 1998), which is related to the bootstrap method used in some NLP applications (Yarowsky, 1995) and to EM (Nigam et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207228399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278841ab0cb24c1abcb75e363aeed1fa741c8cc4",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu"
            },
            "slug": "Combining-labeled-and-unlabeled-data-with-Blum-Mitchell",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data with co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A PAC-style analysis is provided for a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views, to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16025939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9755f9993553131e5cc796d34ecaa624fe0ddffa",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, there has been increasing interest in using unlabeled data for classiica-tion. However, whether these unlabeled data are truly useful is still under debate. In order to have a better understanding of relevant issues, it is worthwhile to precisely formulate the problem and carefully analyze the value of unlabeled data under certain learning models. In this paper, we approach this problem from the statistical point of view, where we assume that a correct model of the underlying distribution is given. We demonstrate that Fisher information matrices can be used to judge the asymp-totic value of unlabeled data. We apply this methodology to both \\passive partially supervised learning\" and \\active learning\", and draw conclusions from this analysis. Experiments will be provided to support our claims."
            },
            "slug": "The-Value-of-Unlabeled-Data-for-Classification-Zhang",
            "title": {
                "fragments": [],
                "text": "The Value of Unlabeled Data for Classification Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is demonstrated that Fisher information matrices can be used to judge the asymp-totic value of unlabeled data and this methodology is applied to both passive partially supervised learning and active learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16629334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4299baa815ca5a815a70fba94a9f6f2b42fff19",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning. The idea is to find \"what good classifiers are like\" by learning from thousands of automatically generated auxiliary classification problems on unlabeled data. By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem. The method produces performance higher than the previous best results on CoNLL'00 syntactic chunking and CoNLL'03 named entity chunking (English and German)."
            },
            "slug": "A-High-Performance-Semi-Supervised-Learning-Method-Ando-Zhang",
            "title": {
                "fragments": [],
                "text": "A High-Performance Semi-Supervised Learning Method for Text Chunking"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel semi-supervised method that employs a learning paradigm which is to find \"what good classifiers are like\" by learning from thousands of automatically generated auxiliary classification problems on unlabeled data, which produces performance higher than the previous best results."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34277946"
                        ],
                        "name": "D. Pierce",
                        "slug": "D.-Pierce",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pierce",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pierce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 31
                            }
                        ],
                        "text": "However, it was pointed out by Pierce and Cardie (2001) that this method may degrade the classification performance when the assumptions of the method are not satisfied (that is when noise is introduced into the labels through non-perfect classification)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13999155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df3520f52fcf42b4f10ed4b35b3b3f9cd050f290",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between co-trained classifiers and fully supervised classifiers trained on a labeled version of all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks."
            },
            "slug": "Limitations-of-Co-Training-for-Natural-Language-Pierce-Cardie",
            "title": {
                "fragments": [],
                "text": "Limitations of Co-Training for Natural Language Learning from Large Datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels and proposes a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312432"
                        ],
                        "name": "T. N. Lal",
                        "slug": "T.-N.-Lal",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lal",
                            "middleNames": [
                                "Navin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. N. Lal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 184
                            }
                        ],
                        "text": "An example of this approach is to use unlabeled data to create a data-manifold (graph structure), on which proper smooth function classes can be defined (Szummer and Jaakkola, 2002; Zhou et al., 2004; Zhu et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 508435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46770a8e7e2af28f5253e5961f709be74e34c1f6",
            "isKey": false,
            "numCitedBy": 3894,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data."
            },
            "slug": "Learning-with-Local-and-Global-Consistency-Zhou-Bousquet",
            "title": {
                "fragments": [],
                "text": "Learning with Local and Global Consistency"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 189
                            }
                        ],
                        "text": "Another popular semi-supervised learning method is co-training (Blum and Mitchell, 1998), which is related to the bootstrap method used in some NLP applications (Yarowsky, 1995) and to EM (Nigam et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 686980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2de29049d62de925cf709024b92774cd82b0a5a",
            "isKey": false,
            "numCitedBy": 3072,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%."
            },
            "slug": "Text-Classification-from-Labeled-and-Unlabeled-EM-Nigam-McCallum",
            "title": {
                "fragments": [],
                "text": "Text Classification from Labeled and Unlabeled Documents using EM"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents, and presents two extensions to the algorithm that improve classification accuracy under these conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the machine learning literature, related work is sometime referred to as multi-task learning, for example, see (Baxter, 2000; Ben-David and Schuller, 2003; Caruana, 1997; Evegniou and Pontil, 2004; Micchelli and Ponti, 2005) and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 719551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e219a61354d972a28954e655a7c53373508a08b6",
            "isKey": false,
            "numCitedBy": 1468,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs."
            },
            "slug": "Regularized-multi--task-learning-Evgeniou-Pontil",
            "title": {
                "fragments": [],
                "text": "Regularized multi--task learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines, that have been successfully used in the past for single-- task learning is presented."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 115
                            }
                        ],
                        "text": "In the machine learning literature, related work is sometime referred to as multi-task learning, for example, see (Baxter, 2000; Ben-David and Schuller, 2003; Caruana, 1997; Evegniou and Pontil, 2004; Micchelli and Ponti, 2005) and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 46
                            }
                        ],
                        "text": "A similar point of view can also be found in (Baxter, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9803204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "727e1e16ede6eaad241bad11c525da07b154c688",
            "isKey": false,
            "numCitedBy": 972,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task."
            },
            "slug": "A-Model-of-Inductive-Bias-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Model of Inductive Bias Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Under certain restrictions on the set of all hypothesis spaces available to the learner, it is shown that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 51
                            }
                        ],
                        "text": "Figure 6: Comparison with similar settings in BN04 (Belkin and Niyogi, 2004) on 20 newsgroup."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 13
                            }
                        ],
                        "text": "BN04 results (Belkin and Niyogi, 2004) are on the unlabeled portion of the training set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 50
                            }
                        ],
                        "text": "As shown in Figure 6, our results outperform BN04 (Belkin and Niyogi, 2004)\u2019s manifoldbased semi-supervised learning method."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 76
                            }
                        ],
                        "text": "3 It also outperforms a manifold-based semi-supervised learning method BN04 (Belkin and Niyogi, 2004) except when the number of labeled data is 100."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17133491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed3c324be93f30797e0f71d5f5fb5417cdd790bc",
            "isKey": true,
            "numCitedBy": 806,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of utilizing both labeled and unlabeled data to improve classification accuracy. Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an algorithmic framework to classify a partially labeled data set in a principled manner. The central idea of our approach is that classification functions are naturally defined only on the submanifold in question rather than the total ambient space. Using the Laplace-Beltrami operator one produces a basis (the Laplacian Eigenmaps) for a Hilbert space of square integrable functions on the submanifold. To recover such a basis, only unlabeled examples are required. Once such a basis is obtained, training can be performed using the labeled data set.Our algorithm models the manifold using the adjacency graph for the data and approximates the Laplace-Beltrami operator by the graph Laplacian. We provide details of the algorithm, its theoretical justification, and several practical applications for image, speech, and text classification."
            },
            "slug": "Semi-Supervised-Learning-on-Riemannian-Manifolds-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning on Riemannian Manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An algorithmic framework to classify a partially labeled data set in a principled manner and models the manifold using the adjacency graph for the data and approximates the Laplace-Beltrami operator by the graph Laplacian."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 201
                            }
                        ],
                        "text": "An example of this approach is to use unlabeled data to create a data-manifold (graph structure), on which proper smooth function classes can be defined (Szummer and Jaakkola, 2002; Zhou et al., 2004; Zhu et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1052837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "125842668eab7decac136db8a59d392dc5e4e395",
            "isKey": false,
            "numCitedBy": 3710,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks."
            },
            "slug": "Semi-Supervised-Learning-Using-Gaussian-Fields-and-Zhu-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model, and methods to incorporate class priors and the predictions of classifiers obtained by supervised learning are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401829700"
                        ],
                        "name": "Shai Ben-David",
                        "slug": "Shai-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shai Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952147"
                        ],
                        "name": "Reba Schuller Borbely",
                        "slug": "Reba-Schuller-Borbely",
                        "structuredName": {
                            "firstName": "Reba",
                            "lastName": "Borbely",
                            "middleNames": [
                                "Schuller"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reba Schuller Borbely"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 129
                            }
                        ],
                        "text": "In the machine learning literature, related work is sometime referred to as multi-task learning, for example, see (Baxter, 2000; Ben-David and Schuller, 2003; Caruana, 1997; Evegniou and Pontil, 2004; Micchelli and Ponti, 2005) and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13967968,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "3a4551508f84a3f5447d3490b2db95b4d87a7969",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The approach of learning of multiple \u201crelated\u201d tasks simultaneously has proven quite successful in practice; however, theoretical justification for this success has remained elusive. The starting point for previous work on multiple task learning has been that the tasks to be learned jointly are somehow \u201calgorithmically related\u201d, in the sense that the results of applying a specific learning algorithm to these tasks are assumed to be similar. We offer an alternative approach, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task."
            },
            "slug": "Exploiting-Task-Relatedness-for-Mulitple-Task-Ben-David-Borbely",
            "title": {
                "fragments": [],
                "text": "Exploiting Task Relatedness for Mulitple Task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work offers an alternative approach to multiple task learning, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 154
                            }
                        ],
                        "text": "An example of this approach is to use unlabeled data to create a data-manifold (graph structure), on which proper smooth function classes can be defined (Szummer and Jaakkola, 2002; Zhou et al., 2004; Zhu et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9743839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e6779bb55f7fbed5684ded55df51747ea678a84",
            "isKey": false,
            "numCitedBy": 669,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems."
            },
            "slug": "Partially-labeled-classification-with-Markov-random-Szummer-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Partially labeled classification with Markov random walks"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work combines a limited number of labeled examples with a Markov random walk representation over the unlabeled examples and develops and compares several estimation criteria/algorithms suited to this representation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 51
                            }
                        ],
                        "text": "Although some success has been reported (e.g., see Joachims, 1999), there has also been criticism pointing out that this method may not behave well under some circumstances (Zhang and Oles, 2000)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14591650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8",
            "isKey": false,
            "numCitedBy": 3047,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more."
            },
            "slug": "Transductive-Inference-for-Text-Classification-Joachims",
            "title": {
                "fragments": [],
                "text": "Transductive Inference for Text Classification using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "An analysis of why Transductive Support Vector Machines are well suited for text classi cation is presented, and an algorithm for training TSVMs, handling 10,000 examples and more is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 201
                            }
                        ],
                        "text": "In the machine learning literature, related work is sometime referred to as multi-task learning, for example, see (Baxter, 2000; Ben-David and Schuller, 2003; Caruana, 1997; Evegniou and Pontil, 2004; Micchelli and Ponti, 2005) and references therein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7051002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1abfc1c96fe3f2dd2b9282835dea1fd6906fedb0",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides a foundation for multi-task learning using reproducing kernel Hilbert spaces of vector-valued functions. In this setting, the kernel is a matrix-valued function. Some explicit examples will be described which go beyond our earlier results in [7]. In particular, we characterize classes of matrix- valued kernels which are linear and are of the dot product or the translation invariant type. We discuss how these kernels can be used to model relations between the tasks and present linear multi-task learning algorithms. Finally, we present a novel proof of the representer theorem for a minimizer of a regularization functional which is based on the notion of minimal norm interpolation."
            },
            "slug": "Kernels-for-Multi--task-Learning-Micchelli-Pontil",
            "title": {
                "fragments": [],
                "text": "Kernels for Multi--task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper provides a foundation for multi-task learning using reproducing kernel Hilbert spaces of vector-valued functions using classes of matrix- valued kernels which are linear and are of the dot product or the translation invariant type."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444394"
                        ],
                        "name": "E. Ziegel",
                        "slug": "E.-Ziegel",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ziegel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ziegel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 200
                            }
                        ],
                        "text": "However, the framework developed in this paper is under the frequentist setting, and the most relevant statistical studies are shrinkage methods in multiple-output linear models (see Section 3.4.6 of Hastie et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46701966,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "e41ba5dc12c79a64dfa905c0328f95976252ffe0",
            "isKey": true,
            "numCitedBy": 12392,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Chapter 11 includes more case studies in other areas, ranging from manufacturing to marketing research. Chapter 12 concludes the book with some commentary about the scienti\u008e c contributions of MTS. The Taguchi method for design of experiment has generated considerable controversy in the statistical community over the past few decades. The MTS/MTGS method seems to lead another source of discussions on the methodology it advocates (Montgomery 2003). As pointed out by Woodall et al. (2003), the MTS/MTGS methods are considered ad hoc in the sense that they have not been developed using any underlying statistical theory. Because the \u201cnormal\u201d and \u201cabnormal\u201d groups form the basis of the theory, some sampling restrictions are fundamental to the applications. First, it is essential that the \u201cnormal\u201d sample be uniform, unbiased, and/or complete so that a reliable measurement scale is obtained. Second, the selection of \u201cabnormal\u201d samples is crucial to the success of dimensionality reduction when OAs are used. For example, if each abnormal item is really unique in the medical example, then it is unclear how the statistical distance MD can be guaranteed to give a consistent diagnosis measure of severity on a continuous scale when the larger-the-better type S/N ratio is used. Multivariate diagnosis is not new to Technometrics readers and is now becoming increasingly more popular in statistical analysis and data mining for knowledge discovery. As a promising alternative that assumes no underlying data model, The Mahalanobis\u2013Taguchi Strategy does not provide suf\u008e cient evidence of gains achieved by using the proposed method over existing tools. Readers may be very interested in a detailed comparison with other diagnostic tools, such as logistic regression and tree-based methods. Overall, although the idea of MTS/MTGS is intriguing, this book would be more valuable had it been written in a rigorous fashion as a technical reference. There is some lack of precision even in several mathematical notations. Perhaps a follow-up with additional theoretical justi\u008e cation and careful case studies would answer some of the lingering questions."
            },
            "slug": "The-Elements-of-Statistical-Learning-Ziegel",
            "title": {
                "fragments": [],
                "text": "The Elements of Statistical Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Chapter 11 includes more case studies in other areas, ranging from manufacturing to marketing research, and a detailed comparison with other diagnostic tools, such as logistic regression and tree-based methods."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150444687"
                        ],
                        "name": "David E. Johnson",
                        "slug": "David-E.-Johnson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Johnson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David E. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 34
                            }
                        ],
                        "text": "90 rule-based post processing ZJ03(Zhang and Johnson, 2003) 71."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 116
                            }
                        ],
                        "text": "1 Feature Representation Our feature representation is a slight modification of a simpler configuration reported in (Zhang and Johnson, 2003), which uses: token strings, parts-of-speech, character types, several characters at the beginning and the ending of the tokens, in a 5-token window around the current position; token strings in a 3-syntactic chunk window; labels of two tokens on the left to the current position; bi-grams of the current token and the label on the left; and the labels assigned to previous occurrences of the current word."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7896577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "207df123e3c24e6e25019d4b86f8efaad5d6f13c",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a robust linear classification system for Named Entity Recognition. A similar system has been applied to the CoNLL text chunking shared task with state of the art performance. By using different linguistic features, we can easily adapt this system to other token-based linguistic tagging problems. The main focus of the current paper is to investigate the impact of various local linguistic features for named entity recognition on the CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) shared task data. We show that the system performance can be enhanced significantly with some relative simple token-based features that are available for many languages. Although more sophisticated linguistic features will also be helpful, they provide much less improvement than might be expected."
            },
            "slug": "A-Robust-Risk-Minimization-based-Named-Entity-Zhang-Johnson",
            "title": {
                "fragments": [],
                "text": "A Robust Risk Minimization based Named Entity Recognition System"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that the system performance can be enhanced significantly with some relative simple token-based features that are available for many languages, and more sophisticated linguistic features provide much less improvement than might be expected."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707117"
                        ],
                        "name": "Radu Florian",
                        "slug": "Radu-Florian",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Florian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Florian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683685"
                        ],
                        "name": "Abraham Ittycheriah",
                        "slug": "Abraham-Ittycheriah",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Ittycheriah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abraham Ittycheriah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40544823"
                        ],
                        "name": "Hongyan Jing",
                        "slug": "Hongyan-Jing",
                        "structuredName": {
                            "firstName": "Hongyan",
                            "lastName": "Jing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongyan Jing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10606201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1ba322f795adbdc706651dbc76ad53b9c5f9468",
            "isKey": false,
            "numCitedBy": 477,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions. When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data."
            },
            "slug": "Named-Entity-Recognition-through-Classifier-Florian-Ittycheriah",
            "title": {
                "fragments": [],
                "text": "Named Entity Recognition through Classifier Combination"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 67
                            }
                        ],
                        "text": "The training algorithm is stochastic gradient descent (SGD) as in (Zhang, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 112
                            }
                        ],
                        "text": "It was recently argued that this simple method can also work well for large scale convex learning formulations (Zhang, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5306879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff",
            "isKey": false,
            "numCitedBy": 1008,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings."
            },
            "slug": "Solving-large-scale-linear-prediction-problems-Zhang",
            "title": {
                "fragments": [],
                "text": "Solving large scale linear prediction problems using stochastic gradient descent algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Stochastic gradient descent algorithms on regularized forms of linear prediction methods, related to online algorithms such as perceptron, are studied, and numerical rate of convergence for such algorithms is obtained."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 69
                            }
                        ],
                        "text": "For example, Vapnik introduced the notion of transductive inference (Vapnik, 1998), which may be regarded as an approach to semi-supervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3024630"
                        ],
                        "name": "Hai Leong Chieu",
                        "slug": "Hai-Leong-Chieu",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Chieu",
                            "middleNames": [
                                "Leong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hai Leong Chieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34789794"
                        ],
                        "name": "H. Ng",
                        "slug": "H.-Ng",
                        "structuredName": {
                            "firstName": "Hwee",
                            "lastName": "Ng",
                            "middleNames": [
                                "Tou"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16619357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03c03dec975554cb02aca1e076106178dbe0a8a0",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The named entity recognition (NER) task involves identifying noun phrases that are names, and assigning a class to each name. This task has its origin from the Message Understanding Conferences (MUC) in the 1990s, a series of conferences aimed at evaluating systems that extract information from natural language texts. It became evident that in order to achieve good performance in information extraction, a system needs to be able to recognize names. A separate subtask on NER was created in MUC-6 and MUC-7 (Chinchor, 1998)."
            },
            "slug": "Named-Entity-Recognition-with-a-Maximum-Entropy-Chieu-Ng",
            "title": {
                "fragments": [],
                "text": "Named Entity Recognition with a Maximum Entropy Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The named entity recognition (NER) task involves identifying noun phrases that are names, and assigning a class to each name."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 162
                            }
                        ],
                        "text": "Another popular semi-supervised learning method is co-training (Blum and Mitchell, 1998), which is related to the bootstrap method used in some NLP applications (Yarowsky, 1995) and to EM (Nigam et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 161
                            }
                        ],
                        "text": "Another popular semi-supervised learning method is co-training (Blum and Mitchell, 1998), which is related to the bootstrap method used in some NLP applications (Yarowsky, 1995) and to EM (Nigam et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1487550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944cba683d10d8c1a902e05cd68e32a9f47b372e",
            "isKey": false,
            "numCitedBy": 2536,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%."
            },
            "slug": "Unsupervised-Word-Sense-Disambiguation-Rivaling-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1855302"
                        ],
                        "name": "Joseph Smarr",
                        "slug": "Joseph-Smarr",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Smarr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Smarr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122619312"
                        ],
                        "name": "Huy Nguyen",
                        "slug": "Huy-Nguyen",
                        "structuredName": {
                            "firstName": "Huy",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huy Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 52
                            }
                        ],
                        "text": "31 gazetteers (also very elaborated features) KSNM03(Klein et al., 2003) 86."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1080545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e958d809a45ba3ae9eef8c5381e8cad8f11de10",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss two named-entity recognition models which use characters and character n-grams either exclusively or as an important part of their data representation. The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features. Our best model achieves an overall F1 of 86.07% on the English test data (92.31% on the development data). This number represents a 25% error reduction over the same model without word-internal (substring) features."
            },
            "slug": "Named-Entity-Recognition-with-Character-Level-Klein-Smarr",
            "title": {
                "fragments": [],
                "text": "Named Entity Recognition with Character-Level Models"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Two named-entity recognition models which use characters and character n-grams either exclusively or as an important part of their data representation are discussed, both of which are a character-level HMM with minimal context information and a maximum-entropy conditional markov model with substantially richer context features."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 104
                            }
                        ],
                        "text": "In particular, the algorithm proposed in Section 3 has a form similar to a shrinkage method proposed by Breiman and Friedman (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121621272,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c5f1b46a306a486bcf91be71f2a726b11f462514",
            "isKey": false,
            "numCitedBy": 474,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We look at the problem of predicting several response variables from the same set of explanatory variables. The question is how to take advantage of correlations between the response variables to improve predictive accuracy compared with the usual procedure of doing individual regressions of each response variable on the common set of predictor variables. A new procedure is introduced called the curds and whey method. Its use can substantially reduce prediction errors when there are correlations between responses while maintaining accuracy even if the responses are uncorrelated. In extensive simulations, the new procedure is compared with several previously proposed methods for predicting multiple responses (including partial least squares) and exhibits superior accuracy. One version can be easily implemented in the context of standard statistical packages."
            },
            "slug": "Predicting-Multivariate-Responses-in-Multiple-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Predicting Multivariate Responses in Multiple Linear Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2508221"
                        ],
                        "name": "A. V. D. Vaart",
                        "slug": "A.-V.-D.-Vaart",
                        "structuredName": {
                            "firstName": "Aad",
                            "lastName": "Vaart",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. V. D. Vaart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144952368"
                        ],
                        "name": "J. Wellner",
                        "slug": "J.-Wellner",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Wellner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wellner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117819015,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "45ee7447b9dd406496c4a5d9d8fb6556366a01c6",
            "isKey": false,
            "numCitedBy": 5375,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1.1. Introduction.- 1.2. Outer Integrals and Measurable Majorants.- 1.3. Weak Convergence.- 1.4. Product Spaces.- 1.5. Spaces of Bounded Functions.- 1.6. Spaces of Locally Bounded Functions.- 1.7. The Ball Sigma-Field and Measurability of Suprema.- 1.8. Hilbert Spaces.- 1.9. Convergence: Almost Surely and in Probability.- 1.10. Convergence: Weak, Almost Uniform, and in Probability.- 1.11. Refinements.- 1.12. Uniformity and Metrization.- 2.1. Introduction.- 2.2. Maximal Inequalities and Covering Numbers.- 2.3. Symmetrization and Measurability.- 2.4. Glivenko-Cantelli Theorems.- 2.5. Donsker Theorems.- 2.6. Uniform Entropy Numbers.- 2.7. Bracketing Numbers.- 2.8. Uniformity in the Underlying Distribution.- 2.9. Multiplier Central Limit Theorems.- 2.10. Permanence of the Donsker Property.- 2.11. The Central Limit Theorem for Processes.- 2.12. Partial-Sum Processes.- 2.13. Other Donsker Classes.- 2.14. Tail Bounds.- 3.1. Introduction.- 3.2. M-Estimators.- 3.3. Z-Estimators.- 3.4. Rates of Convergence.- 3.5. Random Sample Size, Poissonization and Kac Processes.- 3.6. The Bootstrap.- 3.7. The Two-Sample Problem.- 3.8. Independence Empirical Processes.- 3.9. The Delta-Method.- 3.10. Contiguity.- 3.11. Convolution and Minimax Theorems.- A. Appendix.- A.1. Inequalities.- A.2. Gaussian Processes.- A.2.1. Inequalities and Gaussian Comparison.- A.2.2. Exponential Bounds.- A.2.3. Majorizing Measures.- A.2.4. Further Results.- A.3. Rademacher Processes.- A.4. Isoperimetric Inequalities for Product Measures.- A.5. Some Limit Theorems.- A.6. More Inequalities.- A.6.1. Binomial Random Variables.- A.6.2. Multinomial Random Vectors.- A.6.3. Rademacher Sums.- Notes.- References.- Author Index.- List of Symbols."
            },
            "slug": "Weak-Convergence-and-Empirical-Processes:-With-to-Vaart-Wellner",
            "title": {
                "fragments": [],
                "text": "Weak Convergence and Empirical Processes: With Applications to Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter discusses Convergence: Weak, Almost Uniform, and in Probability, which focuses on the part of Convergence of the Donsker Property which is concerned with Uniformity and Metrization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14725378"
                        ],
                        "name": "M. Ledoux",
                        "slug": "M.-Ledoux",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Ledoux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ledoux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2837737"
                        ],
                        "name": "M. Talagrand",
                        "slug": "M.-Talagrand",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Talagrand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Talagrand"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118526268,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d61540a96c22a1ed4a2b78558a2ebdd39f221a90",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Notation.- 0. Isoperimetric Background and Generalities.- 1. Isoperimetric Inequalities and the Concentration of Measure Phenomenon.- 2. Generalities on Banach Space Valued Random Variables and Random Processes.- I. Banach Space Valued Random Variables and Their Strong Limiting Properties.- 3. Gaussian Random Variables.- 4. Rademacher Averages.- 5. Stable Random Variables.- 6 Sums of Independent Random Variables.- 7. The Strong Law of Large Numbers.- 8. The Law of the Iterated Logarithm.- II. Tightness of Vector Valued Random Variables and Regularity of Random Processes.- 9. Type and Cotype of Banach Spaces.- 10. The Central Limit Theorem.- 11. Regularity of Random Processes.- 12. Regularity of Gaussian and Stable Processes.- 13. Stationary Processes and Random Fourier Series.- 14. Empirical Process Methods in Probability in Banach Spaces.- 15. Applications to Banach Space Theory.- References."
            },
            "slug": "Probability-in-Banach-Spaces:-Isoperimetry-and-Ledoux-Talagrand",
            "title": {
                "fragments": [],
                "text": "Probability in Banach Spaces: Isoperimetry and Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737631"
                        ],
                        "name": "J. Kuelbs",
                        "slug": "J.-Kuelbs",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kuelbs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kuelbs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118021177,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5546b2d40ca53da8d34d8cd3c3ea4d736cd1a3b2",
            "isKey": false,
            "numCitedBy": 750,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probability-on-Banach-spaces-Kuelbs",
            "title": {
                "fragments": [],
                "text": "Probability on Banach spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679035"
                        ],
                        "name": "C. McDiarmid",
                        "slug": "C.-McDiarmid",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "McDiarmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. McDiarmid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116663483,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6bbb79cc026ecca4264d95c4551fc58205b09533",
            "isKey": false,
            "numCitedBy": 1710,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Surveys-in-Combinatorics,-1989:-On-the-method-of-McDiarmid",
            "title": {
                "fragments": [],
                "text": "Surveys in Combinatorics, 1989: On the method of bounded differences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Named entity recognition with a maximum entropy approach Regularized multi \u2013 task learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Named entity recognition with a maximum entropy approach Regularized multi \u2013 task learning"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weak convergence and empirical processes. Springer Series in Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Weak convergence and empirical processes. Springer Series in Statistics"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weak convergence and empirical processes. Springer Series in Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Weak convergence and empirical processes. Springer Series in Statistics"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 74
                            }
                        ],
                        "text": "The lemma is a direct consequence of McDiarmid\u2019s concentration inequality (McDiarmid, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the method of bounded differences. In Surveys in Combinatorics, pages 148\u2013188"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Named entity recognition with a maximum entropy approach Regularized multi \u2013 task learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 11,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Framework-for-Learning-Predictive-Structures-from-Ando-Zhang/944e1a7b2c5c62e952418d7684e3cade89c76f87?sort=total-citations"
}