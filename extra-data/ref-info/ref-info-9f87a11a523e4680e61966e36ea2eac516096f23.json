{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690163"
                        ],
                        "name": "G. McLachlan",
                        "slug": "G.-McLachlan",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "McLachlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McLachlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143870001"
                        ],
                        "name": "T. Krishnan",
                        "slug": "T.-Krishnan",
                        "structuredName": {
                            "firstName": "Thriyambakam",
                            "lastName": "Krishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Krishnan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 68
                            }
                        ],
                        "text": "The scope of the algorithm'sapplications are evident in the book by McLachlan and Krishnan (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122530182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d50991b693fc23edda316fb1487f114f6cc6706",
            "isKey": false,
            "numCitedBy": 6113,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The first unified account of the theory, methodology, and applications of the EM algorithm and its extensionsSince its inception in 1977, the Expectation-Maximization (EM) algorithm has been the subject of intense scrutiny, dozens of applications, numerous extensions, and thousands of publications. The algorithm and its extensions are now standard tools applied to incomplete data problems in virtually every field in which statistical methods are used. Until now, however, no single source offered a complete and unified treatment of the subject.The EM Algorithm and Extensions describes the formulation of the EM algorithm, details its methodology, discusses its implementation, and illustrates applications in many statistical contexts. Employing numerous examples, Geoffrey McLachlan and Thriyambakam Krishnan examine applications both in evidently incomplete data situations-where data are missing, distributions are truncated, or observations are censored or grouped-and in a broad variety of situations in which incompleteness is neither natural nor evident. They point out the algorithm's shortcomings and explain how these are addressed in the various extensions.Areas of application discussed include: Regression Medical imaging Categorical data analysis Finite mixture analysis Factor analysis Robust statistical modeling Variance-components estimation Survival analysis Repeated-measures designs For theoreticians, practitioners, and graduate students in statistics as well as researchers in the social and physical sciences, The EM Algorithm and Extensions opens the door to the tremendous potential of this remarkably versatile statistical tool."
            },
            "slug": "The-EM-algorithm-and-extensions-McLachlan-Krishnan",
            "title": {
                "fragments": [],
                "text": "The EM algorithm and extensions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The EM Algorithm and Extensions describes the formulation of the EM algorithm, details its methodology, discusses its implementation, and illustrates applications in many statistical contexts, opening the door to the tremendous potential of this remarkably versatile statistical tool."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695292"
                        ],
                        "name": "R. Hathaway",
                        "slug": "R.-Hathaway",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hathaway",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hathaway"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 54
                            }
                        ],
                        "text": "In many cases, partial implementation of the E step is also natural."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 33
                            }
                        ],
                        "text": "Csisz ar and Tusn ady (1984) and Hathaway (1986) have alsoviewed EM in this light."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 119523289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f866fa9a6c13d72a87b8ff2da7c582e987db53a6",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Another-interpretation-of-the-EM-algorithm-for-Hathaway",
            "title": {
                "fragments": [],
                "text": "Another interpretation of the EM algorithm for mixture distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097256"
                        ],
                        "name": "Xiao-Li Meng",
                        "slug": "Xiao-Li-Meng",
                        "structuredName": {
                            "firstName": "Xiao-Li",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-Li Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1569622302"
                        ],
                        "name": "D. V. van Dyk",
                        "slug": "D.-V.-van-Dyk",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "van Dyk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. V. van Dyk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17461647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0dc01248f2e19124afaac8f62e3ed2d935d652d",
            "isKey": false,
            "numCitedBy": 791,
            "numCiting": 219,
            "paperAbstract": {
                "fragments": [],
                "text": "Celebrating the 20th anniversary of the presentation of the paper by Dempster, Laird and Rubin which popularized the EM algorithm, we investigate, after a brief historical account, strategies that aim to make the EM algorithm converge faster while maintaining its simplicity and stability (e.g. automatic monotone convergence in likelihood). First we introduce the idea of a \u2018working parameter\u2019 to facilitate the search for efficient data augmentation schemes and thus fast EM implementations. Second, summarizing various recent extensions of the EM algorithm, we formulate a general alternating expectation\u2013conditional maximization algorithm AECM that couples flexible data augmentation schemes with model reduction schemes to achieve efficient computations. We illustrate these methods using multivariate t\u2010models with known or unknown degrees of freedom and Poisson models for image reconstruction. We show, through both empirical and theoretical evidence, the potential for a dramatic reduction in computational time with little increase in human effort. We also discuss the intrinsic connection between EM\u2010type algorithms and the Gibbs sampler, and the possibility of using the techniques presented here to speed up the latter. The main conclusion of the paper is that, with the help of statistical considerations, it is possible to construct algorithms that are simple, stable and fast."
            },
            "slug": "The-EM-Algorithm\u2014an-Old-Folk\u2010song-Sung-to-a-Fast-Meng-Dyk",
            "title": {
                "fragments": [],
                "text": "The EM Algorithm\u2014an Old Folk\u2010song Sung to a Fast New Tune"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A general alternating expectation\u2013conditional maximization algorithm AECM is formulated that couples flexible data augmentation schemes with model reduction schemes to achieve efficient computations and shows the potential for a dramatic reduction in computational time with little increase in human effort."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "An incremental variant of the EM algorithm somewhat similar to thatof (9) was investigated by Nowlan (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 70
                            }
                        ],
                        "text": "Anincremental algorithm along these general lines was investigated by Nowlan(1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60866167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59fa47fc237a0781b4bf1c84fedb728d20db26a1",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this thesis, we consider learning algorithms for neural networks which are based on fitting a mixture probability density to a set of data. \nWe begin with an unsupervised algorithm which is an alternative to the classical winner-take-all competitive algorithms. Rather than updating only the parameters of the \"winner\" on each case, the parameters of all competitors are updated in proportion to their relative responsibility for the case. Use of such a \"soft\" competitive algorithm is shown to give better performance than the more traditional algorithms, with little additional cost. \nWe then consider a supervised modular architecture in which a number of simple \"expert\" networks compete to solve distinct pieces of a large task. A soft competitive mechanism is used to determine how much an expert learns on a case, based on how well the expert performs relative to the other expert networks. At the same time, a separate gating network learns to weight the output of each expert according to a prediction of its relative performance based on the input to the system. Experiments on a number of tasks illustrate that this architecture is capable of uncovering interesting task decompositions and of generalizing better than a single network with small training sets. \nFinally, we consider learning algorithms in which we assume that the actual output of the network should fall into one of a small number of classes or clusters. The objective of learning is to make the variance of these classes as small as possible. In the classical decision-directed algorithm, we decide that an output belongs to the class it is closest to and minimize the squared distance between the output and the center (mean) of this closest class. In the \"soft\" version of this algorithm, we minimize the squared distance between the actual output and a weighted average of the means of all of the classes. The weighting factors are the relative probability that the output belongs to each class. This idea may also be used to model the weights of a network, to produce networks which generalize better from small training sets."
            },
            "slug": "Soft-competitive-adaptation:-neural-network-based-Nowlan",
            "title": {
                "fragments": [],
                "text": "Soft competitive adaptation: neural network learning algorithms based on fitting statistical mixtures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An unsupervised algorithm which is an alternative to the classical winner-take-all competitive algorithms and a supervised modular architecture in which a number of simple \"expert\" networks compete to solve distinct pieces of a large task are considered."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 158
                            }
                        ],
                        "text": "Special cases of the algorithm date back several decades, and its use hasgrown even more since its generality and widespread applicability were dis-cussed by Dempster, Laird, and Rubin (1977)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48403,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "14 RADFORD M. NEAL AND GEOFFREY E. HINTONREFERENCESCsisz ar I. and Tusn ady, G. (1984) \\Information geometry and alternatingminimization procedures\", in E. J. Dudewicz, et al (editors) Recent Re-sults in Estimation Theory and Related Topics (Statistics and Decisions,Supplement Issue No. 1, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1940,
                                "start": 122
                            }
                        ],
                        "text": "It can be shown that each such iteration improves the true likelihood, or leaves it unchanged (if a local maximum has already been reached, or in uncommon cases, before then). The M step of the algorithm may be only partially implemented, with the new estimate for the parameters improving the likelihood given the distribution found in the E step, but not necessarily maximizing it. Such a partial M step always results in the true likelihood improving as well. Dempster, et al refer to such variants as \\generalized EM (GEM)\" algorithms. A sub-class of GEM algorithms of wide applicability, the \\ExpectationConditional Maximization (ECM)\" algorithms, have been developed by Meng and Rubin (1992), and further generalized by Meng and van Dyk (1997). In many cases, partial implementation of the E step is also natural. The unobserved variables are commonly independent, and in uence the likelihood of the parameters only through simple su cient statistics. If these statistics can be updated incrementally when the distribution for one of the variables is re-calculated, it makes sense to immediately re-estimate the parameters before performing the E step for the next unobserved variable, as this utilizes the new information immediately, speeding convergence. An incremental algorithm along these general lines was investigated by Nowlan (1991). However, such incremental variants of the EM algorithm have not previously received any formal justi cation. We present here a view of the EM algorithm in which it is seen as maximizing a joint function of the parameters and of the distribution over the unobserved variables that is analogous to the \\free energy\" function used in statistical physics, and which can also be viewed in terms of a KullbackLiebler divergence. The E step maximizes this function with respect to the distribution over unobserved variables; the M step with respect to the parameters. Csisz ar and Tusn ady (1984) and Hathaway (1986) have also viewed EM in this light."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 750,
                                "start": 122
                            }
                        ],
                        "text": "It can be shown that each such iteration improves the true likelihood, or leaves it unchanged (if a local maximum has already been reached, or in uncommon cases, before then). The M step of the algorithm may be only partially implemented, with the new estimate for the parameters improving the likelihood given the distribution found in the E step, but not necessarily maximizing it. Such a partial M step always results in the true likelihood improving as well. Dempster, et al refer to such variants as \\generalized EM (GEM)\" algorithms. A sub-class of GEM algorithms of wide applicability, the \\ExpectationConditional Maximization (ECM)\" algorithms, have been developed by Meng and Rubin (1992), and further generalized by Meng and van Dyk (1997). In many cases, partial implementation of the E step is also natural."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 18
                            }
                        ],
                        "text": "Csisz ar and Tusn ady (1984) and Hathaway (1986) have alsoviewed EM in this light."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 698,
                                "start": 122
                            }
                        ],
                        "text": "It can be shown that each such iteration improves the true likelihood, or leaves it unchanged (if a local maximum has already been reached, or in uncommon cases, before then). The M step of the algorithm may be only partially implemented, with the new estimate for the parameters improving the likelihood given the distribution found in the E step, but not necessarily maximizing it. Such a partial M step always results in the true likelihood improving as well. Dempster, et al refer to such variants as \\generalized EM (GEM)\" algorithms. A sub-class of GEM algorithms of wide applicability, the \\ExpectationConditional Maximization (ECM)\" algorithms, have been developed by Meng and Rubin (1992), and further generalized by Meng and van Dyk (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1349,
                                "start": 122
                            }
                        ],
                        "text": "It can be shown that each such iteration improves the true likelihood, or leaves it unchanged (if a local maximum has already been reached, or in uncommon cases, before then). The M step of the algorithm may be only partially implemented, with the new estimate for the parameters improving the likelihood given the distribution found in the E step, but not necessarily maximizing it. Such a partial M step always results in the true likelihood improving as well. Dempster, et al refer to such variants as \\generalized EM (GEM)\" algorithms. A sub-class of GEM algorithms of wide applicability, the \\ExpectationConditional Maximization (ECM)\" algorithms, have been developed by Meng and Rubin (1992), and further generalized by Meng and van Dyk (1997). In many cases, partial implementation of the E step is also natural. The unobserved variables are commonly independent, and in uence the likelihood of the parameters only through simple su cient statistics. If these statistics can be updated incrementally when the distribution for one of the variables is re-calculated, it makes sense to immediately re-estimate the parameters before performing the E step for the next unobserved variable, as this utilizes the new information immediately, speeding convergence. An incremental algorithm along these general lines was investigated by Nowlan (1991). However, such incremental variants of the EM algorithm have not previously received any formal justi cation."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information geometry and alternating"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3147156"
                        ],
                        "name": "E. Dudewicz",
                        "slug": "E.-Dudewicz",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Dudewicz",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Dudewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101739731"
                        ],
                        "name": "D. Plachky",
                        "slug": "D.-Plachky",
                        "structuredName": {
                            "firstName": "Detlef",
                            "lastName": "Plachky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Plachky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731053"
                        ],
                        "name": "P. Sen",
                        "slug": "P.-Sen",
                        "structuredName": {
                            "firstName": "Pranab",
                            "lastName": "Sen",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118623261,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "29d4b9ce9a738861350f82adbcd736d7c44c3de6",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recent-results-in-estimation-theory-and-related-Dudewicz-Plachky",
            "title": {
                "fragments": [],
                "text": "Recent results in estimation theory and related topics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 158
                            }
                        ],
                        "text": "Special cases of the algorithm date back several decades, and its use hasgrown even more since its generality and widespread applicability were dis-cussed by Dempster, Laird, and Rubin (1977)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Maximum likelihood from incomplete data via the EM algorithm\" (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society B"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 186
                            }
                        ],
                        "text": "A sub-class of GEM algorithms of wide applicability, the \\Expectation-Conditional Maximization (ECM)\" algorithms, have been developed byMeng and Rubin (1992), and further generalized by Meng and van Dyk(1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\The EM algorithm | an old folksong sung to a fast new tune\" (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society B"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recent extensions of the EM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Information geometry and alternating minimization procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Recent Results in Estimation Theory and Related Topics (Statistics and Decisions"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 69
                            }
                        ],
                        "text": "The scope of the algorithm's applications are evident in the book by McLachlan and Krishnan (1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 68
                            }
                        ],
                        "text": "The scope of the algorithm'sapplications are evident in the book by McLachlan and Krishnan (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The EM Algorithm and Exten"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Recent extensions of the EM algorithm (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "\\Recent extensions of the EM algorithm (with discussion)"
            },
            "year": 1992
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 13,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton/9f87a11a523e4680e61966e36ea2eac516096f23?sort=total-citations"
}