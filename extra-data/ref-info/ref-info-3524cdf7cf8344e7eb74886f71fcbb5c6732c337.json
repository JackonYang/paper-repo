{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "It is instructive to explore further the differences between the current Nilsson book and his earlier one [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "As someone who learned much of his AI from Nilsson\u2019s classic books, such as [4], I opened this new textbook with pleasurable anticipation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "He explicitly compares this book to his earlier book [4] in terms of its approach and explains that he has left out much in the way of bibliographic and historical remarks partly because \u201cthe longer text by Russell & Norvig has already done such a thorough job in that regard\u201d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20370792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "affcf19551b01c4c8009d061750700d91c2f79e9",
            "isKey": true,
            "numCitedBy": 3832,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A classic introduction to artificial intelligence intended to bridge the gap between theory and practice, \"Principles of Artificial Intelligence\" describes fundamental AI ideas that underlie applications such as natural language processing, automatic programming, robotics, machine vision, automatic theorem proving, and intelligent data retrieval. Rather than focusing on the subject matter of the applications, the book is organized around general computational concepts involving the kinds of data structures used, the types of operations performed on the data structures, and the properties of the control strategies used. \"Principles of Artificial Intelligence\"evolved from the author's courses and seminars at Stanford University and University of Massachusetts, Amherst, and is suitable for text use in a senior or graduate AI course, or for individual study."
            },
            "slug": "Principles-of-Artificial-Intelligence-Nilsson",
            "title": {
                "fragments": [],
                "text": "Principles of Artificial Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This classic introduction to artificial intelligence describes fundamental AI ideas that underlie applications such as natural language processing, automatic programming, robotics, machine vision, automatic theorem proving, and intelligent data retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725040"
                        ],
                        "name": "I. Bratko",
                        "slug": "I.-Bratko",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Bratko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Bratko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "If you want a book that is based on Prolog that goes further in AI terms than, say, [1] and is based around a small number of recurring examples, then Poole et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61139322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d839753b1de2070810b97c3ca05c6b78f861667",
            "isKey": false,
            "numCitedBy": 823,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Prolog-Programming-for-Artificial-Intelligence-Bratko",
            "title": {
                "fragments": [],
                "text": "Prolog Programming for Artificial Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145313556"
                        ],
                        "name": "D. McDermott",
                        "slug": "D.-McDermott",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "McDermott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. McDermott"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 61
                            }
                        ],
                        "text": "These books thus fall into the same general category as, say [2,5,6], but have been updated for the mid\u2013late 1990s."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9375882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e626c8016a5ae06198045fabcef09e3d00ad50b",
            "isKey": false,
            "numCitedBy": 1629,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book is an introduction on artificial intelligence. Topics include reasoning under uncertainty, robot plans, language understanding, and learning. The history of the field as well as intellectual ties to related disciplines are presented."
            },
            "slug": "Introduction-to-artificial-intelligence-Charniak-McDermott",
            "title": {
                "fragments": [],
                "text": "Introduction to artificial intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "Addison-Wesley series in computer science"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4940444"
                        ],
                        "name": "M. Genesereth",
                        "slug": "M.-Genesereth",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Genesereth",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Genesereth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29423399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23945b28552a35a03ae65f282cc84da0dc5dd332",
            "isKey": false,
            "numCitedBy": 1633,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Logical-foundations-of-artificial-intelligence-Genesereth-Nilsson",
            "title": {
                "fragments": [],
                "text": "Logical foundations of artificial intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 683,
                                "start": 25
                            }
                        ],
                        "text": "stances cannot be solved in any reasonable time. Therefore, one should strive to divide the overall problem of generating intelligent behavior into tractable subproblems rather than intractable ones. REDUCTION The second important concept in the theory of complexity is reduction, which also emerged in the 1960s (Dantzig, 1960; Edmonds, 1962). A reduction is a general transformation from one class of problems to another, such that solutions to the first class can be found by reducing them to problems of the second class and solving the latter problems. NP COMPLETENESS How can one recognize an intractable problem? The theory of NP-completeness, pioneered by Steven Cook (1971) and Richard Karp (1972), provides a method."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 948,
                                "start": 6
                            }
                        ],
                        "text": "logic included Cordell Green's question answering and planning systems (Green, 1969b), and the Shakey robotics project at the new Stanford Research Institute (SRI). The latter project, discussed further in Chapter 25, was the first to demonstrate the complete integration of logical reasoning and physical activity. Minsky supervised a series of students who chose limited problems that appeared to require intelligence to solve. These limited domains became known as microworlds. James Slagle's SAINT program (1963a) was able to solve closed-form integration problems typical of first-year college calculus courses. Tom Evans's ANALOGY program (1968) solved geometric analogy problems that appear in IQ tests, such as the one in Figure 1.2. Bertram Raphael's (1968) SIR (Semantic Information Retrieval) was able to accept input statements in a very restricted subset of English and answer questions thereon. Daniel Bobrow's STUDENT program (1967) solved algebra story problems such as"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 7
                            }
                        ],
                        "text": "The Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory operational definition of intelligence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3073,
                                "start": 25
                            }
                        ],
                        "text": "stances cannot be solved in any reasonable time. Therefore, one should strive to divide the overall problem of generating intelligent behavior into tractable subproblems rather than intractable ones. REDUCTION The second important concept in the theory of complexity is reduction, which also emerged in the 1960s (Dantzig, 1960; Edmonds, 1962). A reduction is a general transformation from one class of problems to another, such that solutions to the first class can be found by reducing them to problems of the second class and solving the latter problems. NP COMPLETENESS How can one recognize an intractable problem? The theory of NP-completeness, pioneered by Steven Cook (1971) and Richard Karp (1972), provides a method. Cook and Karp showed the existence of large classes of canonical combinatorial search and reasoning problems that are NP-complete. Any problem class to which an NP-complete problem class can be reduced is likely to be intractable. (Although it has not yet been proved that NP-complete problems are necessarily intractable, few theoreticians believe otherwise.) These results contrast sharply with the \"Electronic Super-Brain\" enthusiasm accompanying the advent of computers. Despite the ever-increasing speed of computers, subtlety and careful use of resources will characterize intelligent systems. Put crudely, the world is an extremely large problem instance! Besides logic and computation, the third great contribution of mathematics to AI is the j theory of probability. The Italian Gerolamo Cardano (1501-1576) first framed the idea of I probability, describing it in terms of the possible outcomes of gambling events. Before his time, j the outcomes of gambling games were seen as the will of the gods rather than the whim of chance, i Probability quickly became an invaluable part of all the quantitative sciences, helping to deal with uncertain measurements and incomplete theories. Pierre Fermat (1601-1665), Blaise Pascal I (1623-1662), James Bernoulli (1654-1705), Pierre Laplace (1749-1827), and others advanced j the theory and introduced new statistical methods. Bernoulli also framed an alternative view] of probability, as a subjective \"degree of belief\" rather than an objective ratio of outcomes.! Subjective probabilities therefore can be updated as new evidence is obtained. Thomas Bayes j (1702-1761) proposed a rule for updating subjective probabilities in the light of new evidence! (published posthumously in 1763). Bayes' rule, and the subsequent field of Bayesian analysis,! form the basis of the modern approach to uncertain reasoning in AI systems. Debate still rages j between supporters of the objective and subjective views of probability, but it is not clear if the! difference has great significance for AI. Both versions obey the same set of axioms. Savage'sJ (1954) Foundations of Statistics gives a good introduction to the field. As with logic, a connection must be made between probabilistic reasoning and action.! DECISION THEORY Decision theory, pioneered by John Von Neumann and Oskar Morgenstern (1944), combines! probability theory with utility theory (which provides a formal and complete framework forl specifying the preferences of an agent) to give the first general theory that can distinguish good! actions from bad ones."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 834,
                                "start": 86
                            }
                        ],
                        "text": "They drew on three sources: knowledge of the basic physiology and function of neurons in the brain; the formal analysis of propositional logic due to Russell and Whitehead; and Turing's theory of computation. They proposed a model of artificial neurons in which each neuron is characterized as being \"on\" or \"off,\" with a switch to \"on\" occurring in response to stimulation by a sufficient number of neighboring neurons. The state of a neuron was conceived of as \"factually equivalent to a proposition which proposed its adequate stimulus.\" They showed, for example, that any computable function could be computed by some network of connected neurons, and that all the logical connectives could be implemented by simple net structures. McCulloch and Pitts also suggested that suitably defined networks could learn. Donald Hebb (1949) demonstrated a simple updating rule for modifying the connection strengths between neurons, such that learning could take place."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 518,
                                "start": 6
                            }
                        ],
                        "text": "logic included Cordell Green's question answering and planning systems (Green, 1969b), and the Shakey robotics project at the new Stanford Research Institute (SRI). The latter project, discussed further in Chapter 25, was the first to demonstrate the complete integration of logical reasoning and physical activity. Minsky supervised a series of students who chose limited problems that appeared to require intelligence to solve. These limited domains became known as microworlds. James Slagle's SAINT program (1963a) was able to solve closed-form integration problems typical of first-year college calculus courses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 767,
                                "start": 6
                            }
                        ],
                        "text": "logic included Cordell Green's question answering and planning systems (Green, 1969b), and the Shakey robotics project at the new Stanford Research Institute (SRI). The latter project, discussed further in Chapter 25, was the first to demonstrate the complete integration of logical reasoning and physical activity. Minsky supervised a series of students who chose limited problems that appeared to require intelligence to solve. These limited domains became known as microworlds. James Slagle's SAINT program (1963a) was able to solve closed-form integration problems typical of first-year college calculus courses. Tom Evans's ANALOGY program (1968) solved geometric analogy problems that appear in IQ tests, such as the one in Figure 1.2. Bertram Raphael's (1968) SIR (Semantic Information Retrieval) was able to accept input statements in a very restricted subset of English and answer questions thereon."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2829,
                                "start": 25
                            }
                        ],
                        "text": "stances cannot be solved in any reasonable time. Therefore, one should strive to divide the overall problem of generating intelligent behavior into tractable subproblems rather than intractable ones. REDUCTION The second important concept in the theory of complexity is reduction, which also emerged in the 1960s (Dantzig, 1960; Edmonds, 1962). A reduction is a general transformation from one class of problems to another, such that solutions to the first class can be found by reducing them to problems of the second class and solving the latter problems. NP COMPLETENESS How can one recognize an intractable problem? The theory of NP-completeness, pioneered by Steven Cook (1971) and Richard Karp (1972), provides a method. Cook and Karp showed the existence of large classes of canonical combinatorial search and reasoning problems that are NP-complete. Any problem class to which an NP-complete problem class can be reduced is likely to be intractable. (Although it has not yet been proved that NP-complete problems are necessarily intractable, few theoreticians believe otherwise.) These results contrast sharply with the \"Electronic Super-Brain\" enthusiasm accompanying the advent of computers. Despite the ever-increasing speed of computers, subtlety and careful use of resources will characterize intelligent systems. Put crudely, the world is an extremely large problem instance! Besides logic and computation, the third great contribution of mathematics to AI is the j theory of probability. The Italian Gerolamo Cardano (1501-1576) first framed the idea of I probability, describing it in terms of the possible outcomes of gambling events. Before his time, j the outcomes of gambling games were seen as the will of the gods rather than the whim of chance, i Probability quickly became an invaluable part of all the quantitative sciences, helping to deal with uncertain measurements and incomplete theories. Pierre Fermat (1601-1665), Blaise Pascal I (1623-1662), James Bernoulli (1654-1705), Pierre Laplace (1749-1827), and others advanced j the theory and introduced new statistical methods. Bernoulli also framed an alternative view] of probability, as a subjective \"degree of belief\" rather than an objective ratio of outcomes.! Subjective probabilities therefore can be updated as new evidence is obtained. Thomas Bayes j (1702-1761) proposed a rule for updating subjective probabilities in the light of new evidence! (published posthumously in 1763). Bayes' rule, and the subsequent field of Bayesian analysis,! form the basis of the modern approach to uncertain reasoning in AI systems. Debate still rages j between supporters of the objective and subjective views of probability, but it is not clear if the! difference has great significance for AI. Both versions obey the same set of axioms. Savage'sJ (1954) Foundations of Statistics gives a good introduction to the field."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 652,
                                "start": 6
                            }
                        ],
                        "text": "logic included Cordell Green's question answering and planning systems (Green, 1969b), and the Shakey robotics project at the new Stanford Research Institute (SRI). The latter project, discussed further in Chapter 25, was the first to demonstrate the complete integration of logical reasoning and physical activity. Minsky supervised a series of students who chose limited problems that appeared to require intelligence to solve. These limited domains became known as microworlds. James Slagle's SAINT program (1963a) was able to solve closed-form integration problems typical of first-year college calculus courses. Tom Evans's ANALOGY program (1968) solved geometric analogy problems that appear in IQ tests, such as the one in Figure 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Language is ambiguous and leaves much unsaid"
            },
            "venue": {
                "fragments": [],
                "text": "This means that understanding language requires an understanding of the subject matter and context, not just an understanding of the structure of sentences. This may seem obvious, but it was not appreciated until the early 1960s. Much of the early work in knowledge representation ( the study of how "
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123881193"
                        ],
                        "name": "E. Rich",
                        "slug": "E.-Rich",
                        "structuredName": {
                            "firstName": "Elaine",
                            "lastName": "Rich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 61
                            }
                        ],
                        "text": "These books thus fall into the same general category as, say [2,5,6], but have been updated for the mid\u2013late 1990s."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5593054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6da97c8b148b06b820b0d65f73caa41df5fa172a",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Artificial-intelligence-(2.-ed.)-Rich-Knight",
            "title": {
                "fragments": [],
                "text": "Artificial intelligence (2. ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "6 Representing Change in the World ........................ 203 Situation calculus ................................. 204 Keeping track of location"
            },
            "venue": {
                "fragments": [],
                "text": "6 Representing Change in the World ........................ 203 Situation calculus ................................. 204 Keeping track of location"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Simple Reflex Agent .............................. 202 Limitations of simple reflex agents"
            },
            "venue": {
                "fragments": [],
                "text": "A Simple Reflex Agent .............................. 202 Limitations of simple reflex agents"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 203
                            }
                        ],
                        "text": "Ironically, the new back-propagation learning algorithms for multilayer networks that were to cause an enormous resurgence in neural net research in the late 1980s were actually discovered first in 1969 (Bryson and Ho, 1969)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The algorithm was applied to many learning problems in computer science and psychology, and the widespread dissemination of the results in the collection Parallel Distributed Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "claiming, \"There is no such thing as syntax,\" which upset a lot of linguists, but did serve to start a useful discussion. Schank and his students built a series of programs"
            },
            "venue": {
                "fragments": [],
                "text": "At Yale,"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "showed how a large number of elements could collectively represent an individual concept, with a corresponding increase in robustness and parallelism"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Toward a Goal-Based Agent"
            },
            "venue": {
                "fragments": [],
                "text": "Toward a Goal-Based Agent"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1966), introduced the internal state of the agent into the picture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "538 Practical uses of decision tree learning"
            },
            "venue": {
                "fragments": [],
                "text": "532 Inducing decision trees from examples ...................... 534 Assessing the performance of the learning algorithm .............. 542 Broadening the applicability of decision Irees ................... 543 18"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence gives a complete history of the field, and Raymond"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "211 18 Learning from Observations 525 18.1 A General Model of Learning Agents ....................... 525 Components"
            },
            "venue": {
                "fragments": [],
                "text": "211 18 Learning from Observations 525 18.1 A General Model of Learning Agents ....................... 525 Components"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deducing Hidden Properties of the World . .................... Preferences Among Actions"
            },
            "venue": {
                "fragments": [],
                "text": "Deducing Hidden Properties of the World . .................... Preferences Among Actions"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "It is important because exponential growth means that even moderate-sized inTo understand why Frege's notation was not universally adopted, see the cover of this book"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1969; 1978b) helped to synthesize these viewpoints into a coherent \"intentional"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 19,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Artificial-Intelligence:-A-Modern-Approach-Russell-Norvig/3524cdf7cf8344e7eb74886f71fcbb5c6732c337?sort=total-citations"
}