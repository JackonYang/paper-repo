{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237361"
                        ],
                        "name": "Ranjay Krishna",
                        "slug": "Ranjay-Krishna",
                        "structuredName": {
                            "firstName": "Ranjay",
                            "lastName": "Krishna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ranjay Krishna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144421225"
                        ],
                        "name": "Michael Stark",
                        "slug": "Michael-Stark",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Stark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760364"
                        ],
                        "name": "David A. Shamma",
                        "slug": "David-A.-Shamma",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shamma",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Shamma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 42
                            }
                        ],
                        "text": "Oracle results Compared to the results of Johnson et al. (2015), the results of our oracle systems are significantly worse."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 7
                            }
                        ],
                        "text": "3As in Johnson et al. (2015), we observed that the results for recall at 1 were very unstable so we only report recall at 5 and 10 which are typically also more relevant for real-world systems that return multiple results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 46
                            }
                        ],
                        "text": "We train our reimplementation of the model by Johnson et al. (2015) on our training set with human-constructed scene graphs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 15
                            }
                        ],
                        "text": "We also follow Johnson et al. (2015) and discard all objects, relations and attributes whose class or type appears less than 30 times in the entire dataset\nfor the following two reasons."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Johnson et al. (2015) try to solve these problems by annotating images with a graph-based semantic representation called a scene graph which explicitly captures the objects in an image, their attributes and the relations between objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 117
                            }
                        ],
                        "text": "We reuse a dataset which we collected for a different task using Amazon Mechanical Turk (AMT) in a similar manner as Johnson et al. (2015) and Plummer et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 59
                            }
                        ],
                        "text": "For the latter, we use a reimplementation of the system by Johnson et al. (2015) which we briefly describe in the next section."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 30
                            }
                        ],
                        "text": "The image retrieval system by Johnson et al. (2015) is based on a conditional random field (CRF) (Lafferty et al., 2001) model which \u2013 unlike the typical CRFs in NLP \u2013 is not a chain model but instead capturing image region proximity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16414666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85ae705ef4353c6854f5be4a4664269d6317c66b",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops a novel framework for semantic image retrieval based on the notion of a scene graph. Our scene graphs represent objects (\u201cman\u201d, \u201cboat\u201d), attributes of objects (\u201cboat is white\u201d) and relationships between objects (\u201cman standing on boat\u201d). We use these scene graphs as queries to retrieve semantically related images. To this end, we design a conditional random field model that reasons about possible groundings of scene graphs to test images. The likelihoods of these groundings are used as ranking scores for retrieval. We introduce a novel dataset of 5,000 human-generated scene graphs grounded to images and use this dataset to evaluate our method for image retrieval. In particular, we evaluate retrieval using full scene graphs and small scene subgraphs, and show that our method outperforms retrieval methods that use only objects or low-level image features. In addition, we show that our full model can be used to improve object localization compared to baseline methods."
            },
            "slug": "Image-retrieval-using-scene-graphs-Johnson-Krishna",
            "title": {
                "fragments": [],
                "text": "Image retrieval using scene graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A conditional random field model that reasons about possible groundings of scene graphs to test images and shows that the full model can be used to improve object localization compared to baseline methods and outperforms retrieval methods that use only objects or low-level image features."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909300"
                        ],
                        "name": "Lucy Vanderwende",
                        "slug": "Lucy-Vanderwende",
                        "structuredName": {
                            "firstName": "Lucy",
                            "lastName": "Vanderwende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucy Vanderwende"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 83
                            }
                        ],
                        "text": "For example, we can reuse a scene graph parser for systems that generate 2D-scenes (Zitnick et al., 2013) or 3Dscenes (Chang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 84
                            }
                        ],
                        "text": "For example, we can reuse a scene graph parser for systems that generate 2D-scenes (Zitnick et al., 2013) or 3Dscenes (Chang et al., 2014) which require input in the form of similar graph-based representations to which a scene graph can be easily converted."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5642345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c39f56c3c21c3972c362f8e752be57a50c41f4f",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentences that describe visual scenes contain a wide variety of information pertaining to the presence of objects, their attributes and their spatial relations. In this paper we learn the visual features that correspond to semantic phrases derived from sentences. Specifically, we extract predicate tuples that contain two nouns and a relation. The relation may take several forms, such as a verb, preposition, adjective or their combination. We model a scene using a Conditional Random Field (CRF) formulation where each node corresponds to an object, and the edges to their relations. We determine the potentials of the CRF using the tuples extracted from the sentences. We generate novel scenes depicting the sentences' visual meaning by sampling from the CRF. The CRF is also used to score a set of scenes for a text-based image retrieval task. Our results show we can generate (retrieve) scenes that convey the desired semantic meaning, even when scenes (queries) are described by multiple sentences. Significant improvement is found over several baseline approaches."
            },
            "slug": "Learning-the-Visual-Interpretation-of-Sentences-Zitnick-Parikh",
            "title": {
                "fragments": [],
                "text": "Learning the Visual Interpretation of Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper extracts predicate tuples that contain two nouns and a relation from sentences to generate novel scenes depicting the sentences' visual meaning by sampling from the Conditional Random Field (CRF)."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50369944"
                        ],
                        "name": "Desmond Elliott",
                        "slug": "Desmond-Elliott",
                        "structuredName": {
                            "firstName": "Desmond",
                            "lastName": "Elliott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Desmond Elliott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143694777"
                        ],
                        "name": "Frank Keller",
                        "slug": "Frank-Keller",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Keller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Elliott et al. (2014)\nparse image descriptions to a dependency grammar representation which they also use for image retrieval."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 304957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4f34bbe4531dd73334b70278e7ac6b167573531",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Image retrieval models typically represent images as bags-of-terms, a representation that is wellsuited to matching images based on the presence or absence of terms. For some information needs, such as searching for images of people performing actions, it may be useful to retain data about how parts of an image relate to each other. If the underlying representation of an image can distinguish between images where objects only co-occur from images where people are interacting with objects, then it should be possible to improve retrieval performance. In this paper we model the spatial relationships between image regions using Visual Dependency Representations, a structured image representation that makes it possible to distinguish between object co-occurrence and interaction. In a query-by-example image retrieval experiment on data set of people performing actions, we find an 8.8% relative increase in MAP and an 8.6% relative increase in Precision@10 when images are represented using the Visual Dependency Representation compared to a bag-of-terms baseline."
            },
            "slug": "Query-by-Example-Image-Retrieval-using-Visual-Elliott-Lavrenko",
            "title": {
                "fragments": [],
                "text": "Query-by-Example Image Retrieval using Visual Dependency Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper model the spatial relationships between image regions using Visual Dependency Representations, a structured image representation that makes it possible to distinguish between object co-occurrence and interaction."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807606"
                        ],
                        "name": "Dahua Lin",
                        "slug": "Dahua-Lin",
                        "structuredName": {
                            "firstName": "Dahua",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dahua Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144332826"
                        ],
                        "name": "Chen Kong",
                        "slug": "Chen-Kong",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Kong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Kong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 117
                            }
                        ],
                        "text": "We originally annotated 4,999 images from the intersection of the YFCC100m (Thomee et al., 2015) and Microsoft COCO (Lin et al., 2014b) datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Lin et al. (2014a) also use rules to transform dependency trees into semantic graphs which they use for video search."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1753262,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7afd833f484c8032e7fdc5f53188d2ebb0fb9934",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we tackle the problem of retrieving videos using complex natural language queries. Towards this goal, we first parse the sentential descriptions into a semantic graph, which is then matched to visual concepts using a generalized bipartite matching algorithm. Our approach exploits object appearance, motion and spatial relations, and learns the importance of each term using structure prediction. We demonstrate the effectiveness of our approach on a new dataset designed for semantic search in the context of autonomous driving, which exhibits complex and highly dynamic scenes with many objects. We show that our approach is able to locate a major portion of the objects described in the query with high accuracy, and improve the relevance in video retrieval."
            },
            "slug": "Visual-Semantic-Search:-Retrieving-Videos-via-Lin-Fidler",
            "title": {
                "fragments": [],
                "text": "Visual Semantic Search: Retrieving Videos via Complex Textual Queries"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper first parse the sentential descriptions into a semantic graph, which is then matched to visual concepts using a generalized bipartite matching algorithm, and learns the importance of each term using structure prediction."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856622"
                        ],
                        "name": "Bryan A. Plummer",
                        "slug": "Bryan-A.-Plummer",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Plummer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan A. Plummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39060743"
                        ],
                        "name": "Liwei Wang",
                        "slug": "Liwei-Wang",
                        "structuredName": {
                            "firstName": "Liwei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liwei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6648406"
                        ],
                        "name": "Christopher M. Cervantes",
                        "slug": "Christopher-M.-Cervantes",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Cervantes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher M. Cervantes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507543"
                        ],
                        "name": "Juan C. Caicedo",
                        "slug": "Juan-C.-Caicedo",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Caicedo",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan C. Caicedo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 143
                            }
                        ],
                        "text": "We reuse a dataset which we collected for a different task using Amazon Mechanical Turk (AMT) in a similar manner as Johnson et al. (2015) and Plummer et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 154
                            }
                        ],
                        "text": "Multi-modal embeddings Recently, multimodal embeddings of natural language and images got a lot of attention (Socher et al., 2014; Karpathy et al., 2014; Plummer et al., 2015; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 172
                            }
                        ],
                        "text": "One proposed solution to these issues is the mapping of image descriptions to multi-modal embeddings of sentences and images and using these embeddings to retrieve images (Plummer et al., 2015; Karpathy et al., 2014; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6941275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0612745dbd292fc0a548a16d39cd73e127faedde",
            "isKey": false,
            "numCitedBy": 664,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research."
            },
            "slug": "Flickr30k-Entities:-Collecting-Region-to-Phrase-for-Plummer-Wang",
            "title": {
                "fragments": [],
                "text": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper presents Flickr30K Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832513"
                        ],
                        "name": "Behjat Siddiquie",
                        "slug": "Behjat-Siddiquie",
                        "structuredName": {
                            "firstName": "Behjat",
                            "lastName": "Siddiquie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Behjat Siddiquie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723233"
                        ],
                        "name": "R. Feris",
                        "slug": "R.-Feris",
                        "structuredName": {
                            "firstName": "Rog\u00e9rio",
                            "lastName": "Feris",
                            "middleNames": [
                                "Schmidt"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Feris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 42
                            }
                        ],
                        "text": "Rui et al. (1999), Liu et al. (2007), and Siddiquie et al. (2011) provide overviews of the developments of this field over the last twenty years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3163675,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc00536941717ddd69b855afb31eb10a7c0043b5",
            "isKey": false,
            "numCitedBy": 368,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach for ranking and retrieval of images based on multi-attribute queries. Existing image retrieval methods train separate classifiers for each word and heuristically combine their outputs for retrieving multiword queries. Moreover, these approaches also ignore the interdependencies among the query terms. In contrast, we propose a principled approach for multi-attribute retrieval which explicitly models the correlations that are present between the attributes. Given a multi-attribute query, we also utilize other attributes in the vocabulary which are not present in the query, for ranking/retrieval. Furthermore, we integrate ranking and retrieval within the same formulation, by posing them as structured prediction problems. Extensive experimental evaluation on the Labeled Faces in the Wild(LFW), FaceTracer and PASCAL VOC datasets show that our approach significantly outperforms several state-of-the-art ranking and retrieval methods."
            },
            "slug": "Image-ranking-and-retrieval-based-on-queries-Siddiquie-Feris",
            "title": {
                "fragments": [],
                "text": "Image ranking and retrieval based on multi-attribute queries"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work proposes a principled approach for multi-attribute retrieval which explicitly models the correlations that are present between the attributes in the vocabulary, and integrates ranking and retrieval within the same formulation."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 110
                            }
                        ],
                        "text": "Multi-modal embeddings Recently, multimodal embeddings of natural language and images got a lot of attention (Socher et al., 2014; Karpathy et al., 2014; Plummer et al., 2015; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2317858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ca7d208ff8d81377e0eaa9723820aeae7a7322d",
            "isKey": false,
            "numCitedBy": 784,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image."
            },
            "slug": "Grounded-Compositional-Semantics-for-Finding-and-Socher-Karpathy",
            "title": {
                "fragments": [],
                "text": "Grounded Compositional Semantics for Finding and Describing Images with Sentences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The DT-RNN model, which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences, outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa."
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the Association for Computational Linguistics"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49420283"
                        ],
                        "name": "Y. Liu",
                        "slug": "Y.-Liu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865467"
                        ],
                        "name": "Dengsheng Zhang",
                        "slug": "Dengsheng-Zhang",
                        "structuredName": {
                            "firstName": "Dengsheng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengsheng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748411"
                        ],
                        "name": "G. Lu",
                        "slug": "G.-Lu",
                        "structuredName": {
                            "firstName": "Guojun",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712167"
                        ],
                        "name": "Wei-Ying Ma",
                        "slug": "Wei-Ying-Ma",
                        "structuredName": {
                            "firstName": "Wei-Ying",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Ying Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 19
                            }
                        ],
                        "text": "Rui et al. (1999), Liu et al. (2007), and Siddiquie et al. (2011) provide overviews of the developments of this field over the last twenty years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 106
                            }
                        ],
                        "text": "Keyword-based image retrieval systems are clearly unable to deal with the rich semantics of such a query (Liu et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9160719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c52a42b68696ba85abb12db3b4bdf9e128ed15a",
            "isKey": false,
            "numCitedBy": 1794,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-survey-of-content-based-image-retrieval-with-Liu-Zhang",
            "title": {
                "fragments": [],
                "text": "A survey of content-based image retrieval with high-level semantics"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319608"
                        ],
                        "name": "Armand Joulin",
                        "slug": "Armand-Joulin",
                        "structuredName": {
                            "firstName": "Armand",
                            "lastName": "Joulin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Armand Joulin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 131
                            }
                        ],
                        "text": "Multi-modal embeddings Recently, multimodal embeddings of natural language and images got a lot of attention (Socher et al., 2014; Karpathy et al., 2014; Plummer et al., 2015; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 194
                            }
                        ],
                        "text": "One proposed solution to these issues is the mapping of image descriptions to multi-modal embeddings of sentences and images and using these embeddings to retrieve images (Plummer et al., 2015; Karpathy et al., 2014; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2315434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54",
            "isKey": false,
            "numCitedBy": 740,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit."
            },
            "slug": "Deep-Fragment-Embeddings-for-Bidirectional-Image-Karpathy-Joulin",
            "title": {
                "fragments": [],
                "text": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work introduces a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data and introduces a structured max-margin objective that allows this model to explicitly associate fragments across modalities."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144683841"
                        ],
                        "name": "Jeffrey Flanigan",
                        "slug": "Jeffrey-Flanigan",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Flanigan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Flanigan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38094552"
                        ],
                        "name": "Sam Thomson",
                        "slug": "Sam-Thomson",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Thomson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sam Thomson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712374"
                        ],
                        "name": "J. Carbonell",
                        "slug": "J.-Carbonell",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Carbonell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carbonell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "However, the performance of AMR parsers is still not competitive with the performance of dependency parsers which makes dependency trees are more stable starting point."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 89
                            }
                        ],
                        "text": ", 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "As scene graphs consist of object instances, attributes, and relations and are therefore similar to Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs, we use Smatch F1 (Cai and Knight, 2013) as an additional intrinsic metric."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 229
                            }
                        ],
                        "text": "\u2026with graphs has recently experienced a resurgence caused by the development of the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 189
                            }
                        ],
                        "text": "Parsing to graph-based representations Representing semantic information with graphs has recently experienced a resurgence caused by the development of the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "Considering that AMR graphs are, like dependency trees, very similar to scene graphs, we could have also used this representation and transformed it to scene graphs."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5000956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33a9d1a702eb75da709d26c44aaeb7c2015c870b",
            "isKey": true,
            "numCitedBy": 283,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Meaning Representation (AMR) is a semantic formalism for which a grow- ing set of annotated examples is avail- able. We introduce the first approach to parse sentences into this representa- tion, providing a strong baseline for fu- ture improvement. The method is based on a novel algorithm for finding a maxi- mum spanning, connected subgraph, em- bedded within a Lagrangian relaxation of an optimization problem that imposes lin- guistically inspired constraints. Our ap- proach is described in the general frame- work of structured prediction, allowing fu- ture incorporation of additional features and constraints, and may extend to other formalisms as well. Our open-source sys- tem, JAMR, is available at: http://github.com/jflanigan/jamr"
            },
            "slug": "A-Discriminative-Graph-Based-Parser-for-the-Meaning-Flanigan-Thomson",
            "title": {
                "fragments": [],
                "text": "A Discriminative Graph-Based Parser for the Abstract Meaning Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The first approach to parse sentences into meaning representation, a semantic formalism for which a grow- ing set of annotated examples is available, is introduced, providing a strong baseline for improvement."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145830541"
                        ],
                        "name": "Angel X. Chang",
                        "slug": "Angel-X.-Chang",
                        "structuredName": {
                            "firstName": "Angel",
                            "lastName": "Chang",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angel X. Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295141"
                        ],
                        "name": "M. Savva",
                        "slug": "M.-Savva",
                        "structuredName": {
                            "firstName": "Manolis",
                            "lastName": "Savva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Savva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3029023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db3c398db2c6d51d413b7e47b285d58bc73acf37",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the grounding of natural language to concrete spatial constraints, and inference of implicit pragmatics in 3D environments. We apply our approach to the task of text-to-3D scene generation. We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data. In text-to3D scene generation, a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene. The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene. We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes."
            },
            "slug": "Learning-Spatial-Knowledge-for-Text-to-3D-Scene-Chang-Savva",
            "title": {
                "fragments": [],
                "text": "Learning Spatial Knowledge for Text to 3D Scene Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The main innovation of this work is to show how to augment explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 27
                            }
                        ],
                        "text": ", 2015) and Microsoft COCO (Lin et al., 2014b) datasets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 117
                            }
                        ],
                        "text": "We originally annotated 4,999 images from the intersection of the YFCC100m (Thomee et al., 2015) and Microsoft COCO (Lin et al., 2014b) datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Lin et al. (2014a) also use rules to transform dependency trees into semantic graphs which they use for video search."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14113767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "isKey": false,
            "numCitedBy": 19779,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
            },
            "slug": "Microsoft-COCO:-Common-Objects-in-Context-Lin-Maire",
            "title": {
                "fragments": [],
                "text": "Microsoft COCO: Common Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding by gathering images of complex everyday scenes containing common objects in their natural context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 176
                            }
                        ],
                        "text": "Multi-modal embeddings Recently, multimodal embeddings of natural language and images got a lot of attention (Socher et al., 2014; Karpathy et al., 2014; Plummer et al., 2015; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 217
                            }
                        ],
                        "text": "One proposed solution to these issues is the mapping of image descriptions to multi-modal embeddings of sentences and images and using these embeddings to retrieve images (Plummer et al., 2015; Karpathy et al., 2014; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7732372,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e36ea91a3c8fbff92be2989325531b4002e2afc",
            "isKey": false,
            "numCitedBy": 1055,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison."
            },
            "slug": "Unifying-Visual-Semantic-Embeddings-with-Multimodal-Kiros-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work introduces the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder, and shows that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2795219"
                        ],
                        "name": "Keenon Werling",
                        "slug": "Keenon-Werling",
                        "structuredName": {
                            "firstName": "Keenon",
                            "lastName": "Werling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keenon Werling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32301760"
                        ],
                        "name": "Gabor Angeli",
                        "slug": "Gabor-Angeli",
                        "structuredName": {
                            "firstName": "Gabor",
                            "lastName": "Angeli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gabor Angeli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "However, the performance of AMR parsers is still not competitive with the performance of dependency parsers which makes dependency trees are more stable starting point."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 89
                            }
                        ],
                        "text": ", 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "As scene graphs consist of object instances, attributes, and relations and are therefore similar to Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs, we use Smatch F1 (Cai and Knight, 2013) as an additional intrinsic metric."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 271
                            }
                        ],
                        "text": "\u2026with graphs has recently experienced a resurgence caused by the development of the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 189
                            }
                        ],
                        "text": "Parsing to graph-based representations Representing semantic information with graphs has recently experienced a resurgence caused by the development of the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "Considering that AMR graphs are, like dependency trees, very similar to scene graphs, we could have also used this representation and transformed it to scene graphs."
                    },
                    "intents": []
                }
            ],
            "corpusId": 500600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e4513ad6d7aa6358acabba868739ac19ae87ce6",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The Abstract Meaning Representation (AMR) is a representation for open-domain rich semantics, with potential use in fields like event extraction and machine translation. Node generation, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs by transformations on spans of text, which allows for more robust learning of this stage. Our set of construction actions generalize better than the previous approach, and can be learned with a simple classifier. We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end performance by 3 F$_1$ on both the LDC2013E117 and LDC2014T12 datasets."
            },
            "slug": "Robust-Subgraph-Generation-Improves-Abstract-Werling-Angeli",
            "title": {
                "fragments": [],
                "text": "Robust Subgraph Generation Improves Abstract Meaning Representation Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work improves on the previous state-of-the-art result for AMR parsing, boosting end-to-end performance by 3 F$_1$ on both the LDC2013E117 and LDC2014T12 datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47074942"
                        ],
                        "name": "Chuan Wang",
                        "slug": "Chuan-Wang",
                        "structuredName": {
                            "firstName": "Chuan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702849"
                        ],
                        "name": "Nianwen Xue",
                        "slug": "Nianwen-Xue",
                        "structuredName": {
                            "firstName": "Nianwen",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nianwen Xue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735131"
                        ],
                        "name": "Sameer Pradhan",
                        "slug": "Sameer-Pradhan",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Pradhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Pradhan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "However, the performance of AMR parsers is still not competitive with the performance of dependency parsers which makes dependency trees are more stable starting point."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 89
                            }
                        ],
                        "text": ", 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "As scene graphs consist of object instances, attributes, and relations and are therefore similar to Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs, we use Smatch F1 (Cai and Knight, 2013) as an additional intrinsic metric."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 252
                            }
                        ],
                        "text": "\u2026with graphs has recently experienced a resurgence caused by the development of the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 189
                            }
                        ],
                        "text": "Parsing to graph-based representations Representing semantic information with graphs has recently experienced a resurgence caused by the development of the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "Considering that AMR graphs are, like dependency trees, very similar to scene graphs, we could have also used this representation and transformed it to scene graphs."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15344879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91830ca68422d6b42446631eeef696c84e602e87",
            "isKey": true,
            "numCitedBy": 152,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a two-stage framework to parse a sentence into its Abstract Meaning Representation (AMR). We first use a dependency parser to generate a dependency tree for the sentence. In the second stage, we design a novel transition-based algorithm that transforms the dependency tree to an AMR graph. There are several advantages with this approach. First, the dependency parser can be trained on a training set much larger than the training set for the tree-to-graph algorithm, resulting in a more accurate AMR parser overall. Our parser yields an improvement of 5% absolute in F-measure over the best previous result. Second, the actions that we design are linguistically intuitive and capture the regularities in the mapping between the dependency structure and the AMR of a sentence. Third, our parser runs in nearly linear time in practice in spite of a worst-case complexity ofO(n 2 )."
            },
            "slug": "A-Transition-based-Algorithm-for-AMR-Parsing-Wang-Xue",
            "title": {
                "fragments": [],
                "text": "A Transition-based Algorithm for AMR Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A two-stage framework to parse a sentence into its Abstract Meaning Representation (AMR) by using a dependency parser to generate a dependency tree and a novel transition-based algorithm that transforms the dependency tree to an AMR graph."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2463875"
                        ],
                        "name": "B. Thomee",
                        "slug": "B.-Thomee",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Thomee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Thomee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760364"
                        ],
                        "name": "David A. Shamma",
                        "slug": "David-A.-Shamma",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shamma",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Shamma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797144"
                        ],
                        "name": "G. Friedland",
                        "slug": "G.-Friedland",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Friedland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Friedland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2532460"
                        ],
                        "name": "Benjamin Elizalde",
                        "slug": "Benjamin-Elizalde",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Elizalde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Elizalde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36845351"
                        ],
                        "name": "Karl S. Ni",
                        "slug": "Karl-S.-Ni",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Ni",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl S. Ni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40429372"
                        ],
                        "name": "Douglas N. Poland",
                        "slug": "Douglas-N.-Poland",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Poland",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas N. Poland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772549"
                        ],
                        "name": "Damian Borth",
                        "slug": "Damian-Borth",
                        "structuredName": {
                            "firstName": "Damian",
                            "lastName": "Borth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damian Borth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": "We originally annotated 4,999 images from the intersection of the YFCC100m (Thomee et al., 2015) and Microsoft COCO (Lin et al., 2014b) datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 75
                            }
                        ],
                        "text": "We originally annotated 4,999 images from the intersection of the YFCC100m (Thomee et al., 2015) and Microsoft COCO (Lin et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195345989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6e695ddd07aad719001c0fc1129328452385949",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M), the largest public multimedia collection that has ever been released. The dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014. In this article we explain the rationale behind its creation, as well as the implications the dataset has for science, research, engineering, and development. We further present several new challenges in multimedia research that can now be expanded upon with our dataset."
            },
            "slug": "The-New-Data-and-New-Challenges-in-Multimedia-Thomee-Shamma",
            "title": {
                "fragments": [],
                "text": "The New Data and New Challenges in Multimedia Research"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The rationale behind the creation of the YFCC100M, the largest public multimedia collection that has ever been released, is explained, as well as the implications the dataset has for science, research, engineering, and development."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 166
                            }
                        ],
                        "text": "To predict classes and types, we use an L2-regularized maximum entropy classifier which uses the original word, the lemma and the 100- dimensional GloVe word vector (Pennington et al., 2014) as features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 163
                            }
                        ],
                        "text": "To predict classes and types, we use an L2-regularized maximum entropy classifier which uses the original word, the lemma and the 100dimensional GloVe word vector (Pennington et al., 2014) as features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1957433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "isKey": false,
            "numCitedBy": 22536,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."
            },
            "slug": "GloVe:-Global-Vectors-for-Word-Representation-Pennington-Socher",
            "title": {
                "fragments": [],
                "text": "GloVe: Global Vectors for Word Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods and produces a vector space with meaningful substructure."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3187096"
                        ],
                        "name": "Jerry R. Hobbs",
                        "slug": "Jerry-R.-Hobbs",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Hobbs",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jerry R. Hobbs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 165
                            }
                        ],
                        "text": "(2013)) were too conservative in resolving pronouns and hence we implement an intrasential pronoun resolver inspired by the first three rules of the Hobbs algorithm (Hobbs, 1978) which we modified to operate on dependency trees instead of constituency trees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 54094307,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "61c0eaf156647ffad23921a65e3d7b3296f7afd5",
            "isKey": false,
            "numCitedBy": 646,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Resolving-pronoun-references-Hobbs",
            "title": {
                "fragments": [],
                "text": "Resolving pronoun references"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116624868"
                        ],
                        "name": "Heeyoung Lee",
                        "slug": "Heeyoung-Lee",
                        "structuredName": {
                            "firstName": "Heeyoung",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heeyoung Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145830541"
                        ],
                        "name": "Angel X. Chang",
                        "slug": "Angel-X.-Chang",
                        "structuredName": {
                            "firstName": "Angel",
                            "lastName": "Chang",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angel X. Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3349066"
                        ],
                        "name": "Yves Peirsman",
                        "slug": "Yves-Peirsman",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Peirsman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Peirsman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729918"
                        ],
                        "name": "Nathanael Chambers",
                        "slug": "Nathanael-Chambers",
                        "structuredName": {
                            "firstName": "Nathanael",
                            "lastName": "Chambers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathanael Chambers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760868"
                        ],
                        "name": "M. Surdeanu",
                        "slug": "M.-Surdeanu",
                        "structuredName": {
                            "firstName": "Mihai",
                            "lastName": "Surdeanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Surdeanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 141
                            }
                        ],
                        "text": "Our pronoun resolver has an accuracy of 88.5% which is significantly higher than the accuracy of 52.8% achieved by the coreference system of Lee et al. (2013)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 67
                            }
                        ],
                        "text": "We found in practice that document-level coreference systems (e.g. Lee et al. (2013)) were too conservative in resolving pronouns and hence we implement an intrasential pronoun resolver inspired by the first three rules of the Hobbs algorithm (Hobbs, 1978) which we modified to operate on dependency\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13475584,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3cd7890d3ad50cd1947d099bbd16f6da3b33a78",
            "isKey": false,
            "numCitedBy": 431,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new deterministic approach to coreference resolution that combines the global information and precise features of modern machine-learning models with the transparency and modularity of deterministic, rule-based systems. Our sieve architecture applies a battery of deterministic coreference models one at a time from highest to lowest precision, where each model builds on the previous model's cluster output. The two stages of our sieve-based architecture, a mention detection stage that heavily favors recall, followed by coreference sieves that are precision-oriented, offer a powerful way to achieve both high precision and high recall. Further, our approach makes use of global information through an entity-centric model that encourages the sharing of features across all mentions that point to the same real-world entity. Despite its simplicity, our approach gives state-of-the-art performance on several corpora and genres, and has also been incorporated into hybrid state-of-the-art coreference systems for Chinese and Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics."
            },
            "slug": "Deterministic-Coreference-Resolution-Based-on-Rules-Lee-Chang",
            "title": {
                "fragments": [],
                "text": "Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The two stages of the sieve-based architecture, a mention detection stage that heavily favors recall, followed by coreference sieves that are precision-oriented, offer a powerful way to achieve both high precision and high recall."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36010601"
                        ],
                        "name": "Junhua Mao",
                        "slug": "Junhua-Mao",
                        "structuredName": {
                            "firstName": "Junhua",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junhua Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145738410"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143686417"
                        ],
                        "name": "Yi Yang",
                        "slug": "Yi-Yang",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152924487"
                        ],
                        "name": "Jiang Wang",
                        "slug": "Jiang-Wang",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 196
                            }
                        ],
                        "text": "Multi-modal embeddings Recently, multimodal embeddings of natural language and images got a lot of attention (Socher et al., 2014; Karpathy et al., 2014; Plummer et al., 2015; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 237
                            }
                        ],
                        "text": "One proposed solution to these issues is the mapping of image descriptions to multi-modal embeddings of sentences and images and using these embeddings to retrieve images (Plummer et al., 2015; Karpathy et al., 2014; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3509328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745",
            "isKey": false,
            "numCitedBy": 1008,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/~junhua.mao/m-RNN.html ."
            },
            "slug": "Deep-Captioning-with-Multimodal-Recurrent-Neural-Mao-Xu",
            "title": {
                "fragments": [],
                "text": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The m-RNN model directly models the probability distribution of generating a word given previous words and an image, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112618394"
                        ],
                        "name": "Shu Cai",
                        "slug": "Shu-Cai",
                        "structuredName": {
                            "firstName": "Shu",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shu Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 33
                            }
                        ],
                        "text": ", 2013) graphs, we use Smatch F1 (Cai and Knight, 2013) as an additional intrinsic metric."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11345321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebe23ed507014dc012c15dea0e1b538538cd8505",
            "isKey": false,
            "numCitedBy": 250,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. However, there is no widely-used metric to evaluate wholesentence semantic structures. In this paper, we present smatch, a metric that calculates the degree of overlap between two semantic feature structures. We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study."
            },
            "slug": "Smatch:-an-Evaluation-Metric-for-Semantic-Feature-Cai-Knight",
            "title": {
                "fragments": [],
                "text": "Smatch: an Evaluation Metric for Semantic Feature Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents smatch, a metric that calculates the degree of overlap between two semantic feature structures, and gives an efficient algorithm to compute the metric and shows the results of an inter-annotator agreement study."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 52
                            }
                        ],
                        "text": "(2015) is based on a conditional random field (CRF) (Lafferty et al., 2001) model which \u2013 unlike the typical CRFs in NLP \u2013 is not a chain model but instead capturing image region proximity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 98
                            }
                        ],
                        "text": "The image retrieval system by Johnson et al. (2015) is based on a conditional random field (CRF) (Lafferty et al., 2001) model which \u2013 unlike the typical CRFs in NLP \u2013 is not a chain model but instead capturing image region proximity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13409,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145459057"
                        ],
                        "name": "Y. Rui",
                        "slug": "Y.-Rui",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Rui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Rui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Rui et al. (1999), Liu et al. (2007), and Siddiquie et al. (2011) provide overviews of the developments of this field over the last twenty years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 97
                            }
                        ],
                        "text": "Further, the coverage of the description highly depends on the subjectivity of human perception (Rui et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2910032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fa15f525a9814247ecd7cd93636f278d6d9ab3b",
            "isKey": false,
            "numCitedBy": 2521,
            "numCiting": 220,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides a comprehensive survey of the technical achievements in the research area of image retrieval, especially content-based image retrieval, an area that has been so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified and future promising research directions are suggested."
            },
            "slug": "Image-Retrieval:-Current-Techniques,-Promising-and-Rui-Huang",
            "title": {
                "fragments": [],
                "text": "Image Retrieval: Current Techniques, Promising Directions, and Open Issues"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2756960"
                        ],
                        "name": "Grzegorz Chrupa\u0142a",
                        "slug": "Grzegorz-Chrupa\u0142a",
                        "structuredName": {
                            "firstName": "Grzegorz",
                            "lastName": "Chrupa\u0142a",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grzegorz Chrupa\u0142a"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828538"
                        ],
                        "name": "\u00c1kos K\u00e1d\u00e1r",
                        "slug": "\u00c1kos-K\u00e1d\u00e1r",
                        "structuredName": {
                            "firstName": "\u00c1kos",
                            "lastName": "K\u00e1d\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c1kos K\u00e1d\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103538973"
                        ],
                        "name": "A. Alishahi",
                        "slug": "A.-Alishahi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Alishahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Alishahi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 214
                            }
                        ],
                        "text": "Multi-modal embeddings Recently, multimodal embeddings of natural language and images got a lot of attention (Socher et al., 2014; Karpathy et al., 2014; Plummer et al., 2015; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 255
                            }
                        ],
                        "text": "One proposed solution to these issues is the mapping of image descriptions to multi-modal embeddings of sentences and images and using these embeddings to retrieve images (Plummer et al., 2015; Karpathy et al., 2014; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13276568,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36da622533db1b55e6e918b81d47b96975d729d7",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Like humans, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases."
            },
            "slug": "Learning-language-through-pictures-Chrupa\u0142a-K\u00e1d\u00e1r",
            "title": {
                "fragments": [],
                "text": "Learning language through pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48151401"
                        ],
                        "name": "Raffaele Simone",
                        "slug": "Raffaele-Simone",
                        "structuredName": {
                            "firstName": "Raffaele",
                            "lastName": "Simone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raffaele Simone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49756792"
                        ],
                        "name": "F. Masini",
                        "slug": "F.-Masini",
                        "structuredName": {
                            "firstName": "Francesca",
                            "lastName": "Masini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Masini"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 126
                            }
                        ],
                        "text": "Several common expressions with light nouns such as a lot of or a dozen of semantically act like quantificational determiners (Simone and Masini, 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123787841,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "49c6623545fe7ae031806f2e01e16c8e8b3a0608",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the hypothesis that word classes are not homogeneous, but rather complex sets of entities with (partially) different properties that can be arranged along a scale. Whereas much work has been done on subclasses of verbs, and on scales of \u2018verbiness\u2019 (e.g., full-fledged verbs vs. verbs with a grammatical function), less attention has been paid to scales of \u2018nouniness\u2019. This paper focuses on a set of nouns\u00a0\u2013 called \u2018Light Nouns\u2019 here\u00a0\u2013 that occur as N1 in binominal constructions of the N1 of N2 type in languages such as Italian, English, and French. In this position Light Nouns tend to lose their lexical content and take on a more grammatical meaning that serves to modify N2. We discuss various types of Light Nouns (among which Taxonomic Nouns, Approximators, Quantifiers, and Support Nouns), measure their referentiality according to a set of parameters, and finally rank them along a scale."
            },
            "slug": "On-Light-Nouns-Simone-Masini",
            "title": {
                "fragments": [],
                "text": "On Light Nouns"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48151401"
                        ],
                        "name": "Raffaele Simone",
                        "slug": "Raffaele-Simone",
                        "structuredName": {
                            "firstName": "Raffaele",
                            "lastName": "Simone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raffaele Simone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49756792"
                        ],
                        "name": "F. Masini",
                        "slug": "F.-Masini",
                        "structuredName": {
                            "firstName": "Francesca",
                            "lastName": "Masini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Masini"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 127
                            }
                        ],
                        "text": "Several common expressions with light nouns such as a lot of or a dozen of semantically act like quantificational determiners (Simone and Masini, 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 218
                            }
                        ],
                        "text": "While the main purpose of the two-pass data collection was to increase the number of objects and relations per image, it also provides as a byproduct a mapping between sentences and partial scene graphs which gives us a corpus of sentence-scene graph pairs that we can use to train a parser."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117835576,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "e1872bc9cd632215711118a68aad7eff5124b2a3",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Foreword and acknowledgements 2. Introduction: New approaches to old Word Class issues (by Simone, Raffaele) 3. PART I. Types of word classes 4. Carving verb classes from corpora (by Lenci, Alessandro) 5. Classes of creation verbs (by Jezek, Elisabetta) 6. On Light Nouns (by Simone, Raffaele) 7. The 'new adjectives' of Tswana (by Creissels, Denis) 8. The Chinese adjective as a word class (by Arcodia, Giorgio F.) 9. Qualifying modifier encoding and adjectival typology (by Alfieri, Luca) 10. PART II. Minor and neglected word classes 11. Hungarian is a classifier language (by Csirmaz, Aniko) 12. Cardinal numerals: A syntax-semantics interface analysis (by Pannain, Rossella) 13. On the borders of neglected word classes: From preverbs to 'satellites' via adverbs and particles (by Sores, Anna) 14. PART III. Across word classes 15. Between adjective and noun: Category/function mismatch, constructional overrides and coercion (by Lauwers, Peter) 16. On decategorization and its relevance in German (by Gaeta, Livio) 17. PART IV. Radical views: Do we really need word classes? 18. Word-class features and reduplicative meaning: The case of Modern Greek (by Kallergi, Haritini) 19. Half and other unique words: Corpus patterns and lexicalist syntax (by Janssen, Maarten) 20. Index of languages 21. Index of names 22. Index of subjects"
            },
            "slug": "Word-classes-:-nature,-typology-and-representations-Simone-Masini",
            "title": {
                "fragments": [],
                "text": "Word classes : nature, typology and representations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2241127"
                        ],
                        "name": "Marie-Catherine de Marneffe",
                        "slug": "Marie-Catherine-de-Marneffe",
                        "structuredName": {
                            "firstName": "Marie-Catherine",
                            "lastName": "Marneffe",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marie-Catherine de Marneffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277385"
                        ],
                        "name": "Timothy Dozat",
                        "slug": "Timothy-Dozat",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Dozat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Dozat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145851426"
                        ],
                        "name": "Natalia Silveira",
                        "slug": "Natalia-Silveira",
                        "structuredName": {
                            "firstName": "Natalia",
                            "lastName": "Silveira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Natalia Silveira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1837036"
                        ],
                        "name": "Katri Haverinen",
                        "slug": "Katri-Haverinen",
                        "structuredName": {
                            "firstName": "Katri",
                            "lastName": "Haverinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katri Haverinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694491"
                        ],
                        "name": "Filip Ginter",
                        "slug": "Filip-Ginter",
                        "structuredName": {
                            "firstName": "Filip",
                            "lastName": "Ginter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filip Ginter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720988"
                        ],
                        "name": "Joakim Nivre",
                        "slug": "Joakim-Nivre",
                        "structuredName": {
                            "firstName": "Joakim",
                            "lastName": "Nivre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joakim Nivre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 29
                            }
                        ],
                        "text": "A Universal Dependencies (de Marneffe et al., 2014) parse is in many ways close to a shallow semantic representation and therefore a good starting point for parsing image descriptions to scene\ngraphs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 109
                            }
                        ],
                        "text": "While one of the principles of the Universal Dependencies representation is the primacy of content words (de Marneffe et al., 2014), light nouns are treated like any other noun."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10914266,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "16b26c3bfb3feef8f2d1889b4e76c15e3f5b6fb7",
            "isKey": false,
            "numCitedBy": 445,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Revisiting the now de facto standard Stanford dependency representation, we propose an improved taxonomy to capture grammatical relations across languages, including morphologically rich ones. We suggest a two-layered taxonomy: a set of broadly attested universal grammatical relations, to which language-specific relations can be added. We emphasize the lexicalist stance of the Stanford Dependencies, which leads to a particular, partially new treatment of compounding, prepositions, and morphology. We show how existing dependency schemes for several languages map onto the universal taxonomy proposed here and close with consideration of practical implications of dependency representation choices for NLP applications, in particular parsing."
            },
            "slug": "Universal-Stanford-dependencies:-A-cross-linguistic-Marneffe-Dozat",
            "title": {
                "fragments": [],
                "text": "Universal Stanford dependencies: A cross-linguistic typology"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a two-layered taxonomy: a set of broadly attested universal grammatical relations, to which language-specific relations can be added, and a lexicalist stance of the Stanford Dependencies, which leads to a particular, partially new treatment of compounding, prepositions, and morphology."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3460261"
                        ],
                        "name": "Laura Banarescu",
                        "slug": "Laura-Banarescu",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Banarescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura Banarescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202888"
                        ],
                        "name": "Claire Bonial",
                        "slug": "Claire-Bonial",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Bonial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Bonial"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112618394"
                        ],
                        "name": "Shu Cai",
                        "slug": "Shu-Cai",
                        "structuredName": {
                            "firstName": "Shu",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shu Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065872210"
                        ],
                        "name": "Madalina Georgescu",
                        "slug": "Madalina-Georgescu",
                        "structuredName": {
                            "firstName": "Madalina",
                            "lastName": "Georgescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Madalina Georgescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3168985"
                        ],
                        "name": "Kira Griffitt",
                        "slug": "Kira-Griffitt",
                        "structuredName": {
                            "firstName": "Kira",
                            "lastName": "Griffitt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kira Griffitt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791311"
                        ],
                        "name": "U. Hermjakob",
                        "slug": "U.-Hermjakob",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Hermjakob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Hermjakob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145755155"
                        ],
                        "name": "Martha Palmer",
                        "slug": "Martha-Palmer",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Palmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martha Palmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145254207"
                        ],
                        "name": "Nathan Schneider",
                        "slug": "Nathan-Schneider",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Schneider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Schneider"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 194
                            }
                        ],
                        "text": "Parsing to graph-based representations Representing semantic information with graphs has recently experienced a resurgence caused by the development of the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 139
                            }
                        ],
                        "text": "As scene graphs consist of object instances, attributes, and relations and are therefore similar to Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs, we use Smatch F1 (Cai and Knight, 2013) as an additional intrinsic metric."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 144
                            }
                        ],
                        "text": "\u2026semantic information with graphs has recently experienced a resurgence caused by the development of the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7771402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e72e5ee5de14fd463ab58ce830474157258e3578",
            "isKey": false,
            "numCitedBy": 962,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it."
            },
            "slug": "Abstract-Meaning-Representation-for-Sembanking-Banarescu-Bonial",
            "title": {
                "fragments": [],
                "text": "Abstract Meaning Representation for Sembanking"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing."
            },
            "venue": {
                "fragments": [],
                "text": "LAW@ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 2
                            }
                        ],
                        "text": "2 (Klein and Manning, 2003)1 and then perform three additional processing steps to deal with complex quantificational modifiers, to resolve pronouns and to handle plural nouns."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11495042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a600850ac0120cb09a0b7de7da80bb6a7a76de06",
            "isKey": false,
            "numCitedBy": 3370,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."
            },
            "slug": "Accurate-Unlexicalized-Parsing-Klein-Manning",
            "title": {
                "fragments": [],
                "text": "Accurate Unlexicalized Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "It is demonstrated that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 66
                            }
                        ],
                        "text": "To re-\n1We augment the parser\u2019s training data with the Brown corpus (Marcus et al., 1993) to improve its performance on image descriptions which are often very different from sentences found in newswire corpora.\ncover all the relations between objects in this sentence it is crucial to know that it\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 60
                            }
                        ],
                        "text": "We augment the parser\u2019s training data with the Brown corpus (Marcus et al., 1993) to improve its performance on image descriptions which are often very different from sentences found in newswire corpora."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8176,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587983"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 73
                            }
                        ],
                        "text": "We ensured diversity in sentence descriptions by a real-time BLEU score (Papineni et al., 2002) threshold between a new sentence and all the previous ones."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11080756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "isKey": false,
            "numCitedBy": 16616,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
            },
            "slug": "Bleu:-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos",
            "title": {
                "fragments": [],
                "text": "Bleu: a Method for Automatic Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 20
                            }
                        ],
                        "text": ", 2013) or 3Dscenes (Chang et al., 2014) which require input in the form of similar graph-based representations to which a scene graph can be easily converted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "For example, we can reuse a scene graph parser for systems that generate 2D-scenes (Zitnick et al., 2013) or 3Dscenes (Chang et al., 2014) which require input in the form of similar graph-based representations to which a scene graph can be easily converted."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 20
                            }
                        ],
                        "text": "As mentioned above, Chang et al. (2014) present a rule-based system to parse natural language descriptions to scene templates, a similar graphbased semantic representation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 70
                            }
                        ],
                        "text": "One of these tasks is generating 3D scenes from textual descriptions (Chang et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning spatial knowledge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cambridge Handbook of Semantics"
            },
            "venue": {
                "fragments": [],
                "text": "Cambridge Handbook of Semantics"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 15,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Generating-Semantically-Precise-Scene-Graphs-from-Schuster-Krishna/2606e6a5759c030e259ebf3f4261b9c04a36a609?sort=total-citations"
}