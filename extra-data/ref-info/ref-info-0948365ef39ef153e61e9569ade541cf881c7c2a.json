{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50263663"
                        ],
                        "name": "D. Boswell",
                        "slug": "D.-Boswell",
                        "structuredName": {
                            "firstName": "Dustin",
                            "lastName": "Boswell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boswell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Kernel-based learning algorithms (see, for example,  Cristianini and Shawe-Taylor, 2000;  Sch\u02dcolkopf and Smola, 2002; Shawe-Taylor and Cristianini, 2004) work by embedding the data into a Hilbert space, and searching for linear relations in such a space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This section describes several such criteria (see, for example,  Cristianini and Shawe-Taylor, 2000;  Sch\u02dcolkopf and Smola, 2002; Shawe-Taylor and Cristianini, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 19
                            }
                        ],
                        "text": "(See, for example, Cristianini and Shawe-Taylor, 2000; Scho\u0308lkopf and Smola, 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "With a flxed kernel, all of these criteria give upper bounds on misclassiflcation probability (see, for example, Chapter 4 of  Cristianini and Shawe-Taylor, 2000 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 126
                            }
                        ],
                        "text": "With a fixed kernel, all of these criteria give upper bounds on misclassification probability (see, for example, Chapter 4 of Cristianini and Shawe-Taylor, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 52
                            }
                        ],
                        "text": "Kernel-based learning algorithms (see, for example, Cristianini and Shawe-Taylor, 2000; Scho\u0308lkopf and Smola, 2002) work by embedding the data into a Hilbert space, and searching for linear relations in such a space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18986102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36",
            "isKey": true,
            "numCitedBy": 2113,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines (SVM\u2019s) are a relatively new learning method used for binary classification. The basic idea is to find a hyperplane which separates the d-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM\u2019s introduce the notion of a \u201ckernel induced feature space\u201d which casts the data into a higher dimensional space where the data is separable. Typically, casting into such a space would cause problems computationally, and with overfitting. The key insight used in SVM\u2019s is that the higher-dimensional space doesn\u2019t need to be dealt with directly (as it turns out, only the formula for the dot-product in that space is needed), which eliminates the above concerns. Furthermore, the VC-dimension (a measure of a system\u2019s likelihood to perform well on unseen data) of SVM\u2019s can be explicitly calculated, unlike other learning methods like neural networks, for which there is no measure. Overall, SVM\u2019s are intuitive, theoretically wellfounded, and have shown to be practically successful. SVM\u2019s have also been extended to solve regression tasks (where the system is trained to output a numerical value, rather than \u201cyes/no\u201d classification)."
            },
            "slug": "Introduction-to-Support-Vector-Machines-Boswell",
            "title": {
                "fragments": [],
                "text": "Introduction to Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "Support Vector Machines (SVM\u2019s) are intuitive, theoretically wellfounded, and have shown to be practically successful."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647543"
                        ],
                        "name": "Erin J. Bredensteiner",
                        "slug": "Erin-J.-Bredensteiner",
                        "structuredName": {
                            "firstName": "Erin",
                            "lastName": "Bredensteiner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erin J. Bredensteiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 128
                            }
                        ],
                        "text": "The classical Lagrange duality theory outlined in the previous section does not directly apply here, since we are not dealing with finitely many constraints in scalar form; as noted earlier, the LMI constraint involves an infinite number of such constraints, of the form (8)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 151
                            }
                        ],
                        "text": "Geometrically, \u03b3 corresponds to the distance between the convex hulls (the smallest convex sets that contain the data in each class) of the two classes (Bennett and Bredensteiner, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1689546,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5cb82f707407336fe4ba82c1e4675670fb668a45",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop an intuitive geometric interpretation of the standard support vector machine (SVM) for classification of both linearly separable and inseparable data and provide a rigorous derivation of the concepts behind the geometry. For the separable case finding the maximum margin between the two sets is equivalent to finding the closest points in the smallest convex sets that contain each class (the convex hulls). We now extend this argument to the inseparable case by using a reduced convex hull reduced away from outliers. We prove that solving the reduced convex hull formulation is exactly equivalent to solving the standard inseparable SVM for appropriate choices of parameters. Some additional advantages of the new formulation are that the effect of the choice of parameters becomes geometrically clear and that the formulation may be solved by fast nearest point algorithms. By changing norms these arguments hold for both the standard 2-norm and 1-norm SVM."
            },
            "slug": "Duality-and-Geometry-in-SVM-Classifiers-Bennett-Bredensteiner",
            "title": {
                "fragments": [],
                "text": "Duality and Geometry in SVM Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An intuitive geometric interpretation of the standard support vector machine for classification of both linearly separable and inseparable data is developed and a rigorous derivation of the concepts behind the geometry is provided."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18608811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "004c20fc5d7fd6f0889fe38f6758db9e8809b61a",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to deal with known limitations of the hard margin support vector machine (SVM) for binary classification \u2014 such as overfitting and the fact that some data sets are not linearly separable \u2014, a soft margin approach has been proposed in literature [2, 4, 5]. The soft margin SVM allows training data to be misclassified to a certain extent, by introducing slack variables and penalizing the cost function with an error term, i.e., the 1-norm or 2-norm of the corresponding slack vector. A regularization parameter C trades off the importance of maximizing the margin versus minimizing the error. While the 2-norm soft margin algorithm itself is well understood, and a generalization bound is known [4, 5], no computationally tractable method for tuning the soft margin parameter C has been proposed so far. In this report we present a convex way to optimize C for the 2-norm soft margin SVM, by maximizing this generalization bound. The resulting problem is a quadratically constrained quadratic programming (QCQP) problem, which can be solved in polynomial time O(l) with l the number of training samples."
            },
            "slug": "Convex-Tuning-of-the-Soft-Margin-Parameter-Bie-Lanckriet",
            "title": {
                "fragments": [],
                "text": "Convex Tuning of the Soft Margin Parameter"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A convex way to optimize C for the 2-norm soft margin SVM, by maximizing this generalization bound, which is a quadratically constrained quadratic programming (QCQP) problem, which can be solved in polynomial time O(l) with l the number of training samples."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145536952"
                        ],
                        "name": "J. Kandola",
                        "slug": "J.-Kandola",
                        "structuredName": {
                            "firstName": "Jaz",
                            "lastName": "Kandola",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kandola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17466014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36aa0d0936b2cf128c646c36a1981807b5a27aaf",
            "isKey": false,
            "numCitedBy": 1048,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction."
            },
            "slug": "On-Kernel-Target-Alignment-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "On Kernel-Target Alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function, is introduced, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on a test set, giving improved classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2014414"
                        ],
                        "name": "L. Vandenberghe",
                        "slug": "L.-Vandenberghe",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Vandenberghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vandenberghe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46238940"
                        ],
                        "name": "Shao-Po Wu",
                        "slug": "Shao-Po-Wu",
                        "structuredName": {
                            "firstName": "Shao-Po",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shao-Po Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8952069,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "97c47de1aae432f90554f3de8f58e90096c54876",
            "isKey": false,
            "numCitedBy": 692,
            "numCiting": 155,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of maximizing the determinant of a matrix subject to linear matrix inequalities (LMIs) arises in many fields, including computational geometry, statistics, system identification, experiment design, and information and communication theory. It can also be considered as a generalization of the semidefinite programming problem. \nWe give an overview of the applications of the determinant maximization problem, pointing out simple cases where specialized algorithms or analytical solutions are known. We then describe an interior-point method, with a simplified analysis of the worst-case complexity and numerical results that indicate that the method is very efficient, both in theory and in practice. Compared to existing specialized algorithms (where they are available), the interior-point method will generally be slower; the advantage is that it handles a much wider variety of problems."
            },
            "slug": "Determinant-Maximization-with-Linear-Matrix-Vandenberghe-Boyd",
            "title": {
                "fragments": [],
                "text": "Determinant Maximization with Linear Matrix Inequality Constraints"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 52
                            }
                        ],
                        "text": "Kernel-based learning algorithms (see, for example, Cristianini and Shawe-Taylor, 2000; Scho\u0308lkopf and Smola, 2002) work by embedding the data into a Hilbert space, and searching for linear relations in such a space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 19
                            }
                        ],
                        "text": "(See, for example, Cristianini and Shawe-Taylor, 2000; Scho\u0308lkopf and Smola, 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 126
                            }
                        ],
                        "text": "With a fixed kernel, all of these criteria give upper bounds on misclassification probability (see, for example, Chapter 4 of Cristianini and Shawe-Taylor, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14727192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c04f8002e24a8c09bfbfedca3c6c346fe1e5d53",
            "isKey": false,
            "numCitedBy": 13352,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software."
            },
            "slug": "An-Introduction-to-Support-Vector-Machines-and-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory, and will guide practitioners to updated literature, new applications, and on-line software."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Such interaction graph allows to establish similarities among proteins through the construction of a corresponding diffusion kernel (Kondor and Lafferty, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2 Protein Function Prediction Here we illustrate the SDP approach for fusing heterogeneous genomic data in order to predict protein function in yeast; see Lanckriet et al. (2004) for more details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5525836,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6320770fe216ebbba769b9f0a006669b616a03d0",
            "isKey": false,
            "numCitedBy": 888,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space."
            },
            "slug": "Diffusion-Kernels-on-Graphs-and-Other-Discrete-Kondor-Lafferty",
            "title": {
                "fragments": [],
                "text": "Diffusion Kernels on Graphs and Other Discrete Input Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea, and focuses on generating kernels on graphs, for which a special class of exponential kernels called diffusion kernels are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1530308,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8aaff7e62c4ce8724298a276a56ac519c804bb74",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) problem. This paper proposes an algorithm for training SVMs: Sequential Minimal Optimization, or SMO. SMO breaks the large QP problem into a series of smallest possible QP problems which are analytically solvable. Thus, SMO does not require a numerical QP library. SMO's computation time is dominated by evaluation of the kernel, hence kernel optimizations substantially quicken SMO. For the MNIST database, SMO is 1.7 times as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be 1500 times faster than the PCG chunking algorithm."
            },
            "slug": "Using-Analytic-QP-and-Sparseness-to-Speed-Training-Platt",
            "title": {
                "fragments": [],
                "text": "Using Analytic QP and Sparseness to Speed Training of Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An algorithm for training SVMs: Sequential Minimal Optimization, or SMO, which breaks the large QP problem into a series of smallest possible QP problems which are analytically solvable and does not require a numerical QP library."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709706"
                        ],
                        "name": "Minghua Deng",
                        "slug": "Minghua-Deng",
                        "structuredName": {
                            "firstName": "Minghua",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghua Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144458655"
                        ],
                        "name": "William Stafford Noble",
                        "slug": "William-Stafford-Noble",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Noble",
                            "middleNames": [
                                "Stafford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Stafford Noble"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Deng et al. (2003),  Lanckriet et al. (2004)  perform two variants of the experiment: one in which the flve kernels are restricted to contain precisely the same binary information as used by the MRF method, and a second experiment in which the richer Pfam and expression kernels are used and the SW kernel is added."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Here we illustrate the SDP approach for fusing heterogeneous genomic data in order to predict protein function in yeast; see  Lanckriet et al. (2004)  for more details."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7412694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "087e8dc46e9ef192e828a2287e153425173e1532",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods provide a principled framework in which to represent many types of data, including vectors, strings, trees and graphs. As such, these methods are useful for drawing inferences about biological phenomena. We describe a method for combining multiple kernel representations in an optimal fashion, by formulating the problem as a convex optimization problem that can be solved using semidefinite programming techniques. The method is applied to the problem of predicting yeast protein functional classifications using a support vector machine (SVM) trained on five types of data. For this problem, the new method performs better than a previously-described Markov random field method, and better than the SVM trained on any single type of data."
            },
            "slug": "Kernel-Based-Data-Fusion-and-Its-Application-to-in-Lanckriet-Deng",
            "title": {
                "fragments": [],
                "text": "Kernel-Based Data Fusion and Its Application to Protein Function Prediction in Yeast"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method for combining multiple kernel representations in an optimal fashion is described, by formulating the problem as a convex optimization problem that can be solved using semidefinite programming techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Pacific Symposium on Biocomputing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14632283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38bdbf7cf0572732bd21b299bdbaf2aab8da959d",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "Approximate Bayesian Gaussian process (GP) classification techniques are powerful non-parametric learning methods, similar in appearance and performance to support vector machines. Based on simple probabilistic models, they render interpretable results and can be embedded in Bayesian frameworks for model selection, feature selection, etc. In this paper, by applying the PAC-Bayesian theorem of McAllester (1999a), we prove distribution-free generalisation error bounds for a wide range of approximate Bayesian GP classification techniques. We also provide a new and much simplified proof for this powerful theorem, making use of the concept of convex duality which is a backbone of many machine learning techniques. We instantiate and test our bounds for two particular GPC techniques, including a recent sparse method which circumvents the unfavourable scaling of standard GP algorithms. As is shown in experiments on a real-world task, the bounds can be very tight for moderate training sample sizes. To the best of our knowledge, these results provide the tightest known distribution-free error bounds for approximate Bayesian GPC methods, giving a strong learning-theoretical justification for the use of these techniques."
            },
            "slug": "PAC-Bayesian-Generalisation-Error-Bounds-for-Seeger",
            "title": {
                "fragments": [],
                "text": "PAC-Bayesian Generalisation Error Bounds for Gaussian Process Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By applying the PAC-Bayesian theorem of McAllester (1999a), this paper proves distribution-free generalisation error bounds for a wide range of approximate Bayesian GP classification techniques, giving a strong learning-theoretical justification for the use of these techniques."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This result is essentially Lemma 3 of (Bartlett and Mendelson, 2002); that lemma contained a similar bound for i.i.d. data, but the same argument holds for fixed data, randomly permuted. Step 4. If the class F of real-valued functions defined on X is closed under negations, R\u03022n(\u03c6 \u25e6 F ) \u2264 R\u03022n(F ), where each f \u2208 F defines a g \u2208 \u03c6 \u25e6 F by g(x, y) = \u03c6(yf(x)). This bound is the contraction lemma of Ledoux and Talagrand (1991). Step 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "See, for example, Proposition 8 and its applications in the work of Bartlett (1998). The result is presented for the 1-norm soft margin classifier, but the proof uses only two properties of the cost function a 5\u2192 max{1 \u2212 a, 0}: that it is an upper bound on the indicator function for a \u2264 0, and that it satisfies a Lipschitz constraint on [0,\u221e)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 685382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015999a72c70a960e59c51078b09c8f672af0d2c",
            "isKey": false,
            "numCitedBy": 1198,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the number of training patterns. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training. The proof techniques appear to be useful for the analysis of other pattern classifiers: when the input domain is a totally bounded metric space, we use the same approach to give upper bounds on misclassification probability for classifiers with decision boundaries that are far from the training examples."
            },
            "slug": "The-Sample-Complexity-of-Pattern-Classification-The-Bartlett",
            "title": {
                "fragments": [],
                "text": "The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This section describes several such criteria (see, for example, Cristianini and Shawe-Taylor, 2000; Sch\u02dcolkopf and Smola, 2002;  Shawe-Taylor and Cristianini, 2004 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Kernel-based learning algorithms (see, for example, Cristianini and Shawe-Taylor, 2000; Sch\u02dcolkopf and Smola, 2002;  Shawe-Taylor and Cristianini, 2004 ) work by embedding the data into a Hilbert space, and searching for linear relations in such a space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 35730151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0f9f3c338dd84b054dabcbd50b725f2b3609ad9",
            "isKey": false,
            "numCitedBy": 4628,
            "numCiting": 184,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods provide a powerful and unified framework for pattern discovery, motivating algorithms that can act on general types of data (e.g. strings, vectors or text) and look for general types of relations (e.g. rankings, classifications, regressions, clusters). The application areas range from neural networks and pattern recognition to machine learning and data mining. This book, developed from lectures and tutorials, fulfils two major roles: firstly it provides practitioners with a large toolkit of algorithms, kernels and solutions ready to use for standard pattern discovery problems in fields such as bioinformatics, text analysis, image analysis. Secondly it provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so."
            },
            "slug": "Kernel-Methods-for-Pattern-Analysis-Shawe-Taylor-Cristianini",
            "title": {
                "fragments": [],
                "text": "Kernel Methods for Pattern Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so."
            },
            "venue": {
                "fragments": [],
                "text": "ICTAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784986"
                        ],
                        "name": "V. Koltchinskii",
                        "slug": "V.-Koltchinskii",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Koltchinskii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltchinskii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611153"
                        ],
                        "name": "D. Panchenko",
                        "slug": "D.-Panchenko",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Panchenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Panchenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The proof technique for the first part of the theorem was introduced by Koltchinskii and Panchenko (2002), who used it to give error bounds for boosting algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The proof of the first part is due to Koltchinskii and Panchenko Koltchinskii and Panchenko (2002), and involves the following five steps: Step 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2307733,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6388150296152c8173fae995e30c80a86d7cf1f7",
            "isKey": false,
            "numCitedBy": 503,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove new probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifiers. Such combinations could be implemented by neural networks or by voting methods of combining the classifiers, such as boosting and bagging. The bounds are in terms of the empirical distribution of the margin of the combined classifier. They are based on the methods of the theory of Gaussian and empirical processes (comparison inequalities, symmetrization method, concentration inequalities) and they improve previous results of Bartlett (1998) on bounding the generalization error of neural networks in terms of 1 -norms of the weights of neurons and of Schapire, Freund, Bartlett and Lee (1998) on bounding the generalization error of boosting. We also obtain rates of convergence in Levy distance of empirical margin distribution to the true margin distribution uniformly over the classes of classifiers and prove the optimality of these rates."
            },
            "slug": "Empirical-margin-distributions-and-bounding-the-of-Koltchinskii-Panchenko",
            "title": {
                "fragments": [],
                "text": "Empirical margin distributions and bounding the generalization error of combined classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "New probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifier combinations, based on the methods of the theory of Gaussian and empirical processes are proved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792144"
                        ],
                        "name": "J. Sturm",
                        "slug": "J.-Sturm",
                        "structuredName": {
                            "firstName": "Jos",
                            "lastName": "Sturm",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sturm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909395"
                        ],
                        "name": "Gerardo A. Guerra",
                        "slug": "Gerardo-A.-Guerra",
                        "structuredName": {
                            "firstName": "Gerardo",
                            "lastName": "Guerra",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerardo A. Guerra"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A general-purpose program such as SeDuMi ( Sturm, 1999 ) handles those problems e\u2010ciently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 146
                            }
                        ],
                        "text": "Note also that the optimal weights \u00b5i, i = 1, . . . , m, can be recovered from the primal-dual solution found by standard software such as SeDuMi (Sturm, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 41
                            }
                        ],
                        "text": "General-purpose programs such as SeDuMi (Sturm, 1999) use interior-point methods to solve SDP problems (Nesterov and Nemirovsky, 1994); they are polynomial time, but have a worst-case complexity O(n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 42
                            }
                        ],
                        "text": "A general-purpose program such as SeDuMi (Sturm, 1999) handles those problems efficiently."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 244
                            }
                        ],
                        "text": "\u2026weights {\u00b5i}3i=1 are optimized according to a hard margin, a 1-norm soft margin and a 2-norm soft margin criterion, respectively; the semi-definite programs (27), (32) and (38) are solved using the general-purpose optimization software SeDuMi (Sturm, 1999), leading to optimal weights {\u00b5\u2217i }3i=1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 135
                            }
                        ],
                        "text": "This QCQP problem is a special form of SDP (Boyd and Vandenberghe, 2001) which can be solved efficiently with programs such as SeDuMi (Sturm, 1999) or Mosek (Andersen and Andersen, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17004369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2ad732c6bc4916c63c9c80eb5cdeb0066bd68b1",
            "isKey": true,
            "numCitedBy": 7068,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "SeDuMi is an add-on for MATLAB, which lets you solve optimization problems with linear, quadratic and semidefiniteness constraints. It is possible to have complex valued data and variables in SeDuMi. Moreover, large scale optimization problems are solved efficiently, by exploiting sparsity. This paper describes how to work with this toolbox."
            },
            "slug": "A-Matlab-toolbox-for-optimization-over-symmetric-Sturm-Guerra",
            "title": {
                "fragments": [],
                "text": "A Matlab toolbox for optimization over symmetric cones"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper describes how to work with SeDuMi, an add-on for MATLAB, which lets you solve optimization problems with linear, quadratic and semidefiniteness constraints by exploiting sparsity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2266267"
                        ],
                        "name": "S. Mendelson",
                        "slug": "S.-Mendelson",
                        "structuredName": {
                            "firstName": "Shahar",
                            "lastName": "Mendelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mendelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 38
                            }
                        ],
                        "text": "This result is essentially Lemma 3 of Bartlett and\nMendelson (2001); that lemma contained a similar bound for i.i.d. Xi, but the same argument holds for fixed Xi, randomly permuted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 78
                            }
                        ],
                        "text": "For the class FK of kernel expansions, notice (as in the proof of Lemma 26 of Bartlett and Mendelson (2001)) that\nR\u03022n(FK) = 1 n E max f\u2208FK 2n\u2211 i=1 \u03c3if(xi)\n= 1 n Emax K\u2208K max \u2016w\u2016\u22641 \u3008w, 2n\u2211 i=1 \u03c3i\u03a6(Xi)\u3009\n= 1 n Emax K\u2208K \u2225\u2225\u2225\u2225\u2225 2n\u2211 i=1 \u03c3i\u03a6(Xi) \u2225\u2225\u2225\u2225\u2225 \u2264 1\nn\n\u221a Emax\nK\u2208K \u03c3\u2032K\u03c3\n= 1 n\n\u221a C(K),\nwhere \u03c3 = (\u03c31, . .\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "So, even when not learning the kernel matrix, this approach can be used to learn the 2-norm soft margin parameter \u03c4 = 1/C automatically."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 463216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "009f35c0e453f2435efd8d8ef8086b76b294967a",
            "isKey": false,
            "numCitedBy": 2177,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines."
            },
            "slug": "Rademacher-and-Gaussian-Complexities:-Risk-Bounds-Bartlett-Mendelson",
            "title": {
                "fragments": [],
                "text": "Rademacher and Gaussian Complexities: Risk Bounds and Structural Results"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This work investigates the use of certain data-dependent estimates of the complexity of a function class called Rademacher and Gaussian complexities and proves general risk bounds in terms of these complexities in a decision theoretic setting."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 65
                            }
                        ],
                        "text": "Data for the twonorm problem data were generated as specified by Breiman (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14669208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "814cf172298d11db0ac9b839440ed8f3db93e438",
            "isKey": false,
            "numCitedBy": 762,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging (Breiman [1996a]) Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire [1995,1996] propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym-arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly"
            },
            "slug": "Arcing-Classifiers-Breiman",
            "title": {
                "fragments": [],
                "text": "Arcing Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two arcing algorithms are explored, they are compared to each other and to bagging, and the definitions of bias and variance for a classifier as components of the test set error are introduced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33133899"
                        ],
                        "name": "E. Andersen",
                        "slug": "E.-Andersen",
                        "structuredName": {
                            "firstName": "Erling",
                            "lastName": "Andersen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Andersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735908"
                        ],
                        "name": "Knud D. Andersen",
                        "slug": "Knud-D.-Andersen",
                        "structuredName": {
                            "firstName": "Knud",
                            "lastName": "Andersen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Knud D. Andersen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 224
                            }
                        ],
                        "text": "Next, the weights {\u00b5i}3i=1 are constrained to be non-negative and optimized according to the same criteria: the second order cone programs (29), (34) and (40) are solved using the general-purpose optimization software Mosek (Andersen and Andersen, 2000), leading to optimal weights {\u00b5\u2217i,+}3i=1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 226
                            }
                        ],
                        "text": "Next, the weights {\u03bci}(3)i=1 are constrained to be non-negative and optimized according to the same criteria: the second order cone programs (29), (34) and (40) are solved using the general-purpose optimization software Mosek (Andersen and Andersen, 2000), leading to optimal weights {\u03bci,+}(3)i=1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 158
                            }
                        ],
                        "text": "This QCQP problem is a special form of SDP (Boyd and Vandenberghe, 2001) which can be solved efficiently with programs such as SeDuMi (Sturm, 1999) or Mosek (Andersen and Andersen, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117484770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2b98fbce490feb02f24013413835736af319da6",
            "isKey": false,
            "numCitedBy": 584,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this work is to present the MOSEK optimizer intended for solution of large-scale sparse linear programs. The optimizer is based on the homogeneous interior-point algorithm which in contrast to the primal-dual algorithm detects a possible primal or dual infeasibility reliably. It employs advanced (parallelized) linear algebra, it handles dense columns in the constraint matrix efficiently, and it has a basis identification procedure."
            },
            "slug": "The-Mosek-Interior-Point-Optimizer-for-Linear-An-of-Andersen-Andersen",
            "title": {
                "fragments": [],
                "text": "The Mosek Interior Point Optimizer for Linear Programming: An Implementation of the Homogeneous Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The MOSEK optimizer is based on the homogeneous interior-point algorithm which in contrast to the primal-dual algorithm detects a possible primal or dual infeasibility reliably reliably."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14725378"
                        ],
                        "name": "M. Ledoux",
                        "slug": "M.-Ledoux",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Ledoux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ledoux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2837737"
                        ],
                        "name": "M. Talagrand",
                        "slug": "M.-Talagrand",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Talagrand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Talagrand"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 39
                            }
                        ],
                        "text": "This bound is the contraction lemma in Ledoux and Talagrand (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This bound is the contraction lemma of  Ledoux and Talagrand (1991) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118526268,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d61540a96c22a1ed4a2b78558a2ebdd39f221a90",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Notation.- 0. Isoperimetric Background and Generalities.- 1. Isoperimetric Inequalities and the Concentration of Measure Phenomenon.- 2. Generalities on Banach Space Valued Random Variables and Random Processes.- I. Banach Space Valued Random Variables and Their Strong Limiting Properties.- 3. Gaussian Random Variables.- 4. Rademacher Averages.- 5. Stable Random Variables.- 6 Sums of Independent Random Variables.- 7. The Strong Law of Large Numbers.- 8. The Law of the Iterated Logarithm.- II. Tightness of Vector Valued Random Variables and Regularity of Random Processes.- 9. Type and Cotype of Banach Spaces.- 10. The Central Limit Theorem.- 11. Regularity of Random Processes.- 12. Regularity of Gaussian and Stable Processes.- 13. Stationary Processes and Random Fourier Series.- 14. Empirical Process Methods in Probability in Banach Spaces.- 15. Applications to Banach Space Theory.- References."
            },
            "slug": "Probability-in-Banach-Spaces:-Isoperimetry-and-Ledoux-Talagrand",
            "title": {
                "fragments": [],
                "text": "Probability in Banach Spaces: Isoperimetry and Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145438513"
                        ],
                        "name": "Yan-Ping Huang",
                        "slug": "Yan-Ping-Huang",
                        "structuredName": {
                            "firstName": "Yan-Ping",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan-Ping Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14723477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa70838bf01aa3c26cd1ea124d597a8c44d5a826",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Text Categorization(TC) is an important component in many information organization and information management tasks. Two key issues in TC are feature coding and classifier design. In this paper Text Categorization via Support Vector Machines(SVMs) approach based on Latent Semantic Indexing(LSI) is described. Latent Semantic Indexing[1][2] is a method for selecting informative subspaces of feature spaces with the goal of obtaining a compact representation of document. Support Vector Machines[3] are powerful machine learning systems, which combine remarkable performance with an elegant theoretical framework. The SVMs well fits the Text Categorization task due to the special properties of text itself. Experiments show that the LSI+SVMs frame improves clustering performance by focusing attention of Support Vector Machines onto informative subspaces of the feature spaces."
            },
            "slug": "Support-Vector-Machines-for-Text-Categorization-on-Huang",
            "title": {
                "fragments": [],
                "text": "Support Vector Machines for Text Categorization Based on Latent Semantic Indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experiments show that the LSI+SVMs frame improves clustering performance by focusing attention of Support Vector Machines onto informative subspaces of the feature spaces."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107389564"
                        ],
                        "name": "T. Smith",
                        "slug": "T.-Smith",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2398669"
                        ],
                        "name": "M. Waterman",
                        "slug": "M.-Waterman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Waterman",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Waterman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, an additional kernel matrix is constructed by applying the Smith-Waterman (SW) pairwise sequence comparison algorithm ( Smith and Waterman, 1981 ) to the yeast protein sequences and applying the empirical kernel map (Tsuda, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20031248,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "40c5441aad96b366996e6af163ca9473a19bb9ad",
            "isKey": false,
            "numCitedBy": 10037,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Identification-of-common-molecular-subsequences.-Smith-Waterman",
            "title": {
                "fragments": [],
                "text": "Identification of common molecular subsequences."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2014414"
                        ],
                        "name": "L. Vandenberghe",
                        "slug": "L.-Vandenberghe",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Vandenberghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vandenberghe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Details and proofs can be found in Boyd and Vandenberghe (2003). Semidefinite programming (Nesterov and Nemirovsky, 1994; Vandenberghe and Boyd, 1996; Boyd and Vandenberghe, 2003) deals with the optimization of convex functions over the convex cone1 of symmetric, positive semidefinite matrices P = { X \u2208 Rp\u00d7p | X = X , X \" 0 } ,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This convex optimization problem, a QCQP more precisely, is a special instance of an SOCP (second-order cone programming problem), which is in turn a special form of SDP (Boyd and Vandenberghe, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Semidefinite programming (Nesterov and Nemirovsky, 1994; Vandenberghe and Boyd, 1996; Boyd and Vandenberghe, 2003) deals with the optimization of convex functions over the convex cone1 of symmetric, positive semidefinite matrices P = { X \u2208 Rp\u00d7p | X = X , X \" 0 } ,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37925315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f607f03272e4d62708f5b2441355f9e005cb452",
            "isKey": true,
            "numCitedBy": 38721,
            "numCiting": 276,
            "paperAbstract": {
                "fragments": [],
                "text": "Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics."
            },
            "slug": "Convex-Optimization-Boyd-Vandenberghe",
            "title": {
                "fragments": [],
                "text": "Convex Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A comprehensive introduction to the subject of convex optimization shows in detail how such problems can be solved numerically with great efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709706"
                        ],
                        "name": "Minghua Deng",
                        "slug": "Minghua-Deng",
                        "structuredName": {
                            "firstName": "Minghua",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghua Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358506"
                        ],
                        "name": "Ting Chen",
                        "slug": "Ting-Chen",
                        "structuredName": {
                            "firstName": "Ting",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ting Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34318607"
                        ],
                        "name": "Fengzhu Sun",
                        "slug": "Fengzhu-Sun",
                        "structuredName": {
                            "firstName": "Fengzhu",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fengzhu Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "5. Expression data: two genes with similar expression proflles are likely to have similar functions; accordingly,  Deng et al. (2003)  convert the expression matrix to a square binary interaction matrix in which a 1 indicates that the corresponding pair of expression proflles exhibits a Pearson correlation greater than 0.8."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Using this setup, we follow the experimental paradigm of  Deng et al. (2003) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " Deng et al. (2003) , Lanckriet et al. (2004) perform two variants of the experiment: one in which the flve kernels are restricted to contain precisely the same binary information as used by the MRF method, and a second experiment in which the richer Pfam and expression kernels are used and the SW kernel is added."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 130115,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "335aba58c9bb90d46e6c2089c44cb8003cb7266d",
            "isKey": true,
            "numCitedBy": 106,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop an integrated probabilistic model to combine protein physical interactions, genetic interactions, highly correlated gene expression network, protein complex data, and domain structures of individual proteins to predict protein functions. The model is an extension of our previous model for protein function prediction based on Markovian random field theory. The model is flexible in that other protein pairwise relationship information and features of individual proteins can be easily incorporated. Two features distinguish the integrated approach from other available methods for protein function prediction. One is that the integrated approach uses all available sources of information with different weights for different sources of data. It is a global approach that takes the whole network into consideration. The second feature is that the posterior probability that a protein has the function of interest is assigned. The posterior probability indicates how confident we are about assigning the function to the protein. We apply our integrated approach to predict functions of yeast proteins based upon MIPS protein function classifications and upon the interaction networks based on MIPS physical and genetic interactions, gene expression profiles, Tandem Affinity Purification (TAP) protein complex data, and protein domain information. We study the sensitivity and specificity of the integrated approach using different sources of information by the leave-one-out approach. In contrast to using MIPS physical interactions only, the integrated approach combining all of the information increases the sensitivity from 57% to 87% when the specificity is set at 57%-an increase of 30%. It should also be noted that enlarging the interaction network greatly increases the number of proteins whose functions can be predicted."
            },
            "slug": "An-integrated-probabilistic-model-for-functional-of-Deng-Chen",
            "title": {
                "fragments": [],
                "text": "An integrated probabilistic model for functional prediction of proteins"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An integrated probabilistic model to combine protein physical interactions, genetic interactions, highly correlated gene expression network, protein complex data, and domain structures of individual proteins to predict protein functions is developed."
            },
            "venue": {
                "fragments": [],
                "text": "RECOMB '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050583"
                        ],
                        "name": "Lijuan Cai",
                        "slug": "Lijuan-Cai",
                        "structuredName": {
                            "firstName": "Lijuan",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lijuan Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " Cai and Hofmann (2003) ; Huang (2003); Eyheramendy et al. (2003)); in particular, we focused on the top flve Reuters-21578 topics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The second kernel K2 is constructed by extracting 500 concepts from documents via probabilistic latent semantic analysis ( Cai and Hofmann, 2003 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8400353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e4d9ef38ff13c6e996b5909ab2ff07b8ef963dc",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Term-based representations of documents have found wide-spread use in information retrieval. However, one of the main shortcomings of such methods is that they largely disregard lexical semantics and, as a consequence, are not sufficiently robust with respect to variations in word usage.In this paper we investigate the use of concept-based document representations to supplement word- or phrase-based features. The utilized concepts are automatically extracted from documents via probabilistic latent semantic analysis. We propose to use AdaBoost to optimally combine weak hypotheses based on both types of features. Experimental results on standard benchmarks confirm the validity of our approach, showing that AdaBoost achieves consistent improvements by including additional semantic features in the learned ensemble."
            },
            "slug": "Text-categorization-by-boosting-automatically-Cai-Hofmann",
            "title": {
                "fragments": [],
                "text": "Text categorization by boosting automatically extracted concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper investigates the use of concept-based document representations to supplement word- or phrase-based features, and proposes to use AdaBoost to optimally combine weak hypotheses based on both types of features."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 88
                            }
                        ],
                        "text": "Kernel methods choose a function that is linear in the feature space by optimizing some criterion over the sample."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 88
                            }
                        ],
                        "text": "Kernel-based learning algorithms (see, for example, Cristianini and Shawe-Taylor, 2000; Scho\u0308lkopf and Smola, 2002) work by embedding the data into a Hilbert space, and searching for linear relations in such a space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 55
                            }
                        ],
                        "text": "(See, for example, Cristianini and Shawe-Taylor, 2000; Scho\u0308lkopf and Smola, 2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 115
                            }
                        ],
                        "text": "To illustrate duality in the case of an SDP, we will first review basic concepts in duality theory and then show how they can be extended to semi-definite programming."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29871328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5051890e501117097eeffbd8ded87694f0d8063",
            "isKey": true,
            "numCitedBy": 6578,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."
            },
            "slug": "Learning-with-kernels-Smola",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This book is intended to be a guide to the art of self-consistency and should not be used as a substitute for a comprehensive guide to self-confidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The flrst kernelK1 is derived as a linear kernel from the \\bag-of-words\" representation of the difierent documents, capturing information about the frequency of terms in the difierent documents ( Salton and McGill, 1983 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": false,
            "numCitedBy": 12605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2333884"
                        ],
                        "name": "S. Eyheramendy",
                        "slug": "S.-Eyheramendy",
                        "structuredName": {
                            "firstName": "Susana",
                            "lastName": "Eyheramendy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eyheramendy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143629224"
                        ],
                        "name": "A. Genkin",
                        "slug": "A.-Genkin",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Genkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Genkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145921755"
                        ],
                        "name": "Wen-Hua Ju",
                        "slug": "Wen-Hua-Ju",
                        "structuredName": {
                            "firstName": "Wen-Hua",
                            "lastName": "Ju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-Hua Ju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710305"
                        ],
                        "name": "D. Madigan",
                        "slug": "D.-Madigan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Madigan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Madigan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15177949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2909fb503b0e1a41104fb3141b830d20e1468c2b",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper empirically compares the performance of different Bayesian models for text categorization. In particular we examine so-called \u201csparse\u201d Bayesian models that explicitly favor simplicity. We present empirical evidence that these models retain good predictive capabilities while offering significant computational advantages."
            },
            "slug": "Sparse-Bayesian-Classifiers-for-Text-Categorization-Eyheramendy-Genkin",
            "title": {
                "fragments": [],
                "text": "Sparse Bayesian Classifiers for Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Empirically compares the performance of different Bayesian models for text categorization and examines so-called \u201csparse\u201d Bayesian model that explicitly favor simplicity, finding empirical evidence that these models retain good predictive capabilities while offering significant computational advantages."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 45
                            }
                        ],
                        "text": "Comparing this to (59), we see that both formulations are identical except for the terms in the objective function that only depend on \u03b1 (and not on the kernel matrix, i.e., not on the weights \u00b5i for K = \u2211m i=1 \u00b5iviv T i ): these are 2\u03b1\nT e \u2212 1C \u03b1T \u03b1 in (65) instead of only 2\u03b1T e in (59)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 130
                            }
                        ],
                        "text": "For the case Ki = vivTi , with vi orthonormal, the learning problem becomes\nmin K\nwS2(Ktr)\nsubject to trace(K) = c, K \u00ba 0,\nK = m\u2211\ni=1\n\u00b5iviv T i ,\nor, according to (6),\nmin K max \u03b1 : \u03b1\u22650,\u03b1T y=0\n2\u03b1T e\u2212 \u03b1T (\nG(Ktr) + 1 C Intr\n) \u03b1 (64)\nsubject to trace(K) = c, K \u00ba 0,\nK = m\u2211\ni=1\n\u00b5iviv T i ,\nwhich is\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 32
                            }
                        ],
                        "text": "More details can be found in De Bie et al. (2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 145
                            }
                        ],
                        "text": "Similarly, we can simultaneously and automatically tune the parameter \u03c4 = 1/C such that the quantity wS2(Ktr, \u03c4) is minimized, as is proposed in De Bie et al. (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convex optimization of the 2-norm soft margin parameter in support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "Convex optimization of the 2-norm soft margin parameter in support vector machines"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 146
                            }
                        ],
                        "text": "Note also that the optimal weights \u00b5i, i = 1, . . . , m, can be recovered from the primal-dual solution found by standard software such as SeDuMi (Sturm, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 41
                            }
                        ],
                        "text": "General-purpose programs such as SeDuMi (Sturm, 1999) use interior-point methods to solve SDP problems (Nesterov and Nemirovsky, 1994); they are polynomial time, but have a worst-case complexity O(n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 42
                            }
                        ],
                        "text": "A general-purpose program such as SeDuMi (Sturm, 1999) handles those problems efficiently."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 244
                            }
                        ],
                        "text": "\u2026weights {\u00b5i}3i=1 are optimized according to a hard margin, a 1-norm soft margin and a 2-norm soft margin criterion, respectively; the semi-definite programs (27), (32) and (38) are solved using the general-purpose optimization software SeDuMi (Sturm, 1999), leading to optimal weights {\u00b5\u2217i }3i=1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 319,
                                "start": 306
                            }
                        ],
                        "text": "After evaluating the initial kernel matrices {Ki}(3)i=1, the weights {\u03bci}(3)i=1 are optimized according to a hard margin, a 1-norm soft margin and a 2-norm soft margin criterion, respectively; the semi-definite programs (27), (32) and (38) are solved using the general-purpose optimization software SeDuMi (Sturm, 1999), leading to optimal weights {\u03bci }(3)i=1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 135
                            }
                        ],
                        "text": "This QCQP problem is a special form of SDP (Boyd and Vandenberghe, 2001) which can be solved efficiently with programs such as SeDuMi (Sturm, 1999) or Mosek (Andersen and Andersen, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": ", m, can be recovered from the primal-dual solution found by standard software such as SeDuMi (Sturm, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optimization Methods and Software, 11\u201312:625\u2013653"
            },
            "venue": {
                "fragments": [],
                "text": "Special issue on Interior Point Methods (CD supplement with software)"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Such solutions would of course also have immediate applications to on-line learning problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 146
                            }
                        ],
                        "text": "Note also that the optimal weights \u00b5i, i = 1, . . . , m, can be recovered from the primal-dual solution found by standard software such as SeDuMi (Sturm, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 41
                            }
                        ],
                        "text": "General-purpose programs such as SeDuMi (Sturm, 1999) use interior-point methods to solve SDP problems (Nesterov and Nemirovsky, 1994); they are polynomial time, but have a worst-case complexity O(n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 42
                            }
                        ],
                        "text": "A general-purpose program such as SeDuMi (Sturm, 1999) handles those problems efficiently."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 244
                            }
                        ],
                        "text": "\u2026weights {\u00b5i}3i=1 are optimized according to a hard margin, a 1-norm soft margin and a 2-norm soft margin criterion, respectively; the semi-definite programs (27), (32) and (38) are solved using the general-purpose optimization software SeDuMi (Sturm, 1999), leading to optimal weights {\u00b5\u2217i }3i=1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 135
                            }
                        ],
                        "text": "This QCQP problem is a special form of SDP (Boyd and Vandenberghe, 2001) which can be solved efficiently with programs such as SeDuMi (Sturm, 1999) or Mosek (Andersen and Andersen, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optimization Methods and Software"
            },
            "venue": {
                "fragments": [],
                "text": "Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optimization Methods and Software"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Such solutions would of course also have immediate applications to on-line learning problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 47
                            }
                        ],
                        "text": "This code uses interior-point methods for SDP (Nesterov and Nemirovsky, 1994); these methods have a worst-case complexity of O(q2p2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 104
                            }
                        ],
                        "text": "General-purpose programs such as SeDuMi (Sturm, 1999) use interior-point methods to solve SDP problems (Nesterov and Nemirovsky, 1994); they are polynomial time, but have a worst-case complexity O(n4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 49
                            }
                        ],
                        "text": "These codes use interior-point methods for QCQP (Nesterov and Nemirovsky, 1994) which yield a worst-case complexity of O(mn2tr + n 3 tr)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 27
                            }
                        ],
                        "text": "Semi-definite programming (Nesterov and Nemirovsky, 1994; Vandenberghe and Boyd, 1996; Boyd and Vandenberghe, 2001) deals with the optimization of convex functions over the convex cone1 of symmetric, positive semi-definite matrices P = {X \u2208 Rp\u00d7p | X = XT , X \u00ba 0} , or affine subsets of this cone."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interior point polynomial methods in convex programming: Theory and applications"
            },
            "venue": {
                "fragments": [],
                "text": "Interior point polynomial methods in convex programming: Theory and applications"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679035"
                        ],
                        "name": "C. McDiarmid",
                        "slug": "C.-McDiarmid",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "McDiarmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. McDiarmid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 42
                            }
                        ],
                        "text": "McDiarmid\u2019s bounded difference inequality (McDiarmid, 1989) implies the result."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116663483,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6bbb79cc026ecca4264d95c4551fc58205b09533",
            "isKey": false,
            "numCitedBy": 1710,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Surveys-in-Combinatorics,-1989:-On-the-method-of-McDiarmid",
            "title": {
                "fragments": [],
                "text": "Surveys in Combinatorics, 1989: On the method of bounded differences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convex optimization. Course notes for EE364, Stanford University"
            },
            "venue": {
                "fragments": [],
                "text": "Convex optimization. Course notes for EE364, Stanford University"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 167
                            }
                        ],
                        "text": "However, there is an important difference: when evaluating the resulting classifier, the actual kernel matrix K is used, instead of the augmented K \u2032 (see, for example, Shawe-Taylor and Cristianini, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soft margin and margin distribution"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Large Margin Classifiers"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using sparseness and analytic QP to speed training of support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 11"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 224
                            }
                        ],
                        "text": "Moreover, an additional kernel matrix is constructed by applying the Smith-Waterman (SW) pairwise sequence comparison algorithm (Smith and Waterman, 1981) to the yeast protein sequences and applying the empirical kernel map (Tsuda, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support vector classification with asymmetric kernel function"
            },
            "venue": {
                "fragments": [],
                "text": "Verleysen, M., editor, Proceedings of the European Symposium on Artificial Neural Networks, pages 183\u2013188."
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 20
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 35,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-the-Kernel-Matrix-with-Semidefinite-Lanckriet-Cristianini/0948365ef39ef153e61e9569ade541cf881c7c2a?sort=total-citations"
}