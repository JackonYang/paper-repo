{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3502855"
                        ],
                        "name": "Marcin Marszalek",
                        "slug": "Marcin-Marszalek",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Marszalek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Marszalek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "When objects are organized along a hierarchical taxonomy, nodes at different levels in the hierarchy share the training samples of all the children [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14412825,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "530b99c3819bd4c4f1884fa89c9a0d6e024156bd",
            "isKey": false,
            "numCitedBy": 309,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose to use lexical semantic networks to extend the state-of-the-art object recognition techniques. We use the semantics of image labels to integrate prior knowledge about inter-class relationships into the visual appearance learning. We show how to build and train a semantic hierarchy of discriminative classifiers and how to use it to perform object detection. We evaluate how our approach influences the classification accuracy and speed on the Pascal VOC challenge 2006 dataset, a set of challenging real-world images. We also demonstrate additional features that become available to object recognition due to the extension with semantic inference tools- we can classify high-level categories, such as animals, and we can train part detectors, for example a window detector, by pure inference in the semantic network."
            },
            "slug": "Semantic-Hierarchies-for-Visual-Object-Recognition-Marszalek-Schmid",
            "title": {
                "fragments": [],
                "text": "Semantic Hierarchies for Visual Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The semantics of image labels are used to integrate prior knowledge about inter-class relationships into the visual appearance learning and to build and train a semantic hierarchy of discriminative classifiers and how to use it to perform object detection."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188179"
                        ],
                        "name": "A. Opelt",
                        "slug": "A.-Opelt",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Opelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Opelt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718587"
                        ],
                        "name": "A. Pinz",
                        "slug": "A.-Pinz",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Pinz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pinz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 52
                            }
                        ],
                        "text": "Other models learn the relations between categories [30, 21, 22, 1, 25, 13, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 34
                            }
                        ],
                        "text": ", parts) across different classes [15, 30, 2, 21, 17]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 673415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3c48c85c9da6441547288f48ae9735f1ff2016e",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of multiclass object detection. Our aims are to enable models for new categories to benefit from the detectors built previously for other categories, and for the complexity of the multiclass system to grow sublinearly with the number of categories. To this end we introduce a visual alphabet representation which can be learnt incrementally, and explicitly shares boundary fragments (contours) and spatial configurations (relation to centroid) across object categories. We develop a learning algorithm with the following novel contributions: (i) AdaBoost is adapted to learn jointly, based on shape features; (ii) a new learning schedule enables incremental additions of new categories; and (iii) the algorithm learns to detect objects (instead of categorizing images). Furthermore, we show that category similarities can be predicted from the alphabet. We obtain excellent experimental results on a variety of complex categories over several visual aspects. We show that the sharing of shape features not only reduces the number of features required per category, but also often improves recognition performance, as compared to individual detectors which are trained on a per-class basis."
            },
            "slug": "Incremental-learning-of-object-detectors-using-a-Opelt-Pinz",
            "title": {
                "fragments": [],
                "text": "Incremental learning of object detectors using a visual shape alphabet"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A visual alphabet representation which can be learnt incrementally, and explicitly shares boundary fragments and spatial configurations across object categories, and shows that category similarities can be predicted from the alphabet."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799035"
                        ],
                        "name": "Erik B. Sudderth",
                        "slug": "Erik-B.-Sudderth",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sudderth",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik B. Sudderth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 119
                            }
                        ],
                        "text": "Hierarchical models also share information across classes by representing objects as compositions of shared components [13, 27, 25, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6153430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5331349557fababfac48d47e49b44583e3bd5f6",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a hierarchical probabilistic model for the detection and recognition of objects in cluttered, natural scenes. The model is based on a set of parts which describe the expected appearance and position, in an object centered coordinate frame, of features detected by a low-level interest operator. Each object category then has its own distribution over these parts, which are shared between objects. We learn the parameters of this model via a Gibbs sampler which uses the graphical model's structure to analytically average over many parameters. Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available. We also extend this hierarchical framework to scenes containing multiple objects"
            },
            "slug": "Learning-hierarchical-models-of-scenes,-objects,-Sudderth-Torralba",
            "title": {
                "fragments": [],
                "text": "Learning hierarchical models of scenes, objects, and parts"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available and this hierarchical probabilistic model is extended to scenes containing multiple objects."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365442"
                        ],
                        "name": "B. Alexe",
                        "slug": "B.-Alexe",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Alexe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Alexe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 28
                            }
                        ],
                        "text": "Models using a global prior [8, 17, 6] ignore this issue by sharing information across all categories, which provides only small benefits."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7664974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42bb241681c4bec1fa36211a204fa0dc8158e5ff",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning a new object class from cluttered training images is very challenging when the location of object instances is unknown. Previous works generally require objects covering a large portion of the images. We present a novel approach that can cope with extensive clutter as well as large scale and appearance variations between object instances. To make this possible we propose a conditional random field that starts from generic knowledge and then progressively adapts to the new class. Our approach simultaneously localizes object instances while learning an appearance model specific for the class. We demonstrate this on the challenging PASCAL VOC 2007 dataset. Furthermore, our method enables to train any state-of-the-art object detector in a weakly supervised fashion, although it would normally require object location annotations."
            },
            "slug": "Localizing-Objects-While-Learning-Their-Appearance-Deselaers-Alexe",
            "title": {
                "fragments": [],
                "text": "Localizing Objects While Learning Their Appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a conditional random field that starts from generic knowledge and then progressively adapts to the new class to enable any state-of-the-art object detector in a weakly supervised fashion, although it would normally require object location annotations."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 52
                            }
                        ],
                        "text": "Other models learn the relations between categories [30, 21, 22, 1, 25, 13, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 34
                            }
                        ],
                        "text": ", parts) across different classes [15, 30, 2, 21, 17]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11194336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42de22c119f25d303032396b8f7d962f62d6498b",
            "isKey": false,
            "numCitedBy": 441,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of detecting a large number of different object classes in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, which can be slow and require much training data. We present a multi-class boosting procedure (joint boosting) that reduces both the computational and sample complexity, by finding common features that can be shared across the classes. The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required is observed to scale approximately logarithmically with the number of classes. In addition, we find that the features selected by independently trained classifiers are often specific to the class, whereas the features selected by the jointly trained classifiers are more generic features, such as lines and edges."
            },
            "slug": "Sharing-features:-efficient-boosting-procedures-for-Torralba-Murphy",
            "title": {
                "fragments": [],
                "text": "Sharing features: efficient boosting procedures for multiclass object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A multi-class boosting procedure (joint boosting) is presented that reduces both the computational and sample complexity, by finding common features that can be shared across the classes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765505"
                        ],
                        "name": "M. Boben",
                        "slug": "M.-Boben",
                        "structuredName": {
                            "firstName": "Marko",
                            "lastName": "Boben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Boben"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732672"
                        ],
                        "name": "A. Leonardis",
                        "slug": "A.-Leonardis",
                        "structuredName": {
                            "firstName": "Ale\u0161",
                            "lastName": "Leonardis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leonardis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39883550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b45584ecebcc040e9a84347f305e1fcd787b9991",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiple object class learning and detection is a challenging problem due to the large number of object classes and their high visual variability. Specialized detectors usually excel in performance, while joint representations optimize sharing and reduce inference time --- but are complex to train. Conveniently, sequential learning of categories cuts down training time by transferring existing knowledge to novel classes, but cannot fully exploit the richness of shareability and might depend on ordering in learning. In hierarchical frameworks these issues have been little explored. In this paper, we show how different types of multi-class learning can be done within one generative hierarchical framework and provide a rigorous experimental analysis of various object class learning strategies as the number of classes grows. Specifically, we propose, evaluate and compare three important types of multi-class learning: 1.) independent training of individual categories, 2.) joint training of classes, 3.) sequential learning of classes. We explore and compare their computational behavior (space and time) and detection performance as a function of the number of learned classes on several recognition data sets."
            },
            "slug": "Evaluating-multi-class-learning-strategies-in-a-for-Fidler-Boben",
            "title": {
                "fragments": [],
                "text": "Evaluating multi-class learning strategies in a generative hierarchical framework for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper shows how different types of multi-class learning can be done within one generative hierarchical framework and provides a rigorous experimental analysis of various object class learning strategies as the number of classes grows."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52118596"
                        ],
                        "name": "S. Krempp",
                        "slug": "S.-Krempp",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Krempp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Krempp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4801654"
                        ],
                        "name": "Y. Amit",
                        "slug": "Y.-Amit",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Amit"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 34
                            }
                        ],
                        "text": ", parts) across different classes [15, 30, 2, 21, 17]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6386555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61b933b8ef5b10ae4f6491a89f89972322534cf0",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Our long-range goal is detecting instances from a large number of object classes in a computationally efficient manner. Detectors involving a hierarchy of tests based on edges have been used elsewhere and shown to be quite fast online. However, significant further gains in efficiency in representation, error rates and computation can be realized if the family of detectors is constructed from common parts. Our parts are flexible, extended edge configurations; they are learned, not pre-designed. In training, object classes are presented sequentially; the objective is then to accommodate new classes by maximally reusing parts. Ideally, the number of distinct parts in the system would grow much more slowly than linearly with the number of classes. Initial experiments on learning to detect several hundred LTEXsymbols are encouraging."
            },
            "slug": "Sequential-Learning-of-Reusable-Parts-for-Object-Krempp-Geman",
            "title": {
                "fragments": [],
                "text": "Sequential Learning of Reusable Parts for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Initial experiments on learning to detect several hundred LTEXsymbols are encouraging, and significant further gains in efficiency in representation, error rates and computation can be realized if the family of detectors is constructed from common parts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "This allows us to reuse an efficient C++ implementation of [10] with minimal changes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "However, many of these systems [4, 10] detect objects by testing sub-windows and scoring corresponding test patches x with a linear function of the form: y = \u03b2\u03a6(x), (1) where \u03a6(x) may represent a vector of different image features at multiple scales (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [10], optimization over \u03b2 is performed using a linear SVM objective with a standard hinge-loss(3) as in Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "In particular, [10] use a binary classification model that scores an example x with a linear function: y = \u03b2\u03a6(x), where \u03a6(x) represents a concatenation of the HOG feature pyramid plus part displacement features, and \u03b2 represents model parameters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 137
                            }
                        ],
                        "text": "Experimental results Our proposed hierarchical classification framework can be directly applied to learning an object detection model of [4, 10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": true,
            "numCitedBy": 9375,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 352,
                                "start": 349
                            }
                        ],
                        "text": "Independently of the task being solved, another aspect that differentiates between multiclass detectors is the type of information shared across categories: Sharing parts: Some of the first models in multiclass object recognition shared information via a global prior that modeled the distribution of appearance and geometry of generic object parts [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "This model, similar in spirit to [8], could learn a set of useful features common to all object categories."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 28
                            }
                        ],
                        "text": "Models using a global prior [8, 17, 6] ignore this issue by sharing information across all categories, which provides only small benefits."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2096065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d044d7d92dd1fb80275d04d035aed71bcd3374e5",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning visual models of object categories notoriously requires thousands of training examples; this is due to the diversity and richness of object appearance which requires models containing hundreds of parameters. We present a method for learning object categories from just a few images (1 /spl sim/ 5). It is based on incorporating \"generic\" knowledge which may be obtained from previously learnt models of unrelated categories. We operate in a variational Bayesian framework: object categories are represented by probabilistic models, and \"prior\" knowledge is represented as a probability density function on the parameters of these models. The \"posterior\" model for an object category is obtained by updating the prior in the light of one or more observations. Our ideas are demonstrated on four diverse categories (human faces, airplanes, motorcycles, spotted cats). Initially three categories are learnt from hundreds of training examples, and a \"prior\" is estimated from these. Then the model of the fourth category is learnt from 1 to 5 training examples, and is used for detecting new exemplars a set of test images."
            },
            "slug": "A-Bayesian-approach-to-unsupervised-one-shot-of-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "A Bayesian approach to unsupervised one-shot learning of object categories"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a method for learning object categories from just a few images, based on incorporating \"generic\" knowledge which may be obtained from previously learnt models of unrelated categories, in a variational Bayesian framework."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748758"
                        ],
                        "name": "H. Nickisch",
                        "slug": "H.-Nickisch",
                        "structuredName": {
                            "firstName": "Hannes",
                            "lastName": "Nickisch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nickisch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734990"
                        ],
                        "name": "S. Harmeling",
                        "slug": "S.-Harmeling",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Harmeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harmeling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Sharing attributes: Representations based on attributes [16] define a finite vocabulary that is common to all categories, with each category using a subset of the attributes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10301835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0566bf06a0368b518b8b474166f7b1dfef3f9283",
            "isKey": false,
            "numCitedBy": 1951,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of object classification when training and test classes are disjoint, i.e. no training examples of the target classes are available. This setup has hardly been studied in computer vision research, but it is the rule rather than the exception, because the world contains tens of thousands of different object classes and for only a very few of them image, collections have been formed and annotated with suitable class labels. In this paper, we tackle the problem by introducing attribute-based classification. It performs object detection based on a human-specified high-level description of the target objects instead of training images. The description consists of arbitrary semantic attributes, like shape, color or even geographic information. Because such properties transcend the specific learning task at hand, they can be pre-learned, e.g. from image datasets unrelated to the current task. Afterwards, new classes can be detected based on their attribute representation, without the need for a new training phase. In order to evaluate our method and to facilitate research in this area, we have assembled a new large-scale dataset, \u201cAnimals with Attributes\u201d, of over 30,000 animal images that match the 50 classes in Osherson's classic table of how strongly humans associate 85 semantic attributes with animal classes. Our experiments show that by using an attribute layer it is indeed possible to build a learning object detection system that does not require any training images of the target classes."
            },
            "slug": "Learning-to-detect-unseen-object-classes-by-Lampert-Nickisch",
            "title": {
                "fragments": [],
                "text": "Learning to detect unseen object classes by between-class attribute transfer"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The experiments show that by using an attribute layer it is indeed possible to build a learning object detection system that does not require any training images of the target classes, and assembled a new large-scale dataset, \u201cAnimals with Attributes\u201d, of over 30,000 animal images that match the 50 classes in Osherson's classic table of how strongly humans associate 85 semantic attributes with animal classes."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1874218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "555bd3cdd261ab42c3d40194be991ddec8b6a14c",
            "isKey": false,
            "numCitedBy": 250,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Objects in the world can be arranged into a hierarchy based on their semantic meaning (e.g. organism - animal - feline - cat). What about defining a hierarchy based on the visual appearance of objects? This paper investigates ways to automatically discover a hierarchical structure for the visual world from a collection of unlabeled images. Previous approaches for unsupervised object and scene discovery focused on partitioning the visual data into a set of non-overlapping classes of equal granularity. In this work, we propose to group visual objects using a multi-layer hierarchy tree that is based on common visual elements. This is achieved by adapting to the visual domain the generative hierarchical latent Dirichlet allocation (hLDA) model previously used for unsupervised discovery of topic hierarchies in text. Images are modeled using quantized local image regions as analogues to words in text. Employing the multiple segmentation framework of Russell et al. [22], we show that meaningful object hierarchies, together with object segmentations, can be automatically learned from unlabeled and unsegmented image collections without supervision. We demonstrate improved object classification and localization performance using hLDA over the previous non-hierarchical method on the MSRC dataset [33]."
            },
            "slug": "Unsupervised-discovery-of-visual-object-class-Sivic-Russell",
            "title": {
                "fragments": [],
                "text": "Unsupervised discovery of visual object class hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes to group visual objects using a multi-layer hierarchy tree that is based on common visual elements by adapting to the visual domain the generative hierarchical latent Dirichlet allocation (hLDA) model previously used for unsupervised discovery of topic hierarchies in text."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1813142"
                        ],
                        "name": "K. Levi",
                        "slug": "K.-Levi",
                        "structuredName": {
                            "firstName": "Kobi",
                            "lastName": "Levi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Levi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105534484"
                        ],
                        "name": "Michael Fink",
                        "slug": "Michael-Fink",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Fink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 34
                            }
                        ],
                        "text": ", parts) across different classes [15, 30, 2, 21, 17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 28
                            }
                        ],
                        "text": "Models using a global prior [8, 17, 6] ignore this issue by sharing information across all categories, which provides only small benefits."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 477017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f0f92d501450088ab9104cefc99a2001ff8c385",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last few years, object detection techniques have progressed immensely. Impressive detection results have been achieved for many objects such as faces [11, 14, 9] and cars [11]. The robustness of these systems emerges from a training stage utilizing thousands of positive examples. One approach to enable learning from a small set of training examples is to find an efficient set of features that accurately represent the target object. Unfortunately, automatically selecting such a feature set is a difficult task in itself. In this paper we present a novel feature selection method that is based on the notion of object categories. We assume that when learning to recognize a new object (like an apple) we also know a category it belongs to (fruit). We further assume that features that are useful for learning other objects in the same category (e.g. pear or orange) will also be useful for learning the novel object. This leads to a simple criterion for selecting features and building classifiers. We show that our method gives significant improvement in detection performance in challenging domains."
            },
            "slug": "Learning-From-a-Small-Number-of-Training-Examples-Levi-Fink",
            "title": {
                "fragments": [],
                "text": "Learning From a Small Number of Training Examples by Exploiting Object Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a novel feature selection method that is based on the notion of object categories, which assumes that when learning to recognize a new object the authors also know a category it belongs to (fruit)."
            },
            "venue": {
                "fragments": [],
                "text": "2004 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3171632"
                        ],
                        "name": "A. Quattoni",
                        "slug": "A.-Quattoni",
                        "structuredName": {
                            "firstName": "Ariadna",
                            "lastName": "Quattoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Quattoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 52
                            }
                        ],
                        "text": "Other models learn the relations between categories [30, 21, 22, 1, 25, 13, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 87
                            }
                        ],
                        "text": "Object classes transfer information by regularizing the space of classifier parameters [22, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2381155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "953e2cfa58679ff6ea8c0bb432afd641f15d3657",
            "isKey": false,
            "numCitedBy": 247,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "To learn a new visual category from few examples, prior knowledge from unlabeled data as well as previous related categories may be useful. We develop a new method for transfer learning which exploits available unlabeled data and an arbitrary kernel function; we form a representation based on kernel distances to a large set of unlabeled data points. To transfer knowledge from previous related problems we observe that a category might be learnable using only a small subset of reference prototypes. Related problems may share a significant number of relevant prototypes; we find such a concise representation by performing a joint loss minimization over the training sets of related problems with a shared regularization penalty that minimizes the total number of prototypes involved in the approximation. This optimization problem can be formulated as a linear program that can be solved efficiently. We conduct experiments on a news-topic prediction task where the goal is to predict whether an image belongs to a particular news topic. Our results show that when only few examples are available for training a target topic, leveraging knowledge learnt from other topics can significantly improve performance."
            },
            "slug": "Transfer-learning-for-image-classification-with-Quattoni-Collins",
            "title": {
                "fragments": [],
                "text": "Transfer learning for image classification with sparse prototype representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results show that when only few examples are available for training a target topic, leveraging knowledge learnt from other topics can significantly improve performance."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38163342"
                        ],
                        "name": "Merrielle Spain",
                        "slug": "Merrielle-Spain",
                        "structuredName": {
                            "firstName": "Merrielle",
                            "lastName": "Spain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Merrielle Spain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 454554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb885a2bcab4d361e6fbcd426ea617f0cab1ef63",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Associating keywords with images automatically is an approachable and useful goal for visual recognition researchers. Keywords are distinctive and informative objects. We argue that keywords need to be sorted by 'importance', which we define as the probability of being mentioned first by an observer. We propose a method for measuring the `importance' of words using the object labels that multiple human observers give an everyday scene photograph. We model object naming as drawing balls from an urn, and fit this model to estimate `importance'; this combines order and frequency, enabling precise prediction under limited human labeling. We explore the relationship between the importance of an object in a particular image and the area, centrality, and saliency of the corresponding image patches. Furthermore, our data shows that many words are associated with even simple environments, and that few frequently appearing objects are shared across environments."
            },
            "slug": "Measuring-and-Predicting-Importance-of-Objects-in-Spain-Perona",
            "title": {
                "fragments": [],
                "text": "Measuring and Predicting Importance of Objects in Our Visual World"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is argued that keywords need to be sorted by 'importance', which is defined as the probability of being mentioned first by an observer, and a method for measuring the `importance' of words using the object labels that multiple human observers give an everyday scene photograph is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084916713"
                        ],
                        "name": "Hector Bernal",
                        "slug": "Hector-Bernal",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Bernal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hector Bernal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 192
                            }
                        ],
                        "text": "Most studies on transfer learning for object recognition have focused on multiclass recognition without a background class (saying if a crop image contains an object out of M possible classes [20, 14, 25, 11, 29])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7086636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffeb684b193afd21aefc5a6b05fb3616cc99418e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In an object recognition scenario with tens of thousands of categories, even a small number of labels per category leads to a very large number of total labels required. We propose a simple method of label sharing between semantically similar categories. We leverage the WordNet hierarchy to define semantic distance between any two categories and use this semantic distance to share labels. Our approach can be used with any classifier. Experimental results on a range of datasets, upto 80 million images and 75,000 categories in size, show that despite the simplicity of the approach, it leads to significant improvements in performance."
            },
            "slug": "Semantic-Label-Sharing-for-Learning-with-Many-Fergus-Bernal",
            "title": {
                "fragments": [],
                "text": "Semantic Label Sharing for Learning with Many Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A simple method of label sharing between semantically similar categories is proposed that leverages the WordNet hierarchy to define semantic distance between any two categories and use this semantic distance to share labels."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1938475"
                        ],
                        "name": "E. Bart",
                        "slug": "E.-Bart",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Bart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 34
                            }
                        ],
                        "text": ", parts) across different classes [15, 30, 2, 21, 17]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18632647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f2b11d835b72b1f6385ede0631d49b10c6f2914",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop an object classification method that can learn a novel class from a single training example. In this method, experience with already learned classes is used to facilitate the learning of novel classes. Our classification scheme employs features that discriminate between class and non-class images. For a novel class, new features are derived by selecting features that proved useful for already learned classification tasks, and adapting these features to the new classification task. This adaptation is performed by replacing the features from already learned classes with similar features taken from the novel class. A single example of a novel class is sufficient to perform feature adaptation and achieve useful classification performance. Experiments demonstrate that the proposed algorithm can learn a novel class from a single training example, using 10 additional familiar classes. The performance is significantly improved compared to using no feature adaptation. The robustness of the proposed feature adaptation concept is demonstrated by similar performance gains across 107 widely varying object categories."
            },
            "slug": "Cross-generalization:-learning-novel-classes-from-a-Bart-Ullman",
            "title": {
                "fragments": [],
                "text": "Cross-generalization: learning novel classes from a single example by feature replacement"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An object classification method that can learn a novel class from a single training example, using 10 additional familiar classes, and the performance is significantly improved compared to using no feature adaptation."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "However, many of these systems [4, 10] detect objects by testing sub-windows and scoring corresponding test patches x with a linear function of the form: y = \u03b2\u03a6(x), (1) where \u03a6(x) may represent a vector of different image features at multiple scales (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 137
                            }
                        ],
                        "text": "Experimental results Our proposed hierarchical classification framework can be directly applied to learning an object detection model of [4, 10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29264,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1938475"
                        ],
                        "name": "E. Bart",
                        "slug": "E.-Bart",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Bart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2562172"
                        ],
                        "name": "I. Porteous",
                        "slug": "I.-Porteous",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Porteous",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Porteous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 52
                            }
                        ],
                        "text": "Other models learn the relations between categories [30, 21, 22, 1, 25, 13, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 119
                            }
                        ],
                        "text": "Hierarchical models also share information across classes by representing objects as compositions of shared components [13, 27, 25, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2073289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0092766d3ea424bcf14e634c03fe64be5cfc570d",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "As more images and categories become available, organizing them becomes crucial. We present a novel statistical method for organizing a collection of images into a tree-shaped hierarchy. The method employs a non-parametric Bayesian model and is completely unsupervised. Each image is associated with a path through a tree. Similar images share initial segments of their paths and therefore have a smaller distance from each other. Each internal node in the hierarchy represents information that is common to images whose paths pass through that node, thus providing a compact image representation. Our experiments show that a disorganized collection of images will be organized into an intuitive taxonomy. Furthermore, we find that the taxonomy allows good image categorization and, in this respect, is superior to the popular LDA model."
            },
            "slug": "Unsupervised-learning-of-visual-taxonomies-Bart-Porteous",
            "title": {
                "fragments": [],
                "text": "Unsupervised learning of visual taxonomies"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The experiments show that a disorganized collection of images will be organized into an intuitive taxonomy and it is found that the taxonomy allows good image categorization and, in this respect, is superior to the popular LDA model."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716876"
                        ],
                        "name": "O. Dekel",
                        "slug": "O.-Dekel",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Dekel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Dekel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771345"
                        ],
                        "name": "Joseph Keshet",
                        "slug": "Joseph-Keshet",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Keshet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Keshet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 84
                            }
                        ],
                        "text": "2, where parameters of each class are given by the sum of parameters along the tree [24, 5, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16308336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "094d9601e6f6c45579647e20b5f7b0eeb4e2819f",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithmic framework for supervised classification learning where the set of labels is organized in a predefined hierarchical structure. This structure is encoded by a rooted tree which induces a metric over the label set. Our approach combines ideas from large margin kernel methods and Bayesian analysis. Following the large margin principle, we associate a prototype with each label in the tree and formulate the learning task as an optimization problem with varying margin constraints. In the spirit of Bayesian methods, we impose similarity requirements between the prototypes corresponding to adjacent labels in the hierarchy. We describe new online and batch algorithms for solving the constrained optimization problem. We derive a worst case loss-bound for the online algorithm and provide generalization analysis for its batch counterpart. We demonstrate the merits of our approach with a series of experiments on synthetic, text and speech data."
            },
            "slug": "Large-margin-hierarchical-classification-Dekel-Keshet",
            "title": {
                "fragments": [],
                "text": "Large margin hierarchical classification"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This work presents an algorithmic framework for supervised classification learning where the set of labels is organized in a predefined hierarchical structure encoded by a rooted tree which induces a metric over the label set."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655614"
                        ],
                        "name": "G. Griffin",
                        "slug": "G.-Griffin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Griffin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 192
                            }
                        ],
                        "text": "Most studies on transfer learning for object recognition have focused on multiclass recognition without a background class (saying if a crop image contains an object out of M possible classes [20, 14, 25, 11, 29])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12805251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49bab85f250ff4c086665691df4971c438336e0b",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The computational complexity of current visual categorization algorithms scales linearly at best with the number of categories. The goal of classifying simultaneously Ncat = 104 - 105 visual categories requires sub-linear classification costs. We explore algorithms for automatically building classification trees which have, in principle, logNcat complexity. We find that a greedy algorithm that recursively splits the set of categories into the two minimally confused subsets achieves 5-20 fold speedups at a small cost in classification performance. Our approach is independent of the specific classification algorithm used. A welcome by-product of our algorithm is a very reasonable taxonomy of the Caltech-256 dataset."
            },
            "slug": "Learning-and-using-taxonomies-for-fast-visual-Griffin-Perona",
            "title": {
                "fragments": [],
                "text": "Learning and using taxonomies for fast visual categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that a greedy algorithm that recursively splits the set of categories into the two minimally confused subsets achieves 5-20 fold speedups at a small cost in classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 105
                            }
                        ],
                        "text": "This setting is artificial as it is becoming increasingly easy to collect large amounts of training data [23, 18], at least for a subset of the object classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1900911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092c275005ae49dc1303214f6d02d134457c7053",
            "isKey": false,
            "numCitedBy": 3076,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nWe seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.\n"
            },
            "slug": "LabelMe:-A-Database-and-Web-Based-Tool-for-Image-Russell-Torralba",
            "title": {
                "fragments": [],
                "text": "LabelMe: A Database and Web-Based Tool for Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A web-based tool that allows easy image annotation and instant sharing of such annotations is developed and a large dataset that spans many object categories, often containing multiple instances over a wide variety of images is collected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865091"
                        ],
                        "name": "Krista A. Ehinger",
                        "slug": "Krista-A.-Ehinger",
                        "structuredName": {
                            "firstName": "Krista",
                            "lastName": "Ehinger",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krista A. Ehinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "that we will use in this paper, the SUN\u201909 database [31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Details of the Dataset We divide SUN\u201909 dataset [31] into two sets: one for training and the other one for testing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1309931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "908091b4a8757c3b2f7d9cfa2c4f616ee12c5157",
            "isKey": false,
            "numCitedBy": 2355,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes."
            },
            "slug": "SUN-database:-Large-scale-scene-recognition-from-to-Xiao-Hays",
            "title": {
                "fragments": [],
                "text": "SUN database: Large-scale scene recognition from abbey to zoo"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images and uses 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642138"
                        ],
                        "name": "B. Shahbaba",
                        "slug": "B.-Shahbaba",
                        "structuredName": {
                            "firstName": "Babak",
                            "lastName": "Shahbaba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shahbaba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 84
                            }
                        ],
                        "text": "2, where parameters of each class are given by the sum of parameters along the tree [24, 5, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10611032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b30dec88ee796cb30236301c66c815434fcebe3e",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new method for building classification models when we have prior knowledge of how the classes can be arranged in a hierarchy, based on how easily they can be distinguished. The new method uses a Bayesian form of the multinomial logit (MNL, a.k.a. \u201csoftmax\u201d) model, with a prior that introduces correlations between the parameters for classes that are nearby in the tree. We compare the performance on simulated data of the new method, the ordinary MNL model, and a model that uses the hierarchy in different way. We also test the new method on a document labelling problem, and find that it performs better than the other methods, particularly when the amount of training data is small."
            },
            "slug": "Improving-Classification-When-a-Class-Hierarchy-is-Shahbaba-Neal",
            "title": {
                "fragments": [],
                "text": "Improving Classification When a Class Hierarchy is Available Using a Hierarchy-Based Prior"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new method for building classification models when the authors have prior knowledge of how the classes can be arranged in a hierarchy, based on how easily they can be distinguished is introduced, and it is found that it performs better than the other methods, particularly when the amount of training data is small."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41224113"
                        ],
                        "name": "E. Miller",
                        "slug": "E.-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1837031"
                        ],
                        "name": "Nicholas E. Matsakis",
                        "slug": "Nicholas-E.-Matsakis",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Matsakis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicholas E. Matsakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 106
                            }
                        ],
                        "text": "These methods can be applied for object classes with well defined sets of transformations such as letters [20, 28], or faces [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 192
                            }
                        ],
                        "text": "Most studies on transfer learning for object recognition have focused on multiclass recognition without a background class (saying if a crop image contains an object out of M possible classes [20, 14, 25, 11, 29])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2699786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7aac1045e6943b4a7978e260a3035662d5b3bf8d",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We define a process called congealing in which elements of a dataset (images) are brought into correspondence with each other jointly, producing a data-defined model. It is based upon minimizing the summed component-wise (pixel-wise) entropies over a continuous set of transforms on the data. One of the biproducts of this minimization is a set of transform, one associated with each original training sample. We then demonstrate a procedure for effectively bringing test data into correspondence with the data-defined model produced in the congealing process. Subsequently; we develop a probability density over the set of transforms that arose from the congealing process. We suggest that this density over transforms may be shared by many classes, and demonstrate how using this density as \"prior knowledge\" can be used to develop a classifier based on only a single training example for each class."
            },
            "slug": "Learning-from-one-example-through-shared-densities-Miller-Matsakis",
            "title": {
                "fragments": [],
                "text": "Learning from one example through shared densities on transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A probability density over the set of transforms that arose from the congealing process is developed, and it is suggested that this density over transforms may be shared by many classes, and used to develop a classifier based on only a single training example for each class."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 84
                            }
                        ],
                        "text": "2, where parameters of each class are given by the sum of parameters along the tree [24, 5, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 719551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e219a61354d972a28954e655a7c53373508a08b6",
            "isKey": false,
            "numCitedBy": 1468,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs."
            },
            "slug": "Regularized-multi--task-learning-Evgeniou-Pontil",
            "title": {
                "fragments": [],
                "text": "Regularized multi--task learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines, that have been successfully used in the past for single-- task learning is presented."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 106
                            }
                        ],
                        "text": "These methods can be applied for object classes with well defined sets of transformations such as letters [20, 28], or faces [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9492646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e85f7d59e37972ec52cbabfef0512588d87f125",
            "isKey": false,
            "numCitedBy": 859,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "Perceptual systems routinely separate content from style, classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, & Bibby, 1979; Hinton & Zemel, 1994; Ghahramani, 1995; Bell & Sejnowski, 1995; Hinton, Dayan, Frey, & Neal, 1995; Dayan, Hinton, Neal, & Zemel, 1995; Hinton & Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants."
            },
            "slug": "Separating-Style-and-Content-with-Bilinear-Models-Tenenbaum-Freeman",
            "title": {
                "fragments": [],
                "text": "Separating Style and Content with Bilinear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401829700"
                        ],
                        "name": "Shai Ben-David",
                        "slug": "Shai-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shai Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952147"
                        ],
                        "name": "Reba Schuller Borbely",
                        "slug": "Reba-Schuller-Borbely",
                        "structuredName": {
                            "firstName": "Reba",
                            "lastName": "Borbely",
                            "middleNames": [
                                "Schuller"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reba Schuller Borbely"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13967968,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "3a4551508f84a3f5447d3490b2db95b4d87a7969",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The approach of learning of multiple \u201crelated\u201d tasks simultaneously has proven quite successful in practice; however, theoretical justification for this success has remained elusive. The starting point for previous work on multiple task learning has been that the tasks to be learned jointly are somehow \u201calgorithmically related\u201d, in the sense that the results of applying a specific learning algorithm to these tasks are assumed to be similar. We offer an alternative approach, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task."
            },
            "slug": "Exploiting-Task-Relatedness-for-Mulitple-Task-Ben-David-Borbely",
            "title": {
                "fragments": [],
                "text": "Exploiting Task Relatedness for Mulitple Task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work offers an alternative approach to multiple task learning, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 86
                            }
                        ],
                        "text": "Our results, which complement existing work on transfer and multitask learning theory (Baxter, 1997; Ben-David & Schuller, 2003; Pentina & Lampert, 2014), show that when the parameters with small regret for a collection of related tasks are either (a) similar or (b) not unexpected under the prior, then the hierarchical model has a smaller regret bound than assuming the tasks are independent."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 656,
                                "start": 115
                            }
                        ],
                        "text": "A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory. Baxter (1997) examines the asymptotic learning properties of hierarchical Bayesian models. Pentina & Lampert (2014) take a PAC-Bayesian approach while Hassan Mahmud & Ray (2007); Hassan Mahmud (2009), and Juba (2006) develop notions of task-relatedness from an (algorithmic) information-theoretic perspective."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 554,
                                "start": 115
                            }
                        ],
                        "text": "A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory. Baxter (1997) examines the asymptotic learning properties of hierarchical Bayesian models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 411,
                                "start": 115
                            }
                        ],
                        "text": "A number of theoretical investigations of MTL and LTL haven been carried out, beginning with a series of papers by Baxter (cf. Baxter, 1997; 2000). Generically, such MTL and LTL frameworks involve two or more learning problems that are related to each other in some manner. The learning properties are investigated as the number of tasks and/or the number of examples from each task is increased. Baxter (2000) and Ben-David & Schuller (2003) give sample complexity bounds based on classical ideas from statistical and PAC learning theory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12565208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bd6e929ed8384ea2212d50ab3c103ec018cc9fd",
            "isKey": true,
            "numCitedBy": 382,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A Bayesian model of learning to learn by sampling from multiple tasks is presented. The multiple tasks are themselves generated by sampling from a distribution over an environment of related tasks. Such an environment is shown to be naturally modelled within a Bayesian context by the concept of an objective prior distribution. It is argued that for many common machine learning problems, although in general we do not know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by learning sufficiently many tasks from the environment. In addition, bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, but the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous. The theory is applied to the problem of learning a common feature set or equivalently a low-dimensional-representation (LDR) for an environment of related tasks."
            },
            "slug": "A-Bayesian/Information-Theoretic-Model-of-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that for many common machine learning problems, although in general the authors do not know the true (objective) prior for the problem, they do have some idea of a set of possible priors to which the true prior belongs."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "Examples borrowed from other classes are given a lower weight in the cost function [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7441811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40903135e329a09c4530bb068130ca9bba02b7a7",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "With the advent of the Internet it is now possible to collect hundreds of millions of images. These images come with varying degrees of label information. \"Clean labels\" can be manually obtained on a small fraction, \"noisy labels\" may be extracted automatically from surrounding text, while for most images there are no labels at all. Semi-supervised learning is a principled framework for combining these different label sources. However, it scales polynomially with the number of images, making it impractical for use on gigantic collections with hundreds of millions of images and thousands of classes. In this paper we show how to utilize recent results in machine learning to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images. Specifically, we use the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators. Our algorithm enables us to apply semi-supervised learning to a database of 80 million images gathered from the Internet."
            },
            "slug": "Semi-Supervised-Learning-in-Gigantic-Image-Fergus-Weiss",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning in Gigantic Image Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper uses the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9578863"
                        ],
                        "name": "Brendan A. Juba",
                        "slug": "Brendan-A.-Juba",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Juba",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brendan A. Juba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 130
                            }
                        ],
                        "text": "Our approach to bounding LBayes(ZT ) follows that of previous work on Bayesian GLM regret bounds with logloss (Kakade & Ng, 2004; Kakade et al., 2005; Seeger et al., 2008), relying on the following well-known result: Proposition 2.1 (Kakade & Ng (2004); Banerjee (2006))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10419296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e405a104716c2146af98eb0de61549d389e508e3",
            "isKey": true,
            "numCitedBy": 39,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that it is possible to use data compression on independently obtained hypotheses from various tasks to algorithmically provide guarantees that the tasks are sufficiently related to benefit from multitask learning. We give uniform bounds in terms of the empirical average error for the true average error of the n hypotheses provided by deterministic learning algorithms drawing independent samples from a set of n unknown computable task distributions over finite sets."
            },
            "slug": "Estimating-relatedness-via-data-compression-Juba",
            "title": {
                "fragments": [],
                "text": "Estimating relatedness via data compression"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "It is shown that it is possible to use data compression on independently obtained hypotheses from various tasks to algorithmically provide guarantees that the tasks are sufficiently related to benefit from multitask learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144774720"
                        ],
                        "name": "M. H. Mahmud",
                        "slug": "M.-H.-Mahmud",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Mahmud",
                            "middleNames": [
                                "M.",
                                "Hassan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Mahmud"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7803839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08b74486974c2e0ae8ef11ec9bf83d05a6bd28c2",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-universal-transfer-learning-Mahmud",
            "title": {
                "fragments": [],
                "text": "On universal transfer learning"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9803204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "727e1e16ede6eaad241bad11c525da07b154c688",
            "isKey": false,
            "numCitedBy": 972,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task."
            },
            "slug": "A-Model-of-Inductive-Bias-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Model of Inductive Bias Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Under certain restrictions on the set of all hypothesis spaces available to the learner, it is shown that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346320"
                        ],
                        "name": "Dean P. Foster",
                        "slug": "Dean-P.-Foster",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Foster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dean P. Foster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 110
                            }
                        ],
                        "text": "Our approach to bounding LBayes(ZT ) follows that of previous work on Bayesian GLM regret bounds with logloss (Kakade & Ng, 2004; Kakade et al., 2005; Seeger et al., 2008), relying on the following well-known result: Proposition 2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 114
                            }
                        ],
                        "text": "4 In the infinite-dimensional case, Gaussian processes can be used while still obtaining meaningful regret bounds (Kakade et al., 2005; Seeger et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 76
                            }
                        ],
                        "text": "Regret bound for a number of Bayesian models have previously been developed (Vovk, 2001; Kakade & Ng, 2004; Kakade et al., 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7081744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29119a67e5a88e9c5822d9e651f55c6d4ec2f325",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a competitive analysis of some non-parametric Bayesian algorithms in a worst-case online learning setting, where no probabilistic assumptions about the generation of the data are made. We consider models which use a Gaussian process prior (over the space of all functions) and provide bounds on the regret (under the log loss) for commonly used non-parametric Bayesian algorithms \u2014 including Gaussian regression and logistic regression \u2014 which show how these algorithms can perform favorably under rather general conditions. These bounds explicitly handle the infinite dimensionality of these non-parametric classes in a natural way. We also make formal connections to the minimax and minimum description length (MDL) framework. Here, we show precisely how Bayesian Gaussian regression is a minimax strategy."
            },
            "slug": "Worst-Case-Bounds-for-Gaussian-Process-Models-Kakade-Seeger",
            "title": {
                "fragments": [],
                "text": "Worst-Case Bounds for Gaussian Process Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Here, it is shown precisely how Bayesian Gaussian regression is a minimax strategy, which explicitly handles the infinite dimensionality of these non-parametric classes in a natural way."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "However, extension to learning multi-level tree structured model can be accomplished using a nested CRP prior [3], which extends CRP to nested sequence of partitions, one for each level of the tree."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1269561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28e245ce0e06f398dd26a9d6ab6bb04ef4c016e6",
            "isKey": false,
            "numCitedBy": 1065,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting\u2014which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts."
            },
            "slug": "Hierarchical-Topic-Models-and-the-Nested-Chinese-Blei-Griffiths",
            "title": {
                "fragments": [],
                "text": "Hierarchical Topic Models and the Nested Chinese Restaurant Process"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A Bayesian approach is taken to generate an appropriate prior via a distribution on partitions that allows arbitrarily large branching factors and readily accommodates growing data collections."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2322407"
                        ],
                        "name": "O. Catoni",
                        "slug": "O.-Catoni",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Catoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Catoni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1207,
                                "start": 133
                            }
                        ],
                        "text": "We now develop a connection between regret and risk bounds via a PAC-Bayesian analysis (McAllester, 2003; Audibert & Bousquet, 2007; Catoni, 2007). Such bounds also have the benefit of applying to any bounded loss (e.g., the 0-1 loss for binary classification), which may be more task-relevant than the log-loss. In the batch setting, the data ZT are received all at once by the learner and are assumed to be distributed i.i.d. according to some distribution D over X \u00d7 Y: (xt, yt) i.i.d. \u223c D, t = 1, . . . , T . Let ` be a bounded loss function taking a probability distribution over Y and an element of Y as arguments. Without loss of generality assume ` \u2208 [0, 1]. Writing `\u03b8(x, y) , `(p(\u00b7 |x,\u03b8), y), for any distribution Q over \u0398, let L(Q) , E(x,y)\u223cDE\u03b8\u223cQ[`\u03b8(x, y)] L\u0302(Q,ZT ) , T\u22121 \u2211T t=1 E\u03b8\u223cQ[`\u03b8(xt, yt)] be, respectively, the expected and empirical losses under Q. PAC-Bayesian analyses consider the risk of the Gibbs predictor for the distribution Q (i.e., sample \u03b8 \u223c Q, predict with p(\u00b7 |x,\u03b8)), not the model average over Q (i.e., predict with \u222b p(\u00b7 |x,\u03b8)Q(d\u03b8)). A typical bound (specialized to the Bayesian setting) is the following (here pT (\u03b8) , p(\u03b8 |ZT )): Theorem 3.1 (Audibert & Bousquet (2007))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 87
                            }
                        ],
                        "text": "We now develop a connection between regret and risk bounds via a PAC-Bayesian analysis (McAllester, 2003; Audibert & Bousquet, 2007; Catoni, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 88517530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "598b49e0b8e6a4c9a38b8e92164397fb83df7ce8",
            "isKey": true,
            "numCitedBy": 320,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This monograph deals with adaptive supervised classification, using tools borrowed from statistical mechanics and information theory, stemming from the PACBayesian approach pioneered by David McAllester and applied to a conception of statistical learning theory forged by Vladimir Vapnik. Using convex analysis on the set of posterior probability measures, we show how to get local measures of the complexity of the classification model involving the relative entropy of posterior distributions with respect to Gibbs posterior measures. We then discuss relative bounds, comparing the generalization error of two classification rules, showing how the margin assumption of Mammen and Tsybakov can be replaced with some empirical measure of the covariance structure of the classification model.We show how to associate to any posterior distribution an effective temperature relating it to the Gibbs prior distribution with the same level of expected error rate, and how to estimate this effective temperature from data, resulting in an estimator whose expected error rate converges according to the best possible power of the sample size adaptively under any margin and parametric complexity assumptions. We describe and study an alternative selection scheme based on relative bounds between estimators, and present a two step localization technique which can handle the selection of a parametric model from a family of those. We show how to extend systematically all the results obtained in the inductive setting to transductive learning, and use this to improve Vapnik's generalization bounds, extending them to the case when the sample is made of independent non-identically distributed pairs of patterns and labels. Finally we review briefly the construction of Support Vector Machines and show how to derive generalization bounds for them, measuring the complexity either through the number of support vectors or through the value of the transductive or inductive margin."
            },
            "slug": "PAC-BAYESIAN-SUPERVISED-CLASSIFICATION:-The-of-Catoni",
            "title": {
                "fragments": [],
                "text": "PAC-BAYESIAN SUPERVISED CLASSIFICATION: The Thermodynamics of Statistical Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An alternative selection scheme based on relative bounds between estimators is described and study, and a two step localization technique which can handle the selection of a parametric model from a family of those is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015507"
                        ],
                        "name": "Jean-Yves Audibert",
                        "slug": "Jean-Yves-Audibert",
                        "structuredName": {
                            "firstName": "Jean-Yves",
                            "lastName": "Audibert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Yves Audibert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17591405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ffbf4c750304d17c864f1f2d6812155501bf36c",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, first, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds."
            },
            "slug": "Combining-PAC-Bayesian-and-Generic-Chaining-Bounds-Audibert-Bousquet",
            "title": {
                "fragments": [],
                "text": "Combining PAC-Bayesian and Generic Chaining Bounds"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work combines the PAC-Bayes approach introduced by McAllester (1998), with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand, in a way that also takes into account the variance of the combined functions."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741460"
                        ],
                        "name": "H. Ishwaran",
                        "slug": "H.-Ishwaran",
                        "structuredName": {
                            "firstName": "Hemant",
                            "lastName": "Ishwaran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ishwaran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144319330"
                        ],
                        "name": "J. S. Rao",
                        "slug": "J.-S.-Rao",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Rao",
                            "middleNames": [
                                "Sunil"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. S. Rao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9004248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "315eb50f7bea92e1f2d569e511e009e8e1ed3aa8",
            "isKey": false,
            "numCitedBy": 850,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure\u2019s ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty."
            },
            "slug": "Spike-and-slab-variable-selection:-Frequentist-and-Ishwaran-Rao",
            "title": {
                "fragments": [],
                "text": "Spike and slab variable selection: Frequentist and Bayesian strategies"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper introduces a variable selection method referred to as a rescaled spike and slab model, and studies the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144205696"
                        ],
                        "name": "A. Banerjee",
                        "slug": "A.-Banerjee",
                        "structuredName": {
                            "firstName": "Arindam",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Banerjee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 2
                            }
                        ],
                        "text": ", Banerjee (2007). In particular, if p , q for some constant 0 < q < 1, then R Bayes(Z,\u03b8 \u2217) is at most \u2016\u03b8\u2217\u20162 2\u03c32 +m ln n 1\u2212 q + ln 1 q + m 2 ln ( 1 + Tc\u03c3(2) m ) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 470,
                                "start": 8
                            }
                        ],
                        "text": ", 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient. For a discussion of more general (but asymptotic) Bayesian regret bounds for exponential families and other sufficiently \u201cregular\u201d model classes, see Gr\u00fcnwald (2007, Chapter 8). We follow the approach originally taken in Kakade & Ng (2004), and further explored in Kakade et al. (2005), Banerjee (2006), and Seeger et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 424,
                                "start": 8
                            }
                        ],
                        "text": ", 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient. For a discussion of more general (but asymptotic) Bayesian regret bounds for exponential families and other sufficiently \u201cregular\u201d model classes, see Gr\u00fcnwald (2007, Chapter 8). We follow the approach originally taken in Kakade & Ng (2004), and further explored in Kakade et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 76
                            }
                        ],
                        "text": "Regret bound for a number of Bayesian models have previously been developed (Vovk, 2001; Kakade & Ng, 2004; Kakade et al., 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 513,
                                "start": 8
                            }
                        ],
                        "text": ", 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient. For a discussion of more general (but asymptotic) Bayesian regret bounds for exponential families and other sufficiently \u201cregular\u201d model classes, see Gr\u00fcnwald (2007, Chapter 8). We follow the approach originally taken in Kakade & Ng (2004), and further explored in Kakade et al. (2005), Banerjee (2006), and Seeger et al. (2008), which applies to a large class of Bayesian generalized linear models (GLMs)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6618209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79b8b874f5604c75460494543d0f4398d4c62097",
            "isKey": true,
            "numCitedBy": 68,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that several important Bayesian bounds studied in machine learning, both in the batch as well as the online setting, arise by an application of a simple compression lemma. In particular, we derive (i) PAC-Bayesian bounds in the batch setting, (ii) Bayesian log-loss bounds and (iii) Bayesian bounded-loss bounds in the online setting using the compression lemma. Although every setting has different semantics for prior, posterior and loss, we show that the core bound argument is the same. The paper simplifies our understanding of several important and apparently disparate results, as well as brings to light a powerful tool for developing similar arguments for other methods."
            },
            "slug": "On-Bayesian-bounds-Banerjee",
            "title": {
                "fragments": [],
                "text": "On Bayesian bounds"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The paper simplifies the understanding of several important and apparently disparate results, as well as bringing to light a powerful tool for developing similar arguments for other methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144774720"
                        ],
                        "name": "M. H. Mahmud",
                        "slug": "M.-H.-Mahmud",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Mahmud",
                            "middleNames": [
                                "M.",
                                "Hassan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Mahmud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1874339"
                        ],
                        "name": "S. Ray",
                        "slug": "S.-Ray",
                        "structuredName": {
                            "firstName": "Sylvian",
                            "lastName": "Ray",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 717220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f63cdc711a4f42a65d3b2b3cbe9e2b919a995f5",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In transfer learning we aim to solve new problems using fewer examples using information gained from solving related problems. Transfer learning has been successful in practice, and extensive PAC analysis of these methods has been developed. However it is not yet clear how to define relatedness between tasks. This is considered as a major problem as it is conceptually troubling and it makes it unclear how much information to transfer and when and how to transfer it. In this paper we propose to measure the amount of information one task contains about another using conditional Kolmogorov complexity between the tasks. We show how existing theory neatly solves the problem of measuring relatedness and transferring the 'right' amount of information in sequential transfer learning in a Bayesian setting. The theory also suggests that, in a very formal and precise sense, no other reasonable transfer method can do much better than our Kolmogorov Complexity theoretic transfer method, and that sequential transfer is always justified. We also develop a practical approximation to the method and use it to transfer information between 8 arbitrarily chosen databases from the UCI ML repository."
            },
            "slug": "Transfer-Learning-using-Kolmogorov-Complexity:-and-Mahmud-Ray",
            "title": {
                "fragments": [],
                "text": "Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper shows how existing theory neatly solves the problem of measuring relatedness and transferring the 'right' amount of information in sequential transfer learning in a Bayesian setting, and suggests that no other reasonable transfer method can do much better than the Kolmogorov Complexity theoretic transfer method."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 942059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "744e7239c7e466468dd928feb3d265ebdd097b40",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a competitive analysis of Bayesian learning algorithms in the online learning setting and show that many simple Bayesian algorithms (such as Gaussian linear regression and Bayesian logistic regression) perform favorably when compared, in retrospect, to the single best model in the model class. The analysis does not assume that the Bayesian algorithms' modeling assumptions are \"correct,\" and our bounds hold even if the data is adversarially chosen. For Gaussian linear regression (using logloss), our error bounds are comparable to the best bounds in the online learning literature, and we also provide a lower bound showing that Gaussian linear regression is optimal in a certain worst case sense. We also give bounds for some widely used maximum a posteriori (MAP) estimation algorithms, including regularized logistic regression."
            },
            "slug": "Online-Bounds-for-Bayesian-Algorithms-Kakade-Ng",
            "title": {
                "fragments": [],
                "text": "Online Bounds for Bayesian Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is shown that many simple Bayesian algorithms (such as Gaussian linear regression and Bayesian logistic regression) perform favorably when compared, in retrospect, to the single best model in the model class."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346320"
                        ],
                        "name": "Dean P. Foster",
                        "slug": "Dean-P.-Foster",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Foster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dean P. Foster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 110
                            }
                        ],
                        "text": "Our approach to bounding LBayes(ZT ) follows that of previous work on Bayesian GLM regret bounds with logloss (Kakade & Ng, 2004; Kakade et al., 2005; Seeger et al., 2008), relying on the following well-known result: Proposition 2."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 114
                            }
                        ],
                        "text": "4 In the infinite-dimensional case, Gaussian processes can be used while still obtaining meaningful regret bounds (Kakade et al., 2005; Seeger et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 76
                            }
                        ],
                        "text": "Regret bound for a number of Bayesian models have previously been developed (Vovk, 2001; Kakade & Ng, 2004; Kakade et al., 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2197379,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8348741ca22058f9955f932bd9d63ccd9e81319",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian nonparametric models are widely and successfully used for statistical prediction. While posterior consistency properties are well studied in quite general settings, results have been proved using abstract concepts such as metric entropy, and they come with subtle conditions which are hard to validate and not intuitive when applied to concrete models. Furthermore, convergence rates are difficult to obtain. By focussing on the concept of information consistency for Bayesian Gaussian process (GP)models, consistency results and convergence rates are obtained via a regret bound on cumulative log loss. These results depend strongly on the covariance function of the prior process, thereby giving a novel interpretation to penalization with reproducing kernel Hilbert space norms and to commonly used covariance function classes and their parameters. The proof of the main result employs elementary convexity arguments only. A theorem of Widom is used in order to obtain precise convergence rates for several covariance functions widely used in practice."
            },
            "slug": "Information-Consistency-of-Nonparametric-Gaussian-Seeger-Kakade",
            "title": {
                "fragments": [],
                "text": "Information Consistency of Nonparametric Gaussian Process Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "By focussing on the concept of information consistency for Bayesian Gaussian process (GP)models, consistency results and convergence rates are obtained via a regret bound on cumulative log loss."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 116
                            }
                        ],
                        "text": "A popular non-Bayesian approach for inducing sparsity is `1 regularization, such as the lasso for linear regression (Tibshirani, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16162039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5",
            "isKey": false,
            "numCitedBy": 36486,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described."
            },
            "slug": "Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Regression Shrinkage and Selection via the Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new method for estimation in linear models called the lasso, which minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144205696"
                        ],
                        "name": "A. Banerjee",
                        "slug": "A.-Banerjee",
                        "structuredName": {
                            "firstName": "Arindam",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Banerjee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5878209,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "a6c908e5846a905628e4b652b28f3e788aa57b7b",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Logistic models are arguably one of the most widely used data analysis techniques. In this paper, we present analyses focussing on two important aspects of logistic models\u2014its relationship with exponential family based generative models, and its performance in online and potentially adversarial settings. In particular, we present two new theoretical results on logistic models focusing on the above two aspects. First, we establish an exact connection between logistic models and exponential family based generative models, resolving a long-standing ambiguity over their relationship. Second, we show that online Bayesian logistic models are competitive to the best batch models, even in potentially adversarial settings. Further, we discuss relevant connections of our analysis to the literature on integral transforms, and also present a new optimality result for Bayesian models. The analysis makes a strong case for using logistic models and partly explains the success of such models for a wide range of practical problems."
            },
            "slug": "An-Analysis-of-Logistic-Models:-Exponential-Family-Banerjee",
            "title": {
                "fragments": [],
                "text": "An Analysis of Logistic Models: Exponential Family Connections and Online Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "An exact connection between logistic models and exponential family based generative models is established, resolving a long-standing ambiguity over their relationship, and it is shown that online Bayesian logistics models are competitive to the best batch models, even in potentially adversarial settings."
            },
            "venue": {
                "fragments": [],
                "text": "SDM"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 87
                            }
                        ],
                        "text": "We now develop a connection between regret and risk bounds via a PAC-Bayesian analysis (McAllester, 2003; Audibert & Bousquet, 2007; Catoni, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14620324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e252a389eedc07622bf6d90159dae2fa496cb53",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The theoretical understanding of support vector machines is largely based on margin bounds for linear classifiers with unit-norm weight vectors and unit-norm feature vectors. Unit-norm margin bounds have been proved previously using fat-shattering arguments and Rademacher complexity. Recently Langford and Shawe-Taylor proved a dimension-independent unit-norm margin bound using a relatively simple PAC-Bayesian argument. Unfortunately, the Langford-Shawe-Taylor bound is stated in a variational form making direct comparison to fat-shattering bounds difficult. This paper provides an explicit solution to the variational problem implicit in the Langford-Shawe-Taylor bound and shows that the PAC-Bayesian margin bounds are significantly tighter. Because a PAC-Bayesian bound is derived from a particular prior distribution over hypotheses, a PAC-Bayesian margin bound also seems to provide insight into the nature of the learning bias underlying the bound."
            },
            "slug": "Simplified-PAC-Bayesian-Margin-Bounds-McAllester",
            "title": {
                "fragments": [],
                "text": "Simplified PAC-Bayesian Margin Bounds"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper provides an explicit solution to the variational problem implicit in the Langford-Shawe-Taylor bound and shows that the PAC-Bayesian margin bounds are significantly tighter."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143982131"
                        ],
                        "name": "Trevor H Park",
                        "slug": "Trevor-H-Park",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Park",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor H Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144898907"
                        ],
                        "name": "G. Casella",
                        "slug": "G.-Casella",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Casella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Casella"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11797924,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "785ede21535ff3a4ccc75daa990e45363db3a363",
            "isKey": false,
            "numCitedBy": 2471,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant."
            },
            "slug": "The-Bayesian-Lasso-Park-Casella",
            "title": {
                "fragments": [],
                "text": "The Bayesian Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055339284"
                        ],
                        "name": "Ulrike Schneider",
                        "slug": "Ulrike-Schneider",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Schneider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ulrike Schneider"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40023580"
                        ],
                        "name": "J. Corcoran",
                        "slug": "J.-Corcoran",
                        "structuredName": {
                            "firstName": "Jem",
                            "lastName": "Corcoran",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Corcoran"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119694870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "845fc005ca1a4e46927325c4dcec2f1540b936a5",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Perfect-sampling-for-Bayesian-variable-selection-in-Schneider-Corcoran",
            "title": {
                "fragments": [],
                "text": "Perfect sampling for Bayesian variable selection in a linear regression model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 206
                            }
                        ],
                        "text": "Specifically, we analyze a canonical use of a hierarchical prior \u2014 to capture greater uncertainty in the value of a parameter by placing a hyperprior on the variance of the Gaussian prior on that parameter (Berger, 1985; Bishop, 2006; Gelman et al., 2013): \u03c3(2) 0 |\u03b1, \u03b2 \u223c \u0393\u22121(\u03b1, \u03b2) and \u03b8i |\u03bc0, \u03c3(2) 0 \u223c N(\u03bc0, \u03c3(2) 0), where \u0393\u22121(\u03b1, \u03b2) is the inverse gamma distribution with shape \u03b1 and scale \u03b2."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31993898,
            "fieldsOfStudy": [
                "Mathematics",
                "Art"
            ],
            "id": "3bb5a439a0d610a7eac68f73068cdd278b8c9775",
            "isKey": false,
            "numCitedBy": 21003,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "the selection of symmetric factorial designs, that is, a design where all factors have the same number of levels. Chapter 3 focuses on selection of two-level factorial designs and discusses complementary design theory and related topics in the selection of designs. Chapter 4 covers the selection of three level designs followed by the general case of s-levels. Chapter 5 discusses estimation capacity, presenting the connections with complementary designs followed by the estimation capacity for two-level and s-level designs. Chapter 6 discusses and presents results for the construction of mixed-level designs. Giving many examples of the use of mixed two and four-level designs. The final unit of the book discusses designs where there are two-different groups of factors. Chapters 7 and 8 discuss factorial designs with restricted randomization. Focusing first on blocked designs for full factorials as well as blocked fractional factorial designs. Chapter 8 focuses on split-plot designs. The booked is concluded with a chapter on robust parameter designs. This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion. However, the authors do a wonderful job of keeping the statistical methodology at the forefront of the book and the mathematical detail is presented as the necessary tool to study these designs. The book will serve as a great text for an advanced graduate level course in design theory for students with the necessary mathematical background. The book will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments. In addition, practitioners will also find the book useful for the comprehensive collection of optimal designs presented at the end of many chapters. Overall, this is a very well written book and a necessary addition to the existing literature on the design of factorial experiments."
            },
            "slug": "Pattern-Recognition-and-Machine-Learning-Neal",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion and will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755694"
                        ],
                        "name": "G. Lugosi",
                        "slug": "G.-Lugosi",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Lugosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lugosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28709420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9fa9777415729cebfcbcb5ee667057e981f9912e",
            "isKey": false,
            "numCitedBy": 3228,
            "numCiting": 302,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction 2. Prediction with expert advice 3. Tight bounds for specific losses 4. Randomized prediction 5. Efficient forecasters for large classes of experts 6. Prediction with limited feedback 7. Prediction and playing games 8. Absolute loss 9. Logarithmic loss 10. Sequential investment 11. Linear pattern recognition 12. Linear classification 13. Appendix."
            },
            "slug": "Prediction,-learning,-and-games-Cesa-Bianchi-Lugosi",
            "title": {
                "fragments": [],
                "text": "Prediction, learning, and games"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This chapter discusses prediction with expert advice, efficient forecasters for large classes of experts, and randomized prediction for specific losses."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "Other models use external non-visual hierarchies such as wordnet [9] (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "The second model, called WordnetTree, uses wordnet [9] to define a fixed tree hierarchy(4)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5958691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d87ceda3042f781c341ac17109d1e94a717f5f60",
            "isKey": false,
            "numCitedBy": 13575,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet."
            },
            "slug": "WordNet-:-an-electronic-lexical-database-Fellbaum",
            "title": {
                "fragments": [],
                "text": "WordNet : an electronic lexical database"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The lexical database: nouns in WordNet, Katherine J. Miller a semantic network of English verbs, and applications of WordNet: building semantic concordances are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804386"
                        ],
                        "name": "A. Raftery",
                        "slug": "A.-Raftery",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Raftery",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Raftery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49113393"
                        ],
                        "name": "Jennifer Chunn",
                        "slug": "Jennifer-Chunn",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Chunn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennifer Chunn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2929365"
                        ],
                        "name": "P. Gerland",
                        "slug": "P.-Gerland",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Gerland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gerland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145042813"
                        ],
                        "name": "H. Sevcikova",
                        "slug": "H.-Sevcikova",
                        "structuredName": {
                            "firstName": "Hana",
                            "lastName": "Sevcikova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sevcikova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 329,
                                "start": 280
                            }
                        ],
                        "text": "For example, one might instead consider the hierarchical models have been used in political science for analyzing polling and census data to predict election outcomes (Ghitza & Gelman, 2013) and in demography for predicting population growth, life expectancy, and fertility rates (Raftery et al., 2013; 2012; Alkema et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15421426,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "cae1907302ae27a4e6077b938736caa7e40f711f",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a Bayesian hierarchical model for producing probabilistic forecasts of male period life expectancy at birth for all the countries of the world to 2100. Such forecasts would be an input to the production of probabilistic population projections for all countries, which is currently being considered by the United Nations. To evaluate the method, we conducted an out-of-sample cross-validation experiment, fitting the model to the data from 1950\u20131995 and using the estimated model to forecast for the subsequent 10 years. The 10-year predictions had a mean absolute error of about 1 year, about 40 % less than the current UN methodology. The probabilistic forecasts were calibrated in the sense that, for example, the 80 % prediction intervals contained the truth about 80 % of the time. We illustrate our method with results from Madagascar (a typical country with steadily improving life expectancy), Latvia (a country that has had a mortality crisis), and Japan (a leading country). We also show aggregated results for South Asia, a region with eight countries. Free, publicly available R software packages called bayesLife and bayesDem are available to implement the method."
            },
            "slug": "Bayesian-Probabilistic-Projections-of-Life-for-All-Raftery-Chunn",
            "title": {
                "fragments": [],
                "text": "Bayesian Probabilistic Projections of Life Expectancy for All Countries"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A Bayesian hierarchical model for producing probabilistic forecasts of male period life expectancy at birth for all the countries of the world to 2100 is proposed and illustrated with results from Madagascar, Latvia, Japan, and Japan."
            },
            "venue": {
                "fragments": [],
                "text": "Demography"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804386"
                        ],
                        "name": "A. Raftery",
                        "slug": "A.-Raftery",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Raftery",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Raftery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50599161"
                        ],
                        "name": "Nan Li",
                        "slug": "Nan-Li",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nan Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145042813"
                        ],
                        "name": "H. Sevcikova",
                        "slug": "H.-Sevcikova",
                        "structuredName": {
                            "firstName": "Hana",
                            "lastName": "Sevcikova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sevcikova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2929365"
                        ],
                        "name": "P. Gerland",
                        "slug": "P.-Gerland",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Gerland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gerland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40399884"
                        ],
                        "name": "G. Heilig",
                        "slug": "G.-Heilig",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Heilig",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Heilig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12771371,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "177817a0c689bdb611ff6238249daf1c760f505f",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Projections of countries\u2019 future populations, broken down by age and sex, are widely used for planning and research. They are mostly done deterministically, but there is a widespread need for probabilistic projections. We propose a Bayesian method for probabilistic population projections for all countries. The total fertility rate and female and male life expectancies at birth are projected probabilistically using Bayesian hierarchical models estimated via Markov chain Monte Carlo using United Nations population data for all countries. These are then converted to age-specific rates and combined with a cohort component projection model. This yields probabilistic projections of any population quantity of interest. The method is illustrated for five countries of different demographic stages, continents and sizes. The method is validated by an out of sample experiment in which data from 1950\u20131990 are used for estimation, and applied to predict 1990\u20132010. The method appears reasonably accurate and well calibrated for this period. The results suggest that the current United Nations high and low variants greatly underestimate uncertainty about the number of oldest old from about 2050 and that they underestimate uncertainty for high fertility countries and overstate uncertainty for countries that have completed the demographic transition and whose fertility has started to recover towards replacement level, mostly in Europe. The results also indicate that the potential support ratio (persons aged 20\u201364 per person aged 65+) will almost certainly decline dramatically in most countries over the coming decades."
            },
            "slug": "Bayesian-probabilistic-population-projections-for-Raftery-Li",
            "title": {
                "fragments": [],
                "text": "Bayesian probabilistic population projections for all countries"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results suggest that the current United Nations high and low variants greatly underestimate uncertainty about the number of oldest old from about 2050 and that they underestimate uncertainty for high fertility countries and overstate uncertainty for countries that have completed the demographic transition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2621402"
                        ],
                        "name": "Yair Ghitza",
                        "slug": "Yair-Ghitza",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Ghitza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Ghitza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144389145"
                        ],
                        "name": "A. Gelman",
                        "slug": "A.-Gelman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6925574,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "6c9dd5cdc711e63bdca77a22072b098f3a23fec7",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Usingmultilevelregressionandpoststratification(MRP),weestimatevoterturnoutandvotechoicewithindeeplyinteracted subgroups: subsets of the population that are defined by multiple demographic and geographic characteristics. This article lays out the models and statistical procedures we use, along with the steps required to fit the model for the 2004 and 2008 presidential elections. Though MRP is an increasingly popular method, we improve upon it in numerous ways: deeper levels ofcovariateinteraction,allowingfornonlinearityandnonmonotonicity,accountingforunequalinclusionprobabilitiesthat areconveyedinsurveyweights,postestimationadjustmentstoturnoutandvotinglevels,andinformativemultidimensional graphical displays as a form of model checking. We use a series of examples to demonstrate the flexibility of our method, including an illustration of turnout and vote choice as subgroups become increasingly detailed, and an analysis of both vote choice changes and turnout changes from 2004 to 2008."
            },
            "slug": "Deep-Interactions-with-MRP:-Election-Turnout-and-Ghitza-Gelman",
            "title": {
                "fragments": [],
                "text": "Deep Interactions with MRP: Election Turnout and Voting Patterns Among Small Electoral Subgroups"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 937,
                                "start": 47
                            }
                        ],
                        "text": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1164,
                                "start": 47
                            }
                        ],
                        "text": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al. (2013) both contain further examples of problems in which hierarchical modeling is critical to obtaining high-quality inferences. Within the machine learning and vision literature, Salakhutdinov et al. (2011) offers an illustrative case study in the benefits and the pitfalls of employing a hierarchical model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 962,
                                "start": 47
                            }
                        ],
                        "text": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al. (2013) both contain further examples of problems in which hierarchical modeling is critical to obtaining high-quality inferences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 46
                            }
                        ],
                        "text": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2180,
                                "start": 47
                            }
                        ],
                        "text": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al. (2013) both contain further examples of problems in which hierarchical modeling is critical to obtaining high-quality inferences. Within the machine learning and vision literature, Salakhutdinov et al. (2011) offers an illustrative case study in the benefits and the pitfalls of employing a hierarchical model. The motivation of Salakhutdinov et al. (2011) was that, for image classification tasks, some categories of objects (e.g., \u201ccar\u201d or \u201cdog\u201d) have many labeled positive and negative examples while other, visually related, categories (e.g., \u201cbus\u201d or \u201canteater\u201d) have only a few labeled examples. Fig. 1(right, a) shows the distribution of training examples for the 200 object categories used while Fig. 1(right, b) shows the same distribution, but now objects are grouped with those with similar appearances. In both cases, the distributions are fat-tailed: there are a few categories with many training examples and many categories with a few training examples. It was hypothesized that by using a hierarchical Bayesian model, the classes with large amounts of labeled data could be used to construct better classifiers for the classes with small amounts of labeled data. The model used by Salakhutdinov et al. (2011), which ar X iv :1 50 5."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 206
                            }
                        ],
                        "text": "Specifically, we analyze a canonical use of a hierarchical prior \u2014 to capture greater uncertainty in the value of a parameter by placing a hyperprior on the variance of the Gaussian prior on that parameter (Berger, 1985; Bishop, 2006; Gelman et al., 2013): \u03c3(2) 0 |\u03b1, \u03b2 \u223c \u0393\u22121(\u03b1, \u03b2) and \u03b8i |\u03bc0, \u03c3(2) 0 \u223c N(\u03bc0, \u03c3(2) 0), where \u0393\u22121(\u03b1, \u03b2) is the inverse gamma distribution with shape \u03b1 and scale \u03b2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1312,
                                "start": 47
                            }
                        ],
                        "text": "the hyperparameters of the prior distribution (Berger, 1985; Bernardo & Smith, 2000; Gelman et al., 2013). By explicitly modeling the additional uncertainty, there is greater \u201crobustness\u201d to misspecification and unexpected data. The second is that hierarchical models permit the \u201csharing of statistical strength\u201d between related observations or cohorts (Gelman et al., 2013). For example, take the recent \u201cBig Bayes Stories\u201d special issue of the journal Statistical Science, which was comprised of short articles describing successful applications of Bayesian models to a diverse range of problems, including political science, astronomy, and public health (Mengersen & Robert, 2014). Most of the Bayesian models were hierarchical, and the need for robustness and sharing of statistical strength because of limited data were commonly cited reasons by the practitioners for choosing a hierarchical Bayesian approach. Gelman & Hill (2006) and Gelman et al. (2013) both contain further examples of problems in which hierarchical modeling is critical to obtaining high-quality inferences. Within the machine learning and vision literature, Salakhutdinov et al. (2011) offers an illustrative case study in the benefits and the pitfalls of employing a hierarchical model. The motivation of Salakhutdinov et al. (2011) was that, for image classification tasks, some categories of objects (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": true,
            "numCitedBy": 7325,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5147805"
                        ],
                        "name": "L. Alkema",
                        "slug": "L.-Alkema",
                        "structuredName": {
                            "firstName": "Leontine",
                            "lastName": "Alkema",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Alkema"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804386"
                        ],
                        "name": "A. Raftery",
                        "slug": "A.-Raftery",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Raftery",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Raftery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2929365"
                        ],
                        "name": "P. Gerland",
                        "slug": "P.-Gerland",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Gerland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gerland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34772466"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Clark",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145411220"
                        ],
                        "name": "F. Pelletier",
                        "slug": "F.-Pelletier",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Pelletier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pelletier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47744924"
                        ],
                        "name": "T. Buettner",
                        "slug": "T.-Buettner",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Buettner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Buettner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40399884"
                        ],
                        "name": "G. Heilig",
                        "slug": "G.-Heilig",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Heilig",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Heilig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 329,
                                "start": 280
                            }
                        ],
                        "text": "For example, one might instead consider the hierarchical models have been used in political science for analyzing polling and census data to predict election outcomes (Ghitza & Gelman, 2013) and in demography for predicting population growth, life expectancy, and fertility rates (Raftery et al., 2013; 2012; Alkema et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15259884,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "a247be70413cd6a2f1767fdc1d402f655ff99342",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a Bayesian projection model to produce country-specific projections of the total fertility rate (TFR) for all countries. The model decomposes the evolution of TFR into three phases: pre-transition high fertility, the fertility transition, and post-transition low fertility. The model for the fertility decline builds on the United Nations Population Division\u2019s current deterministic projection methodology, which assumes that fertility will eventually fall below replacement level. It models the decline in TFR as the sum of two logistic functions that depend on the current TFR level, and a random term. A Bayesian hierarchical model is used to project future TFR based on both the country\u2019s TFR history and the pattern of all countries. It is estimated from United Nations estimates of past TFR in all countries using a Markov chain Monte Carlo algorithm. The post-transition low fertility phase is modeled using an autoregressive model, in which long-term TFR projections converge toward and oscillate around replacement level. The method is evaluated using out-of-sample projections for the period since 1980 and the period since 1995, and is found to be well calibrated."
            },
            "slug": "Probabilistic-Projections-of-the-Total-Fertility-Alkema-Raftery",
            "title": {
                "fragments": [],
                "text": "Probabilistic Projections of the Total Fertility Rate for All Countries"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A Bayesian projection model is described to produce country-specific projections of the total fertility rate (TFR) for all countries using an autoregressive model, in which long-term TFR projections converge toward and oscillate around replacement level."
            },
            "venue": {
                "fragments": [],
                "text": "Demography"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8359988"
                        ],
                        "name": "J. Hilbe",
                        "slug": "J.-Hilbe",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Hilbe",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hilbe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 61817962,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "264ce4449468f38bc6f3d14d363cb434bb80537a",
            "isKey": false,
            "numCitedBy": 4575,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Data-Analysis-Using-Regression-and-Models-Hilbe",
            "title": {
                "fragments": [],
                "text": "Data Analysis Using Regression and Multilevel/Hierarchical Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 136
                            }
                        ],
                        "text": "\u03a3 , s(2)\u03c11K + s (2)(1\u2212 \u03c1)I (9) s(2) , \u03c3(2) 0 + \u03c3 (2), \u03c1 , \u03c3(2) 0/(\u03c3 2 0 + \u03c3 (2)), (10) This prior corresponds to the one-level prior in Salakhutdinov et al. (2011). Similar results, which will be discussed qualitatively below, can be obtained for the two-level prior at the cost of a significantly more complicated bound."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1463,
                                "start": 182
                            }
                        ],
                        "text": "Why the different performance characteristics for the two hierarchical models? Why do some categories have improved accuracy while others decreased accuracy? In a post-hoc analysis, Salakhutdinov et al. (2011) note that the \u201cobjects with the largest improvement...borrow visual appearance from other frequent objects\u201d while \u201cobjects with the largest decrease [such as \u2018umbrella\u2019 and \u2018merchandise\u2019] are abstract, and their visual appearance is very different from other object categories.\u201d The results just described lead to numerous theoretical questions of practical consequence: Q1 Can we formalize why for some object classes there was a beneficial sharing of statistical strength, while for other classes the sharing was detrimental? Q2 Can we understand when a flat model should be preferred to a hierarchical one to avoid unfavorable sharing? Q3 More generally, can we obtain guidance on the best type of prior for the problem at hand? Perhaps a different hierarchical prior would have been better suited to learning the image classifiers. For example, could placing hyperpriors on the variance parameters lead to greater \u201crobustness\u201d for object categories such as \u2018umbrella\u2019 and \u2018merchandise,\u2019 whose visual appearance differs from other object categories? Q4 Once the form of the prior has been chosen, how should hyperparameters be set to maximize learning? The settings of the variance hyperparameters was left unspecified by Salakhutdinov et al. (2011), and it is not clear a priori how they should be set, or how much effect their choice will have on learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 56
                            }
                        ],
                        "text": "The two-level regret bound well-explains the results of Salakhutdinov et al. (2011). The poor performance on image classes with very different visual appearance from the other classes is unsurprising since the parameter vectors that predict these classes well are going to have large `(2) distance from the parameter vectors of other object classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 182
                            }
                        ],
                        "text": "Why the different performance characteristics for the two hierarchical models? Why do some categories have improved accuracy while others decreased accuracy? In a post-hoc analysis, Salakhutdinov et al. (2011) note that the \u201cobjects with the largest improvement."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 44
                            }
                        ],
                        "text": "The regret bound for the two-level prior in Salakhutdinov et al. (2011) is quite similar to that for the one-level prior."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 68
                            }
                        ],
                        "text": "This setting is exactly that of the image classification problem of Salakhutdinov et al. (2011). For concreteness, consider a \u201clarge data\u201d task with T (1) n cs2 and a \u201csmall data\u201d task T (2) = 2, so that ln ( 4 3n+T ((1))cs(2) n+T (1)cs2 ) \u2248 0 and (12) becomes (approximately)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to share visual appearance for multiclass object detection"
            },
            "venue": {
                "fragments": [],
                "text": "In Conference on Computer Vision and Pattern Recognition,"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702954"
                        ],
                        "name": "R. Schiffer",
                        "slug": "R.-Schiffer",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Schiffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schiffer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 145019771,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "54a62b853cb80c1de49a7e8d4154d77b6b3369d3",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Psychobiology-of-Language-Schiffer",
            "title": {
                "fragments": [],
                "text": "Psychobiology of Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4632093"
                        ],
                        "name": "G. Zipf",
                        "slug": "G.-Zipf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zipf",
                            "middleNames": [
                                "Kingsley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zipf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "The distribution of words approximates the Zipf\u2019s law [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59628089,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d835c665aa0a14f4269d6ad9c8c2eec0dffc1f9c",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Psychobiology-of-Language-Zipf",
            "title": {
                "fragments": [],
                "text": "The Psychobiology of Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26228557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "615eca6e046814159fffd9bd14852819bcd16d1a",
            "isKey": false,
            "numCitedBy": 857,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Minimum-Description-Length-Principle-Rissanen",
            "title": {
                "fragments": [],
                "text": "Minimum Description Length Principle"
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Machine Learning"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087226"
                        ],
                        "name": "T. Tommasi",
                        "slug": "T.-Tommasi",
                        "structuredName": {
                            "firstName": "Tatiana",
                            "lastName": "Tommasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tommasi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721068"
                        ],
                        "name": "Francesco Orabona",
                        "slug": "Francesco-Orabona",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Orabona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesco Orabona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3033284"
                        ],
                        "name": "B. Caputo",
                        "slug": "B.-Caputo",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Caputo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caputo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 52
                            }
                        ],
                        "text": "Other models learn the relations between categories [30, 21, 22, 1, 25, 13, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 87
                            }
                        ],
                        "text": "Object classes transfer information by regularizing the space of classifier parameters [22, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 192
                            }
                        ],
                        "text": "Most studies on transfer learning for object recognition have focused on multiclass recognition without a background class (saying if a crop image contains an object out of M possible classes [20, 14, 25, 11, 29])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52827789,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04ffc7f4cd64253ccaefd6451474fe3102cafdbb",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning object categories from small samples is a challenging problem, where machine learning tools can in general provide very few guarantees. Exploiting prior knowledge may be useful to reproduce the human capability of recognizing objects even from only one single view. This paper presents an SVM-based model adaptation algorithm able to select and weight appropriately prior knowledge coming from different categories. The method relies on the solution of a convex optimization problem which ensures to have the minimal leave-one-out error on the training set. Experiments on a subset of the Caltech-256 database show that the proposed method produces better results than both choosing one single prior model, and transferring from all previous experience in a flat uninformative way."
            },
            "slug": "Safety-in-numbers:-Learning-categories-from-few-Tommasi-Orabona",
            "title": {
                "fragments": [],
                "text": "Safety in numbers: Learning categories from few examples with multi model knowledge transfer"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents an SVM-based model adaptation algorithm able to select and weight appropriately prior knowledge coming from different categories, which relies on the solution of a convex optimization problem which ensures to have the minimal leave-one-out error on the training set."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 105
                            }
                        ],
                        "text": "This setting is artificial as it is becoming increasingly easy to collect large amounts of training data [23, 18], at least for a subset of the object classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Imagenet"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR,"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 76
                            }
                        ],
                        "text": "Regret bound for a number of Bayesian models have previously been developed (Vovk, 2001; Kakade & Ng, 2004; Kakade et al., 2005; Banerjee, 2006; 2007; Seeger et al., 2008), with a particular focus on regression and simple priors such as independent Gaussian distributions for each regression coefficient."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Competitive Online Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "International Statistical Review,"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our experimental results on the challenging object localization and detection task demonstrate that the proposed model substantially improves the accuracy of the standard single object detectors that ignore hierarchical structure altogether."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CVPR"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 28,
            "methodology": 17,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 63,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-to-share-visual-appearance-for-multiclass-Salakhutdinov-Torralba/47d588f746949b066af6957a524d82ca3c6c961b?sort=total-citations"
}