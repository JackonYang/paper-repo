{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1424216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c0ade464620ed604ff58d9ad64bcfa1bc37a86f",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Preface-to-the-Special-Issue-on-Connectionist-Hinton",
            "title": {
                "fragments": [],
                "text": "Preface to the Special Issue on Connectionist Symbol Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5309076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf895330739ec25aa4077ca375daa2cf3d265215",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A class of fast, supervised learning algorithms is presented. They use local representations, hashing, and multiple scales of resolution to approximate functions which are piece-wise continuous. Inspired by Albus's CMAC model, the algorithms learn orders of magnitude more rapidly than typical implementations of back propagation, while often achieving comparable qualities of generalization. Furthermore, unlike most traditional function approximation methods, the algorithms are well suited for use in real time adaptive signal processing. Unlike simpler adaptive systems, such as linear predictive coding, the adaptive linear combiner, and the Kalman filter, the new algorithms are capable of efficiently capturing the structure of complicated non-linear systems. As an illustration, the algorithm is applied to the prediction of a chaotic timeseries."
            },
            "slug": "Fast-Learning-in-Multi-Resolution-Hierarchies-Moody",
            "title": {
                "fragments": [],
                "text": "Fast Learning in Multi-Resolution Hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A class of fast, supervised learning algorithms inspired by Albus's CMAC model that use local representations, hashing, and multiple scales of resolution to approximate functions which are piece-wise continuous are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112105858"
                        ],
                        "name": "William Y. Huang",
                        "slug": "William-Y.-Huang",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Huang",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Y. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 22
                            }
                        ],
                        "text": "The data (provided by Huang and Lippmann (1988)) consists of a training set with 338 phoneme exemplars and a test set with 333 exemplars."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11607279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1382a29539b3de419d567f679b5f28cee459a49",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on nets with continuous-valued inputs led to generative procedures to construct convex decision regions with two-layer perceptrons (one hidden layer) and arbitrary decision regions with three-layer perceptrons (two hidden layers). Here we demonstrate that two-layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions. Such classifiers are robust, train rapidly, and provide good performance with simple decision regions. When complex decision regions are required, however, convergence time can be excessively long and performance is often no better than that of k-nearest neighbor classifiers. Three neural net classifiers are presented that provide more rapid training under such situations. Two use fixed weights in the first one or two layers and are similar to classifiers that estimate probability density functions using histograms. A third \"feature map classifier\" uses both unsupervised and supervised training. It provides good performance with little supervised training in situations such as speech recognition where much unlabeled training data is available. The architecture of this classifier can be used to implement a neural net k-nearest neighbor classifier."
            },
            "slug": "Neural-Net-and-Traditional-Classifiers-Huang-Lippmann",
            "title": {
                "fragments": [],
                "text": "Neural Net and Traditional Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that two-layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020074"
                        ],
                        "name": "A. Lapedes",
                        "slug": "A.-Lapedes",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Lapedes",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lapedes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542113"
                        ],
                        "name": "R. Farber",
                        "slug": "R.-Farber",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Farber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Farber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18474528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d7de94252e5040a38ebaaf535841d3500791c79",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "There is presently great interest in the abilities of neural networks to mimic \"qualitative reasoning\" by manipulating neural incodings of symbols. Less work has been performed on using neural networks to process floating point numbers and it is sometimes stated that neural networks are somehow inherently inaccurate and therefore best suited for \"fuzzy\" qualitative reasoning. Nevertheless, the potential speed of massively parallel operations make neural net \"number crunching\" an interesting topic to explore. In this paper we discuss some of our work in which we demonstrate that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques. In particular, prediction of future values of a chaotic time series can be performed with exceptionally high accuracy. We analyze how a neural net is able to do this, and in the process show that a large class of functions from Rn \u2192 Rm may be accurately approximated by a backpropagation neural net with just two \"hidden\" layers. The network uses this functional approximation to perform either interpolation (signal processing applications) or extrapolation (symbol processing applications). Neural nets therefore use quite familiar methods to perform their tasks. The geometrical viewpoint advocated here seems to be a useful approach to analyzing neural network operation and relates neural networks to well studied topics in functional approximation."
            },
            "slug": "How-Neural-Nets-Work-Lapedes-Farber",
            "title": {
                "fragments": [],
                "text": "How Neural Nets Work"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper demonstrates that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques, and shows that prediction of future values of a chaotic time series can be performed with exceptionally high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8275028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8778bb692cf105254fe767ef11a3a8afac4a068",
            "isKey": false,
            "numCitedBy": 3816,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets."
            },
            "slug": "An-introduction-to-computing-with-neural-nets-Lippmann",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification and exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14892653,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "089a76dbc62a06ad30ae1925530e8733e850268e",
            "isKey": false,
            "numCitedBy": 3702,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >"
            },
            "slug": "Networks-for-approximation-and-learning-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Networks for approximation and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758350"
                        ],
                        "name": "M. Kramer",
                        "slug": "M.-Kramer",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Kramer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kramer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31970911"
                        ],
                        "name": "J. A. Leonard",
                        "slug": "J.-A.-Leonard",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Leonard",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. A. Leonard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62570293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d564b9ffa5ac79bfb9d53b846eaf53d4f201f8de",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Diagnosis-using-backpropagation-neural-and-Kramer-Leonard",
            "title": {
                "fragments": [],
                "text": "Diagnosis using backpropagation neural networks\u2014analysis and criticism"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020074"
                        ],
                        "name": "A. Lapedes",
                        "slug": "A.-Lapedes",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Lapedes",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lapedes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542113"
                        ],
                        "name": "R. Farber",
                        "slug": "R.-Farber",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Farber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Farber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 17
                            }
                        ],
                        "text": "After this manuscript was accepted for publication, we learned that Hanson and Burr (1987) had suggested using a single layer of locally-tuned units in place of two layers of sigmoidal or threshold units."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 195
                            }
                        ],
                        "text": "Figure 5 contrasts the prediction accuracy E (Normalized Prediction Error) versus number of internal units for three versions of our algorithm to the backpropagation benchmark (A) of Lapedes and Farber (1987). The three versions of the learning algorithm are:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60720876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d981c7637fc39335cf53cfa792a0f8d5b66ec6e",
            "isKey": false,
            "numCitedBy": 626,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The backpropagation learning algorithm for neural networks is developed into a formalism for nonlinear signal processing. We illustrate the method by selecting two common topics in signal processing, prediction and system modelling, and show that nonlinear applications can be handled extremely well by using neural networks. The formalism is a natural, nonlinear extension of the linear Least Mean Squares algorithm commonly used in adaptive signal processing. Simulations are presented that document the additional performance achieved by using nonlinear neural networks. First, we demonstrate that the formalism may be used to predict points in a highly chaotic time series with orders of magnitude increase in accuracy over conventional methods including the Linear Predictive Method and the Gabor-Volterra-Weiner Polynomial Method. Deterministic chaos is thought to be involved in many physical situations including the onset of turbulence in fluids, chemical reactions and plasma physics. Secondly, we demonstrate the use of the formalism in nonlinear system modelling by providing a graphic example in which it is clear that the neural network has accurately modelled the nonlinear transfer function. It is interesting to note that the formalism provides explicit, analytic, global, approximations to the nonlinear maps underlying the various time series. Furthermore, the neural net more\u00a0\u00bb seems to be extremely parsimonious in its requirements for data points from the time series. We show that the neural net is able to perform well because it globally approximates the relevant maps by performing a kind of generalized mode decomposition of the maps. 24 refs., 13 figs. \u00ab\u00a0less"
            },
            "slug": "Nonlinear-signal-processing-using-neural-networks:-Lapedes-Farber",
            "title": {
                "fragments": [],
                "text": "Nonlinear signal processing using neural networks: Prediction and system modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "It is demonstrated that the backpropagation learning algorithm for neural networks may be used to predict points in a highly chaotic time series with orders of magnitude increase in accuracy over conventional methods including the Linear Predictive Method and the Gabor-Volterra-Weiner Polynomial Method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074723820"
                        ],
                        "name": "Bo U Curry",
                        "slug": "Bo-U-Curry",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Curry",
                            "middleNames": [
                                "U"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo U Curry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57418580,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d21af3b7c2c971c338818d1ee47c4cef3fc88f3",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "MSnet:-A-Neural-Network-which-Classifies-Mass-Curry-Rumelhart",
            "title": {
                "fragments": [],
                "text": "MSnet: A Neural Network which Classifies Mass Spectra"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143736719"
                        ],
                        "name": "E. Hartman",
                        "slug": "E.-Hartman",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Hartman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hartman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1952587"
                        ],
                        "name": "J. Keeler",
                        "slug": "J.-Keeler",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Keeler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Keeler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39221643"
                        ],
                        "name": "J. Kowalski",
                        "slug": "J.-Kowalski",
                        "structuredName": {
                            "firstName": "Jacek",
                            "lastName": "Kowalski",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kowalski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44931577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68ec8a1e9aea1916e2280489729bab74d5bf6631",
            "isKey": false,
            "numCitedBy": 773,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural network with a single layer of hidden units of gaussian type is proved to be a universal approximator for real-valued maps defined on convex, compact sets of Rn."
            },
            "slug": "Layered-Neural-Networks-with-Gaussian-Hidden-Units-Hartman-Keeler",
            "title": {
                "fragments": [],
                "text": "Layered Neural Networks with Gaussian Hidden Units as Universal Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A neural network with a single layer of hidden units of gaussian type is proved to be a universal approximator for real-valued maps defined on convex, compact sets of Rn."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142809"
                        ],
                        "name": "K. Grajski",
                        "slug": "K.-Grajski",
                        "structuredName": {
                            "firstName": "Kamil",
                            "lastName": "Grajski",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grajski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2392704"
                        ],
                        "name": "M. Merzenich",
                        "slug": "M.-Merzenich",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Merzenich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Merzenich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45705401,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "c798789172c6d0831eb257af484cbbdf875a8df4",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The inverse magnification rule in cortical somatotopy is the experimentally derived inverse relationship between cortical magnification (area of somatotopic map representing a unit area of skin surface) and receptive field size (area of restricted skin surface driving a cortical neuron). We show by computer simulation of a simple, multilayer model that Hebb-type synaptic modification subject to competitive constraints is sufficient to account for the inverse magnification rule."
            },
            "slug": "Hebb-Type-Dynamics-is-Sufficient-to-Account-for-the-Grajski-Merzenich",
            "title": {
                "fragments": [],
                "text": "Hebb-Type Dynamics is Sufficient to Account for the Inverse Magnification Rule in Cortical Somatotopy"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown by computer simulation of a simple, multilayer model that Hebb-type synaptic modification subject to competitive constraints is sufficient to account for the inverse magnification rule in cortical somatotopy."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32466125"
                        ],
                        "name": "J. Howell",
                        "slug": "J.-Howell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Howell",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Howell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143959576"
                        ],
                        "name": "C. Barnes",
                        "slug": "C.-Barnes",
                        "structuredName": {
                            "firstName": "Cris",
                            "lastName": "Barnes",
                            "middleNames": [
                                "W"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Barnes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107290247"
                        ],
                        "name": "S. Brown",
                        "slug": "S.-Brown",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Brown",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687227"
                        ],
                        "name": "G. Flake",
                        "slug": "G.-Flake",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Flake",
                            "middleNames": [
                                "William"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Flake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157480812"
                        ],
                        "name": "R. D. Jones",
                        "slug": "R.-D.-Jones",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Jones",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. D. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116687606"
                        ],
                        "name": "Y. C. Lee",
                        "slug": "Y.-C.-Lee",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Lee",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114887926"
                        ],
                        "name": "S. Qian",
                        "slug": "S.-Qian",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144430406"
                        ],
                        "name": "R. Wright",
                        "slug": "R.-Wright",
                        "structuredName": {
                            "firstName": "Rozelle",
                            "lastName": "Wright",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120459295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f8bb3e0c39041cf01a3a72b6ad6b77a94573837",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Control-of-a-negative-ion-accelerator-source-using-Howell-Barnes",
            "title": {
                "fragments": [],
                "text": "Control of a negative-ion accelerator source using neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083386048"
                        ],
                        "name": "Farmer",
                        "slug": "Farmer",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Farmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Farmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30111504"
                        ],
                        "name": "Sidorowich",
                        "slug": "Sidorowich",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sidorowich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sidorowich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 569,
                                "start": 51
                            }
                        ],
                        "text": "As a second representative test problem, we follow Farmer and Sidorowich (1987) and consider the prediction of a chaotic time series. As it is usually formulated, this problem requires finding a real-valued mapping f : Rn H R which takes a sequence of n recent samples of a time series and predicts the value of the time series at a future moment. It is assumed that the underlying process which generates the time series is unknown. We shall compare our network's learning and generalizing capabilities to a three-layer perceptron studied by Lapedes and Farber (1987) (see figure lb)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 105
                            }
                        ],
                        "text": "Approximation methods based on local linear and local quadratic fitting have been championed recently by Farmer and Sidorowich (1987). These algorithms utilize local representations in the input space, but are not appropriate for real-time use since they require multi-dimensional tree data structures which are cumbersome to modify on the fly and would be extremely difficult to implement in special purpose hardware."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 51
                            }
                        ],
                        "text": "As a second representative test problem, we follow Farmer and Sidorowich (1987) and consider the prediction of a chaotic time series."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44464211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38918f38a875bc2ede6e6865552bcf736c67dc95",
            "isKey": true,
            "numCitedBy": 1792,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a forecasting technique for chaotic data. After embedding a time series in a state space using delay coordinates, we ''learn'' the induced nonlinear mapping using local approximation. This allows us to make short-term predictions of the future behavior of a time series, using information based only on past values. We present an error estimate for this technique, and demonstrate its effectiveness by applying it to several examples, including data from the Mackey-Glass delay differential equation, Rayleigh-Benard convection, and Taylor-Couette flow."
            },
            "slug": "Predicting-chaotic-time-series.-Farmer-Sidorowich",
            "title": {
                "fragments": [],
                "text": "Predicting chaotic time series."
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An error estimate is presented for this forecasting technique for chaotic data, and its effectiveness is demonstrated by applying it to several examples, including data from the Mackey-Glass delay differential equation, Rayleigh-Benard convection, and Taylor-Couette flow."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review letters"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222292199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10055eb6f2f711a36d9aa8f759d3b3f01ebddb5d",
            "isKey": false,
            "numCitedBy": 6561,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Various Aspects of Memory.- 1.1 On the Purpose and Nature of Biological Memory.- 1.1.1 Some Fundamental Concepts.- 1.1.2 The Classical Laws of Association.- 1.1.3 On Different Levels of Modelling.- 1.2 Questions Concerning the Fundamental Mechanisms of Memory.- 1.2.1 Where Do the Signals Relating to Memory Act Upon?.- 1.2.2 What Kind of Encoding is Used for Neural Signals?.- 1.2.3 What are the Variable Memory Elements?.- 1.2.4 How are Neural Signals Addressed in Memory?.- 1.3 Elementary Operations Implemented by Associative Memory.- 1.3.1 Associative Recall.- 1.3.2 Production of Sequences from the Associative Memory.- 1.3.3 On the Meaning of Background and Context.- 1.4 More Abstract Aspects of Memory.- 1.4.1 The Problem of Infinite-State Memory.- 1.4.2 Invariant Representations.- 1.4.3 Symbolic Representations.- 1.4.4 Virtual Images.- 1.4.5 The Logic of Stored Knowledge.- 2. Pattern Mathematics.- 2.1 Mathematical Notations and Methods.- 2.1.1 Vector Space Concepts.- 2.1.2 Matrix Notations.- 2.1.3 Further Properties of Matrices.- 2.1.4 Matrix Equations.- 2.1.5 Projection Operators.- 2.1.6 On Matrix Differential Calculus.- 2.2 Distance Measures for Patterns.- 2.2.1 Measures of Similarity and Distance in Vector Spaces.- 2.2.2 Measures of Similarity and Distance Between Symbol Strings.- 2.2.3 More Accurate Distance Measures for Text.- 3. Classical Learning Systems.- 3.1 The Adaptive Linear Element (Adaline).- 3.1.1 Description of Adaptation by the Stochastic Approximation.- 3.2 The Perceptron.- 3.3 The Learning Matrix.- 3.4 Physical Realization of Adaptive Weights.- 3.4.1 Perceptron and Adaline.- 3.4.2 Classical Conditioning.- 3.4.3 Conjunction Learning Switches.- 3.4.4 Digital Representation of Adaptive Circuits.- 3.4.5 Biological Components.- 4. A New Approach to Adaptive Filters.- 4.1 Survey of Some Necessary Functions.- 4.2 On the \"Transfer Function\" of the Neuron.- 4.3 Models for Basic Adaptive Units.- 4.3.1 On the Linearization of the Basic Unit.- 4.3.2 Various Cases of Adaptation Laws.- 4.3.3 Two Limit Theorems.- 4.3.4 The Novelty Detector.- 4.4 Adaptive Feedback Networks.- 4.4.1 The Autocorrelation Matrix Memory.- 4.4.2 The Novelty Filter.- 5. Self-Organizing Feature Maps.- 5.1 On the Feature Maps of the Brain.- 5.2 Formation of Localized Responses by Lateral Feedback.- 5.3 Computational Simplification of the Process.- 5.3.1 Definition of the Topology-Preserving Mapping.- 5.3.2 A Simple Two-Dimensional Self-Organizing System.- 5.4 Demonstrations of Simple Topology-Preserving Mappings.- 5.4.1 Images of Various Distributions of Input Vectors.- 5.4.2 \"The Magic TV\".- 5.4.3 Mapping by a Feeler Mechanism.- 5.5 Tonotopic Map.- 5.6 Formation of Hierarchical Representations.- 5.6.1 Taxonomy Example.- 5.6.2 Phoneme Map.- 5.7 Mathematical Treatment of Self-Organization.- 5.7.1 Ordering of Weights.- 5.7.2 Convergence Phase.- 5.8 Automatic Selection of Feature Dimensions.- 6. Optimal Associative Mappings.- 6.1 Transfer Function of an Associative Network.- 6.2 Autoassociative Recall as an Orthogonal Projection.- 6.2.1 Orthogonal Projections.- 6.2.2 Error-Correcting Properties of Projections.- 6.3 The Novelty Filter.- 6.3.1 Two Examples of Novelty Filter.- 6.3.2 Novelty Filter as an Autoassociative Memory.- 6.4 Autoassociative Encoding.- 6.4.1 An Example of Autoassociative Encoding.- 6.5 Optimal Associative Mappings.- 6.5.1 The Optimal Linear Associative Mapping.- 6.5.2 Optimal Nonlinear Associative Mappings.- 6.6 Relationship Between Associative Mapping, Linear Regression, and Linear Estimation.- 6.6.1 Relationship of the Associative Mapping to Linear Regression.- 6.6.2 Relationship of the Regression Solution to the Linear Estimator.- 6.7 Recursive Computation of the Optimal Associative Mapping.- 6.7.1 Linear Corrective Algorithms.- 6.7.2 Best Exact Solution (Gradient Projection).- 6.7.3 Best Approximate Solution (Regression).- 6.7.4 Recursive Solution in the General Case.- 6.8 Special Cases.- 6.8.1 The Correlation Matrix Memory.- 6.8.2 Relationship Between Conditional Averages and Optimal Estimator.- 7. Pattern Recognition.- 7.1 Discriminant Functions.- 7.2 Statistical Formulation of Pattern Classification.- 7.3 Comparison Methods.- 7.4 The Subspace Methods of Classification.- 7.4.1 The Basic Subspace Method.- 7.4.2 The Learning Subspace Method (LSM).- 7.5 Learning Vector Quantization.- 7.6 Feature Extraction.- 7.7 Clustering.- 7.7.1 Simple Clustering (Optimization Approach).- 7.7.2 Hierarchical Clustering (Taxonomy Approach).- 7.8 Structural Pattern Recognition Methods.- 8. More About Biological Memory.- 8.1 Physiological Foundations of Memory.- 8.1.1 On the Mechanisms of Memory in Biological Systems.- 8.1.2 Structural Features of Some Neural Networks.- 8.1.3 Functional Features of Neurons.- 8.1.4 Modelling of the Synaptic Plasticity.- 8.1.5 Can the Memory Capacity Ensue from Synaptic Changes?.- 8.2 The Unified Cortical Memory Model.- 8.2.1 The Laminar Network Organization.- 8.2.2 On the Roles of Interneurons.- 8.2.3 Representation of Knowledge Over Memory Fields.- 8.2.4 Self-Controlled Operation of Memory.- 8.3 Collateral Reading.- 8.3.1 Physiological Results Relevant to Modelling.- 8.3.2 Related Modelling.- 9. Notes on Neural Computing.- 9.1 First Theoretical Views of Neural Networks.- 9.2 Motives for the Neural Computing Research.- 9.3 What Could the Purpose of the Neural Networks be?.- 9.4 Definitions of Artificial \"Neural Computing\" and General Notes on Neural Modelling.- 9.5 Are the Biological Neural Functions Localized or Distributed?.- 9.6 Is Nonlinearity Essential to Neural Computing?.- 9.7 Characteristic Differences Between Neural and Digital Computers.- 9.7.1 The Degree of Parallelism of the Neural Networks is Still Higher than that of any \"Massively Parallel\" Digital Computer.- 9.7.2 Why the Neural Signals Cannot be Approximated by Boolean Variables.- 9.7.3 The Neural Circuits do not Implement Finite Automata.- 9.7.4 Undue Views of the Logic Equivalence of the Brain and Computers on a High Level.- 9.8 \"Connectionist Models\".- 9.9 How can the Neural Computers be Programmed?.- 10. Optical Associative Memories.- 10.1 Nonholographic Methods.- 10.2 General Aspects of Holographic Memories.- 10.3 A Simple Principle of Holographic Associative Memory.- 10.4 Addressing in Holographic Memories.- 10.5 Recent Advances of Optical Associative Memories.- Bibliography on Pattern Recognition.- References."
            },
            "slug": "Self-Organization-and-Associative-Memory-Kohonen",
            "title": {
                "fragments": [],
                "text": "Self-Organization and Associative Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The purpose and nature of Biological Memory, as well as some of the aspects of Memory Aspects, are explained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104537710"
                        ],
                        "name": "J. MacQueen",
                        "slug": "J.-MacQueen",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "MacQueen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. MacQueen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6278891,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed",
            "isKey": false,
            "numCitedBy": 24206,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The main purpose of this paper is to describe a process for partitioning an N-dimensional population into k sets on the basis of a sample. The process, which is called 'k-means,' appears to give partitions which are reasonably efficient in the sense of within-class variance. That is, if p is the probability mass function for the population, S = {S1, S2, * *, Sk} is a partition of EN, and ui, i = 1, 2, * , k, is the conditional mean of p over the set Si, then W2(S) = ff=ISi f z u42 dp(z) tends to be low for the partitions S generated by the method. We say 'tends to be low,' primarily because of intuitive considerations, corroborated to some extent by mathematical analysis and practical computational experience. Also, the k-means procedure is easily programmed and is computationally economical, so that it is feasible to process very large samples on a digital computer. Possible applications include methods for similarity grouping, nonlinear prediction, approximating multivariate distributions, and nonparametric tests for independence among several variables. In addition to suggesting practical classification methods, the study of k-means has proved to be theoretically interesting. The k-means concept represents a generalization of the ordinary sample mean, and one is naturally led to study the pertinent asymptotic behavior, the object being to establish some sort of law of large numbers for the k-means. This problem is sufficiently interesting, in fact, for us to devote a good portion of this paper to it. The k-means are defined in section 2.1, and the main results which have been obtained on the asymptotic behavior are given there. The rest of section 2 is devoted to the proofs of these results. Section 3 describes several specific possible applications, and reports some preliminary results from computer experiments conducted to explore the possibilities inherent in the k-means idea. The extension to general metric spaces is indicated briefly in section 4. The original point of departure for the work described here was a series of problems in optimal classification (MacQueen [9]) which represented special"
            },
            "slug": "Some-methods-for-classification-and-analysis-of-MacQueen",
            "title": {
                "fragments": [],
                "text": "Some methods for classification and analysis of multivariate observations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101639166"
                        ],
                        "name": "S. P. Lloyd",
                        "slug": "S.-P.-Lloyd",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Lloyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. P. Lloyd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10833328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9241ea3d8cb85633d314ecb74b31567b8e73f6af",
            "isKey": false,
            "numCitedBy": 11643,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for 2^{b} quanta, b=1,2, \\cdots, 7 , are given numerically for Gaussian and for Laplacian distribution of signal amplitudes."
            },
            "slug": "Least-squares-quantization-in-PCM-Lloyd",
            "title": {
                "fragments": [],
                "text": "Least squares quantization in PCM"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Although our discussion has focused on algorithms which can be implemented as real-time adaptive systems, such as backpropagation and the networks of locally-tuned units we have presented, a number of offline algorithms for multidimensional function modeling achieve excellent performance both in terms of efficiency and precision. symmetric basis functions ( Powell 1985 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118224933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c71ca26b183025b9f39f940f5e730f2c9a64e414",
            "isKey": false,
            "numCitedBy": 1425,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Radial-basis-functions-for-multivariable-a-review-Powell",
            "title": {
                "fragments": [],
                "text": "Radial basis functions for multivariable interpolation: a review"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62186699,
            "fieldsOfStudy": [],
            "id": "3302a19539ccfa8ed3a8361ace8947ddbba1acf5",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59773108,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "015e5c48abbd59309e6986aaa94550e40562f100",
            "isKey": false,
            "numCitedBy": 3128,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Self-organization-and-associative-memory:-3rd-Kohonen",
            "title": {
                "fragments": [],
                "text": "Self-organization and associative memory: 3rd edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44717168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff80b7820fbc54926946c245e139c382266489ae",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-Algorithms-with-Neural-Network-Behavior-Omohundro",
            "title": {
                "fragments": [],
                "text": "Efficient Algorithms with Neural Network Behavior"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with localized receptive fields"
            },
            "venue": {
                "fragments": [],
                "text": "In: Proceedings of the 1988 Connectionist Models Summer School, eds. Touretzky, Hinton, and Sejnowski. Morgan-Kaufmann, Publishers."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Radial basis functions for multivariate interpolation: a review"
            },
            "venue": {
                "fragments": [],
                "text": "Department of Applied Mathematics and Theoretical Physics"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Knowledge representation in connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": "Bellcore Technical Report."
            },
            "year": 1987
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 1,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Fast-Learning-in-Networks-of-Locally-Tuned-Units-Moody-Darken/1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76?sort=total-citations"
}