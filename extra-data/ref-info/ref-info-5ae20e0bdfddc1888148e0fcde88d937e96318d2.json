{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 167
                            }
                        ],
                        "text": "For this noun phrase corefer\u00adence task, the new formulation with Latent \nStructural SVM improves both the prediction performance and training e.ciency over conventional Structural \nSVMs. 5.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 14
                            }
                        ],
                        "text": "When \ntraining Structural SVMs, the parameter vec\u00adtor w is determined by minimizing the (regularized) risk \non the training set (x1,y1),...,(xn,yn)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 31
                            }
                        ],
                        "text": "The Structural SVM formulation (Tsochantaridis et al., 2004) overcomes these difficulties by replacing the loss function \u2206 with a piecewise linear convex upper bound (margin rescaling) \u2206(yi, \u0177i(w)) \u2264 max \u0177\u2208Y [\u2206(yi, \u0177)+w\u00b7\u03a6(xi, \u0177)]\u2212w\u00b7\u03a6(xi, yi)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Structural SVMs Suppose we are given a training set \nof input-output structure pairs S= {(x1,y1),..., (xn,yn)}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "In (Finley &#38; Joachims, 2005) the \ntask is formulated as a correlation clustering problem trained with Structural SVMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 9
                            }
                        ],
                        "text": "Learning Structural SVMs with Latent Variables Chun-Nam John Yu cnyu@cs.cornell.edu Thorsten Joachims \ntj@cs.cornell.edu Department of Computer Science, Cornell University, Ithaca, NY 14850 USA Abstract \nWe present a large-margin formulation and algorithm for structured output prediction that allows the \nuse of latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 45
                            }
                        ],
                        "text": "Y where yi(w)= argmaxF(xi,y). y.Y w \u00b7 To train Structural SVMs \nwe then solve the following convex optimization problem: ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 23
                            }
                        ],
                        "text": "(3) h.H In the case of Structural SVMs without latent vari\u00adables, the complex dependence on w within \nthe loss. can be removed using the following inequality, com\u00admonly referred to as loss-augmented inference \nin Structural SVM training:  max w \u00b7 F(x,y ) +.(yi,y i(w)) y ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 50
                            }
                        ],
                        "text": "Y\u00d7H Using the same reasoning \nas for fully observed Struc\u00adtural SVMs, this gives rise to the following optimiza\u00adtion problem for Structural \nSVMs with latent vari\u00adables: 0 n 1 1w12 min +C max[w\u00b7 F(xi, h)+.(yi, h)] y,y, w 2 ( y,h)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 242
                            }
                        ],
                        "text": "As the cost of each CCCP iteration is no more than solving a standard Structural SVM optimization problem \n(with the completion of latent variables), the total number of CCCP iterations gives us a rough estimate \nof the cost of training Latent Structural SVMs, which is not particularly expensive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 93
                            }
                        ],
                        "text": "large-margin structured output learning such as MaxMargin Markov Networks or Structural SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 32
                            }
                        ],
                        "text": "The Structural SVM formulation (Tsochan\u00adtaridis et al., 2004) overcomes these di.culties by \nre\u00adplacing the loss function.witha piecewise linear con\u00advex upper bound (margin rescaling) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Struc\u00adtural SVMsgive excellentperformance on many \nstruc\u00adtured prediction tasks, especially when the model F is high-dimensional and it is necessary to \noptimize to non-standard loss functions .. 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "Conclusions We have presented a framework and formulation for learning Structural \nSVMs with latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 158
                            }
                        ],
                        "text": "Copy\u00adright 2009 by the author(s)/owner(s). large-margin \nstructured output learning such as Max-Margin Markov Networks or Structural SVMs (Taskar et al., 2003; \nTsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 564746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93aa298b40bb3ec23c25239089284fdf61ded917",
            "isKey": false,
            "numCitedBy": 1455,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "slug": "Support-vector-machine-learning-for-interdependent-Tsochantaridis-Hofmann",
            "title": {
                "fragments": [],
                "text": "Support vector machine learning for interdependent and structured output spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs, and demonstrates the versatility and effectiveness of the method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754497"
                        ],
                        "name": "Slav Petrov",
                        "slug": "Slav-Petrov",
                        "structuredName": {
                            "firstName": "Slav",
                            "lastName": "Petrov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Slav Petrov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14428799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b10973281836b437b3db7c14bc95a4cd974ab046",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that log-linear grammars with latent variables can be practically trained using discriminative methods. Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient-based procedure. We compare L1 and L2 regularization and show that L1 regularization is superior, requiring fewer iterations to converge, and yielding sparser solutions. On full-scale treebank parsing experiments, the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non-latent baselines."
            },
            "slug": "Discriminative-Log-Linear-Grammars-with-Latent-Petrov-Klein",
            "title": {
                "fragments": [],
                "text": "Discriminative Log-Linear Grammars with Latent Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "It is demonstrated that log-linear grammars with latent variables can be practically trained using discriminative methods, and it is shown that L1 regularization is superior, requiring fewer iterations to converge, and yielding sparser solutions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145106110"
                        ],
                        "name": "Vincent Ng",
                        "slug": "Vincent-Ng",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794075"
                        ],
                        "name": "Claire Gardent",
                        "slug": "Claire-Gardent",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Gardent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Gardent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 52
                            }
                        ],
                        "text": "The pairwise features xij are the same as those in (Ng &#38; \nCardie, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 144
                            }
                        ],
                        "text": "Following the intuition that humans might determine \nif two noun phrases are coreferent by reasoning tran\u00adsitively over strong coreference links (Ng &#38; \nCardie, 2002), we model the problem of noun phrase corefer\u00adence as a single-link agglomerative clustering \nproblem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1189640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08c81389b3ac4b8253d718a7cebe04a5536efa78",
            "isKey": false,
            "numCitedBy": 759,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge."
            },
            "slug": "Improving-Machine-Learning-Approaches-to-Resolution-Ng-Gardent",
            "title": {
                "fragments": [],
                "text": "Improving Machine Learning Approaches to Coreference Resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A noun phrase coreference system that extends the work of Soon et al. (2001) and produces the best results to date on the M UC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 167
                            }
                        ],
                        "text": "For this noun phrase corefer\u00adence task, the new formulation with Latent \nStructural SVM improves both the prediction performance and training e.ciency over conventional Structural \nSVMs. 5.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 14
                            }
                        ],
                        "text": "When \ntraining Structural SVMs, the parameter vec\u00adtor w is determined by minimizing the (regularized) risk \non the training set (x1,y1),...,(xn,yn)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Structural SVMs Suppose we are given a training set \nof input-output structure pairs S= {(x1,y1),..., (xn,yn)}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "In (Finley &#38; Joachims, 2005) the \ntask is formulated as a correlation clustering problem trained with Structural SVMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 9
                            }
                        ],
                        "text": "Learning Structural SVMs with Latent Variables Chun-Nam John Yu cnyu@cs.cornell.edu Thorsten Joachims \ntj@cs.cornell.edu Department of Computer Science, Cornell University, Ithaca, NY 14850 USA Abstract \nWe present a large-margin formulation and algorithm for structured output prediction that allows the \nuse of latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 45
                            }
                        ],
                        "text": "Y where yi(w)= argmaxF(xi,y). y.Y w \u00b7 To train Structural SVMs \nwe then solve the following convex optimization problem: ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 23
                            }
                        ],
                        "text": "(3) h.H In the case of Structural SVMs without latent vari\u00adables, the complex dependence on w within \nthe loss. can be removed using the following inequality, com\u00admonly referred to as loss-augmented inference \nin Structural SVM training:  max w \u00b7 F(x,y ) +.(yi,y i(w)) y ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 50
                            }
                        ],
                        "text": "Y\u00d7H Using the same reasoning \nas for fully observed Struc\u00adtural SVMs, this gives rise to the following optimiza\u00adtion problem for Structural \nSVMs with latent vari\u00adables: 0 n 1 1w12 min +C max[w\u00b7 F(xi, h)+.(yi, h)] y,y, w 2 ( y,h)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 242
                            }
                        ],
                        "text": "As the cost of each CCCP iteration is no more than solving a standard Structural SVM optimization problem \n(with the completion of latent variables), the total number of CCCP iterations gives us a rough estimate \nof the cost of training Latent Structural SVMs, which is not particularly expensive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Struc\u00adtural SVMsgive excellentperformance on many \nstruc\u00adtured prediction tasks, especially when the model F is high-dimensional and it is necessary to \noptimize to non-standard loss functions .. 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "Conclusions We have presented a framework and formulation for learning Structural \nSVMs with latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 137
                            }
                        ],
                        "text": "Copy\u00adright 2009 by the author(s)/owner(s). large-margin \nstructured output learning such as Max-Margin Markov Networks or Structural SVMs (Taskar et al., 2003; \nTsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": true,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042259"
                        ],
                        "name": "Chuong B. Do",
                        "slug": "Chuong-B.-Do",
                        "structuredName": {
                            "firstName": "Chuong",
                            "lastName": "Do",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuong B. Do"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37077406"
                        ],
                        "name": "C. Teo",
                        "slug": "C.-Teo",
                        "structuredName": {
                            "firstName": "Choon",
                            "lastName": "Teo",
                            "middleNames": [
                                "Hui"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Teo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 119
                            }
                        ],
                        "text": "Very recently the CCCP algorithm has also been applied to obtain tighter non-convex loss bounds on structured learning (Chapelle et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 119
                            }
                        ],
                        "text": "Very recently \nthe CCCP algorithm has alsobeen applied to obtain tighter non-convex loss bounds on structured learning \n(Chapelle et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5810122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6df0fd29ab42bff1f0239510237b413d296c3390",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Large-margin structured estimation methods minimize a convex upper bound of loss functions. While they allow for efficient optimization algorithms, these convex formulations are not tight and sacrifice the ability to accurately model the true loss. We present tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classification to structured estimation. We show that a small modification of existing optimization algorithms suffices to solve this modified problem. On structured prediction tasks such as protein sequence alignment and web page ranking, our algorithm leads to improved accuracy."
            },
            "slug": "Tighter-Bounds-for-Structured-Estimation-Chapelle-Do",
            "title": {
                "fragments": [],
                "text": "Tighter Bounds for Structured Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classification to structured estimation and shows that a small modification of existing optimization algorithms suffices to solve this modified problem."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50095217"
                        ],
                        "name": "Fabian H Sinz",
                        "slug": "Fabian-H-Sinz",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Sinz",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabian H Sinz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 146
                            }
                        ],
                        "text": "In recentyears therehavebeennumerous appli\u00adcations \nof the algorithm in machine learning, includ\u00ading training non-convex SVMs and transductive SVMs (Collobert \net al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 103
                            }
                        ],
                        "text": "cations of the algorithm in machine learning, including training non-convex SVMs and transductive SVMs (Collobert et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5175370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33fd991b4e05becfdec93585d25b6369a0519133",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide scalability advantages over convexity. We show how concave-convex programming can be applied to produce (i) faster SVMs where training errors are no longer support vectors, and (ii) much faster Transductive SVMs."
            },
            "slug": "Trading-convexity-for-scalability-Collobert-Sinz",
            "title": {
                "fragments": [],
                "text": "Trading convexity for scalability"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown how concave-convex programming can be applied to produce faster SVMs where training errors are no longer support vectors, and much faster Transductive SVMs."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50256971"
                        ],
                        "name": "Thomas Finley",
                        "slug": "Thomas-Finley",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Finley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Finley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3166569"
                        ],
                        "name": "C. Yu",
                        "slug": "C.-Yu",
                        "structuredName": {
                            "firstName": "Chun-Nam",
                            "lastName": "Yu",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14211670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f30aba767d71c1db5ea70b041d9fcc2b9b1ddad4",
            "isKey": false,
            "numCitedBy": 1083,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminative training approaches like structural SVMs have shown much promise for building highly complex and accurate models in areas like natural language processing, protein structure prediction, and information retrieval. However, current training algorithms are computationally expensive or intractable on large datasets. To overcome this bottleneck, this paper explores how cutting-plane methods can provide fast training not only for classification SVMs, but also for structural SVMs. We show that for an equivalent \u201c1-slack\u201d reformulation of the linear SVM training problem, our cutting-plane method has time complexity linear in the number of training examples. In particular, the number of iterations does not depend on the number of training examples, and it is linear in the desired precision and the regularization parameter. Furthermore, we present an extensive empirical evaluation of the method applied to binary classification, multi-class classification, HMM sequence tagging, and CFG parsing. The experiments show that the cutting-plane algorithm is broadly applicable and fast in practice. On large datasets, it is typically several orders of magnitude faster than conventional training methods derived from decomposition methods like SVM-light, or conventional cutting-plane methods. Implementations of our methods are available at www.joachims.org."
            },
            "slug": "Cutting-plane-training-of-structural-SVMs-Joachims-Finley",
            "title": {
                "fragments": [],
                "text": "Cutting-plane training of structural SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper explores how cutting-plane methods can provide fast training not only for classification SVMs, but also for structural SVMs and presents an extensive empirical evaluation of the method applied to binary classification, multi-class classification, HMM sequence tagging, and CFG parsing."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765114"
                        ],
                        "name": "Ulf Brefeld",
                        "slug": "Ulf-Brefeld",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Brefeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ulf Brefeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751348"
                        ],
                        "name": "T. Scheffer",
                        "slug": "T.-Scheffer",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Scheffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Scheffer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 247
                            }
                        ],
                        "text": "When the loss \u2206 depends only on the fully observed label yi, it rules out the possibility of transductive learning, but the restriction also results in simpler optimization problems compared to the transductive cases (for example, the approach in (Zien et al., 2007) involves constraint removals to deal with dependence on h\u2217i (w) within the loss \u2206)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 191
                            }
                        ],
                        "text": "\u2026yi, it \nrules out thepossi\u00adbility of transductive learning, but the restriction also results in simpler optimization \nproblems compared to the transductive cases (for example, the approach in (Zien et al., 2007) involves \nconstraint removals to deal with dependence on h*(w)within the loss .). i 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 110
                            }
                        ],
                        "text": "Finally, note that the redefined loss distinguishes our approach from transductive structured output learning (Zien et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 110
                            }
                        ],
                        "text": "Finally, note that the rede.ned loss distinguishes our approach from transductive structured \noutput learn\u00ading (Zien et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8791045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daaeb4f63877a9902498999bdbd020c06f52b77d",
            "isKey": true,
            "numCitedBy": 49,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of learning kernel machines transductively for structured output variables. Transductive learning can be reduced to combinatorial optimization problems over all possible labelings of the unlabeled data. In order to scale transductive learning to structured variables, we transform the corresponding non-convex, combinatorial, constrained optimization problems into continuous, unconstrained optimization problems. The discrete optimization parameters are eliminated and the resulting differentiable problems can be optimized efficiently. We study the effectiveness of the generalized TSVM on multiclass classification and label-sequence learning problems empirically."
            },
            "slug": "Transductive-support-vector-machines-for-structured-Zien-Brefeld",
            "title": {
                "fragments": [],
                "text": "Transductive support vector machines for structured variables"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "In order to scale transductive learning to structured variables, this work transforms the corresponding non-convex, combinatorial, constrained optimization problems into continuous, unconstrained optimization problems."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 123
                            }
                        ],
                        "text": "We can see from Table \n3 that our Latent Struc\u00adtural SVM approachperformsbetter than the Ranking SVM (Herbrich et al., 2000; \nJoachims, 2002) on pre\u00adcision@1,3,5,10, one of the stronger baselines in the LETOR 3.0benchmark."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 101
                            }
                        ],
                        "text": "We can see from Table 3 that our Latent Structural SVM approach performs better than the Ranking SVM (Herbrich et al., 2000; Joachims, 2002) on precision@1,3,5,10, one of the stronger baselines in the LETOR 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 68
                            }
                        ],
                        "text": "Precision@k on OHSUMED dataset (5-fold CV) OHSUMED P@1 P@3 P@5 P@10 Ranking SVM 0.597 \n0.543 0.532 0.486 ListNet 0.652 0.602 0.550 0.498 Latent Structural SVM 0.680 0.573 0.567 0.494 Initial \nWeight Vector 0.626 0.557 0.524 0.464  C Figure 3."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207605508,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfd4259d305a00f13d5f08841230389f61322422",
            "isKey": false,
            "numCitedBy": 4305,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples."
            },
            "slug": "Optimizing-search-engines-using-clickthrough-data-Joachims",
            "title": {
                "fragments": [],
                "text": "Optimizing search engines using clickthrough data"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 167
                            }
                        ],
                        "text": "For this noun phrase corefer\u00adence task, the new formulation with Latent \nStructural SVM improves both the prediction performance and training e.ciency over conventional Structural \nSVMs. 5.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 14
                            }
                        ],
                        "text": "When \ntraining Structural SVMs, the parameter vec\u00adtor w is determined by minimizing the (regularized) risk \non the training set (x1,y1),...,(xn,yn)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Structural SVMs Suppose we are given a training set \nof input-output structure pairs S= {(x1,y1),..., (xn,yn)}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "In (Finley &#38; Joachims, 2005) the \ntask is formulated as a correlation clustering problem trained with Structural SVMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 9
                            }
                        ],
                        "text": "Learning Structural SVMs with Latent Variables Chun-Nam John Yu cnyu@cs.cornell.edu Thorsten Joachims \ntj@cs.cornell.edu Department of Computer Science, Cornell University, Ithaca, NY 14850 USA Abstract \nWe present a large-margin formulation and algorithm for structured output prediction that allows the \nuse of latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 45
                            }
                        ],
                        "text": "Y where yi(w)= argmaxF(xi,y). y.Y w \u00b7 To train Structural SVMs \nwe then solve the following convex optimization problem: ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 23
                            }
                        ],
                        "text": "(3) h.H In the case of Structural SVMs without latent vari\u00adables, the complex dependence on w within \nthe loss. can be removed using the following inequality, com\u00admonly referred to as loss-augmented inference \nin Structural SVM training:  max w \u00b7 F(x,y ) +.(yi,y i(w)) y ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 50
                            }
                        ],
                        "text": "Y\u00d7H Using the same reasoning \nas for fully observed Struc\u00adtural SVMs, this gives rise to the following optimiza\u00adtion problem for Structural \nSVMs with latent vari\u00adables: 0 n 1 1w12 min +C max[w\u00b7 F(xi, h)+.(yi, h)] y,y, w 2 ( y,h)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 242
                            }
                        ],
                        "text": "As the cost of each CCCP iteration is no more than solving a standard Structural SVM optimization problem \n(with the completion of latent variables), the total number of CCCP iterations gives us a rough estimate \nof the cost of training Latent Structural SVMs, which is not particularly expensive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Struc\u00adtural SVMsgive excellentperformance on many \nstruc\u00adtured prediction tasks, especially when the model F is high-dimensional and it is necessary to \noptimize to non-standard loss functions .. 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "Conclusions We have presented a framework and formulation for learning Structural \nSVMs with latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 137
                            }
                        ],
                        "text": "Copy\u00adright 2009 by the author(s)/owner(s). large-margin \nstructured output learning such as Max-Margin Markov Networks or Structural SVMs (Taskar et al., 2003; \nTsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maxmargin Markov networks. Adv. in Neural Inf. Process . Syst"
            },
            "venue": {
                "fragments": [],
                "text": "Maxmargin Markov networks. Adv. in Neural Inf. Process . Syst"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50256971"
                        ],
                        "name": "Thomas Finley",
                        "slug": "Thomas-Finley",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Finley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Finley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 4
                            }
                        ],
                        "text": "In (Finley &#38; Joachims, 2005) the \ntask is formulated as a correlation clustering problem trained with Structural SVMs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 98
                            }
                        ],
                        "text": "Table 2 shows the result of our algorithm compared \nto the SVM correlation clustering approach in (Finley &#38; Joachims, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 60
                            }
                        ],
                        "text": "We present the results using \nthe same loss functions as in (Finley &#38; Joachims, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8292657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9616433b227763c28f31820ec05b174fcd577af",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised clustering is the problem of training a clustering algorithm to produce desirable clusterings: given sets of items and complete clusterings over these sets, we learn how to cluster future sets of items. Example applications include noun-phrase coreference clustering, and clustering news articles by whether they refer to the same topic. In this paper we present an SVM algorithm that trains a clustering algorithm by adapting the item-pair similarity measure. The algorithm may optimize a variety of different clustering functions to a variety of clustering performance measures. We empirically evaluate the algorithm for noun-phrase and news article clustering."
            },
            "slug": "Supervised-clustering-with-support-vector-machines-Finley-Joachims",
            "title": {
                "fragments": [],
                "text": "Supervised clustering with support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper presents an SVM algorithm that trains a clustering algorithm by adapting the item-pair similarity measure, and empirically evaluates the algorithm for noun-phrase and news article clustering."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 111
                            }
                        ],
                        "text": "In the computer vision community there are recent works on training Hidden \nCRF using the max-margin criterion (Felzenszwalb et al., 2008; Wang &#38; Mori, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 33
                            }
                        ],
                        "text": "Interestingly, the algorithm in (Felzenszwalb et al., 2008) coincides with our approach for binary \nclassi.cation but was derived in a di.erent way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14327585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "860a9d55d87663ca88e74b3ca357396cd51733d0",
            "isKey": false,
            "numCitedBy": 2616,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose."
            },
            "slug": "A-discriminatively-trained,-multiscale,-deformable-Felzenszwalb-McAllester",
            "title": {
                "fragments": [],
                "text": "A discriminatively trained, multiscale, deformable part model"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A discriminatively trained, multiscale, deformable part model for object detection, which achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge and outperforms the best results in the 2007 challenge in ten out of twenty categories."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110264337"
                        ],
                        "name": "Tie-Yan Liu",
                        "slug": "Tie-Yan-Liu",
                        "structuredName": {
                            "firstName": "Tie-Yan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tie-Yan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150635498"
                        ],
                        "name": "Jun Xu",
                        "slug": "Jun-Xu",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143826491"
                        ],
                        "name": "Tao Qin",
                        "slug": "Tao-Qin",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Qin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8574112"
                        ],
                        "name": "Wen-Ying Xiong",
                        "slug": "Wen-Ying-Xiong",
                        "structuredName": {
                            "firstName": "Wen-Ying",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-Ying Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145572104"
                        ],
                        "name": "Hang Li",
                        "slug": "Hang-Li",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 93
                            }
                        ],
                        "text": "To evaluate our algorithm, we \nran experiments on the OHSUMED tasks of the LETOR 3.0 dataset (Liu et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14596754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbcd79bd7edcdcbb5912a50796fc3c2746729eb5",
            "isKey": true,
            "numCitedBy": 500,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with learning to rank for information retrieval (IR). Ranking is the central problem for information retrieval, and employing machine learning techniques to learn the ranking function is viewed as a promising approach to IR. Unfortunately, there was no benchmark dataset that could be used in comparison of existing learning algorithms and in evaluation of newly proposed algorithms, which stood in the way of the related research. To deal with the problem, we have constructed a benchmark dataset referred to as LETOR and distributed it to the research communities. Specifically we have derived the LETOR data from the existing data sets widely used in IR, namely, OHSUMED and TREC data. The two collections contain queries, the contents of the retrieved documents, and human judgments on the relevance of the documents with respect to the queries. We have extracted features from the datasets, including both conventional features, such as term frequency, inverse document frequency, BM25, and language models for IR, and features proposed recently at SIGIR, such as HostRank, feature propagation, and topical PageRank. We have then packaged LETOR with the extracted features, queries, and relevance judgments. We have also provided the results of several state-ofthe-arts learning to rank algorithms on the data. This paper describes in details about LETOR."
            },
            "slug": "LETOR:-Benchmark-Dataset-for-Research-on-Learning-Liu-Xu",
            "title": {
                "fragments": [],
                "text": "LETOR: Benchmark Dataset for Research on Learning to Rank for Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper has constructed a benchmark dataset referred to as LETOR, derived the LETOR data from the existing data sets widely used in IR, namely, OHSUMED and TREC data and provided the results of several state-ofthe-arts learning to rank algorithms on the data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713876"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 16
                            }
                        ],
                        "text": "The approachin (Smola et al., 2005) employs CCCP to handle missing data in SVMs and Gaussian \nProcesses and is closely related to our work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 16
                            }
                        ],
                        "text": "The approach in (Smola et al., 2005) employs CCCP to handle missing data in SVMs and Gaussian Processes and is closely related to our work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17096229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16c65bc854666bb0fe19631061d13a07734d2465",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present methods for dealing with missing variables in the context of Gaussian Processes and Support Vector Machines. This solves an important problem which has largely been ignored by kernel methods: How to systematically deal with incomplete data? Our method can also be applied to problems with partially observed labels as well as to the transductive setting where we view the labels as missing data. Our approach relies on casting kernel methods as an estimation problem in exponential families. Hence, estimation with missing variables becomes a problem of computing marginal distributions, and finding efficient optimization methods. To that extent we propose an optimization scheme which extends the Concave Convex Procedure (CCP) of Yuille and Rangarajan, and present a simplified and intuitive proof of its convergence. We show how our algorithm can be specialized to various cases in order to efficiently solve the optimization problems that arise. Encouraging preliminary experimental results on the USPS dataset are also presented."
            },
            "slug": "Kernel-Methods-for-Missing-Variables-Smola-Vishwanathan",
            "title": {
                "fragments": [],
                "text": "Kernel Methods for Missing Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An optimization scheme is proposed which extends the Concave Convex Procedure (CCP) of Yuille and Rangarajan and it is shown how the algorithm can be specialized to various cases in order to efficiently solve the optimization problems that arise."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113999576"
                        ],
                        "name": "Zhe Cao",
                        "slug": "Zhe-Cao",
                        "structuredName": {
                            "firstName": "Zhe",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhe Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143826491"
                        ],
                        "name": "Tao Qin",
                        "slug": "Tao-Qin",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Qin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110264337"
                        ],
                        "name": "Tie-Yan Liu",
                        "slug": "Tie-Yan-Liu",
                        "structuredName": {
                            "firstName": "Tie-Yan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tie-Yan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793168"
                        ],
                        "name": "Ming-Feng Tsai",
                        "slug": "Ming-Feng-Tsai",
                        "structuredName": {
                            "firstName": "Ming-Feng",
                            "lastName": "Tsai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Feng Tsai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145572104"
                        ],
                        "name": "Hang Li",
                        "slug": "Hang-Li",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 29
                            }
                        ],
                        "text": "We also \nessentially tie with ListNet(Caoetal., 2007), oneofthebestoverall rank\u00ading method in the LETOR 3.0 benchmark."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 104
                            }
                        ],
                        "text": "Precision@k on OHSUMED dataset (5-fold CV) OHSUMED P@1 P@3 P@5 P@10 Ranking SVM 0.597 \n0.543 0.532 0.486 ListNet 0.652 0.602 0.550 0.498 Latent Structural SVM 0.680 0.573 0.567 0.494 Initial \nWeight Vector 0.626 0.557 0.524 0.464  C Figure 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 37
                            }
                        ],
                        "text": "We also essentially tie with ListNet (Cao et al., 2007), one of the best overall ranking method in the LETOR 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207163577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b64de5584c28fbcfbaf3168d8185221f30ed27e0",
            "isKey": true,
            "numCitedBy": 1718,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative filtering, and many other applications. Several methods for learning to rank have been proposed, which take object pairs as 'instances' in learning. We refer to them as the pairwise approach in this paper. Although the pairwise approach offers advantages, it ignores the fact that ranking is a prediction task on list of objects. The paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as 'instances' in learning. The paper proposes a new probabilistic method for the approach. Specifically it introduces two probability models, respectively referred to as permutation probability and top k probability, to define a listwise loss function for learning. Neural Network and Gradient Descent are then employed as model and algorithm in the learning method. Experimental results on information retrieval show that the proposed listwise approach performs better than the pairwise approach."
            },
            "slug": "Learning-to-rank:-from-pairwise-approach-to-Cao-Qin",
            "title": {
                "fragments": [],
                "text": "Learning to rank: from pairwise approach to listwise approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proposed that learning to rank should adopt the listwise approach in which lists of objects are used as 'instances' in learning, and introduces two probability models, respectively referred to as permutation probability and top k probability, to define a listwise loss function for learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108648318"
                        ],
                        "name": "Sy Bor Wang",
                        "slug": "Sy-Bor-Wang",
                        "structuredName": {
                            "firstName": "Sy",
                            "lastName": "Wang",
                            "middleNames": [
                                "Bor"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sy Bor Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3171632"
                        ],
                        "name": "A. Quattoni",
                        "slug": "A.-Quattoni",
                        "structuredName": {
                            "firstName": "Ariadna",
                            "lastName": "Quattoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Quattoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49933077"
                        ],
                        "name": "Louis-Philippe Morency",
                        "slug": "Louis-Philippe-Morency",
                        "structuredName": {
                            "firstName": "Louis-Philippe",
                            "lastName": "Morency",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Louis-Philippe Morency"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444581"
                        ],
                        "name": "D. Demirdjian",
                        "slug": "D.-Demirdjian",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Demirdjian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Demirdjian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 64
                            }
                        ],
                        "text": "Recently, there has been some work on Conditional Random Fields (Wang et al., 2006) with latent variables."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 64
                            }
                        ],
                        "text": "Re\u00adcently, \nthere hasbeen some work on Conditional Ran\u00addom Fields (Wang et al., 2006) with latent variables."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 12
                            }
                        ],
                        "text": "The work in (Wang et al., 2006) introduces Hidden Conditional Random Fields, a discriminative probabilistic latent variable model for structured prediction, with applications to two computer vision tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 13
                            }
                        ],
                        "text": "The work in (Wang et al., 2006) introduces Hidden Conditional \nRandom Fields, a discriminative probabilistic latent variable model for structured prediction, with applications \nto two com\u00adputer vision tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 7
                            }
                        ],
                        "text": "Hidden Conditional Random Fields for Gesture Recognition."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1171329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c4f1bbaa1eac7bc1e0b8a662ff295ce4963532d",
            "isKey": true,
            "numCitedBy": 536,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a discriminative hidden-state approach for the recognition of human gestures. Gesture sequences often have a complex underlying structure, and models that can incorporate hidden structures have proven to be advantageous for recognition tasks. Most existing approaches to gesture recognition with hidden states employ a Hidden Markov Model or suitable variant (e.g., a factored or coupled state model) to model gesture streams; a significant limitation of these models is the requirement of conditional independence of observations. In addition, hidden states in a generative model are selected to maximize the likelihood of generating all the examples of a given gesture class, which is not necessarily optimal for discriminating the gesture class against other gestures. Previous discriminative approaches to gesture sequence recognition have shown promising results, but have not incorporated hidden states nor addressed the problem of predicting the label of an entire sequence. In this paper, we derive a discriminative sequence model with a hidden state structure, and demonstrate its utility both in a detection and in a multi-way classification formulation. We evaluate our method on the task of recognizing human arm and head gestures, and compare the performance of our method to both generative hidden state and discriminative fully-observable models."
            },
            "slug": "Hidden-Conditional-Random-Fields-for-Gesture-Wang-Quattoni",
            "title": {
                "fragments": [],
                "text": "Hidden Conditional Random Fields for Gesture Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper derives a discriminative sequence model with a hidden state structure, and demonstrates its utility both in a detection and in a multi-way classification formulation."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11955007"
                        ],
                        "name": "Zhengdong Lu",
                        "slug": "Zhengdong-Lu",
                        "structuredName": {
                            "firstName": "Zhengdong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengdong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400347470"
                        ],
                        "name": "M. A. Carreira-Perpi\u00f1\u00e1n",
                        "slug": "M.-A.-Carreira-Perpi\u00f1\u00e1n",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Carreira-Perpi\u00f1\u00e1n",
                            "middleNames": [
                                "\u00c1."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Carreira-Perpi\u00f1\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6068544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "906fbff169dbb5eb774bdc43220c3c199c3b1b11",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Reliably recovering 3D human pose from monocular video requires models that bias the estimates towards typical human poses and motions. We construct priors for people tracking using the Laplacian Eigenmaps Latent Variable Model (LELVM). LELVM is a recently introduced probabilistic dimensionality reduction model that combines the advantages of latent variable models\u2014a multimodal probability density for latent and observed variables, and globally differentiable nonlinear mappings for reconstruction and dimensionality reduction\u2014with those of spectral manifold learning methods\u2014no local optima, ability to unfold highly nonlinear manifolds, and good practical scaling to latent spaces of high dimension. LELVM is computationally efficient, simple to learn from sparse training data, and compatible with standard probabilistic trackers such as particle filters. We analyze the performance of a LELVM-based probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that LELVM not only provides sufficient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements, but also compares favorably with alternative trackers based on PCA or GPLVM priors."
            },
            "slug": "People-Tracking-with-the-Laplacian-Eigenmaps-Latent-Lu-Carreira-Perpi\u00f1\u00e1n",
            "title": {
                "fragments": [],
                "text": "People Tracking with the Laplacian Eigenmaps Latent Variable Model"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is demonstrated that LELVM not only provides sufficient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements, but also compares favorably with alternative trackers based on PCA or GPLVM priors."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145257017"
                        ],
                        "name": "Anand Rangarajan",
                        "slug": "Anand-Rangarajan",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rangarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Rangarajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 137
                            }
                        ],
                        "text": "The general template \nfor a CCCP algorithm for minimizing a function f(w) - g(w), where f and g are convex, works as follows: \nAlgorithm 1 Concave-Convex Procedure (CCCP) 1: Set t =0 and initialize w0 2: repeat 3: Find hyperplane \nvt such that -g(w) = -g(wt)+(w -wt)\u00b7 vt for all w 4: Solve wt+1 = argminw f(w)+w \u00b7 vt 5: Set t = t +1 \n6: until [f(wt)-g(wt)] -[f(wt-1)-g(wt-1)]  E The CCCP algorithm is guaranteed to decrease the objective \nfunction at every iteration and to converge toalocal minimumor saddlepoint(Yuille&#38;Rangara\u00adjan, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 30
                            }
                        ],
                        "text": "The Concave-Convex Procedure (Yuille &#38; Rangara\u00adjan, \n2003) employed in our work is a general frame\u00adwork for minimizing non-convex functions which falls into \nthe class of DC (Di.erence of Convex) program\u00adming."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 88
                            }
                        ],
                        "text": "C max w \u00b7 F(xi,yi,h) . h.H i=1 This allows us to solve the optimization \nproblem us\u00ading the Concave-Convex Procedure (CCCP)(Yuille&#38; Rangarajan, 2003)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 135
                            }
                        ],
                        "text": "We identify a particular, yet rather general, \nformulation for which there exists an e.cient algorithm to .nd a local optimum using the Concave-Convex \nProcedure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 4
                            }
                        ],
                        "text": "The Concave-Convex \nProcedure."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1668136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffb607e61e10a3bb54463b334aaf5ea9c7c04be6",
            "isKey": true,
            "numCitedBy": 1065,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The concave-convex procedure (CCCP) is a way to construct discrete-time iterative dynamical systems that are guaranteed to decrease global optimization and energy functions monotonically. This procedure can be applied to almost any optimization problem, and many existing algorithms can be interpreted in terms of it. In particular, we prove that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP. We show that many existing neural network and mean-field theory algorithms are also examples of CCCP. The generalized iterative scaling algorithm and Sinkhorn's algorithm can also be expressed as CCCP by changing variables. CCCP can be used both as a new way to understand, and prove the convergence of, existing optimization algorithms and as a procedure for generating new algorithms."
            },
            "slug": "The-Concave-Convex-Procedure-Yuille-Rangarajan",
            "title": {
                "fragments": [],
                "text": "The Concave-Convex Procedure"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "It is proved that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145878390"
                        ],
                        "name": "Patrick Ng",
                        "slug": "Patrick-Ng",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2864261"
                        ],
                        "name": "U. Keich",
                        "slug": "U.-Keich",
                        "structuredName": {
                            "firstName": "Uri",
                            "lastName": "Keich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Keich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14073953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6768be93809707ec8abe48aff1f970caf3959cd",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "UNLABELLED\nWe present GIMSAN (GIbbsMarkov with Significance ANalysis): a novel tool for de novo motif finding. GIMSAN combines GibbsMarkov, our variant of the Gibbs Sampler, described here for the first time, with our recently introduced significance analysis.\n\n\nAVAILABILITY\nGIMSAN is currently available as a web application and a stand-alone application on Unix and PBS (Portable Batch System) cluster through links from http://www.cs.cornell.edu/~keich."
            },
            "slug": "GIMSAN:-a-Gibbs-motif-finder-with-significance-Ng-Keich",
            "title": {
                "fragments": [],
                "text": "GIMSAN: a Gibbs motif finder with significance analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "GIMSAN combines GibbsMarkov, the authors' variant of the Gibbs Sampler, described here for the first time, with the recently introduced significance analysis to create a novel tool for de novo motif finding."
            },
            "venue": {
                "fragments": [],
                "text": "Bioinform."
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 167
                            }
                        ],
                        "text": "For this noun phrase corefer\u00adence task, the new formulation with Latent \nStructural SVM improves both the prediction performance and training e.ciency over conventional Structural \nSVMs. 5.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 14
                            }
                        ],
                        "text": "When \ntraining Structural SVMs, the parameter vec\u00adtor w is determined by minimizing the (regularized) risk \non the training set (x1,y1),...,(xn,yn)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Structural SVMs Suppose we are given a training set \nof input-output structure pairs S= {(x1,y1),..., (xn,yn)}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "In (Finley &#38; Joachims, 2005) the \ntask is formulated as a correlation clustering problem trained with Structural SVMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 9
                            }
                        ],
                        "text": "Learning Structural SVMs with Latent Variables Chun-Nam John Yu cnyu@cs.cornell.edu Thorsten Joachims \ntj@cs.cornell.edu Department of Computer Science, Cornell University, Ithaca, NY 14850 USA Abstract \nWe present a large-margin formulation and algorithm for structured output prediction that allows the \nuse of latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 45
                            }
                        ],
                        "text": "Y where yi(w)= argmaxF(xi,y). y.Y w \u00b7 To train Structural SVMs \nwe then solve the following convex optimization problem: ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 23
                            }
                        ],
                        "text": "(3) h.H In the case of Structural SVMs without latent vari\u00adables, the complex dependence on w within \nthe loss. can be removed using the following inequality, com\u00admonly referred to as loss-augmented inference \nin Structural SVM training:  max w \u00b7 F(x,y ) +.(yi,y i(w)) y ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 50
                            }
                        ],
                        "text": "Y\u00d7H Using the same reasoning \nas for fully observed Struc\u00adtural SVMs, this gives rise to the following optimiza\u00adtion problem for Structural \nSVMs with latent vari\u00adables: 0 n 1 1w12 min +C max[w\u00b7 F(xi, h)+.(yi, h)] y,y, w 2 ( y,h)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 242
                            }
                        ],
                        "text": "As the cost of each CCCP iteration is no more than solving a standard Structural SVM optimization problem \n(with the completion of latent variables), the total number of CCCP iterations gives us a rough estimate \nof the cost of training Latent Structural SVMs, which is not particularly expensive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Struc\u00adtural SVMsgive excellentperformance on many \nstruc\u00adtured prediction tasks, especially when the model F is high-dimensional and it is necessary to \noptimize to non-standard loss functions .. 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "Conclusions We have presented a framework and formulation for learning Structural \nSVMs with latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 137
                            }
                        ],
                        "text": "Copy\u00adright 2009 by the author(s)/owner(s). large-margin \nstructured output learning such as Max-Margin Markov Networks or Structural SVMs (Taskar et al., 2003; \nTsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maxmargin Markov networks. Adv. in Neural Inf. Process . Syst"
            },
            "venue": {
                "fragments": [],
                "text": "Maxmargin Markov networks. Adv. in Neural Inf. Process . Syst"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "According to the Empirical Risk Minimization (ERM) principle [9], we should search for a parameter vector ~ w with low empirical risk \u2211n i=1 \u2206(yi, f~ w(xi))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2142161"
                        ],
                        "name": "K. Kiwiel",
                        "slug": "K.-Kiwiel",
                        "structuredName": {
                            "firstName": "Krzysztof",
                            "lastName": "Kiwiel",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kiwiel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 116
                            }
                        ],
                        "text": "In our implementation, we used an improved version of the cutting plane algorithm \ncalled the proximal bundle method(Kiwiel, 1990) to solve the standard Structural SVM problem in Equation \n(7)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 116
                            }
                        ],
                        "text": "In our implementation, we used an improved version of the cutting plane algorithm called the proximal bundle method (Kiwiel, 1990) to solve the standard Structural SVM problem in Equation (7)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 32870608,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8f61d9b0282ae4fc244707293fc61c955354834e",
            "isKey": false,
            "numCitedBy": 461,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractProximal bundle methods for minimizing a convex functionf generate a sequence {xk} by takingxk+1 to be the minimizer of\n$$\\hat f^k (x) + u^k |x - x^k |^2 /2$$\n, where\n$$\\hat f^k $$\n is a sufficiently accurate polyhedral approximation tof anduk > 0. The usual choice ofuk = 1 may yield very slow convergence. A technique is given for choosing {uk} adaptively that eliminates sensitivity to objective scaling. Some encouraging numerical experience is reported."
            },
            "slug": "Proximity-control-in-bundle-methods-for-convex-Kiwiel",
            "title": {
                "fragments": [],
                "text": "Proximity control in bundle methods for convex nondifferentiable minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "A technique is given for choosing {uk} adaptively that eliminates sensitivity to objective scaling and some encouraging numerical experience is reported."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is similar to the iterative process of Expectation Maximization (EM) [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 167
                            }
                        ],
                        "text": "For this noun phrase corefer\u00adence task, the new formulation with Latent \nStructural SVM improves both the prediction performance and training e.ciency over conventional Structural \nSVMs. 5.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 14
                            }
                        ],
                        "text": "When \ntraining Structural SVMs, the parameter vec\u00adtor w is determined by minimizing the (regularized) risk \non the training set (x1,y1),...,(xn,yn)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Structural SVMs Suppose we are given a training set \nof input-output structure pairs S= {(x1,y1),..., (xn,yn)}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "In (Finley &#38; Joachims, 2005) the \ntask is formulated as a correlation clustering problem trained with Structural SVMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 9
                            }
                        ],
                        "text": "Learning Structural SVMs with Latent Variables Chun-Nam John Yu cnyu@cs.cornell.edu Thorsten Joachims \ntj@cs.cornell.edu Department of Computer Science, Cornell University, Ithaca, NY 14850 USA Abstract \nWe present a large-margin formulation and algorithm for structured output prediction that allows the \nuse of latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 45
                            }
                        ],
                        "text": "Y where yi(w)= argmaxF(xi,y). y.Y w \u00b7 To train Structural SVMs \nwe then solve the following convex optimization problem: ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 23
                            }
                        ],
                        "text": "(3) h.H In the case of Structural SVMs without latent vari\u00adables, the complex dependence on w within \nthe loss. can be removed using the following inequality, com\u00admonly referred to as loss-augmented inference \nin Structural SVM training:  max w \u00b7 F(x,y ) +.(yi,y i(w)) y ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 50
                            }
                        ],
                        "text": "Y\u00d7H Using the same reasoning \nas for fully observed Struc\u00adtural SVMs, this gives rise to the following optimiza\u00adtion problem for Structural \nSVMs with latent vari\u00adables: 0 n 1 1w12 min +C max[w\u00b7 F(xi, h)+.(yi, h)] y,y, w 2 ( y,h)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 242
                            }
                        ],
                        "text": "As the cost of each CCCP iteration is no more than solving a standard Structural SVM optimization problem \n(with the completion of latent variables), the total number of CCCP iterations gives us a rough estimate \nof the cost of training Latent Structural SVMs, which is not particularly expensive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Struc\u00adtural SVMsgive excellentperformance on many \nstruc\u00adtured prediction tasks, especially when the model F is high-dimensional and it is necessary to \noptimize to non-standard loss functions .. 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "Conclusions We have presented a framework and formulation for learning Structural \nSVMs with latent variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 137
                            }
                        ],
                        "text": "Copy\u00adright 2009 by the author(s)/owner(s). large-margin \nstructured output learning such as Max-Margin Markov Networks or Structural SVMs (Taskar et al., 2003; \nTsochantaridis et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maxmargin Markov networks. Adv. in Neural Inf. Process . Syst"
            },
            "venue": {
                "fragments": [],
                "text": "Maxmargin Markov networks. Adv. in Neural Inf. Process . Syst"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39962304"
                        ],
                        "name": "Darren Gehring",
                        "slug": "Darren-Gehring",
                        "structuredName": {
                            "firstName": "Darren",
                            "lastName": "Gehring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darren Gehring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686971"
                        ],
                        "name": "T. Graepel",
                        "slug": "T.-Graepel",
                        "structuredName": {
                            "firstName": "Thore",
                            "lastName": "Graepel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Graepel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 100
                            }
                        ],
                        "text": "We can see from Table \n3 that our Latent Struc\u00adtural SVM approachperformsbetter than the Ranking SVM (Herbrich et al., 2000; \nJoachims, 2002) on pre\u00adcision@1,3,5,10, one of the stronger baselines in the LETOR 3.0benchmark."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 101
                            }
                        ],
                        "text": "We can see from Table 3 that our Latent Structural SVM approach performs better than the Ranking SVM (Herbrich et al., 2000; Joachims, 2002) on precision@1,3,5,10, one of the stronger baselines in the LETOR 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 68
                            }
                        ],
                        "text": "Precision@k on OHSUMED dataset (5-fold CV) OHSUMED P@1 P@3 P@5 P@10 Ranking SVM 0.597 \n0.543 0.532 0.486 ListNet 0.652 0.602 0.550 0.498 Latent Structural SVM 0.680 0.573 0.567 0.494 Initial \nWeight Vector 0.626 0.557 0.524 0.464  C Figure 3."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60533697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "958f001b6f348f7c353260b289bed185fffac847",
            "isKey": true,
            "numCitedBy": 980,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Large-Margin-Rank-Boundaries-for-Ordinal-Regression-Gehring-Graepel",
            "title": {
                "fragments": [],
                "text": "Large Margin Rank Boundaries for Ordinal Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 111
                            }
                        ],
                        "text": "In the computer vision community there are recent works on training Hidden \nCRF using the max-margin criterion (Felzenszwalb et al., 2008; Wang &#38; Mori, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 33
                            }
                        ],
                        "text": "Interestingly, the algorithm in (Felzenszwalb et al., 2008) coincides with our approach for binary \nclassi.cation but was derived in a di.erent way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 110
                            }
                        ],
                        "text": "In the computer vision community there are recent works on training Hidden CRF using the max-margin criterion (Felzenszwalb et al., 2008; Wang & Mori, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 32
                            }
                        ],
                        "text": "Interestingly, the algorithm in (Felzenszwalb et al., 2008) coincides with our approach for binary classification but was derived in a different way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Discriminatively Trained, Multiscale"
            },
            "venue": {
                "fragments": [],
                "text": "Deformable Part Model. Proc. Computer Vision and Pattern Recognition Conf. (pp. 1\u20138)"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718084"
                        ],
                        "name": "T. Bailey",
                        "slug": "T.-Bailey",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Bailey",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bailey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722831"
                        ],
                        "name": "C. Elkan",
                        "slug": "C.-Elkan",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Elkan",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Elkan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 63
                            }
                        ],
                        "text": "Popular methods for motif .nding includes methods based on EM (Bailey &#38; Elkan, 1995) \nand Gibbs\u00adsampling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122209731,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "63e9b1123197b8a49e9be36d95a49fd024e548d1",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The MEME algorithm extends the expectation maximization (EM) algorithm for identifying motifs in unaligned biopolymer sequences. The aim of MEME is to discover new motifs in a set of biopolymer sequences where little or nothing is known in advance about any motifs that may be present. MEME innovations expand the range of problems which can be solved using EM and increase the chance of finding good solutions. First, subsequences which actually occur in the biopolymer sequences are used as starting points for the EM algorithm to increase the probability of finding globally optimal motifs. Second, the assumption that each sequence contains exactly one occurrence of the shared motif is removed. This allows multiple appearances of a motif to occur in any sequence and permits the algorithm to ignore sequences with no appearance of the shared motif, increasing its resistance to noisy data. Third, a method for probabilistically erasing shared motifs after they are found is incorporated so that several distinct motifs can be found in the same set of sequences, both when different motifs appear in different sequences and when a single sequence may contain multiple motifs. Experiments show that MEME can discover both the CRP and LexA binding sites from a set of sequences which contain one or both sites, and that MEME can discover both the \u221210 and \u221235 promoter regions in a set ofE. coli sequences."
            },
            "slug": "Unsupervised-learning-of-multiple-motifs-in-using-Bailey-Elkan",
            "title": {
                "fragments": [],
                "text": "Unsupervised learning of multiple motifs in biopolymers using expectation maximization"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "The MEME algorithm extends the expectation maximization (EM) algorithm for identifying motifs in unaligned biopolymer sequences and can discover both the CRP and LexA binding sites from a set of sequences which contain one or both sites."
            },
            "venue": {
                "fragments": [],
                "text": "Mach. Learn."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Sim\u00adilarly, in machine translation, one may be given the translation \nyof sentence x, but not the linguistic struc\u00adture h (e.g. parse trees, word alignments) that con\u00adnects \nthem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Max-margin Markov networks. Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Max-margin Markov networks. Advances in Neural Information Processing Systems"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "imum likelihood from incomplete data via the EM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Royal Statistical Society"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 212
                            }
                        ],
                        "text": "For example, it has been used for capturing interesting substructures or parts in object recognition [10], for automatic refinement of grammars in parsing [6], and for dimensionality reduction in people tracking [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and C"
            },
            "venue": {
                "fragments": [],
                "text": "Sminchisescu. People Tracking with the Laplacian Eigenmaps Latent Variable Model"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adv. in Neural Inf. Process. Syst"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. in Neural Inf. Process. Syst"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 19
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 31,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-structural-SVMs-with-latent-variables-Yu-Joachims/5ae20e0bdfddc1888148e0fcde88d937e96318d2?sort=total-citations"
}