{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052321363"
                        ],
                        "name": "Matheus Palhares Viana",
                        "slug": "Matheus-Palhares-Viana",
                        "structuredName": {
                            "firstName": "Matheus",
                            "lastName": "Viana",
                            "middleNames": [
                                "Palhares"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matheus Palhares Viana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34475405"
                        ],
                        "name": "Dario Augusto Borges Oliveira",
                        "slug": "Dario-Augusto-Borges-Oliveira",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Oliveira",
                            "middleNames": [
                                "Augusto",
                                "Borges"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dario Augusto Borges Oliveira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4741889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdcdf4df32a753523d82dd5c2daa55b11ac73749",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic document layout analysis is a crucial step in cognitive computing and processes that extract information out of document images, such as specific-domain knowledge database creation, graphs and images understanding, extraction of structured data from tables, and others. Even with the progress observed in this field in the last years, challenges are still open and range from accurately detecting content boxes to classifying them into semantically meaningful classes. With the popularization of mobile devices and cloud-based services, the need for approaches that are both fast and economic in data usage is a reality. In this paper we propose a fast one-dimensional approach for automatic document layout analysis considering text, figures and tables based on convolutional neural networks (CNN). We take advantage of the inherently one-dimensional pattern observed in text and table blocks to reduce the dimension analysis from bi-dimensional documents images to 1D signatures, improving significantly the overall performance: we present considerably faster execution times and more compact data usage with no loss in overall accuracy if compared with a classical bidimensional CNN approach."
            },
            "slug": "Fast-CNN-Based-Document-Layout-Analysis-Viana-Oliveira",
            "title": {
                "fragments": [],
                "text": "Fast CNN-Based Document Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper takes advantage of the inherently one-dimensional pattern observed in text and table blocks to reduce the dimension analysis from bi-dimensional documents images to 1D signatures, improving significantly the overall performance."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision Workshops (ICCVW)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51226106"
                        ],
                        "name": "Pranaydeep Singh",
                        "slug": "Pranaydeep-Singh",
                        "structuredName": {
                            "firstName": "Pranaydeep",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pranaydeep Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26961805"
                        ],
                        "name": "Srikrishna Varadarajan",
                        "slug": "Srikrishna-Varadarajan",
                        "structuredName": {
                            "firstName": "Srikrishna",
                            "lastName": "Varadarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srikrishna Varadarajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2140475450"
                        ],
                        "name": "A. Singh",
                        "slug": "A.-Singh",
                        "structuredName": {
                            "firstName": "Ankit",
                            "lastName": "Singh",
                            "middleNames": [
                                "Narayan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27088137"
                        ],
                        "name": "Muktabh Mayank Srivastava",
                        "slug": "Muktabh-Mayank-Srivastava",
                        "structuredName": {
                            "firstName": "Muktabh",
                            "lastName": "Srivastava",
                            "middleNames": [
                                "Mayank"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muktabh Mayank Srivastava"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 159
                            }
                        ],
                        "text": "\u2026images, charts and tables in newspapers, Oliveira and Viana (2017), who uss 1D CNNs to recognize text, images, and tables in scientific articles, and finally Singh et al. (2018), who use a LSTD model to recognize various customizable text and image classes in multiple domains, with mixed results."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52069216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12eede12e8673888e07eb59b922227b2d0f92969",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We try to address the problem of document layout understanding using a simple algorithm which generalizes across multiple domains while training on just few examples per domain. We approach this problem via supervised object detection method and propose a methodology to overcome the requirement of large datasets. We use the concept of transfer learning by pre-training our object detector on a simple artificial (source) dataset and fine-tuning it on a tiny domain specific (target) dataset. We show that this methodology works for multiple domains with training samples as less as 10 documents. We demonstrate the effect of each component of the methodology in the end result and show the superiority of this methodology over simple object detectors."
            },
            "slug": "Multidomain-Document-Layout-Understanding-using-Few-Singh-Varadarajan",
            "title": {
                "fragments": [],
                "text": "Multidomain Document Layout Understanding using Few Shot Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes a methodology to overcome the requirement of large datasets by using the concept of transfer learning by pre-training the object detector on a simple artificial dataset and fine-tuning it on a tiny domain specific dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ICIAR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 92
                            }
                        ],
                        "text": ", and has been highly successful even when objects are cropped or occluded by other objects (Lin et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 220
                            }
                        ],
                        "text": "This approach is advantageous to general purpose object detection, because objects may be located anywhere in a photo or video., and has been highly successful even when objects are cropped or occluded by other objects (Lin et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14113767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "isKey": false,
            "numCitedBy": 19779,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
            },
            "slug": "Microsoft-COCO:-Common-Objects-in-Context-Lin-Maire",
            "title": {
                "fragments": [],
                "text": "Microsoft COCO: Common Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding by gathering images of complex everyday scenes containing common objects in their natural context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153819461"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2700495"
                        ],
                        "name": "Mathias Seuret",
                        "slug": "Mathias-Seuret",
                        "structuredName": {
                            "firstName": "Mathias",
                            "lastName": "Seuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathias Seuret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109393868"
                        ],
                        "name": "Hao Wei",
                        "slug": "Hao-Wei",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722800"
                        ],
                        "name": "J. Hennebert",
                        "slug": "J.-Hennebert",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Hennebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hennebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680326"
                        ],
                        "name": "R. Ingold",
                        "slug": "R.-Ingold",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Ingold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ingold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 132
                            }
                        ],
                        "text": "Alternatively, visual-based techniques for document layout analysis have tended to focus on text segmentation (Tran et al., 2015), (Chen et al., 2015a), (Chen et al., 2015b), (Garz et al., 2016), especially for historical documents; Eskenazi et al. (2017) survey dozens of such methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35615788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a595205493266d1fd7d3575a10c2f87e4d7c1dca",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a new dataset and a ground-truthing methodology for layout analysis of historical documents with complex layouts. The dataset is based on a generic model for ground-truth presentation of the complex layout structure of historical documents. For the purpose of extracting uniformly the document contents, our model defines five types of regions of interest: page, text block, text line, decoration, and comment. Unconstrained polygons are used to outline the regions. A performance metric is proposed in order to evaluate various page segmentation methods based on this model. We have analysed four state-of-the-art ground-truthing tools: TRUVIZ, GEDI, WebGT, and Aletheia. From this analysis, we conceptualized and developed Divadia, a new tool that overcomes some of the drawbacks of these tools, targeting the simplicity and the efficiency of the layout ground truthing process on historical document images. With Divadia, we have created a new public dataset. This dataset contains 120 pages from three historical document image collections of different styles and is made freely available to the scientific community for historical document layout analysis research."
            },
            "slug": "Ground-truth-model,-tool,-and-dataset-for-layout-of-Chen-Seuret",
            "title": {
                "fragments": [],
                "text": "Ground truth model, tool, and dataset for layout analysis of historical documents"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A new dataset and a ground-truthing methodology for layout analysis of historical documents with complex layouts, targeting the simplicity and the efficiency of the layout ground truthing process on historical document images is proposed and developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 97
                            }
                        ],
                        "text": "The baseline technique used for this work is the highly successful object detector Faster R-CNN (Ren et al., 2015) \u2013 a so-called two stage detection model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": false,
            "numCitedBy": 32561,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2134774148"
                        ],
                        "name": "Thomas Lang",
                        "slug": "Thomas-Lang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Lang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715229"
                        ],
                        "name": "Markus Diem",
                        "slug": "Markus-Diem",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Diem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Diem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729636"
                        ],
                        "name": "Florian Kleber",
                        "slug": "Florian-Kleber",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Kleber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Kleber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 60
                            }
                        ],
                        "text": "More closely aligned with the aim of this work are those by Lang et al. (2018), who use HOG features and random forests to recognize text, images, charts and tables in newspapers, Oliveira and Viana (2017), who uss 1D CNNs to recognize text, images, and tables in scientific articles, and finally\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 201798849,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "2d9cf08a81252b753c0f6e09c42efd18c4f0c7a3",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Even though modern newspapers are born-digital, layout information is often not stored or not distributed along with the documents. Techniques for region segmentation and classification have been developed that use incomplete region information extracted from digital newspapers available in the PDF format. Extracted image regions which are too large or overlap each other can be used along with the document image to obtain region bounding boxes that fit the visible images in the document. Known and manually labeled document regions are used to train a random forest classifier using HOG features, in order to classify the final region bounding boxes, resulting in classification error rates of 0.05 for text/table regions and 0.1 for image/chart regions."
            },
            "slug": "Physical-Layout-Analysis-of-Partly-Annotated-Images-Lang-Diem",
            "title": {
                "fragments": [],
                "text": "Physical Layout Analysis of Partly Annotated Newspaper Images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Techniques for region segmentation and classification have been developed that use incomplete region information extracted from digital newspapers available in the PDF format to train a random forest classifier using HOG features."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145431050"
                        ],
                        "name": "Mayank Singh",
                        "slug": "Mayank-Singh",
                        "structuredName": {
                            "firstName": "Mayank",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mayank Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3468443"
                        ],
                        "name": "B. Barua",
                        "slug": "B.-Barua",
                        "structuredName": {
                            "firstName": "Barnopriyo",
                            "lastName": "Barua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Barua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3469144"
                        ],
                        "name": "Priyank Palod",
                        "slug": "Priyank-Palod",
                        "structuredName": {
                            "firstName": "Priyank",
                            "lastName": "Palod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Priyank Palod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1453433376"
                        ],
                        "name": "M. Garg",
                        "slug": "M.-Garg",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Garg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Garg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3469142"
                        ],
                        "name": "Sidhartha Satapathy",
                        "slug": "Sidhartha-Satapathy",
                        "structuredName": {
                            "firstName": "Sidhartha",
                            "lastName": "Satapathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sidhartha Satapathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3469059"
                        ],
                        "name": "Samuel Bushi",
                        "slug": "Samuel-Bushi",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bushi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel Bushi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50430041"
                        ],
                        "name": "Kumar Ayush",
                        "slug": "Kumar-Ayush",
                        "structuredName": {
                            "firstName": "Kumar",
                            "lastName": "Ayush",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kumar Ayush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3468903"
                        ],
                        "name": "K. S. Rohith",
                        "slug": "K.-S.-Rohith",
                        "structuredName": {
                            "firstName": "Krishna",
                            "lastName": "Rohith",
                            "middleNames": [
                                "Sai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. S. Rohith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3469040"
                        ],
                        "name": "Tulasi Gamidi",
                        "slug": "Tulasi-Gamidi",
                        "structuredName": {
                            "firstName": "Tulasi",
                            "lastName": "Gamidi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tulasi Gamidi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111980452"
                        ],
                        "name": "P. Goyal",
                        "slug": "P.-Goyal",
                        "structuredName": {
                            "firstName": "Pawan",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33392067"
                        ],
                        "name": "Animesh Mukherjee",
                        "slug": "Animesh-Mukherjee",
                        "structuredName": {
                            "firstName": "Animesh",
                            "lastName": "Mukherjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Animesh Mukherjee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 18
                            }
                        ],
                        "text": ", 2015) and OCR++ (Singh et al., 2016) extract the raw or processed markup from \u201cborn-digital\u201d PDFs"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 55
                            }
                        ],
                        "text": "Systems like CERMINE (Tkaczyk et al., 2015) and OCR++ (Singh et al., 2016) extract the raw or processed markup from \u201cborn-digital\u201d PDFs (e.g. using tools like pdf2xml) and apply a variety of text processing methods to deduce docu-\nment structure and apply semantic labels to text blocks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3187359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b2b2afaf6e2b7b84d93c62d2efcf6577ddd9aab",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes OCR++, an open-source framework designed for a variety of information extraction tasks from scholarly articles including metadata (title, author names, affiliation and e-mail), structure (section headings and body text, table and figure headings, URLs and footnotes) and bibliography (citation instances and references). We analyze a diverse set of scientific articles written in English to understand generic writing patterns and formulate rules to develop this hybrid framework. Extensive evaluations show that the proposed framework outperforms the existing state-of-the-art tools by a large margin in structural information extraction along with improved performance in metadata and bibliography extraction tasks, both in terms of accuracy (around 50% improvement) and processing time (around 52% improvement). A user experience study conducted with the help of 30 researchers reveals that the researchers found this system to be very helpful. As an additional objective, we discuss two novel use cases including automatically extracting links to public datasets from the proceedings, which would further accelerate the advancement in digital libraries. The result of the framework can be exported as a whole into structured TEI-encoded documents. Our framework is accessible online at http://www.cnergres.iitkgp.ac.in/OCR++/home/."
            },
            "slug": "OCR++:-A-Robust-Framework-For-Information-from-Singh-Barua",
            "title": {
                "fragments": [],
                "text": "OCR++: A Robust Framework For Information Extraction from Scholarly Articles"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Extensive evaluations show that the proposed framework outperforms the existing state-of-the-art tools by a large margin in structural information extraction along with improved performance in metadata and bibliography extraction tasks, both in terms of accuracy and processing time."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47316088"
                        ],
                        "name": "Priya Goyal",
                        "slug": "Priya-Goyal",
                        "structuredName": {
                            "firstName": "Priya",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Priya Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 208
                            }
                        ],
                        "text": "Figure 4 shows per-class performance over 30 training epochs, as well as comparative performance against the baseline Faster R-CNN model and reference model implementations of YOLOv3 (Linder-Noren, 2018) and RetinaNet (Henon, 2018)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 151
                            }
                        ],
                        "text": "Another important consideration is the existence of newer, even more successful detection models like YOLOv3 (Redmon and Farhadi, 2018) and RetinaNet (Lin et al., 2017), which use a single-stage detection paradigm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 47252984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72564a69bf339ff1d16a639c86a764db2321caab",
            "isKey": false,
            "numCitedBy": 8230,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron."
            },
            "slug": "Focal-Loss-for-Dense-Object-Detection-Lin-Goyal",
            "title": {
                "fragments": [],
                "text": "Focal Loss for Dense Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to address the extreme foreground-background class imbalance encountered during training of dense detectors by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples, and develops a novel Focal Loss, which focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153819461"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2700495"
                        ],
                        "name": "Mathias Seuret",
                        "slug": "Mathias-Seuret",
                        "structuredName": {
                            "firstName": "Mathias",
                            "lastName": "Seuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathias Seuret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722800"
                        ],
                        "name": "J. Hennebert",
                        "slug": "J.-Hennebert",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Hennebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hennebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680326"
                        ],
                        "name": "R. Ingold",
                        "slug": "R.-Ingold",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Ingold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ingold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 132
                            }
                        ],
                        "text": "Alternatively, visual-based techniques for document layout analysis have tended to focus on text segmentation (Tran et al., 2015), (Chen et al., 2015a), (Chen et al., 2015b), (Garz et al., 2016), especially for historical documents; Eskenazi et al. (2017) survey dozens of such methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9814021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b90411acf9a597f139651133d42bffce3df78044",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an unsupervised feature learning method for page segmentation of historical handwritten documents available as color images. We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as either periphery, background, text block, or decoration. Traditional methods in this area rely on carefully hand-crafted features or large amounts of prior knowledge. In contrast, we apply convolutional autoencoders to learn features directly from pixel intensity values. Then, using these features to train an SVM, we achieve high quality segmentation without any assumption of specific topologies and shapes. Experiments on three public datasets demonstrate the effectiveness and superiority of the proposed approach."
            },
            "slug": "Page-segmentation-of-historical-document-images-Chen-Seuret",
            "title": {
                "fragments": [],
                "text": "Page segmentation of historical document images with convolutional autoencoders"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper considers page segmentation as a pixel labeling problem, i.e., each pixel is classified as either periphery, background, text block, or decoration, and applies convolutional autoencoders to learn features directly from pixel intensity values."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143959908"
                        ],
                        "name": "H. Bast",
                        "slug": "H.-Bast",
                        "structuredName": {
                            "firstName": "Hannah",
                            "lastName": "Bast",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bast"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285788"
                        ],
                        "name": "Claudius Korzen",
                        "slug": "Claudius-Korzen",
                        "structuredName": {
                            "firstName": "Claudius",
                            "lastName": "Korzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claudius Korzen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 145
                            }
                        ],
                        "text": "Standard tools often mix headers, footers, table and figure captions, page numbers, and other extraneous text into the main text being extracted (Bast and Korzen, 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4910532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18efceeda452f50e81e0ca31cf2e959a673094c0",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting the body text from a PDF document is an important but surprisingly difficult task. The reason is that PDF is a layout-based format which specifies the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). There is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data. We construct such a benchmark of 12,098 scientific articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which significantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to finally make text extraction from PDF a \"solved problem\"."
            },
            "slug": "A-Benchmark-and-Evaluation-for-Text-Extraction-from-Bast-Korzen",
            "title": {
                "fragments": [],
                "text": "A Benchmark and Evaluation for Text Extraction from PDF"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper shows how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data, and establishes a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool."
            },
            "venue": {
                "fragments": [],
                "text": "2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697354"
                        ],
                        "name": "A. Garz",
                        "slug": "A.-Garz",
                        "structuredName": {
                            "firstName": "Angelika",
                            "lastName": "Garz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Garz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2700495"
                        ],
                        "name": "Mathias Seuret",
                        "slug": "Mathias-Seuret",
                        "structuredName": {
                            "firstName": "Mathias",
                            "lastName": "Seuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathias Seuret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2860677"
                        ],
                        "name": "Fotini Simistira",
                        "slug": "Fotini-Simistira",
                        "structuredName": {
                            "firstName": "Fotini",
                            "lastName": "Simistira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fotini Simistira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153745355"
                        ],
                        "name": "Andreas Fischer",
                        "slug": "Andreas-Fischer",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680326"
                        ],
                        "name": "R. Ingold",
                        "slug": "R.-Ingold",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Ingold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ingold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 176
                            }
                        ],
                        "text": "Alternatively, visual-based techniques for document layout analysis have tended to focus on text segmentation (Tran et al., 2015), (Chen et al., 2015a), (Chen et al., 2015b), (Garz et al., 2016), especially for historical documents; Eskenazi et al. (2017) survey dozens of such methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 10
                            }
                        ],
                        "text": ", 2015b), (Garz et al., 2016), especially for historical documents; Eskenazi et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23806350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd61ee05c320ec40d67ed8cf857de2799dbd3174",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Ground truth is both - indispensable for training and evaluating document analysis methods, and yet very tedious to create manually. This especially holds true for complex historical manuscripts that exhibit challenging layouts with interfering and overlapping handwriting. In this paper, we propose a novel semi-automatic system to support layout annotations in such a scenario based on document graphs and a pen-based scribbling interaction. On the one hand, document graphs provide a sparse page representation that is already close to the desired ground truth and on the other hand, scribbling facilitates an efficient and convenient pen-based interaction with the graph. The performance of the system is demonstrated in the context of a newly introduced database of historical manuscripts with complex layouts."
            },
            "slug": "Creating-Ground-Truth-for-Historical-Manuscripts-Garz-Seuret",
            "title": {
                "fragments": [],
                "text": "Creating Ground Truth for Historical Manuscripts with Document Graphs and Scribbling Interaction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel semi-automatic system to support layout annotations in such a scenario based on document graphs and a pen-based scribbling interaction and the performance is demonstrated in the context of a newly introduced database of historical manuscripts with complex layouts."
            },
            "venue": {
                "fragments": [],
                "text": "2016 12th IAPR Workshop on Document Analysis Systems (DAS)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2458171"
                        ],
                        "name": "S\u00e9bastien Eskenazi",
                        "slug": "S\u00e9bastien-Eskenazi",
                        "structuredName": {
                            "firstName": "S\u00e9bastien",
                            "lastName": "Eskenazi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S\u00e9bastien Eskenazi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399368454"
                        ],
                        "name": "Petra Gomez-Kr\u00e4mer",
                        "slug": "Petra-Gomez-Kr\u00e4mer",
                        "structuredName": {
                            "firstName": "Petra",
                            "lastName": "Gomez-Kr\u00e4mer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petra Gomez-Kr\u00e4mer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695766"
                        ],
                        "name": "J. Ogier",
                        "slug": "J.-Ogier",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Ogier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ogier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 233
                            }
                        ],
                        "text": "Alternatively, visual-based techniques for document layout analysis have tended to focus on text segmentation (Tran et al., 2015), (Chen et al., 2015a), (Chen et al., 2015b), (Garz et al., 2016), especially for historical documents; Eskenazi et al. (2017) survey dozens of such methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4311339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08ab557e132322a5f161d7ddee4ad2a42b806621",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-comprehensive-survey-of-mostly-textual-document-Eskenazi-Gomez-Kr\u00e4mer",
            "title": {
                "fragments": [],
                "text": "A comprehensive survey of mostly textual document segmentation algorithms since 2008"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35119829"
                        ],
                        "name": "Ruihua Song",
                        "slug": "Ruihua-Song",
                        "structuredName": {
                            "firstName": "Ruihua",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruihua Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144321224"
                        ],
                        "name": "Haifeng Liu",
                        "slug": "Haifeng-Liu",
                        "structuredName": {
                            "firstName": "Haifeng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haifeng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259699"
                        ],
                        "name": "Ji-Rong Wen",
                        "slug": "Ji-Rong-Wen",
                        "structuredName": {
                            "firstName": "Ji-Rong",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji-Rong Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712167"
                        ],
                        "name": "Wei-Ying Ma",
                        "slug": "Wei-Ying-Ma",
                        "structuredName": {
                            "firstName": "Wei-Ying",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Ying Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 74
                            }
                        ],
                        "text": "Analogous approaches for web pages \u2013 such as those by Cai et al. (2003) and Song et al. (2004) \u2013 also examine the document source (in this case the page DOM) to build representations for further processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6251615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f53944274eea918dbc0e0ee50e2774894eb9a7e",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work shows that a web page can be partitioned into multiple segments or blocks, and often the importance of those blocks in a page is not equivalent. Also, it has been proven that differentiating noisy or unimportant blocks from pages can facilitate web mining, search and accessibility. However, no uniform approach and model has been presented to measure the importance of different segments in web pages. Through a user study, we found that people do have a consistent view about the importance of blocks in web pages. In this paper, we investigate how to find a model to automatically assign importance values to blocks in a web page. We define the block importance estimation as a learning problem. First, we use a vision-based page segmentation algorithm to partition a web page into semantic blocks with a hierarchical structure. Then spatial features (such as position and size) and content features (such as the number of images and links) are extracted to construct a feature vector for each block. Based on these features, learning algorithms are used to train a model to assign importance to different segments in the web page. In our experiments, the best model can achieve the performance with Micro-F1 79% and Micro-Accuracy 85.9%, which is quite close to a person's view."
            },
            "slug": "Learning-block-importance-models-for-web-pages-Song-Liu",
            "title": {
                "fragments": [],
                "text": "Learning block importance models for web pages"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper uses a vision-based page segmentation algorithm to partition a web page into semantic blocks with a hierarchical structure, then spatial features and content features are extracted and used to construct a feature vector for each block."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695024"
                        ],
                        "name": "Dominika Tkaczyk",
                        "slug": "Dominika-Tkaczyk",
                        "structuredName": {
                            "firstName": "Dominika",
                            "lastName": "Tkaczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dominika Tkaczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2670951"
                        ],
                        "name": "P. Szostek",
                        "slug": "P.-Szostek",
                        "structuredName": {
                            "firstName": "Pawel",
                            "lastName": "Szostek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Szostek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745854"
                        ],
                        "name": "Mateusz Fedoryszak",
                        "slug": "Mateusz-Fedoryszak",
                        "structuredName": {
                            "firstName": "Mateusz",
                            "lastName": "Fedoryszak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mateusz Fedoryszak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3146651"
                        ],
                        "name": "Piotr Jan Dendek",
                        "slug": "Piotr-Jan-Dendek",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Dendek",
                            "middleNames": [
                                "Jan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Jan Dendek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721046"
                        ],
                        "name": "Lukasz Bolikowski",
                        "slug": "Lukasz-Bolikowski",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Bolikowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Bolikowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 21
                            }
                        ],
                        "text": "Systems like CERMINE (Tkaczyk et al., 2015) and OCR++ (Singh et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 22
                            }
                        ],
                        "text": "Systems like CERMINE (Tkaczyk et al., 2015) and OCR++ (Singh et al., 2016) extract the raw or processed markup from \u201cborn-digital\u201d PDFs (e.g. using tools like pdf2xml) and apply a variety of text processing methods to deduce docu-\nment structure and apply semantic labels to text blocks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 13
                            }
                        ],
                        "text": "By contrast, CERMINE averaged 9.4 seconds per article on the same set of articles, on the same machine."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17892962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "256043cc6aa645cb67a9fa11d8258c07be39c3a5",
            "isKey": true,
            "numCitedBy": 143,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "CERMINE is a comprehensive open-source system for extracting structured metadata from scientific articles in a born-digital form. The system is based on a modular workflow, whose loosely coupled architecture allows for individual component evaluation and adjustment, enables effortless improvements and replacements of independent parts of the algorithm and facilitates future architecture expanding. The implementations of most steps are based on supervised and unsupervised machine learning techniques, which simplifies the procedure of adapting the system to new document layouts and styles. The evaluation of the extraction workflow carried out with the use of a large dataset showed good performance for most metadata types, with the average F score of 77.5\u00a0%. CERMINE system is available under an open-source licence and can be accessed at http://cermine.ceon.pl. In this paper, we outline the overall workflow architecture and provide details about individual steps implementations. We also thoroughly compare CERMINE to similar solutions, describe evaluation methodology and finally report its results."
            },
            "slug": "CERMINE:-automatic-extraction-of-structured-from-Tkaczyk-Szostek",
            "title": {
                "fragments": [],
                "text": "CERMINE: automatic extraction of structured metadata from scientific literature"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The overall workflow architecture of CERMINE is outlined, details about individual steps implementations are provided and the evaluation of the extraction workflow carried out with the use of a large dataset showed good performance for most metadata types."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition (IJDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695024"
                        ],
                        "name": "Dominika Tkaczyk",
                        "slug": "Dominika-Tkaczyk",
                        "structuredName": {
                            "firstName": "Dominika",
                            "lastName": "Tkaczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dominika Tkaczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2670951"
                        ],
                        "name": "P. Szostek",
                        "slug": "P.-Szostek",
                        "structuredName": {
                            "firstName": "Pawel",
                            "lastName": "Szostek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Szostek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721046"
                        ],
                        "name": "Lukasz Bolikowski",
                        "slug": "Lukasz-Bolikowski",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Bolikowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Bolikowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 51
                            }
                        ],
                        "text": "Training the model with the full GROTOAP2 dataset (Tkaczyk et al., 2014), for example, yielded a best overall detection performance of 5.1% mean average precision (mAP) over all 22 labels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 50
                            }
                        ],
                        "text": "Training the model with the full GROTOAP2 dataset (Tkaczyk et al., 2014), for example, yielded a best overall detection performance of 5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 35081909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d538dc4cba5fae4049f75885316b1aeb9324bb5",
            "isKey": true,
            "numCitedBy": 20,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientific literature analysis improves knowledge propagation and plays a key role in understanding and assessment of scholarly communication in scientific world. In recent years many tools and services for analysing the content of scientific articles have been developed. One of the most important tasks in this research area is understanding the roles of different parts of the document. It is impossible to build effective solutions for problems related to document fragments classification and evaluate their performance without a reliable test set, that contains both input documents and the expected results of classification. In this paper we present GROTOAP2 \ufffd a large dataset of ground truth files containing labelled fragments of scientific articles in PDF format, useful for training and evaluation of document content analysis-related solutions. GROTOAP2 was successfully used for training CERMINE \ufffd our system for extracting metadata and content from scientific articles. The dataset is based on articles from PubMed Central Open Access Subset. GROTOAP2 is published under Open Access license. The semi-automatic method used to construct GROTOAP2 is scalable and can be adjusted for building large datasets from other data sources. The article presents the content of GROTOAP2, describes the entire creation process and reports the evaluation methodology and results."
            },
            "slug": "GROTOAP2-The-Methodology-of-Creating-a-Large-Ground-Tkaczyk-Szostek",
            "title": {
                "fragments": [],
                "text": "GROTOAP2 - The Methodology of Creating a Large Ground Truth Dataset of Scientific Articles"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A large dataset of ground truth files containing labelled fragments of scientific articles in PDF format, useful for training and evaluation of document content analysis-related solutions and was successfully used for training CERMINE, the authors' system for extracting metadata and content from scientific articles."
            },
            "venue": {
                "fragments": [],
                "text": "D Lib Mag."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724421"
                        ],
                        "name": "Deng Cai",
                        "slug": "Deng-Cai",
                        "structuredName": {
                            "firstName": "Deng",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deng Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911562"
                        ],
                        "name": "Shipeng Yu",
                        "slug": "Shipeng-Yu",
                        "structuredName": {
                            "firstName": "Shipeng",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shipeng Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259699"
                        ],
                        "name": "Ji-Rong Wen",
                        "slug": "Ji-Rong-Wen",
                        "structuredName": {
                            "firstName": "Ji-Rong",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji-Rong Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712167"
                        ],
                        "name": "Wei-Ying Ma",
                        "slug": "Wei-Ying-Ma",
                        "structuredName": {
                            "firstName": "Wei-Ying",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Ying Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 52
                            }
                        ],
                        "text": "Analogous approaches for web pages \u2013 such as those by Cai et al. (2003) and Song et al. (2004) \u2013 also examine the document source (in this case the page DOM) to build representations for further processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1442273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93c2712a8d21b69cab7db3b9337d32e5f118a130",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A new web content structure analysis based on visual representation is proposed in this paper. Many web applications such as information retrieval, information extraction and automatic page adaptation can benefit from this structure. This paper presents an automatic top-down, tag-tree independent approach to detect web content structure. It simulates how a user understands web layout structure based on his visual perception. Comparing to other existing techniques such as DOM tree, our approach is independent to the HTML documentation representation. Our method can work well even when the HTML structure is quite different from the visual layout structure. Several experiments show the effectiveness of our method."
            },
            "slug": "VIPS:-a-Vision-based-Page-Segmentation-Algorithm-Cai-Yu",
            "title": {
                "fragments": [],
                "text": "VIPS: a Vision-based Page Segmentation Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An automatic top-down, tag-tree independent approach to detect web content structure that simulates how a user understands web layout structure based on his visual perception."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072578366"
                        ],
                        "name": "T. A. Tran",
                        "slug": "T.-A.-Tran",
                        "structuredName": {
                            "firstName": "Tuan",
                            "lastName": "Tran",
                            "middleNames": [
                                "Anh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. A. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31725329"
                        ],
                        "name": "In Seop Na",
                        "slug": "In-Seop-Na",
                        "structuredName": {
                            "firstName": "In",
                            "lastName": "Na",
                            "middleNames": [
                                "Seop"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "In Seop Na"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2355626"
                        ],
                        "name": "Soohyung Kim",
                        "slug": "Soohyung-Kim",
                        "structuredName": {
                            "firstName": "Soohyung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soohyung Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 111
                            }
                        ],
                        "text": "Alternatively, visual-based techniques for document layout analysis have tended to focus on text segmentation (Tran et al., 2015), (Chen et al., 2015a), (Chen et al., 2015b), (Garz et al., 2016), especially for historical documents; Eskenazi et al. (2017) survey dozens of such methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 110
                            }
                        ],
                        "text": "Alternatively, visual-based techniques for document layout analysis have tended to focus on text segmentation (Tran et al., 2015), (Chen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27306191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "401ced23c0d24e6cb2e93c9e5a08ef4ad022440d",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A separation of text and non-text elements plays an important role in document layout analysis. A number of approaches have been proposed but the quality of separation result is still limited due to the complex of the document layout. In this paper, we present an efficient method for the classification of text and non-text components in document image. It is the combination of whitespace analysis with multi-layer homogeneous regions which called recursive filter. Firstly, the input binary document is analyzed by connected components analysis and whitespace extraction. Secondly, a heuristic filter is applied to identify non-text components. After that, using statistical method, we implement the recursive filter on multi-layer homogeneous regions to identify all text and non-text elements of the binary image. Finally, all regions will be reshaped and remove noise to get the text document and non-text document. Experimental results on the ICDAR2009 page segmentation competition dataset and other datasets prove the effectiveness and superiority of proposed method."
            },
            "slug": "Separation-of-Text-and-Non-text-in-Document-Layout-Tran-Na",
            "title": {
                "fragments": [],
                "text": "Separation of Text and Non-text in Document Layout Analysis using a Recursive Filter"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An efficient method for the classification of text and non-text components in document image using a combination of whitespace analysis with multi-layer homogeneous regions which is called recursive filter."
            },
            "venue": {
                "fragments": [],
                "text": "KSII Trans. Internet Inf. Syst."
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192178"
                        ],
                        "name": "Olga Russakovsky",
                        "slug": "Olga-Russakovsky",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Russakovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Russakovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285165"
                        ],
                        "name": "J. Krause",
                        "slug": "J.-Krause",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145031342"
                        ],
                        "name": "S. Satheesh",
                        "slug": "S.-Satheesh",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Satheesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satheesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145423516"
                        ],
                        "name": "S. Ma",
                        "slug": "S.-Ma",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3109481"
                        ],
                        "name": "Zhiheng Huang",
                        "slug": "Zhiheng-Huang",
                        "structuredName": {
                            "firstName": "Zhiheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 113
                            }
                        ],
                        "text": "and tested on the remaining 222 in 5 randomized sessions, using a ResNet-101 base network pretrained on ImageNet (Russakovsky et al., 2015), with a batch size of 8, Adam optimizer (Kingma and Ba, 2014), and a starting learning rate of 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 148
                            }
                        ],
                        "text": "\u2026GPU for 30 epochs on 600 images, and tested on the remaining 222 in 5 randomized sessions, using a ResNet-101 base network pretrained on ImageNet (Russakovsky et al., 2015), with a batch size of 8, Adam optimizer (Kingma and Ba, 2014), and a starting learning rate of 0.0001, with decay of 0.1\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 186
                            }
                        ],
                        "text": "The model was trained using a single\nNVIDIA P100 GPU for 30 epochs on 600 images, and tested on the remaining 222 in 5 randomized sessions, using a ResNet-101 base network pretrained on ImageNet (Russakovsky et al., 2015), with a batch size of 8, Adam optimizer (Kingma and Ba, 2014), and a starting learning rate of 0.0001, with decay of 0.1 every 5 epochs."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2930547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "isKey": true,
            "numCitedBy": 25491,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5\u00a0years of the challenge, and propose future directions and improvements."
            },
            "slug": "ImageNet-Large-Scale-Visual-Recognition-Challenge-Russakovsky-Deng",
            "title": {
                "fragments": [],
                "text": "ImageNet Large Scale Visual Recognition Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The creation of this benchmark dataset and the advances in object recognition that have been possible as a result are described, and the state-of-the-art computer vision accuracy with human accuracy is compared."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 138
                            }
                        ],
                        "text": "Algorithmically, Faster R-CNN relies on several component neural networks, beginning with a deep convolutional neural network \u2013 ResNet-101 (He et al., 2016), specifically \u2013 to perform feature extraction on the input image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 86
                            }
                        ],
                        "text": "nent neural networks, beginning with a deep convolutional neural network \u2013 ResNet-101 (He et al., 2016), specifically \u2013 to perform feature extraction on the input image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 148
                            }
                        ],
                        "text": "The model was trained using a single\nNVIDIA P100 GPU for 30 epochs on 600 images, and tested on the remaining 222 in 5 randomized sessions, using a ResNet-101 base network pretrained on ImageNet (Russakovsky et al., 2015), with a batch size of 8, Adam optimizer (Kingma and Ba, 2014), and a starting learning rate of 0.0001, with decay of 0.1 every 5 epochs."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 95318,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 48
                            }
                        ],
                        "text": ", 2015), with a batch size of 8, Adam optimizer (Kingma and Ba, 2014), and a starting learning rate of 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90052,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497777"
                        ],
                        "name": "Joseph Redmon",
                        "slug": "Joseph-Redmon",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Redmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Redmon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 109
                            }
                        ],
                        "text": "Another important consideration is the existence of newer, even more successful detection models like YOLOv3 (Redmon and Farhadi, 2018) and RetinaNet (Lin et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 176
                            }
                        ],
                        "text": "Figure 4 shows per-class performance over 30 training epochs, as well as comparative performance against the baseline Faster R-CNN model and reference model implementations of YOLOv3 (Linder-Noren, 2018) and RetinaNet (Henon, 2018)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 102
                            }
                        ],
                        "text": "Another important consideration is the existence of newer, even more successful detection models like YOLOv3 (Redmon and Farhadi, 2018) and RetinaNet (Lin et al., 2017), which use a single-stage detection paradigm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 58
                            }
                        ],
                        "text": "Most models plateaued early on this small dataset, except YOLOv3 which peaking at 68.9% after 49 epochs (beyond the figure bounds, but still below our model\u2019s results)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4714433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4845fb1e624965d4f036d7fd32e8dcdd2408148",
            "isKey": true,
            "numCitedBy": 8616,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL"
            },
            "slug": "YOLOv3:-An-Incremental-Improvement-Redmon-Farhadi",
            "title": {
                "fragments": [],
                "text": "YOLOv3: An Incremental Improvement"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2724875"
                        ],
                        "name": "Roya Rastan",
                        "slug": "Roya-Rastan",
                        "structuredName": {
                            "firstName": "Roya",
                            "lastName": "Rastan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roya Rastan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33548540"
                        ],
                        "name": "H. Paik",
                        "slug": "H.-Paik",
                        "structuredName": {
                            "firstName": "Hye-young",
                            "lastName": "Paik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Paik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145534891"
                        ],
                        "name": "J. Shepherd",
                        "slug": "J.-Shepherd",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shepherd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shepherd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 182
                            }
                        ],
                        "text": "Visual region detection can serve as a precursor to numerous existing information extraction techniques or adaptations of them, including parsing of reference (Lammey, 2015), tables (Rastan et al., 2015), and equations (Smithies et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 183
                            }
                        ],
                        "text": "Visual region detection can serve as a precursor to numerous existing information extraction techniques or adaptations of them, including parsing of reference (Lammey, 2015), tables (Rastan et al., 2015), and equations (Smithies et al., 2001), as well as data extraction from figures (Tummers, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 292476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8cd18e63e99fe91dd4d24c3ec9fcb9af0497901f",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a precise, comprehensive model of table processing which aims to remedy some of the problems in the discussion of table processing in the literature. The model targets application-independent, end-to-end table processing, and thus encompasses a large subset of the work in the area. The model can be used to aid the design of table processing systems (We provide an example of such a system), can be considered as a reference framework for evaluating the performance of table processing systems, and can assist in clarifying terminological differences in the table processing literature."
            },
            "slug": "TEXUS:-A-Task-based-Approach-for-Table-Extraction-Rastan-Paik",
            "title": {
                "fragments": [],
                "text": "TEXUS: A Task-based Approach for Table Extraction and Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The model targets application-independent, end-to-end table processing, and thus encompasses a large subset of the work in the area, and can be used to aid the design of table processing systems and assist in clarifying terminological differences in the table processing literature."
            },
            "venue": {
                "fragments": [],
                "text": "DocEng"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103989073"
                        ],
                        "name": "Steve Smithies",
                        "slug": "Steve-Smithies",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Smithies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steve Smithies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2169548"
                        ],
                        "name": "K. Novins",
                        "slug": "K.-Novins",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Novins",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Novins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739434"
                        ],
                        "name": "J. Arvo",
                        "slug": "J.-Arvo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Arvo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Arvo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 23
                            }
                        ],
                        "text": ", 2015), and equations (Smithies et al., 2001), as well as data extraction from figures (Tummers, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 220
                            }
                        ],
                        "text": "Visual region detection can serve as a precursor to numerous existing information extraction techniques or adaptations of them, including parsing of reference (Lammey, 2015), tables (Rastan et al., 2015), and equations (Smithies et al., 2001), as well as data extraction from figures (Tummers, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16853059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac09e48a4ff3b87e8273be8044f446d87d1282c1",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a system for freehand entry and editing of mathematical expressions using a pen and tablet. The expressions are entered in the same way that they would be written on paper. The system interprets the results and generates output in a form suitable for use in other applications, such as word processors or symbolic manipulators. Interpretation includes character segmentation, character recognition, and formula parsing. Our interface incorporates easy to use tools for correcting interpretation errors at any stage. The user can also edit the handwritten representation and ask the system to reinterpret the results. By recovering the formula's structure directly from its handwritten form, the user is free to use common conventions of mathematical notation without regard to internal representation. We report the results of a small user study, which indicate that the new style of interaction is effective."
            },
            "slug": "Equation-entry-and-editing-via-handwriting-and-Smithies-Novins",
            "title": {
                "fragments": [],
                "text": "Equation entry and editing via handwriting and gesture recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By recovering the formula's structure directly from its handwritten form, the user is free to use common conventions of mathematical notation without regard to internal representation."
            },
            "venue": {
                "fragments": [],
                "text": "Behav. Inf. Technol."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3090510"
                        ],
                        "name": "Rachael Lammey",
                        "slug": "Rachael-Lammey",
                        "structuredName": {
                            "firstName": "Rachael",
                            "lastName": "Lammey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rachael Lammey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 159
                            }
                        ],
                        "text": "Visual region detection can serve as a precursor to numerous existing information extraction techniques or adaptations of them, including parsing of reference (Lammey, 2015), tables (Rastan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60670143,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c489eacda3b85e8f2063fab35f84590fb77d113",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "CrossRef (http://www.crossref.org/) is an association of scholarly publishers that develops shared infrastructure to support more effective scholarly communications. In May 2014, CrossRef launched CrossRef Text and Data Mining Services for its members. This article covers the thinking behind CrossRef launching this service, and the particular problems it aims to address around the collection of full\u2010text content for the purposes of text and data mining (TDM). It explains the technical aspects of the service for researchers and lets publishers know what they need to provide to CrossRef in order to participate and how to do so. It will also describe the pilot of CrossRef's TDM Services, and information on publisher uptake since the launch and how this can be measured, and the costs for joining or using the service."
            },
            "slug": "CrossRef's-Text-and-Data-Mining-Services-Lammey",
            "title": {
                "fragments": [],
                "text": "CrossRef text and data mining services"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The thinking behind CrossRef launching this service, and the particular problems it aims to address around the collection of full\u2010text content for the purposes of text and data mining (TDM), are covered."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 49
                            }
                        ],
                        "text": ", 2001), as well as data extraction from figures (Tummers, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "DataThief III: A program to extract (reverse engineer) data points from a graph"
            },
            "venue": {
                "fragments": [],
                "text": "https://datathief.org, accessed: 2019-05-01."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 218
                            }
                        ],
                        "text": "Figure 4 shows per-class performance over 30 training epochs, as well as comparative performance against the baseline Faster R-CNN model and reference model implementations of YOLOv3 (Linder-Noren, 2018) and RetinaNet (Henon, 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 140
                            }
                        ],
                        "text": "Another important consideration is the existence of newer, even more successful detection models like YOLOv3 (Redmon and Farhadi, 2018) and RetinaNet (Lin et al., 2017), which use a single-stage detection paradigm."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pytorch implementation of retinanet object detection"
            },
            "venue": {
                "fragments": [],
                "text": "https://github.com/yhenon/pytorch-retinanet."
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 220
                            }
                        ],
                        "text": "This approach is advantageous to general purpose object detection, because objects may be located anywhere in a photo or video., and has been highly successful even when objects are cropped or occluded by other objects (Lin et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A minimal pytorch implementation of yolov3"
            },
            "venue": {
                "fragments": [],
                "text": "European conference on computer vision"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 130
                            }
                        ],
                        "text": "Using the novel labeled dataset described in Section 4, a baseline model was trained using a standard Faster R-CNN implementation (Yang et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A faster pytorch implementation of faster r-cnn"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 11
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 28,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Visual-Detection-with-Context-for-Document-Layout-Soto-Yoo/e0211c1a138724238359bcd3e0f3012e3a49845b?sort=total-citations"
}