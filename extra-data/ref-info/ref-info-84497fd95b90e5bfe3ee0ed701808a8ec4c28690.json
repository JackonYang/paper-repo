{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140335"
                        ],
                        "name": "A. Shashua",
                        "slug": "A.-Shashua",
                        "structuredName": {
                            "firstName": "Amnon",
                            "lastName": "Shashua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shashua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "A preliminary version of this work has appeared as a conference paper(Wolf and Shashua, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14334740,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "95fcb70d663adca1855c71bdd57e5cfd48419cd2",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning with instances defined over a space of sets of vectors. We derive a new positive definite kernel f(A, B) defined over pairs of matrices A, B based on the concept of principal angles between two linear subspaces. We show that the principal angles can be recovered using only inner-products between pairs of column vectors of the input matrices thereby allowing the original column vectors of A, B to be mapped onto arbitrarily high-dimensional feature spaces. We apply this technique to inference over image sequences applications of face recognition and irregular motion trajectory detection."
            },
            "slug": "Kernel-principal-angles-for-classification-machines-Wolf-Shashua",
            "title": {
                "fragments": [],
                "text": "Kernel principal angles for classification machines with applications to image sequence interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A new positive definite kernel f(A, B) defined over pairs of matrices A, B is derived based on the concept of principal angles between two linear subspaces and it is shown that the principal angles can be recovered using only inner-products between pairs of column vectors of the input matrices thereby allowing the original column vectors to be mapped onto arbitrarily high-dimensional feature spaces."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490189"
                        ],
                        "name": "Gregory Shakhnarovich",
                        "slug": "Gregory-Shakhnarovich",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Shakhnarovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Shakhnarovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31496901"
                        ],
                        "name": "John W. Fisher III",
                        "slug": "John-W.-Fisher-III",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Fisher III",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John W. Fisher III"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 55
                            }
                        ],
                        "text": "The conventional approach to representing a signal for classification tasks \u2014 be it a 2D image, a string of characters or any 1D signal \u2014 is to form a one-dimensional attribute vectorxi in some spaceRn defined as the instance space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 96
                            }
                        ],
                        "text": "The last method we compared to was the method based on Kullback-Leibler divergence presented in Shakhnarovich et al. (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 36
                            }
                        ],
                        "text": "Another recent approach proposed by Shakhnarovich et al. (2002) to match two image sequences is to compute the covariance matrices of the two input sets and use theKullback-Leibler divergence metric (algebraically speaking, a function ofAA\u22a4,BB\u22a4 assuming zero mean column spaces) assuming the input\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Kernel Machines, Large margin classifiers, Canonical Correlation Analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 164
                            }
                        ],
                        "text": "\u2026other hand, contemporary face tracking systems can provide long sequences of images of a person, thus for better recognition performance it has been argued (e.g., Shakhnarovich et al., 2002, Yamaguchi et al., 1998) that the information from all images should be used in the classification process."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15164089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31cc777d48b61d1186bae221f1f7a3321441a741",
            "isKey": true,
            "numCitedBy": 317,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of face recognition from a large set of images obtained over time - a task arising in many surveillance and authentication applications. A set or a sequence of images provides information about the variability in the appearance of the face which can be used for more robust recognition. We discuss different approaches to the use of this information, and show that when cast as a statistical hypothesis testing problem, the classification task leads naturally to an information-theoretic algorithm that classifies sets of images using the relative entropy (Kullback-Leibler divergence) between the estimated density of the input set and that of stored collections of images for each class. We demonstrate the performance of the proposed algorithm on two medium-sized data sets of approximately frontal face images, and describe an application of the method as part of a view-independent recognition system."
            },
            "slug": "Face-Recognition-from-Long-Term-Observations-Shakhnarovich-Fisher",
            "title": {
                "fragments": [],
                "text": "Face Recognition from Long-Term Observations"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work addresses the problem of face recognition from a large set of images obtained over time - a task arising in many surveillance and authentication applications and proposes an information-theoretic algorithm that classifies sets of images using the relative entropy between the estimated density of the input set and that of stored collections of images for each class."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Kernel Machines, Large margin classifiers, Canonical Correlation Analysis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 246
                            }
                        ],
                        "text": "It has been obsrved that an effective way to make a classifier invariant is to generate synthetic training examples by transfo ming them according to the desired invariances (see Baird, 1990, DeCoste and Scho\u0308lkopf, 2002, Poggio and Vetter, 1992, Simard et al., 1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2184474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff32cebbdb8a436ccd8ae797647428615ae32d74",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images (translations, rotations, scale changes, etcetera). \n \nWe have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform."
            },
            "slug": "Tangent-Prop-A-Formalism-for-Specifying-Selected-in-Simard-Victorri",
            "title": {
                "fragments": [],
                "text": "Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A scheme is implemented that allows a network to learn the derivative of its outputs with respect to distortion operators of their choosing, which not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations the authors wish the network to perform."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 42
                            }
                        ],
                        "text": "We used the Support Vector Machine (SVM) (Boser et al., 1992, Vapnik, 1998) algorithm for our classification engine."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "Existing kernel algorithms like the Support Vector Machine (SVM) and \u201ckernelPCA\u201d (to mention a few) rely on the use of a positive definite kernel to replacthe inner-products between the input vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "\u201cF\u201d means that theSVM classifier failed to converge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "We use the SVM approach usingf (A,B) where an input matrix represents the motion trajectory of a group of indiviuals over a certain (fixed) time frame."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "\u201cF\u201d means that the SVM classifier failed to converge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SVM results on the vector representation are displayed in the last three columns of the table."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "The input to the SVM algorithm is a \u201cmeasurement\u201d matrixM whose entries Mi j = yiy j f (Ai ,A j) and the output is a set of \u201csupport vectors\u201d which consist of the subset of instances which lie on the margin of the positive and negative examples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SVM was given a training set of inputma rices A1, ...,Al with labelsy1, ...,yl whereyi = \u00b11, where the columns of a matrixAi represent the trajectories of the i\u2019th \u201cinstance\u201d example."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SVM was re-run over the three kernel functions (linear, polynomial, RBF) where instead of using our proposed kernel function over sets we represented the set of vectors as a single concatenated vector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "Note that it is crucial that our measuref () is a positive definite kernel because otherwise we could not have plugged it in the SVM."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": true,
            "numCitedBy": 10843,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8231010"
                        ],
                        "name": "M. Kuss",
                        "slug": "M.-Kuss",
                        "structuredName": {
                            "firstName": "Malte",
                            "lastName": "Kuss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kuss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 224
                            }
                        ],
                        "text": "The fact that only input space dimensionRn is used constrains the applicability of the technique to relatively small input sets, and the assumption of a Gaussian distribution limits the kind of variability alongthe input sequence which can be effectively tolerated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "The standard convention of Canonical Correlation Analysis (CCA) is to treat he row vectors of the matricesA,B as feature vectors \u2014 for example, a row vector ofA would represent the measurements of an object, and the corresponding row vector ofB would represent the class affiliation of the object."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 252
                            }
                        ],
                        "text": "\u2026numerical instability problem have be n suggested such as performing a Kernel Principal Component analysis (KPCA) on the Gram matricesA\u22a4A andB\u22a4B or using a regularization approach by adding small multiples of the identity matrices to the Gram matrices (Kuss and Graepel, 2003, Gestel et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 50
                            }
                        ],
                        "text": "With this convention, the kernel versions of CCA (Kuss and Graepel, 2003, Melzer et al., 2001, Gestel et al., 2001, Bach and Jordan, 2002) therefore map the rows ofA andB using \u03c6(\u00b7) whereas in our work we map thecolumns."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 20
                            }
                        ],
                        "text": "Ways around this numerical instability problem have be n suggested such as performing a Kernel Principal Component analysis (KPCA) on the Gram matricesA\u22a4A andB\u22a4B or using a regularization approach by adding small multiples of the identity matrices to the Gram matrices (Kuss and Graepel, 2003,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 86
                            }
                        ],
                        "text": "As a result this formulation is widely used for obtaining Kernel versions of CCA (see Kuss and Graepel, 2003, Melzer et al., 2001, Gestel et al., 2001, Bach and Jordan, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6840581,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "40f5d93bf10c285ec45d928d61d4f3c029bfbbfe",
            "isKey": true,
            "numCitedBy": 105,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Canonical correlation analysis (CCA) is a classical multivariate method concerned with describing linear dependencies between sets of variables. After a short exposition of the linear sample CCA problem and its analytical solution, the article proceeds with a detailed characterization of its geometry. Projection operators are used to illustrate the relations between canonical vectors and variates. The article then addresses the problem of CCA between spaces spanned by objects mapped into kernel feature spaces. An exact solution for this kernel canonical correlation (KCCA) problem is derived from a geometric point of view. It shows that the expansion coefficients of the canonical vectors in their respective feature space can be found by linear CCA in the basis induced by kernel principal component analysis. The effect of mappings into higher dimensional feature spaces is considered critically since it simplifies the CCA problem in general. Then two regularized variants of KCCA are discussed. Relations to other methods are illustrated, e.g., multicategory kernel Fisher discriminant analysis, kernel principal component regression and possible applications thereof in blind source separation."
            },
            "slug": "The-Geometry-Of-Kernel-Canonical-Correlation-Kuss",
            "title": {
                "fragments": [],
                "text": "The Geometry Of Kernel Canonical Correlation Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738575"
                        ],
                        "name": "T. V. Gestel",
                        "slug": "T.-V.-Gestel",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Gestel",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. V. Gestel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744439"
                        ],
                        "name": "J. Suykens",
                        "slug": "J.-Suykens",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Suykens",
                            "middleNames": [
                                "A.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Suykens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145355963"
                        ],
                        "name": "J. D. Brabanter",
                        "slug": "J.-D.-Brabanter",
                        "structuredName": {
                            "firstName": "Jos",
                            "lastName": "Brabanter",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Brabanter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750713"
                        ],
                        "name": "B. Moor",
                        "slug": "B.-Moor",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Moor",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704135"
                        ],
                        "name": "J. Vandewalle",
                        "slug": "J.-Vandewalle",
                        "structuredName": {
                            "firstName": "Joos",
                            "lastName": "Vandewalle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vandewalle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 276
                            }
                        ],
                        "text": "\u2026numerical instability problem have be n suggested such as performing a Kernel Principal Component analysis (KPCA) on the Gram matricesA\u22a4A andB\u22a4B or using a regularization approach by adding small multiples of the identity matrices to the Gram matrices (Kuss and Graepel, 2003, Gestel et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 95
                            }
                        ],
                        "text": "With this convention, the kernel versions of CCA (Kuss and Graepel, 2003, Melzer et al., 2001, Gestel et al., 2001, Bach and Jordan, 2002) therefore map the rows ofA andB using \u03c6(\u00b7) whereas in our work we map thecolumns."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 3
                            }
                        ],
                        "text": "Other ideas published in the context of matching image sequences are farther away from the concepts we propose in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 44
                            }
                        ],
                        "text": "Ways around this numerical instability problem have be n suggested such as performing a Kernel Principal Component analysis (KPCA) on the Gram matricesA\u22a4A andB\u22a4B or using a regularization approach by adding small multiples of the identity matrices to the Gram matrices (Kuss and Graepel, 2003,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 131
                            }
                        ],
                        "text": "As a result this formulation is widely used for obtaining Kernel versions of CCA (see Kuss and Graepel, 2003, Melzer et al., 2001, Gestel et al., 2001, Bach and Jordan, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33829764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbf59bfc45ca789d7fb729b12220c8f6718e808f",
            "isKey": true,
            "numCitedBy": 43,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A key idea of nonlinear Support Vector Machines (SVMs) is to map the inputs in a nonlinear way to a high dimensional feature space, while Mercer's condition is applied in order to avoid an explicit expression for the nonlinear mapping. In SVMs for nonlinear classification a large margin classifier is constructed in the feature space. For regression a linear regressor is constructed in the feature space. Other kernel extensions of linear algorithms have been proposed like kernel Principal Component Analysis (PCA) and kernel Fisher Discriminant Analysis. In this paper, we discuss the extension of linear Canonical Correlation Analysis (CCA) to a kernel CCA with application of the Mercer condition. We also discuss links with single output Least Squares SVM (LS-SVM) Regression and Classification."
            },
            "slug": "Kernel-Canonical-Correlation-Analysis-and-Least-Gestel-Suykens",
            "title": {
                "fragments": [],
                "text": "Kernel Canonical Correlation Analysis and Least Squares Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper discusses the extension of linear Canonical Correlation Analysis (CCA) to a kernel CCA with application of the Mercer condition and discusses links with single output Least Squares SVM (LS-SVM) Regression and Classification."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 116
                            }
                        ],
                        "text": "With this convention, the kernel versions of CCA (Kuss and Graepel, 2003, Melzer et al., 2001, Gestel et al., 2001, Bach and Jordan, 2002) therefore map the rows ofA andB using \u03c6(\u00b7) whereas in our work we map thecolumns."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 24
                            }
                        ],
                        "text": "Other ideas published in the context of matching image sequences are farther away from the concepts we propose in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 152
                            }
                        ],
                        "text": "As a result this formulation is widely used for obtaining Kernel versions of CCA (see Kuss and Graepel, 2003, Melzer et al., 2001, Gestel et al., 2001, Bach and Jordan, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7691428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d4f4601940d5b13455541a643a39538bb54b6f3",
            "isKey": false,
            "numCitedBy": 1015,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms."
            },
            "slug": "Kernel-independent-component-analysis-Bach-Jordan",
            "title": {
                "fragments": [],
                "text": "Kernel independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A class of algorithms for independent component analysis which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space is presented, showing that these algorithms outperform many of the presently known algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2610790"
                        ],
                        "name": "C. MacInnes",
                        "slug": "C.-MacInnes",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "MacInnes",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. MacInnes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 76
                            }
                        ],
                        "text": "Proof: This is a result related to a theorem by (Brualdi et al., 1995) and (MacInnes, 1999) which follows directly from the Binet-Cauchy theorem, as follows: LetA= QARA andB= QBRB represent the QR factorization of both matrices."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33143830,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7e1812d6ae3d15f022c264b6dc31e6e652e3efeb",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for finding the best approximation of a matrix A by a full rank Hankel matrix is given. The initial problem of best approximation of one matrix by another is transformed to a problem involving best approximation of a given vector by a second vector whose elements are constrained so that its inverse image is a Hankel matrix. The map from a matrix to a vector is the invertible map between a subspace represented as the row space of the matrix A and the Grassman vector representing that subspace. The relation between the principle angles associated with a pair of subspaces and the angle between the Grassman vectors associated with the subspaces is established."
            },
            "slug": "The-Solution-to-a-Structured-Matrix-Approximation-MacInnes",
            "title": {
                "fragments": [],
                "text": "The Solution to a Structured Matrix Approximation Problem Using GrassmanCoordinates"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The map from a matrix to a vector is the invertible map between a subspace represented as the row space of the matrix A and the Grassman vector representing that subspace."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Matrix Anal. Appl."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72818387"
                        ],
                        "name": "Zoran Biuk",
                        "slug": "Zoran-Biuk",
                        "structuredName": {
                            "firstName": "Zoran",
                            "lastName": "Biuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoran Biuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2350869"
                        ],
                        "name": "S. Lon\u010dari\u0107",
                        "slug": "S.-Lon\u010dari\u0107",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Lon\u010dari\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lon\u010dari\u0107"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 120
                            }
                        ],
                        "text": "Most of those ideas are related to capturing \u201cdynamics\u201d and \u201ctemporal signatures\u201d (Edwards et al., 1999, Gong et al., 1994, Biuk and Loncaric, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Face recognition has been traditionally posed as the problem of identifying a face from a single image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9556383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc86de4b93975c8d1b5f959844df8bad3fa97dc4",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel approach to face recognition based on a multi-pose image sequence is presented in this paper. In this approach, faces are represented by their pattern vectors (projections to eigenfaces) in eigenspace. Instead of recognising a face from a single view, a sequence of images showing face movement (from left to the right profile) is used for recognition. Pattern vectors corresponding to multiple poses build a trajectory in eigenspace where each trajectory belongs to one face sequence (profile to profile). In the training phase, sequences of poses construct prototype trajectories, and in recognition phase, an unknown face trajectory is compared with prototypes. New matching models are presented and analysed as well as the influence of some parameters on the recognition ratio."
            },
            "slug": "Face-recognition-from-multi-pose-image-sequence-Biuk-Lon\u010dari\u0107",
            "title": {
                "fragments": [],
                "text": "Face recognition from multi-pose image sequence"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A novel approach to face recognition based on a multi-pose image sequence is presented, where instead of recognising a face from a single view, a sequence of images showing face movement is used for recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ISPA 2001. Proceedings of the 2nd International Symposium on Image and Signal Processing and Analysis. In conjunction with 23rd International Conference on Information Technology Interfaces (IEEE Cat."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2639990"
                        ],
                        "name": "A. Psarrou",
                        "slug": "A.-Psarrou",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Psarrou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Psarrou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153631058"
                        ],
                        "name": "I. Katsoulis",
                        "slug": "I.-Katsoulis",
                        "structuredName": {
                            "firstName": "Ilias",
                            "lastName": "Katsoulis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Katsoulis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67334160"
                        ],
                        "name": "P. Palavouzis",
                        "slug": "P.-Palavouzis",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Palavouzis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Palavouzis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 101
                            }
                        ],
                        "text": "Most of those ideas are related to capturing \u201cdynamics\u201d and \u201ctemporal signatures\u201d (Edwards et al., 1999, Gong et al., 1994, Biuk and Loncaric, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 80
                            }
                        ],
                        "text": "Face recognition has been traditionally posed as the problem of identifying a face from a single image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15205944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd43dcf6d7e0bac14fff77ff44d97d980a22763f",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we address the issue of encoding and recognition of face sequences that arise from continuous head movement. We detect and track a moving head before segmenting face images from on-line camera inputs. We measure temporal changes in the pattern vectors of eigenface projections of successive image frames of a face sequence and introduce the concept of \u201ctemporal signature\u201d of a face class. We exploit two different supervised learning algorithms with feedforward and partially recurrent neural networks to learn possible temporal signatures. We discuss our experimental results and draw conclusions."
            },
            "slug": "Tracking-and-Recognition-of-Face-Sequences-Gong-Psarrou",
            "title": {
                "fragments": [],
                "text": "Tracking and Recognition of Face Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work detects and track a moving head before segmenting face images from on-line camera inputs and measures temporal changes in the pattern vectors of eigenface projections of successive image frames of a face sequence and introduces the concept of \u201ctemporal signature\u201d of aFace class."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3893740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82962da5c273a9e6627a040d56c8a7973fe22440",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this note we discuss how recognition can be achieved from a single 2D model view exploiting prior knowledge of an object''s structure (e.g. symmetry). We prove that for any bilaterally symmetric 3D object one non- accidental 2D model view is sufficient for recognition. Symmetries of higher order allow the recovery of structure from one 2D view. Linear transformations can be learned exactly from a small set of examples in the case of \"linear object classes\" and used to produce new views of an object from a single view."
            },
            "slug": "Recognition-and-Structure-from-one-2D-Model-View:-Poggio-Vetter",
            "title": {
                "fragments": [],
                "text": "Recognition and Structure from one 2D Model View: Observations on Prototypes, Object Classes and Symmetries"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proved that for any bilaterally symmetric 3D object one non- accidental 2D model view is sufficient for recognition and linear transformations can be learned exactly from a small set of examples in the case of \"linear object classes\"."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143842629"
                        ],
                        "name": "T. Melzer",
                        "slug": "T.-Melzer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Melzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Melzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060020641"
                        ],
                        "name": "M. Reiter",
                        "slug": "M.-Reiter",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Reiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144746444"
                        ],
                        "name": "H. Bischof",
                        "slug": "H.-Bischof",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bischof",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bischof"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 74
                            }
                        ],
                        "text": "With this convention, the kernel versions of CCA (Kuss and Graepel, 2003, Melzer et al., 2001, Gestel et al., 2001, Bach and Jordan, 2002) therefore map the rows ofA andB using \u03c6(\u00b7) whereas in our work we map thecolumns."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 248
                            }
                        ],
                        "text": "The fact that only input space dimensionRn is used constrains the applicability of the technique to relatively small input sets, and the assumption of a Gaussian distribution limits the kind of variability alongthe input sequence which can be effectively tolerated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 110
                            }
                        ],
                        "text": "As a result this formulation is widely used for obtaining Kernel versions of CCA (see Kuss and Graepel, 2003, Melzer et al., 2001, Gestel et al., 2001, Bach and Jordan, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33570889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "260e6ff494c26202bcd2929de40cf7d1c732b4f9",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new non-linear feature extraction technique based on Canonical Correlation Analysis (CCA) with applications in regression and object recognition. The non-linear transformation of the input data is performed using kernel-methods. Although, in this respect, our approach is similar to other generalized linear methods like kernel-PCA, our method is especially well suited for relating two sets of measurements. The benefits of our method compared to standard feature extraction methods based on PCA will be illustrated with several experiments from the field of object recognition and pose estimation."
            },
            "slug": "Nonlinear-Feature-Extraction-Using-Generalized-Melzer-Reiter",
            "title": {
                "fragments": [],
                "text": "Nonlinear Feature Extraction Using Generalized Canonical Correlation Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new non-linear feature extraction technique based on Canonical Correlation Analysis (CCA), which is especially well suited for relating two sets of measurements, with applications in regression and object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706942"
                        ],
                        "name": "R. Brualdi",
                        "slug": "R.-Brualdi",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Brualdi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brualdi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219302"
                        ],
                        "name": "S. Friedland",
                        "slug": "S.-Friedland",
                        "structuredName": {
                            "firstName": "Shmuel",
                            "lastName": "Friedland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Friedland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753507"
                        ],
                        "name": "A. Pothen",
                        "slug": "A.-Pothen",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pothen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pothen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 49
                            }
                        ],
                        "text": "Proof: This is a result related to a theorem by (Brualdi et al., 1995) and (MacInnes, 1999) which follows directly from the Binet-Cauchy theorem, as follows: LetA= QARA andB= QBRB represent the QR factorization of both matrices."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17556824,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6b11abfaa13126fa4c979f5edb186938b5d94b7e",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Let $A$ be a $k \\by n$ underdetermined matrix. The sparse basis problem for the row space $W$ of $A$ is to find a basis of $W$ with the fewest number of nonzeros. Suppose that all the entries of $A$ are nonzero, and that they are algebraically independent over the rational number field. Then every nonzero vector in $W$ has at least $n-k+1$ nonzero entries. Those vectors in $W$ with exactly $n-k+1$ nonzero entries are the elementary vectors of $W$. A simple combinatorial condition that is both necessary and sufficient for a set of $k$ elementary vectors of $W$ to form a basis of $W$ is presented here. A similar result holds for the null space of $A$ where the elementary vectors now have exactly $k+1$ nonzero entries. These results follow from a theorem about nonzero minors of order $m$ of the $(m-1)$st compound of an $m \\by n$ matrix with algebraically independent entries, which is proved using multilinear algebra techniques. This combinatorial condition for linear independence is a first step towards the design of algorithms that compute sparse bases for the row and null space without imposing artificial structure constraints to ensure linear independence."
            },
            "slug": "The-Sparse-Basis-Problem-and-Multilinear-Algebra-Brualdi-Friedland",
            "title": {
                "fragments": [],
                "text": "The Sparse Basis Problem and Multilinear Algebra"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple combinatorial condition for linear independence is a first step towards the design of algorithms that compute sparse bases for the row and null space without imposing artificial structure constraints to ensure linear independence."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Matrix Anal. Appl."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703049"
                        ],
                        "name": "D. DeCoste",
                        "slug": "D.-DeCoste",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "DeCoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeCoste"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 261
                            }
                        ],
                        "text": "In the context of \u201ckernel jittering\u201d, where synthetic copies of the input vectors are created in order to simulate invariances of interest, the matching measurement over sets (or over a vector against a set of vectors) is based on the nearest neighbor principle (DeCoste and Sch\u0308olkopf, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 43
                            }
                        ],
                        "text": "Finally, in the \u201ckernel jittering\u201d approach (DeCoste and Scho\u0308lkopf, 2002) for obtaining invariances over a class of transformations, two instance vectorsxi andx j are matched by creating additional synthetic examplesxip andx jq centered around the original input instances and selecting k(x\u2032,x\u2032\u2032)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 37
                            }
                        ],
                        "text": "For instance the \u201ckernel jittering\u201d of DeCoste and Sch\u0308olkopf (2002) performs the synthetic transformations within the matching process between pairs of training examples, thereby effectively matching between two sets of vectors (orbetween a vector and a set)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Kernel Machines, Large margin classifiers, Canonical Correlation Analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 192
                            }
                        ],
                        "text": "It has been obsrved that an effective way to make a classifier invariant is to generate synthetic training examples by transfo ming them according to the desired invariances (see Baird, 1990, DeCoste and Scho\u0308lkopf, 2002, Poggio and Vetter, 1992, Simard et al., 1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 85843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd14aa8bef5afc7d248a04de681242f2e64c6a1e",
            "isKey": true,
            "numCitedBy": 594,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods."
            },
            "slug": "Training-Invariant-Support-Vector-Machines-DeCoste-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Training Invariant Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work reports the recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47652868"
                        ],
                        "name": "H. Hotelling",
                        "slug": "H.-Hotelling",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Hotelling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hotelling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 207
                            }
                        ],
                        "text": "\u2026\u2264 \u03b8k \u2264 (\u03c0/2)\nbetween the two subspaces are uniquely defined as:\ncos(\u03b8k) = max u\u2208UA max v\u2208UB u\u22a4v (1)\nsubject to:\nu\u22a4u = v\u22a4v = 1, u\u22a4ui = 0,v\u22a4vi = 0, i = 1, ...,k\u22121\nThe concept of principal angles is due to Jordan in 1875, where Hotelling(1936) is the first to introduce the recursive definition above."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "With this convention, the kernel versions of CCA (Kuss and Graepel, 2003, Melzer et al., 2001, Gestel et al., 2001, Bach and Jordan, 2002) therefore map the rows ofA andB using \u03c6(\u00b7) whereas in our work we map thecolumns."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 217
                            }
                        ],
                        "text": "The principal angles 0\u2264 \u03b81 \u2264 ... \u2264 \u03b8k \u2264 (\u03c0/2)\nbetween the two subspaces are uniquely defined as:\ncos(\u03b8k) = max u\u2208UA max v\u2208UB u\u22a4v (1)\nsubject to:\nu\u22a4u = v\u22a4v = 1, u\u22a4ui = 0,v\u22a4vi = 0, i = 1, ...,k\u22121\nThe concept of principal angles is due to Jordan in 1875, where Hotelling(1936) is the first to introduce the recursive definition above."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "The first approach is based on the Lagrange formulation (Hotelling, 1936) where the principal angles are thegen ralized eigenvalues of an expanded 2k\u00d72k matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 29
                            }
                        ],
                        "text": "The original formulation by (Hotelling, 1936) was based on Lagrange multipliers g nerating the principal angles as the set of generalized eigenvalues of a block diagonal matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 161
                            }
                        ],
                        "text": "As a result this formulation is widely used for obtaining Kernel versions of CCA (see Kuss and Graepel, 2003, Melzer et al., 2001, Gestel et al., 2001, Bach and Jordan, 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122166830,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "45db76270416a42517a21c63a77e9c4260fa979a",
            "isKey": true,
            "numCitedBy": 5597,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Concepts of correlation and regression may be applied not only to ordinary one-dimensional variates but also to variates of two or more dimensions. Marksmen side by side firing simultaneous shots at targets, so that the deviations are in part due to independent individual errors and in part to common causes such as wind, provide a familiar introduction to the theory of correlation; but only the correlation of the horizontal components is ordinarily discussed, whereas the complex consisting of horizontal and vertical deviations may be even more interesting. The wind at two places may be compared, using both components of the velocity in each place. A fluctuating vector is thus matched at each moment with another fluctuating vector. The study of individual differences in mental and physical traits calls for a detailed study of the relations between sets of correlated variates. For example the scores on a number of mental tests may be compared with physical measurements on the same persons. The questions then arise of determining the number and nature of the independent relations of mind and body shown by these data to exist, and of extracting from the multiplicity of correlations in the system suitable characterizations of these independent relations. As another example, the inheritance of intelligence in rats might be studied by applying not one but s different mental tests to N mothers and to a daughter of each"
            },
            "slug": "Relations-Between-Two-Sets-of-Variates-Hotelling",
            "title": {
                "fragments": [],
                "text": "Relations Between Two Sets of Variates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1936
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50564384"
                        ],
                        "name": "G. Edwards",
                        "slug": "G.-Edwards",
                        "structuredName": {
                            "firstName": "Gareth",
                            "lastName": "Edwards",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Edwards"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "Most of those ideas are related to capturing \u201cdynamics\u201d and \u201ctemporal signatures\u201d (Edwards et al., 1999, Gong et al., 1994, Biuk and Loncaric, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 58
                            }
                        ],
                        "text": "Face recognition has been traditionally posed as the problem of identifying a face from a single image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15d467137dea2b21388c6b9646b76f45b34fcc23",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a quantitative evaluation of an algorithm for model-based face recognition. The algorithm actively learns how individual faces vary through video sequences, providing on-line suppression of confounding factors such as expression, lighting and pose. By actively decoupling sources of image variation, the algorithm provides a framework in which identity evidence can be integrated over a sequence. We demonstrate that face recognition can be considerably improved by the analysis of video sequences. The method presented is widely applicable in many multi-class interpretation problems."
            },
            "slug": "Improving-identification-performance-by-integrating-Edwards-Taylor",
            "title": {
                "fragments": [],
                "text": "Improving identification performance by integrating evidence from sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that face recognition can be considerably improved by the analysis of video sequences, and the method presented is widely applicable in many multi-class interpretation problems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144141891"
                        ],
                        "name": "Osamu Yamaguchi",
                        "slug": "Osamu-Yamaguchi",
                        "structuredName": {
                            "firstName": "Osamu",
                            "lastName": "Yamaguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Osamu Yamaguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770128"
                        ],
                        "name": "K. Fukui",
                        "slug": "K.-Fukui",
                        "structuredName": {
                            "firstName": "Kazuhiro",
                            "lastName": "Fukui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072417013"
                        ],
                        "name": "Ken-ichi Maeda",
                        "slug": "Ken-ichi-Maeda",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Maeda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Maeda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Kernel Machines, Large margin classifiers, Canonical Correlation Analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 97
                            }
                        ],
                        "text": "The idea of using principal angles as a measure for matching two image sequenc s was proposed by Yamaguchi et al. (1998) with dissimilarity between the two subspaces measured by the smallest principal angle \u2014 thereby effectively measuring whether the subspacesinter ectwhich is somewhat similar to a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 192
                            }
                        ],
                        "text": "\u2026other hand, contemporary face tracking systems can provide long sequences of images of a person, thus for better recognition performance it has been argued (e.g., Shakhnarovich et al., 2002, Yamaguchi et al., 1998) that the information from all images should be used in the classification process."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 97
                            }
                        ],
                        "text": "We consider the task of obtaining a similarity function which operates on pairs of setsof vectors \u2014 where a vector can represent an image and a set of vectors could represent a video sequence for example \u2014 in such a way that the function can be plugged into a variety of existing classification\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17342425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab157ed033d25355870943d38475f6be18ca9ce6",
            "isKey": true,
            "numCitedBy": 486,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a face recognition method using image sequence. As input we utilize plural face images rather than a \"single-shot\", so that the input reflects variation of facial expression and face direction. For the identification of the face, we essentially form a subspace with the image sequence and apply the Mutual Subspace Method in which the similarity is defined by the angle between the subspace of input and those of references. We demonstrate the effectiveness of the proposed method through several experimental results."
            },
            "slug": "Face-recognition-using-temporal-image-sequence-Yamaguchi-Fukui",
            "title": {
                "fragments": [],
                "text": "Face recognition using temporal image sequence"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A face recognition method using image sequence that essentially form a subspace with the image sequence and applies the Mutual Subspace Method in which the similarity is defined by the angle between the subspace of input and those of references."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: Kernel Machines, Large margin classifiers, Canonical Correlation Analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 179
                            }
                        ],
                        "text": "It has been obsrved that an effective way to make a classifier invariant is to generate synthetic training examples by transfo ming them according to the desired invariances (see Baird, 1990, DeCoste and Scho\u0308lkopf, 2002, Poggio and Vetter, 1992, Simard et al., 1992)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61076153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0eb7232dc0ceae32aad26c918a24e2775020d46",
            "isKey": false,
            "numCitedBy": 339,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A lack of explicit quantitative models of imaging defects due to printing, optics, and digitization has retarded progress in some areas of document image analysis, including syntactic and structural approaches. Establishing the essential properties of such models, such as completeness (expressive power) and calibration (closeness of fit to actual image populations) remain open research problems. Work-in-progress towards a parameterized model of local imaging defects is described, together with a variety of motivating theoretical arguments and empirical evidence. A pseudo-random image generator implementing the model has been built. Applications of the generator are described, including a polyfont classifier for ASCII and a single-font classifier for a large alphabet (Tibetan U-Chen), both of which which were constructed with a minimum of manual effort. Image defect models and their associated generators permit a new kind of image database which is explicitly parameterized and indefinitely extensible, alleviating some drawbacks of existing databases."
            },
            "slug": "Document-image-defect-models-Baird",
            "title": {
                "fragments": [],
                "text": "Document image defect models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Work-in-progress towards a parameterized model of local imaging defects is described, together with a variety of motivating theoretical arguments and empirical evidence, and a pseudo-random image generator implementing the model has been built."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52047522"
                        ],
                        "name": "P. Graefe",
                        "slug": "P.-Graefe",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Graefe",
                            "middleNames": [
                                "W.",
                                "U."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Graefe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 281
                            }
                        ],
                        "text": "\u2026especially in the context of computer visionapplications, but\n(a)\n(b)\n(c)\nApplications of principal angles are found in numerous branches of science including data analysis (Gittins, 1985), random processes (Kailath, 1974, Gelfand and Yaglom, 1959) and stochastic processes (c.f. Caines, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116701780,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9d41f5df35c45bdff0180b40fef372fe34eeafbc",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This Report presents an investigation of the behaviour of randomly excited linear-dynamical systems whose parameters are subject to either incremental Brownian or Gaussian, 'physical white noise' variations. The Fokker-Planck equation governing the first probability density of the response is derived, and from it the equations that determine the response moments are obtained. Expressions are found for the correIation functions and spectral densities of the response. Various examples are given to illustrate the methods used, and results from an analog computer simulation are shown to agree c10sely with theoretical results."
            },
            "slug": "Linear-stochastic-systems-Graefe",
            "title": {
                "fragments": [],
                "text": "Linear stochastic systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145026287"
                        ],
                        "name": "R. Gittins",
                        "slug": "R.-Gittins",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gittins",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gittins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 175
                            }
                        ],
                        "text": "\u2026especially in the context of computer visionapplications, but\n(a)\n(b)\n(c)\nApplications of principal angles are found in numerous branches of science including data analysis (Gittins, 1985), random processes (Kailath, 1974, Gelfand and Yaglom, 1959) and stochastic processes (c.f. Caines, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123004967,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "74aa92085e7563fc63f7be4da54e63f237798233",
            "isKey": false,
            "numCitedBy": 337,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction.- 1.1 The study of relationships.- 1.2 Objectives.- 1.3 Canonical analysis: overview.- I. Theory.- 2. Canonical correlations and canonical variates.- 2.1 Introduction.- 2.2 Formulation.- 2.3 Derivation of canonical correlation coefficients and canonical variates.- 2.3.1 Eigenanalysis.- 2.3.2 Singular value decompositon.- 2.3.3 Other derivations.- 2.3.4 Concluding remarks.- 2.4 Properties of canonical correlation coefficients, weights and variates.- 2.4.1 Properties of canonical correlation coefficients.- 2.4.2 Properties of canonical weights.- 2.4.3 Properties of canonical variates.- 2.5 Computation.- 2.5.1 Numerical methods.- 2.5.2 Further remarks.- 3. Extensions and generalizations.- 3.1 Introduction.- 3.2 Further interpretive devices.- 3.2.1 Correlations between canonical variates and the original variables.- 3.2.2 Variance extracted by a canonical variate.- 3.2.3 Redundancy.- 3.2.4 Total redundancy.- 3.2.5 Variable communalities.- 3.2.6 Concluding remarks.- 3.3 Extensions and generalizations.- 3.3.1 Redundancy analysis: an alternative to canonical analysis.- 3.3.2 Improving the interpretability of canonical weights.- 3.3.3 Rotation of canonical variates.- 3.3.4 Validation.- 3.3.5 Predicting a criterion of maximum utility.- 3.3.6 Generalizations of canonical analysis.- 3.3.7 Concluding remarks.- 3.4 Hypothesis testing.- 3.4.1 Independence.- 3.4.2 Dimensionality.- 3.4.3 The contribution of particular variables.- 3.4.4 Hypothesis tests for nonnormal data.- 3.4.5 Residuals from a fitted model.- 4. Canonical variate analysis.- 4.1 Introduction.- 4.2 Binary-valued dummy variables.- 4.3 Formulation and derivation.- 4.3.1 Point conceptualizations of NXp and NZq.- 4.3.2 Derivation.- 4.4 Further aspects of canonical variate analysis.- 4.5 Hypothesis testing.- 4.5.1 Equality of g vector-means.- 4.5.2 Dimensionality.- 4.6 Affinities with other methods.- 4.6.1 Canonical variate analysis, multivariate analysis of variance and multiple discriminant analysis.- 4.6.2 Canonical variate analysis and principal component analysis.- 4.7 Imposition of structure.- 4.7.1 Designed comparisons.- 4.7.2 Separating the sources of variation.- 4.7.3 Further comments.- 4.8 Concluding remarks.- 5. Dual scaling.- 5.1 Introduction.- 5.2 Formulation and derivation.- 5.2.1 Maximizing the correlation between rows and columns.- 5.2.2 Maximizing the separation between rows and columns.- 5.3 Further aspects of dual scaling.- 5.4 Hypothesis testing.- 5.4.1 Independence.- 5.4.2 Dimensionality.- 5.5 Affinities with other methods.- 5.5.1 Dual scaling and the analysis of contingency tables.- 5.5.2 Dual scaling, correspondence analysis and principal component analysis.- 5.6 Relationships among statistical methods.- 5.7 Concluding remarks.- II. Applications.- General introduction.- 6. Experiment 1: an investigation of spatial variation.- 6.1 Introduction.- 6.2 Results.- 6.2.1 The canonical correlation coefficients.- 6.2.2 Independence.- 6.2.3 Dimensionality.- 6.2.4 The canonical variates.- 6.2.5 Variable communalities.- 6.3 Conclusions.- 7. Experiment 2: soil-species relationships in a limestone grassland community.- 7.1 Introduction.- 7.2 Results.- 7.2.1 The canonical correlation coefficients.- 7.2.2 Independence.- 7.2.3 Dimensionality.- 7.2.4 The canonical variates.- 7.2.5 Variable communalities.- 7.3 Conclusions.- 8. Soil-vegetation relationships in a lowland tropical rain forest.- 8.1 Introduction.- 8.2 Results.- 8.2.1 The canonical correlation coefficients.- 8.2.2 Independence.- 8.2.3 Dimensionality.- 8.2.4 The canonical variates.- 8.2.5 Variable communalities.- 8.3 Ecological assessment of the results.- 8.4 Conclusions.- 9. Dynamic status of a lowland tropical rain forest.- 9.1 Introduction.- 9.2 Results.- 9.2.1 The canonical correlation coefficients.- 9.2.2 Independence.- 9.2.3 Dimensionality.- 9.2.4 The canonical variates.- 9.2.5 Variable communalities.- 9.3 Ecological assessment of the results.- 9.4 Conclusions.- 10. The structure of grassland vegetation in Anglesey, North Wales.- 10.1 Introduction.- 10.2 Results.- 10.2.1 The canonical correlation coefficients.- 10.2.2 Equality of community centroids.- 10.2.3 Collinearity.- 10.2.4 The canonical variates.- 10.2.5 Variable communalities.- 10.3 Ecological assessment of the results.- 10.4 Conclusions.- 11. The nitrogen nutrition of eight grass species.- 11.1 Introduction.- 11.2 Multivariate analysis of variance.- 11.2.1 Results.- 11.2.2 Designed comparisons.- 11.3 Canonical variate analysis.- 11.3.1 Results.- 11.4 Relationships between multivariate analysis of variance, discriminant analysis and canonical variate analysis.- 11.5 Ecological assessment of the results.- 11.6 Conclusions.- 12. Herbivore-environment relationships in the Rwenzori National Park, Uganda.- 12.1 Introduction.- 12.2 Contingency table analysis.- 12.2.1 Results.- 12.3 Dual scaling.- 12.3.1 Results.- 12.4 Relationships between contingency table analysis and dual scaling.- 12.5 Ecological assessment of the results.- 12.6 Conclusions.- III. Appraisal and Prospect.- 13. Applications: assessment and conclusions.- 13.1 Introduction.- 13.2 Assessment.- 13.3 Conclusions.- 14. Research issues and future developments.- 14.1 Introduction.- 14.2 Data collection.- 14.2.1 Choice of variables.- 14.2.2 Experimental design.- 14.3 Initial data exploration.- 14.3.1 Homogeneity.- 14.3.2 Assessing joint distribution.- 14.3.3 Re-expressing variables.- 14.4 Potential data problems.- 14.4.1 Outlying or influential observations.- 14.4.2 Long-tailed distributions.- 14.4.3 Collinearity.- 14.5 Statistical assessment.- 14.5.1 Residuals in canonical analysis.- 14.5.2 Does the model fit?.- 14.5.3 Stability of results.- 14.5.4 Miscellanea.- 14.6 Concluding remarks.- Appendices.- A.1 Multivariate regression.- A.2 Data sets used in worked applications.- A.3 Species composition of a limestone grassland community.- References.- Species' index.- Author index."
            },
            "slug": "Canonical-Analysis:-A-Review-with-Applications-in-Gittins",
            "title": {
                "fragments": [],
                "text": "Canonical Analysis: A Review with Applications in Ecology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720911"
                        ],
                        "name": "T. Kailath",
                        "slug": "T.-Kailath",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kailath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kailath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 209
                            }
                        ],
                        "text": "\u2026especially in the context of computer visionapplications, but\n(a)\n(b)\n(c)\nApplications of principal angles are found in numerous branches of science including data analysis (Gittins, 1985), random processes (Kailath, 1974, Gelfand and Yaglom, 1959) and stochastic processes (c.f. Caines, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14837516,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "0379b3842e4d504fe0cc927bbaed67ccb64ef61f",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 572,
            "paperAbstract": {
                "fragments": [],
                "text": "Developments in the theory of linear least-squares estimation in the last thirty years or so are outlined. Particular attention is paid to early mathematica[ work in the field and to more modern developments showing some of the many connections between least-squares filtering and other fields."
            },
            "slug": "A-view-of-three-decades-of-linear-filtering-theory-Kailath",
            "title": {
                "fragments": [],
                "text": "A view of three decades of linear filtering theory"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Developments in the theory of linear least-squares estimation in the last thirty years or so are outlined and particular attention is paid to early mathematica[ work in the field and to more modern developments showing some of the many connections between least-Squares filtering and other fields."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 99
                            }
                        ],
                        "text": "Neverth l ss, the product of two positive definite kernels is also a positive definite kernel (see Scho\u0308lkopf and Smola (2002), for example), then\nf (A,B) = det(Q\u22a4A QB) 2 = \u03a0ki=1cos(\u03b8i) 2\nis ourchosen positive definite kernel function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29871328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5051890e501117097eeffbd8ded87694f0d8063",
            "isKey": false,
            "numCitedBy": 6578,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."
            },
            "slug": "Learning-with-kernels-Smola",
            "title": {
                "fragments": [],
                "text": "Learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This book is intended to be a guide to the art of self-consistency and should not be used as a substitute for a comprehensive guide to self-confidence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38757,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40603356"
                        ],
                        "name": "A. C. Aitken",
                        "slug": "A.-C.-Aitken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aitken",
                            "middleNames": [
                                "Craig"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. C. Aitken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122196483,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "983668f3b2aad2821bea5260082724bf795d4a21",
            "isKey": false,
            "numCitedBy": 384,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "$$\\left[ {\\begin{array}{*{20}{c}} {10} \\\\ { - v{{a}^{{ - 1}}}1} \\\\ \\end{array} } \\right]\\left[ {\\begin{array}{*{20}{c}} {au} \\\\ {vb} \\\\ \\end{array} } \\right]\\left[ {\\begin{array}{*{20}{c}} {1 - {{a}^{{ - 1}}}u} \\\\ {01} \\\\ \\end{array} } \\right] = \\left[ {\\begin{array}{*{20}{c}} {a0} \\\\ {0b - v{{a}^{{ - 1}}}u} \\\\ \\end{array} } \\right]$$ \n \n(1.1) \n \nprovided a is regular."
            },
            "slug": "Determinants-and-matrices-Aitken",
            "title": {
                "fragments": [],
                "text": "Determinants and matrices"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1939
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077007866"
                        ],
                        "name": "Aitken. A.c",
                        "slug": "Aitken.-A.c",
                        "structuredName": {
                            "firstName": "Aitken.",
                            "lastName": "A.c",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aitken. A.c"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222413585,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "25469c0084d028ddd6aef41bb9fe1bdc7444226f",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Determinants-And-Matrices-A.c",
            "title": {
                "fragments": [],
                "text": "Determinants And Matrices"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1944
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 62
                            }
                        ],
                        "text": "We used the Support Vector Machine (SVM) (Boser et al., 1992, Vapnik, 1998) algorithm for our classification engine."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59752996,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5451278e1a11cf3f1be28a05f38d36c8641e68f7",
            "isKey": false,
            "numCitedBy": 4580,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Nature-of-Statistical-Learning-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 224
                            }
                        ],
                        "text": "\u2026especially in the context of computer visionapplications, but\n(a)\n(b)\n(c)\nApplications of principal angles are found in numerous branches of science including data analysis (Gittins, 1985), random processes (Kailath, 1974, Gelfand and Yaglom, 1959) and stochastic processes (c.f. Caines, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Calculation of the amount of information about a random function contained in another such function"
            },
            "venue": {
                "fragments": [],
                "text": "Calculation of the amount of information about a random function contained in another such function"
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Calculation of the amount of information about a random function contained in another such function"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kernel canonical correlation analysis and least squares support vector achines Cannonical analysis : A review with applications in ecology"
            },
            "venue": {
                "fragments": [],
                "text": "Amer . Math . Soc . Translations , series"
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 121
                            }
                        ],
                        "text": "The result we will need for the remainder of this section is the Binet-Cauchy theorem on the productf compound matrices (Aitken, 1946, pp.93) attributed by Binet and Cauchy in 1812 \u2014 describednext."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Determinants and Matrices. nterscience Publishers Inc"
            },
            "venue": {
                "fragments": [],
                "text": "Determinants and Matrices. nterscience Publishers Inc"
            },
            "year": 1946
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Yaglom. Calculation of the amount of information about a random function contained in another such function"
            },
            "venue": {
                "fragments": [],
                "text": "Amer. Math. Soc. Translations,"
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 175
                            }
                        ],
                        "text": "\u2026especially in the context of computer visionapplications, but\n(a)\n(b)\n(c)\nApplications of principal angles are found in numerous branches of science including data analysis (Gittins, 1985), random processes (Kailath, 1974, Gelfand and Yaglom, 1959) and stochastic processes (c.f. Caines, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 99
                            }
                        ],
                        "text": "Applications of principal angles are found in numerous branches of science including data analysis (Gittins, 1985), random processes (Kailath, 1974, Gelfand and Yaglom, 1959) and stochastic processes (c."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cannonical analysis: A review with applications"
            },
            "venue": {
                "fragments": [],
                "text": "in ecology. Biomathematics,"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eigen faces for recognition"
            },
            "venue": {
                "fragments": [],
                "text": "J. of Cognitive Neuroscience"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 13
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-over-Sets-using-Kernel-Principal-Angles-Wolf-Shashua/84497fd95b90e5bfe3ee0ed701808a8ec4c28690?sort=total-citations"
}