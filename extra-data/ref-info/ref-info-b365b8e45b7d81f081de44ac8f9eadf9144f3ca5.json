{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 68
                            }
                        ],
                        "text": "One\napplication is to tree-based models, as reported in LeBlanc and Tibshirani (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 32
                            }
                        ],
                        "text": "Results reported in LeBlanc and Tibshirani (1994) suggest that the shrinkage procedure gives more accurate trees than pruning, while still producing interpretable subtrees."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 104
                            }
                        ],
                        "text": "APPLICATION TO GENERALIZED REGRESSION MODELS The lasso can be applied to many other models: for example Tibshirani (1994)\ndescribed an application to the proportional hazards model."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14959891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9c6bf49b8e287569b335081a6f51765a1e5f783",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new method for variable selection and estimation in Cox's proportional hazards model. Our proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant. Because of the nature of this constraint it tends to produce some coeecients that are exactly zero and hence gives interpretable models. The method is a variation of the \\lasso\" proposal of Tibshirani (1994), designed for the linear regression context. Simulations indicate that the lasso can be more accurate than stepwise selection in this setting."
            },
            "slug": "A-proposal-for-variable-selection-in-the-Cox-model-Tibshirani",
            "title": {
                "fragments": [],
                "text": "A proposal for variable selection in the Cox model"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Simulations indicate that the lasso can be more accurate than stepwise selection in this setting and give interpretable models in Cox's proportional hazards model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2347035"
                        ],
                        "name": "E. George",
                        "slug": "E.-George",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "George",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. George"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2840298"
                        ],
                        "name": "R. McCulloch",
                        "slug": "R.-McCulloch",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McCulloch",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McCulloch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 143
                            }
                        ],
                        "text": "(8)\nWe estimate the prediction error for the lasso procedure by fivefold crossvalidation as described (for example) in chapter 17 of Efron and Tibshirani (1993)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15250963,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b7134309fc923d0395edb6c6c7d6695c82f053fc",
            "isKey": false,
            "numCitedBy": 2571,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability\u2014the promising ones\u2014can then be identified by their more frequent appearance in the Gibbs sample."
            },
            "slug": "Variable-selection-via-Gibbs-sampling-George-McCulloch",
            "title": {
                "fragments": [],
                "text": "Variable selection via Gibbs sampling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49286079"
                        ],
                        "name": "Philip C. Spector",
                        "slug": "Philip-C.-Spector",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Spector",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip C. Spector"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4113957,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cae7f8b4ee917793ad310ac571f1ed181f41c7c0",
            "isKey": false,
            "numCitedBy": 672,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary Often, in a regression situation with many variables, a sequence of submodels is generated containing fewer variables by using such methods as stepwise addition or deletion of variables, or 'best subsets'. The question is which of this sequence of submodels is 'best', and how can submodel performance be evaluated. This was explored in Breiman (1988) for a fixed X-design. This is a sequel exploring the case of random X-designs. Analytical results are difficult, if not impossible. This study involved an extensive simulation. The basis of the study is the theoretical definition of prediction error (PE) as the expected squared error produced by applying a prediction equation to the distributional universe of (y, x) values. This definition is used throughout to compare various submodels. There can be startling differences between the x-fixed and x-random situations and different PE estimates are appropriate. Non-resampling estimates such as C,, adjusted RZ,etc. turn out to be highly biased methods for submodel selection. The two best methods are cross-validation and bootstrap. One surprise is that 5 fold cross-validation (leave out 20% of the data) is better at submodel selection and evaluation than leave-one-out cross-validation. There are a number of other surprises."
            },
            "slug": "Submodel-selection-and-evaluation-in-regression.-Breiman-Spector",
            "title": {
                "fragments": [],
                "text": "Submodel selection and evaluation in regression. The X-random case"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49895764"
                        ],
                        "name": "J. Shao",
                        "slug": "J.-Shao",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Shao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16073176,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e0149f4926373368a0d88f7fb57c5549500fd83a",
            "isKey": false,
            "numCitedBy": 1597,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider the problem of selecting a model having the best predictive ability among a class of linear models. The popular leave-one-out cross-validation method, which is asymptotically equivalent to many other model selection methods such as the Akaike information criterion (AIC), the C p , and the bootstrap, is asymptotically inconsistent in the sense that the probability of selecting the model with the best predictive ability does not converge to 1 as the total number of observations n \u2192 \u221e. We show that the inconsistency of the leave-one-out cross-validation can be rectified by using a leave-n v -out cross-validation with n v , the number of observations reserved for validation, satisfying n v /n \u2192 1 as n \u2192 \u221e. This is a somewhat shocking discovery, because nv/n \u2192 1 is totally opposite to the popular leave-one-out recipe in cross-validation. Motivations, justifications, and discussions of some practical aspects of the use of the leave-n v -out cross-validation method are provided, and results ..."
            },
            "slug": "Linear-Model-Selection-by-Cross-validation-Shao",
            "title": {
                "fragments": [],
                "text": "Linear Model Selection by Cross-validation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157209686"
                        ],
                        "name": "Ping Zhang",
                        "slug": "Ping-Zhang",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 37
                            }
                        ],
                        "text": "This latter procedure isdescribed in Zhang (1993) and Shao (1992), and can lead to inconsistent model selection unless the cross-validation test set Tv grows at an appropriate asymptotic rate."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122661960,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4aad883fa1f7c8a55912534804a96c6cc0f39b00",
            "isKey": false,
            "numCitedBy": 499,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In model selection, it is known that the simple leave one out cross validation method is apt to select overfitted models. In an attempt to remedy this problem, we consider two notions of multi-fold cross validation (MCV and MCV*) criteria. In the case of linear regression models, their performance is studied and compared with the simple CV method. As expected, it turns out that MCV indeed reduces the chance of overfitting. The intent ofMCV' is rather different from that of MCV. The differences between these two notions of MCV are also discussed. Our result explains the phenomena observed by Breiman & Spector."
            },
            "slug": "Model-Selection-Via-Multifold-Cross-Validation-Zhang",
            "title": {
                "fragments": [],
                "text": "Model Selection Via Multifold Cross Validation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Two notions of multi-fold cross validation (MCV and MCV*) criteria are considered and it turns out that MCV indeed reduces the chance of overfitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550392"
                        ],
                        "name": "B. Efron",
                        "slug": "B.-Efron",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Efron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Efron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 227312712,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "24f3e29d9feab95e78f3f47364c4edbae34adfd2",
            "isKey": false,
            "numCitedBy": 8062,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the following problem given a random sample X = (X 1, X 2,\u2026, X n) from an unknown probability distribution F, estimate the sampling distribution of some prespecified random variable R(X, F), on the basis of the observed data x. (Standard jackknife theory gives an approximate mean and variance in the case R(X, F) = \\(\\theta \\left( {\\hat F} \\right) - \\theta \\left( F \\right)\\), \u03b8 some parameter of interest.) A general method, called the \u201cbootstrap\u201d, is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc."
            },
            "slug": "Bootstrap-Methods:-Another-Look-at-the-Jackknife-Efron",
            "title": {
                "fragments": [],
                "text": "Bootstrap Methods: Another Look at the Jackknife"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145064923"
                        ],
                        "name": "C. Lawson",
                        "slug": "C.-Lawson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Lawson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lawson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144308260"
                        ],
                        "name": "R. Hanson",
                        "slug": "R.-Hanson",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hanson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hanson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122862057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b35e036e0226d923d03c06ae393084b40c9dec7e",
            "isKey": false,
            "numCitedBy": 4447,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Since the lm function provides a lot of features it is rather complicated. So we are going to instead use the function lsfit as a model. It computes only the coefficient estimates and the residuals. Now would be a good time to read the help file for lsfit. Note that lsfit supports the fitting of multiple least squares models and weighted least squares. Our function will not, hence we can omit the arguments wt, weights and yname. Also, changing tolerances is a little advanced so we will trust the default values and omit the argument tolerance as well."
            },
            "slug": "Solving-least-squares-problems-Lawson-Hanson",
            "title": {
                "fragments": [],
                "text": "Solving least squares problems"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Since the lm function provides a lot of features it is rather complicated so it is going to instead use the function lsfit as a model, which computes only the coefficient estimates and the residuals."
            },
            "venue": {
                "fragments": [],
                "text": "Classics in applied mathematics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 214
                            }
                        ],
                        "text": "\u2026from equation (11) we may derive the formula\nR{f(y)} p - 2 #(j; I7/TI <Y) + E max(I8/TI, Y)2}\nas an approximately unbiased estimate of the risk or mean-square rror E{,3(y) - 3}2, where P8(y) = sign(p,8)(fi/I- y)+ Donoho and Johnstone (1994) gave a similar formula in the function estimation setting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Donoho and Johnstone (1994) proved that the hard threshold (subset selection) estimator f,B = I(,7(1?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 47
                            }
                        ],
                        "text": "This is called a 'soft threshold' estimator by Donoho and Johnstone (1994); they applied this estimator to the coefficients of a wavelet transform of a function measured with noise."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Donoho and Johnstone (1994) gave a similar formula in the function gstimation setting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 81
                            }
                        ],
                        "text": "Interestingly, this has exactly the same form as the soft shrinkage proposals of Donoho and Johnstone (1994) and Donoho et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 81
                            }
                        ],
                        "text": "Interestingly, this has exactly the same form as the soft shrinkage proposals of Donoho and Johnstone (1994) and Donoho et al. (1995), applied to wavelet coefficients in the context of function estimation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 176
                            }
                        ],
                        "text": "\u2026shown to be\npf = sign (1 ) (I6j -j y)I (3)\nwhere y is determined by the condition 2filjl = t. Interestingly, this has exactly the same form as the soft shrinkage proposals of Donoho and Johnstone (1994) and Donoho et al. (1995), applied to wavelet coefficients in the context of function estimation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 319,
                                "start": 81
                            }
                        ],
                        "text": "Interestingly, this has exactly the same form as the soft shrinkage proposals of Donoho and Johnstone (1994) and Donoho et al. (1995), applied to wavelet coefficients in the context of function estimation. The connection between soft shrinkage and a minimum L1-norm penalty was also pointed out by Donoho et al. (1992) for non-negative parameters in the context of signal or image recovery."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 109
                            }
                        ],
                        "text": "Then the lasso\nestimate has the form\n,B = sign(f)(I6 Y-) (13)\nThis is called a 'soft threshold' estimator by Donoho and Johnstone (1994); they applied this estimator to the coefficients of a wavelet transform of a function measured with noise."
                    },
                    "intents": []
                }
            ],
            "corpusId": 239520,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "12e5939a80ec59f81214d7e729c577c350af9501",
            "isKey": true,
            "numCitedBy": 7818,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel, to the unknown function. Estimation with the aid of an oracle offers dramatic advantages over traditional linear estimation by nonadaptive kernels; however, it is a priori unclear whether such performance can be obtained by a procedure relying on the data alone. We describe a new principle for spatially-adaptive estimation: selective wavelet reconstruction. We show that variable-knot spline fits and piecewise-polynomial fits, when equipped with an oracle to select the knots, are not dramatically more powerful than selective wavelet reconstruction with an oracle. We develop a practical spatially adaptive method, RiskShrink, which works by shrinkage of empirical wavelet coefficients. RiskShrink mimics the performance of an oracle for selective wavelet reconstruction as well as it is possible to do so. A new inequality in multivariate normal decision theory which we call the oracle inequality shows that attained performance differs from ideal performance by at most a factor of approximately 2 log n, where n is the sample size. Moreover no estimator can give a better guarantee than this. Within the class of spatially adaptive procedures, RiskShrink is essentially optimal. Relying only on the data, it comes within a factor log 2 n of the performance of piecewise polynomial and variableknot spline methods equipped with an oracle. In contrast, it is unknown how or if piecewise polynomial methods could be made to function this well when denied access to an oracle and forced to rely on data alone."
            },
            "slug": "Ideal-spatial-adaptation-by-wavelet-shrinkage-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Ideal spatial adaptation by wavelet shrinkage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818391"
                        ],
                        "name": "G. Kerkyacharian",
                        "slug": "G.-Kerkyacharian",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Kerkyacharian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kerkyacharian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145897897"
                        ],
                        "name": "D. Picard",
                        "slug": "D.-Picard",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Picard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Picard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12737710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b20b4095236f461ab067d0c23bc9653d3d1f9c0",
            "isKey": false,
            "numCitedBy": 1647,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "Much recent effort has sought asymptotically minimax methods for recovering infinite dimensional objects-curves, densities, spectral densities, images-from noisy data. A now rich and complex body of work develops nearly or exactly minimax estimators for an array of interesting problems. Unfortunately, the results have rarely moved into practice, for a variety of reasons-among them being similarity to known methods, computational intractability and lack of spatial adaptivity. We discuss a method for curve estimation based on n noisy data: translate the empirical wavelet coefficients towards the origin by an amount \u221a(2 log n) /\u221an. The proposal differs from those in current use, is computationally practical and is spatially adaptive; it thus avoids several of the previous objections. Further, the method is nearly minimax both for a wide variety of loss functions-pointwise error, global error measured in L p -norms, pointwise and global error in estimation of derivatives-and for a wide range of smoothness classes, including standard Holder and Sobolev classes, and bounded variation. This is a much broader near optimality than anything previously proposed: we draw loose parallels with near optimality in robustness and also with the broad near eigenfunction properties of wavelets themselves. Finally, the theory underlying the method is interesting, as it exploits a correspondence between statistical questions and questions of optimal recovery and information-based complexity"
            },
            "slug": "Wavelet-Shrinkage:-Asymptopia-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Wavelet Shrinkage: Asymptopia?"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A method for curve estimation based on n noisy data: translate the empirical wavelet coefficients towards the origin by an amount \u221a(2 log n) /\u221an and draw loose parallels with near optimality in robustness and also with the broad near eigenfunction properties of wavelets themselves."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103118511"
                        ],
                        "name": "lldiko E. Frank",
                        "slug": "lldiko-E.-Frank",
                        "structuredName": {
                            "firstName": "lldiko",
                            "lastName": "Frank",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "lldiko E. Frank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 147
                            }
                        ],
                        "text": "This content downloaded on Tue, 12 Mar 2013 11:11:11 AM All use subject to JSTOR Terms and Conditions\n1996] REGRESSION SHRINKAGE AND SELECTION 269\nFrank and Friedman (1993) proposed using a bound on the Lq-norm of the parameters, where q is some number greater than or equal to 0; the lasso\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Frank and Friedman (1993) discuss a generalization fridge regression and subset selection, through the addition of a penalty of the form X YiflIq to the residual sum of squares."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122946044,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "53dc97756369cd1f9300116d6aabdffb7072f2ed",
            "isKey": false,
            "numCitedBy": 2139,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Chemometrics is a field of chemistry that studies the application of statistical methods to chemical data analysis. In addition to borrowing many techniques from the statistics and engineering literatures, chemometrics itself has given rise to several new data-analytical methods. This article examines two methods commonly used in chemometrics for predictive modeling\u2014partial least squares and principal components regression\u2014from a statistical perspective. The goal is to try to understand their apparent successes and in what situations they can be expected to work well and to compare them with other statistical methods intended for those situations. These methods include ordinary least squares, variable subset selection, and ridge regression."
            },
            "slug": "A-Statistical-View-of-Some-Chemometrics-Regression-Frank-Friedman",
            "title": {
                "fragments": [],
                "text": "A Statistical View of Some Chemometrics Regression Tools"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145392702"
                        ],
                        "name": "P. Green",
                        "slug": "P.-Green",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Green",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Green"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17132495,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "961e2156d523e3901c491cc2a1f65764c976fc44",
            "isKey": false,
            "numCitedBy": 5895,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments."
            },
            "slug": "Reversible-jump-Markov-chain-Monte-Carlo-and-model-Green",
            "title": {
                "fragments": [],
                "text": "Reversible jump Markov chain Monte Carlo computation and Bayesian model determination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4348359"
                        ],
                        "name": "J. Hoch",
                        "slug": "J.-Hoch",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Hoch",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hoch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35113244"
                        ],
                        "name": "A. Stern",
                        "slug": "A.-Stern",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Stern",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stern"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "The connection between soft shrinkage and a minimum LI-norm penalty was also pointed out by Donoho et al. (1992) for non-negative parameters in the context of signal or image recovery."
                    },
                    "intents": []
                }
            ],
            "corpusId": 26751866,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "838541412b68548eb98bf05f00293208d4d512c0",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY Maximum entropy (ME) inversion is a non-linear inversion technique for inverse problems where the object to be recovered is known to be positive. It has been applied in areas ranging from radio astronomy to various forms of spectroscopy, sometimes with dramatic success. In some cases, ME has attained an order of magnitude finer resolution and/or an order of magnitude smaller noise level than that obtainable by standard linear methods. The dramatic successes all seem to occur in cases where the object to be recovered is 'nearly black': essentially zero in the vast majority of samples. We show that near-blackness is required, both for signal-to-noise enhancements and for superresolution. However, other methods-in particular, minimum /1-norm reconstruction-may exploit near-blackness to an even greater extent."
            },
            "slug": "Maximum-Entropy-and-the-Nearly-Black-Object-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy and the Nearly Black Object"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378652"
                        ],
                        "name": "R. Olshen",
                        "slug": "R.-Olshen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Olshen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Olshen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103556459"
                        ],
                        "name": "C. J. Stone",
                        "slug": "C.-J.-Stone",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Stone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Rather than prune a large tree as in the classification and regression tree approach of Breiman et al. (1984), we use the lasso idea to shrink it."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29458883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8017699564136f93af21575810d557dba1ee6fc6",
            "isKey": false,
            "numCitedBy": 16308,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Background. Introduction to Tree Classification. Right Sized Trees and Honest Estimates. Splitting Rules. Strengthening and Interpreting. Medical Diagnosis and Prognosis. Mass Spectra Classification. Regression Trees. Bayes Rules and Partitions. Optimal Pruning. Construction of Trees from a Learning Sample. Consistency. Bibliography. Notation Index. Subject Index."
            },
            "slug": "Classification-and-Regression-Trees-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Classification and Regression Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This chapter discusses tree classification in the context of medicine, where right Sized Trees and Honest Estimates are considered and Bayes Rules and Partitions are used as guides to optimal pruning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155551370"
                        ],
                        "name": "Shaobing Chen",
                        "slug": "Shaobing-Chen",
                        "structuredName": {
                            "firstName": "Shaobing",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaobing Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122514140"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Donoho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 11
                            }
                        ],
                        "text": "Donoho and Johnstone (1994) proved that the hard threshold (subset selection) estimator f,B = I(,7(1?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 225
                            }
                        ],
                        "text": "\u2026from equation (11) we may derive the formula\nR{f(y)} p - 2 #(j; I7/TI <Y) + E max(I8/TI, Y)2}\nas an approximately unbiased estimate of the risk or mean-square rror E{,3(y) - 3}2, where P8(y) = sign(p,8)(fi/I- y)+ Donoho and Johnstone (1994) gave a similar formula in the function estimation setting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 187
                            }
                        ],
                        "text": "\u2026shown to be\npf = sign (1 ) (I6j -j y)I (3)\nwhere y is determined by the condition 2filjl = t. Interestingly, this has exactly the same form as the soft shrinkage proposals of Donoho and Johnstone (1994) and Donoho et al. (1995), applied to wavelet coefficients in the context of function estimation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 120
                            }
                        ],
                        "text": "Then the lasso\nestimate has the form\n,B = sign(f)(I6 Y-) (13)\nThis is called a 'soft threshold' estimator by Donoho and Johnstone (1994); they applied this estimator to the coefficients of a wavelet transform of a function measured with noise."
                    },
                    "intents": []
                }
            ],
            "corpusId": 96447294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30f3567eeb13079a1a02ac1342f610cbd95df3bc",
            "isKey": true,
            "numCitedBy": 655,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed an enormous number of over-complete signal dictionaries, wavelets, wavelet packets, cosine packets, Wilson bases, chirplets, warped bases, and hyperbolic cross bases being a few examples. Basis pursuit is a technique for decomposing a signal into an \"optimal\" superposition of dictionary elements. The optimization criterion is the l/sup 1/ norm of coefficients. The method has several advantages over matching pursuit and best ortho basis, including super-resolution and stability.<<ETX>>"
            },
            "slug": "Basis-pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Basis pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Basis pursuit is a technique for decomposing a signal into an \"optimal\" superposition of dictionary elements, which has several advantages over matching pursuit and best ortho basis, including super-resolution and stability."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3548596"
                        ],
                        "name": "T. Stamey",
                        "slug": "T.-Stamey",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Stamey",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Stamey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7797218"
                        ],
                        "name": "J. Kabalin",
                        "slug": "J.-Kabalin",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kabalin",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kabalin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143620798"
                        ],
                        "name": "J. McNeal",
                        "slug": "J.-McNeal",
                        "structuredName": {
                            "firstName": "John E.",
                            "lastName": "McNeal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McNeal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071308568"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8528278"
                        ],
                        "name": "F. Freiha",
                        "slug": "F.-Freiha",
                        "structuredName": {
                            "firstName": "Fuad",
                            "lastName": "Freiha",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Freiha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8100812"
                        ],
                        "name": "E. Redwine",
                        "slug": "E.-Redwine",
                        "structuredName": {
                            "firstName": "Elise",
                            "lastName": "Redwine",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Redwine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145648206"
                        ],
                        "name": "N. Yang",
                        "slug": "N.-Yang",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Yang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 76
                            }
                        ],
                        "text": "EXAMPLE -PROSTATE CANCER DATA\nThe prostate cancer data come from a study by Stamey et al. (1989) that examined the correlation between the level of prostate specific antigen and a number of clinical measures, in men who were about to receive a radical prostatectomy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 23452347,
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "id": "7adbaea0b1987e3f4b8e39929f7057e1f03560d5",
            "isKey": false,
            "numCitedBy": 854,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Prostate-specific-antigen-in-the-diagnosis-and-of-Stamey-Kabalin",
            "title": {
                "fragments": [],
                "text": "Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate. II. Radical prostatectomy treated patients."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of urology"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744288"
                        ],
                        "name": "P. Gill",
                        "slug": "P.-Gill",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Gill",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873253"
                        ],
                        "name": "W. Murray",
                        "slug": "W.-Murray",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685738"
                        ],
                        "name": "M. H. Wright",
                        "slug": "M.-H.-Wright",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "LASSO AS BAYES ESTIMATE The lasso constraint EI6il < t is equivalent to the addition of a penalty term X Ib,I4 to the residual sum of squares (see Murray et al. (1981), chapter 5)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 20611582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8d9abd1c078573188b13d36c1b1efb7cb2fa865",
            "isKey": false,
            "numCitedBy": 7627,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical Optimization MethodsFree eBook: Practical Aspects of Structural Optimization [1701.01450] Practical optimization for hybrid quantum Practical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Acces PDF Practical OptimizationPractical Bayesian Optimization of Machine Learning Particle Swarm Optimization (PSO) An Overview Practical Issues Optimization Algorithms in Physics Practical Mathematical Optimization Universit T BremenA Practical Price Optimization Approach for Omnichannel A Gentle Introduction to Stochastic Optimization AlgorithmsApplied Sciences | Free Full-Text | Evolutionary 0387986316 Practical Optimization Methods: with A Lecture on Model Predictive ControlPractical Optimization : Algorithms and Engineering Wiley Series in Discrete Mathematics and Optimization Ser PRACTICAL OPTIMIZATION uCozEvolutionary practical optimization | DeepDyveA Practical Guide To Hyperparameter Optimization.Blood platelet production: a novel approach for practical [PDF] Practical Bilevel Optimization Download and Read Stability and Sample-based Approximations of Composite Practical portfolio optimization in Python (2/3) machine (PDF) Practical Financial Optimization. Decision making A Multiobjective Optimization Model for Prevention and Particle swarm optimization WikipediaPractical Methods Of Optimization|RPractical Portfolio Optimization London Business SchoolBao: Making Learned Query Optimization PracticalApache Spark Core Practical Optimization DatabricksPractical Methods of Optimization by R. FletcherChapter 11 Nonlinear Optimization Examples4.7 Applied Optimization Problems \u2013 Calculus Volume 1Practical bayesian optimization using Goptuna | by Masashi Practical Optimization Methods For 4th Generation Cellular Facility location problems \u2014 Mathematical Optimization Practical optimization (2004 edition) | Open Library[J726.Ebook] PDF Download Practical Optimization of Multi-objective Exploration for Practical Optimization Practical Optimization: a Gentle Introduction has moved!?Practical Rod Pumping Optimization on Apple Books(PDF) Practical Optimization with MATLAB The Free StudyPractical portfolio optimization in Python (3/3) code (PDF) Practical, Fast and Robust Point Cloud Registration Numerical Optimization Stanford UniversityPractical Optimization Methods with Mathematica ApplicationsPractical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Search Engine Optimization: Practical Marketing TechniquesLagout.orgMeter Placement in Active Distribution System using Manual: Practical guide to optimization for mobiles Unity"
            },
            "slug": "Practical-optimization-Gill-Murray",
            "title": {
                "fragments": [],
                "text": "Practical optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This ebook Practical Optimization by Philip E. Gill is presented in pdf format and the full version of this ebook in DjVu, ePub, doc, txt, PDF forms is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2655181"
                        ],
                        "name": "C. Mallows",
                        "slug": "C.-Mallows",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Mallows",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mallows"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 145
                            }
                        ],
                        "text": "\u2026downloaded on Tue, 12 Mar 2013 11:11:11 AM All use subject to JSTOR Terms and Conditions\n1996] REGRESSION SHRINKAGE AND SELECTION 269\nFrank and Friedman (1993) proposed using a bound on the Lq-norm of the parameters, where q is some number greater than or equal to 0; the lasso corresponds to\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 10
                            }
                        ],
                        "text": "Frank and Friedman (1993) discuss a generalization fridge regression and subset selection, through the addition of a penalty of the form X YiflIq to the residual sum of squares."
                    },
                    "intents": []
                }
            ],
            "corpusId": 124670694,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "393c86ea81bd21951c7ac6f4c4e5d3b66edc9c08",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "[A-Statistical-View-of-Some-Chemometrics-Regression-Hastie-Mallows",
            "title": {
                "fragments": [],
                "text": "[A Statistical View of Some Chemometrics Regression Tools]: Discussion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 94
                            }
                        ],
                        "text": "A different application is to the multivariate adaptive regression splines (MARS) proposal of Friedman (1991)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 67798719,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1bc555309bd1fbce5d487463312bb3a5a7b7a719",
            "isKey": false,
            "numCitedBy": 3734,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multivariate-Adaptive-Regression-Splines-Friedman",
            "title": {
                "fragments": [],
                "text": "Multivariate Adaptive Regression Splines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144294100"
                        ],
                        "name": "C. Stein",
                        "slug": "C.-Stein",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Stein",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121087237,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dc423c0563ac6c80ae8af840c8560438644a0e59",
            "isKey": false,
            "numCitedBy": 2575,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimation-of-the-Mean-of-a-Multivariate-Normal-Stein",
            "title": {
                "fragments": [],
                "text": "Estimation of the Mean of a Multivariate Normal Distribution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 88
                            }
                        ],
                        "text": "Rather than prune a large tree as in the classification a d regression tree approach of Breiman et al. (1984), we use the lasso idea to shrink it."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model selection via multifold cv"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Statist.,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 94
                            }
                        ],
                        "text": "A different application is to the multivariate adaptive regression splines (MARS) proposal of Friedman (1991)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariate adaptive regression splines ( with discussion )"
            },
            "venue": {
                "fragments": [],
                "text": "Ann . Statist ."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "The connection between soft shrinkage and a minimum LI-norm penalty was also pointed out by Donoho et al. (1992) for non-negative parameters in the context of signal or image recovery."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum entropy and the nearly black object ( with discussion )"
            },
            "venue": {
                "fragments": [],
                "text": "J . R . Statist . Soc . B"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 37
                            }
                        ],
                        "text": "This latter procedure isdescribed in Zhang (1993) and Shao (1992), and can lead to inconsistent model selection unless the cross-validation test set Tv grows at an appropriate asymptotic rate."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model selection via multifold cv"
            },
            "venue": {
                "fragments": [],
                "text": "Ann . Statist ."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "The connection between soft shrinkage and a minimum LI-norm penalty was also pointed out by Donoho et al. (1992) for non-negative parameters in the context of signal or image recovery."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum entropy and the nearly"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 66
                            }
                        ],
                        "text": "The motivation for the lasso came from an interesting proposal of Breiman (1993)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Better subset selection using the nonnegative garotte"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 66
                            }
                        ],
                        "text": "The motivation for the lasso came from an interesting proposal of Breiman (1993)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Better subset selection using the non-negative garotte"
            },
            "venue": {
                "fragments": [],
                "text": "Univ. of Cal"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani/b365b8e45b7d81f081de44ac8f9eadf9144f3ca5?sort=total-citations"
}