{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 106
                            }
                        ],
                        "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost (Freund & Schapire, 1996a, 1997) and others in reducing generalization error has not been well understood."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 21
                            }
                        ],
                        "text": "Example 1: Adaboost (Freund & Schapire, 1996a, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "gorithms (arcing) such as Adaboost ( Freund & Schapire, 1996a, 1997 ) and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccf5208521cb8c35f50ee8873df89294b8ed7292",
            "isKey": false,
            "numCitedBy": 13129,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Arc-x4 is a Type II arcing algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 19
                            }
                        ],
                        "text": "Example 2: Arc-x4 (Breiman, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 83
                            }
                        ],
                        "text": "The synthetic data sets, called twonorm, threenorm, and ringnorm, are described in Breiman (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "Relation 2.2 also follows from the duality theorem of linear programming (Breiman, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7732239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d414438926b73bde0313948d8b074cb5360a0e6f",
            "isKey": false,
            "numCitedBy": 637,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. To study this, the concepts of bias and variance of a classifier are defined. Unstable classifiers can have universally low bias. Their problem is high variance. Combining multiple versions is a variance reducing device. One of the most effective is bagging (Breiman [1996a]) Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire [1995,1996] propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym-arcing) so that the weights in the resampling are increased for those cases most often missclassified and the combining is done by weighted voting. Arcing is more sucessful than bagging in variance reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works."
            },
            "slug": "Bias,-Variance-,-And-Arcing-Classifiers-Breiman",
            "title": {
                "fragments": [],
                "text": "Bias, Variance , And Arcing Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work explores two arcing algorithms, compares them to each other and to bagging, and tries to understand how arcing works, which is more sucessful than bagging in variance reduction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 21
                            }
                        ],
                        "text": "Example 1: Adaboost (Freund & Schapire, 1996a, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 42
                            }
                        ],
                        "text": "The classification game was introduced in Freund and Schapire (1996b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 106
                            }
                        ],
                        "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost (Freund & Schapire, 1996a, 1997) and others in reducing generalization error has not been well understood."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Freund and Schapire (1996a) introduced a combination algorithm, called Adaboost, that was designed to drive the training error rapidly to zero."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 148
                            }
                        ],
                        "text": "The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The definition of the matrix game for classification appeared in  Freund and Schapire[1996b] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1638095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "888c09de60ce427669fe5a264fa3e787803eb9d2",
            "isKey": true,
            "numCitedBy": 397,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the close connections between game theory, on-line prediction and boosting. After a brief review of game theory, we describe an algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth. The analysis of this algorithm yields a simple proof of von Neumann\u2019s famous minmax theorem, as well as a provable method of approximately solving a game. We then show that the on-line prediction model is obtained by applying this gameplaying algorithm to an appropriate choice of game and that boosting is obtained by applying the same algorithm to the \u201cdual\u201d of this game."
            },
            "slug": "Game-theory,-on-line-prediction-and-boosting-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Game theory, on-line prediction and boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth is described, which yields a simple proof of von Neumann\u2019s famous minmax theorem, as well as a provable method of approximately solving a game."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "For interesting recent work in this direction see Gollea, Bartlett, Lee, and Mason (1998) and Freund (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For interesting recent work in this direction see Gollea, Bartlett, Lee, and Mason (1998) and  Freund (1998) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7236405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eaf290e5eca3fd440f6fd5ad39600231a28e269f",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Most of the work which attempts to give bounds on the generalization error of the hypothesis generated by a learning algorithm is based on methods from the theory of uniform convergence. These bounds are a-priori bounds that hold for any distribution of examples and are calculated before any data is observed. In this paper we propose a different approach for bounding the generalization error after the data has been observed. A self-bounding learning algorithm is an algorithm which, in addition to the hypothesis that it outputs, outputs a reliable upper bound on the generalization error of this hypothesis. We first explore the idea in the statistical query learning framework of Keams [lo]. After that we give an explicit self bounding algorithm for learning algorithms that are based on local search. Permission to make digital or h,ard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prolit or commercial advantage and that copies bear this notice and the full citation on the first page. To COPY otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific pemlission .and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-581 13-057--0/9X/ 7...$5.00"
            },
            "slug": "Self-bounding-learning-algorithms-Freund",
            "title": {
                "fragments": [],
                "text": "Self bounding learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A self-bounding learning algorithm is an algorithm which, in addition to the hypothesis that it outputs, outputs a reliable upper bound on the generalization error of this hypothesis."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 109
                            }
                        ],
                        "text": "The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a;  Drucker & Cortes, 1995;  Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1266014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dfeb731fc0c79e04523cd655413c223f6fa102",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to find a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input dimensions. We derive asymptotic results for our method. In a variety of simulations the properties of the algorithm are demonstrated with respect to interference, learning speed, prediction accuracy, feature detection, and task oriented incremental learning."
            },
            "slug": "Boosting-Decision-Trees-Drucker-Cortes",
            "title": {
                "fragments": [],
                "text": "Boosting Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A constructive, incremental learning system for regression problems that models data by means of locally linear experts that does not compete for data during learning and derives asymptotic results for this method."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 21
                            }
                        ],
                        "text": "Example 1: Adaboost (Freund & Schapire, 1996a, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 42
                            }
                        ],
                        "text": "The classification game was introduced in Freund and Schapire (1996b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 106
                            }
                        ],
                        "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost (Freund & Schapire, 1996a, 1997) and others in reducing generalization error has not been well understood."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Freund and Schapire (1996a) introduced a combination algorithm, called Adaboost, that was designed to drive the training error rapidly to zero."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 148
                            }
                        ],
                        "text": "The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "gorithms (arcing) such as Adaboost ( Freund & Schapire, 1996a, 1997 ) and, Freund and Schapire (1996a)  introduced a combination algorithm, called,The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996;  Freund & Schapire, 1996a;  Kong & Dietterich, 1996; Bauer & Kohavi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": true,
            "numCitedBy": 8626,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3291954"
                        ],
                        "name": "M. Golea",
                        "slug": "M.-Golea",
                        "structuredName": {
                            "firstName": "Mostefa",
                            "lastName": "Golea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Golea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586148"
                        ],
                        "name": "L. Mason",
                        "slug": "L.-Mason",
                        "structuredName": {
                            "firstName": "Llew",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mason"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12088396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcc802dcc8b4a953d5c3880a3117449e23ee29ea",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent theoretical results for pattern classification with thresholded real-valued functions (such as support vector machines, sigmoid networks, and boosting) give bounds on misclassification probability that do not depend on the size of the classifier, and hence can be considerably smaller than the bounds that follow from the VC theory. In this paper, we show that these techniques can be more widely applied, by representing other boolean functions as two-layer neural networks (thresholded convex combinations of boolean functions). For example, we show that with high probability any decision tree of depth no more than d that is consistent with m training examples has misclassification probability no more than O((1/m(Neff VCdim(U) log2 m log d))1/2), where U is the class of node decision functions, and Neff \u2264 N can be thought of as the effective number of leaves (it becomes small as the distribution on the leaves induced by the training data gets far from uniform). This bound is qualitatively different from the VC bound and can be considerably smaller. \n \nWe use the same technique to give similar results for DNF formulae."
            },
            "slug": "Generalization-in-Decision-Trees-and-DNF:-Does-Size-Golea-Bartlett",
            "title": {
                "fragments": [],
                "text": "Generalization in Decision Trees and DNF: Does Size Matter?"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper shows that with high probability any decision tree of depth no more than d that is consistent with m training examples has misclassification probabilityNo more than O((1/m(Neff VCdim(U) log2 m log d))1/2), where U is the class of node decision functions, and Neff \u2264 N can be thought of as the effective number of leaves."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708645"
                        ],
                        "name": "C. Ji",
                        "slug": "C.-Ji",
                        "structuredName": {
                            "firstName": "Chuanyi",
                            "lastName": "Ji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152273460"
                        ],
                        "name": "Sheng-Fei Ma",
                        "slug": "Sheng-Fei-Ma",
                        "structuredName": {
                            "firstName": "Sheng-Fei",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng-Fei Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 120
                            }
                        ],
                        "text": "Another was an algorithm that used hyperplanes as the class of predictors and produced low error on some hard problems (Ji & Ma, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "Example 3: Random hyperplanes (Ji & Ma, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 107
                            }
                        ],
                        "text": "Interestingly, the individual predictors can be very simple\u2014single hyperplanes in two-class classification (Ji & Ma, 1997) or two terminal-node trees (stumps) (Schapire, Freund, Bartlett, & Lee, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206458358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31f4c8f0850e8c64b0e684bae8bde0dd24ace6a0",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "To obtain classification systems with both good generalization performance and efficiency in space and time, we propose a learning method based on combinations of weak classifiers, where weak classifiers are linear classifiers (perceptrons) which can do a little better than making random guesses. A randomized algorithm is proposed to find the weak classifiers. They are then combined through a majority vote. As demonstrated through systematic experiments, the method developed is able to obtain combinations of weak classifiers with good generalization performance and a fast training time on a variety of test problems and real applications. Theoretical analysis on one of the test problems investigated in our experiments provides insights on when and why the proposed method works. In particular, when the strength of weak classifiers is properly chosen, combinations of weak classifiers can achieve a good generalization performance with polynomial space- and time-complexity."
            },
            "slug": "Combinations-of-Weak-Classifiers-Ji-Ma",
            "title": {
                "fragments": [],
                "text": "Combinations of Weak Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The method developed is able to obtain combinations of weak classifiers with good generalization performance and a fast training time on a variety of test problems and real applications and when the strength of strong classifiers is properly chosen, combinations ofWeak classifiers can achieve a good generalized performance with polynomial space- and time-complexity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 133
                            }
                        ],
                        "text": "The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 937841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79ea6a5a68e05065f82acd11a478aa7eac5f6c06",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Breiman's bagging and Freund and Schapire's boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered."
            },
            "slug": "Bagging,-Boosting,-and-C4.5-Quinlan",
            "title": {
                "fragments": [],
                "text": "Bagging, Boosting, and C4.5"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Results of applying Breiman's bagging and Freund and Schapire's boosting to a system that learns decision trees and testing on a representative collection of datasets show boosting shows the greater benefit."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 1"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913246"
                        ],
                        "name": "E. B. Kong",
                        "slug": "E.-B.-Kong",
                        "structuredName": {
                            "firstName": "Eun",
                            "lastName": "Kong",
                            "middleNames": [
                                "Bae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. B. Kong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a;  Kong & Dietterich, 1996;  Bauer & Kohavi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 174
                            }
                        ],
                        "text": "The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17043461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25744dbb4294fe7abb2d9b1b0d39006482ebb4ab",
            "isKey": false,
            "numCitedBy": 443,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Error-Correcting-Output-Coding-Corrects-Bias-and-Kong-Dietterich",
            "title": {
                "fragments": [],
                "text": "Error-Correcting Output Coding Corrects Bias and Variance"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49826711"
                        ],
                        "name": "F. Leisch",
                        "slug": "F.-Leisch",
                        "structuredName": {
                            "firstName": "Friedrich",
                            "lastName": "Leisch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Leisch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The existing arcing algorithms (see also  Leisch and Hornik[1997] ) fall into two different types which will be defined and discussed in the next two sections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 41
                            }
                        ],
                        "text": "All existing arcing algorithms (see also Leisch & Hornik, 1997) fall into one of these two types."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15967582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71835db06d36cce12b43b2dae8dd3ccbe57ea12b",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce arc-lh, a new algorithm for improvement of ANN classifier performance, which measures the importance of patterns by aggregated network output errors. On several artificial benchmark problems, this algorithm compares favorably with other resample and combine techniques."
            },
            "slug": "ARC-LH:-A-New-Adaptive-Resampling-Algorithm-for-ANN-Leisch-Hornik",
            "title": {
                "fragments": [],
                "text": "ARC-LH: A New Adaptive Resampling Algorithm for Improving ANN Classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145047283"
                        ],
                        "name": "J. Robinson",
                        "slug": "J.-Robinson",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Robinson",
                            "middleNames": [
                                "Jean"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Robinson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 30
                            }
                        ],
                        "text": "Its convergence was proved by Robinson (1951)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 33
                            }
                        ],
                        "text": "This work has important seeds in Schapire et al. (1997) and thought-provoking talks with Yoav Freund at the Newton Institute, Cambridge University, during the summer of 1997."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 32
                            }
                        ],
                        "text": "\u201d Its convergence was proved by Robinson (1951). A more accessible reference is Szep and Forgo (1985). It is an arcing algorithm but appears considerably less efficient than the arc-gv method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 32
                            }
                        ],
                        "text": "\u201d Its convergence was proved by Robinson (1951). A more accessible reference is Szep and Forgo (1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122681347,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "5264306051b8c15980e05339d764d9367b66b8e0",
            "isKey": true,
            "numCitedBy": 843,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : In the paper, demonstration is made of the validity of an iterative procedure suggested by George W. Brown for a two-person game. This method corresponds to each player choosing in turn the best pure strategy against the accumulated mixed strategy of his opponent up to then."
            },
            "slug": "AN-ITERATIVE-METHOD-OF-SOLVING-A-GAME-Robinson",
            "title": {
                "fragments": [],
                "text": "An Iterative Method of Solving a Game"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Drucker[1997]  has applied adaboost ideas to regression in an ad hoc fashion and gotten generally better results than bagging."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 16242966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d8226a52ebc70c8d97ccae10a74e1b0a3908ec1",
            "isKey": false,
            "numCitedBy": 542,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In the regression context, boosting and bagging are techniques to build a committee of regressors that may be superior to a single regressor. We use regression trees as fundamental building blocks in bagging committee machines and boosting committee machines. Performance is analyzed on three non-linear functions and the Boston housing database. In all cases, boosting is at least equivalent, and in most cases better than bagging in terms of prediction error."
            },
            "slug": "Improving-Regressors-using-Boosting-Techniques-Drucker",
            "title": {
                "fragments": [],
                "text": "Improving Regressors using Boosting Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work uses regression trees as fundamental building blocks in bagging committee machines and boosting committee machines to build a committee of regressors that may be superior to a single regressor."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 144
                            }
                        ],
                        "text": "Using the inequality mg(Z, c) \u2265 1\u22122er(Z, c) and setting1 = 1/2\u2212 top(c) gives\nP(mg(Z, c) \u2264 0) \u2264 R(1+ log(1/R)+ log(2N))+ (log(M)/\u03b4)/N. (6.2)\nThe bound in Schapire et al. (1997) depends on PT(mg(z, c) \u2264 \u03b8) where PT is the uniform distribution over the training set and \u03b8 can be varied."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 792,
                                "start": 16
                            }
                        ],
                        "text": "2) The bound in Schapire et al. (1997) depends on PT(mg(z, c) \u2264 \u03b8) where PT is the uniform distribution over the training set and \u03b8 can be varied. If \u03b8 is taken to be the minimum value of the margin over the training set, then in the two-class case, their bound is about the square root of the bound in equation 6.2. If the bound is nontrivial and < 1, then equation 6.2 is less than the Schapire et al. bound. The additional sharpness comes from using the uniform bound given by top(c). We give this theorem and its proof mainly as a factor hopefully pointing in the right direction. Generalization to infinite sets of predictors can be given in terms of their VC-dimension (see Schapire et al., 1997). The motivation for proving this theorem is partly the following: Schapire et al. (1997) draw the conclusion from their bound that for a fixed set of predictors, the margin distribution governs the generalization error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 57
                            }
                        ],
                        "text": "Another explanation for Adaboost\u2019s success was offered by Schapire et al. (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 4
                            }
                        ],
                        "text": "But Schapire et al. (1997) gave examples of data where two-node trees (stumps) had high bias and the main effect of Adaboost was to reduce the bias."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 33
                            }
                        ],
                        "text": "The proof is patterned after the Schapire et al. (1997) proof."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Schapire et al. (1997) derived a bound on the generalization error of a combination of classifiers that did not depend on how many classifiers were combined, but only on the training set margin distribution, the sample size\nand VC-dimension of the set of classifiers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 39
                            }
                        ],
                        "text": "The bound is sharper than the bound in Schapire et al. (1997) based on the margin distribution but uses the same elegant device in its derivation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 95
                            }
                        ],
                        "text": "Generalization to infinite sets of predictors can be given in terms of their VC-dimension (see Schapire et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Schapire et al. (1997) note that the Adaboost algorithm produces a c sequence so that lim supc top(c) = \u03c61 where \u03c61 is less than 1/2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 16
                            }
                        ],
                        "text": "2) The bound in Schapire et al. (1997) depends on PT(mg(z, c) \u2264 \u03b8) where PT is the uniform distribution over the training set and \u03b8 can be varied."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Schapire et al. (1997) derived a bound on the classification generalization error in terms of the distribution of mg(z, c) on the instances of T."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 65
                            }
                        ],
                        "text": "The motivation for proving this theorem is partly the following: Schapire et al. (1997) draw the conclusion from their bound that for a fixed set of predictors, the margin distribution governs the generalization error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Schapire et al. (1997) derived a bound on the generalization error of a combination of classifiers that did not depend on how many classifiers were combined, but only on the training set margin distribution, the sample size"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 33
                            }
                        ],
                        "text": "This work has important seeds in Schapire et al. (1997) and thought-provoking talks with Yoav Freund at the Newton Institute, Cambridge University, during the summer of 1997."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting the margin, (available from http://www.research.att.com/\u223cyoav; look under \u201cpublications\u201d"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Arc-x4 is a Type II arcing algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "Relation 2.2 also follows from the duality theorem of linear programming (Breiman, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 19
                            }
                        ],
                        "text": "Example 2: Arc-x4 (Breiman, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 83
                            }
                        ],
                        "text": "The synthetic data sets, called twonorm, threenorm, and ringnorm, are described in Breiman (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 62
                            }
                        ],
                        "text": "2 also follows from the duality theorem of linear programming (Breiman, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arcing the edge (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep. No. 486). Statistics Department, University of California. Available from: www.stat.berkeley.edu."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 144
                            }
                        ],
                        "text": "Using the inequality mg(Z, c) \u2265 1\u22122er(Z, c) and setting1 = 1/2\u2212 top(c) gives\nP(mg(Z, c) \u2264 0) \u2264 R(1+ log(1/R)+ log(2N))+ (log(M)/\u03b4)/N. (6.2)\nThe bound in Schapire et al. (1997) depends on PT(mg(z, c) \u2264 \u03b8) where PT is the uniform distribution over the training set and \u03b8 can be varied."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 57
                            }
                        ],
                        "text": "Another explanation for Adaboost\u2019s success was offered by Schapire et al. (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 4
                            }
                        ],
                        "text": "But Schapire et al. (1997) gave examples of data where two-node trees (stumps) had high bias and the main effect of Adaboost was to reduce the bias."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 33
                            }
                        ],
                        "text": "The proof is patterned after the Schapire et al. (1997) proof."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Schapire et al. (1997) derived a bound on the generalization error of a combination of classifiers that did not depend on how many classifiers were combined, but only on the training set margin distribution, the sample size\nand VC-dimension of the set of classifiers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 39
                            }
                        ],
                        "text": "The bound is sharper than the bound in Schapire et al. (1997) based on the margin distribution but uses the same elegant device in its derivation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 95
                            }
                        ],
                        "text": "Generalization to infinite sets of predictors can be given in terms of their VC-dimension (see Schapire et al., 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Schapire et al. (1997) note that the Adaboost algorithm produces a c sequence so that lim supc top(c) = \u03c61 where \u03c61 is less than 1/2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Schapire et al. (1997) derived a bound on the classification generalization error in terms of the distribution of mg(z, c) on the instances of T."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 65
                            }
                        ],
                        "text": "The motivation for proving this theorem is partly the following: Schapire et al. (1997) draw the conclusion from their bound that for a fixed set of predictors, the margin distribution governs the generalization error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 33
                            }
                        ],
                        "text": "This work has important seeds in Schapire et al. (1997) and thought-provoking talks with Yoav Freund at the Newton Institute, Cambridge University, during the summer of 1997."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting the Margin"
            },
            "venue": {
                "fragments": [],
                "text": "Boosting the Margin"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Arc-x4 is a Type II arcing algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 19
                            }
                        ],
                        "text": "Example 2: Arc-x4 (Breiman, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 83
                            }
                        ],
                        "text": "The synthetic data sets, called twonorm, threenorm, and ringnorm, are described in Breiman (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "Relation 2.2 also follows from the duality theorem of linear programming (Breiman, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arcing the Edge University of California (available at www.stat.berkeley.edu) Drucker, H. and Cortes, C. [1995] Boosting decision trees"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1430,
                                "start": 56
                            }
                        ],
                        "text": "The first well-known combination algorithm was bagging (Breiman, 1996b). The altered training sets were taken to be bootstrap samples from the original training set, and each predictor grown had equal weighting. It proved quite effective in reducing generalization error. The explanation given for its success was in terms of the bias-variance components of the generalization error. The variance is the scatter in the predictions gotten from using different training sets, each one drawn from the same distribution. Average all of these predictions (or take their most probable value in classification) and compute how much this average differs from the target function. The result is bias. Breiman (1996a) shows that tree algorithms have small bias, and the effect of combination is to reduce the variance. Freund and Schapire (1996a) introduced a combination algorithm, called Adaboost, that was designed to drive the training error rapidly to zero. But experiments showed that Adaboost kept lowering the generalization error long after the training set error was zero. The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998). The Adaboost algorithm differed significantly from bagging and begged the question of why it worked as well as it did. Breiman (1996b) showed that for trees, Adaboost reduced variance more than bagging did while keeping bias low, leading to the possible conclusion that it was a more effective variance reduction algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 22
                            }
                        ],
                        "text": "One was arc-x4, which Breiman (1996b) showed had error performance comparable to Adaboost."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 83
                            }
                        ],
                        "text": "The synthetic data sets, called twonorm, threenorm, and ringnorm, are described in Breiman (1997). The real data sets are all in the repository at the University of California, Irvine."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 56
                            }
                        ],
                        "text": "The first well-known combination algorithm was bagging (Breiman, 1996b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 63
                            }
                        ],
                        "text": "2 also follows from the duality theorem of linear programming (Breiman, 1997). The classification game was introduced in Freund and Schapire (1996b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 837,
                                "start": 56
                            }
                        ],
                        "text": "The first well-known combination algorithm was bagging (Breiman, 1996b). The altered training sets were taken to be bootstrap samples from the original training set, and each predictor grown had equal weighting. It proved quite effective in reducing generalization error. The explanation given for its success was in terms of the bias-variance components of the generalization error. The variance is the scatter in the predictions gotten from using different training sets, each one drawn from the same distribution. Average all of these predictions (or take their most probable value in classification) and compute how much this average differs from the target function. The result is bias. Breiman (1996a) shows that tree algorithms have small bias, and the effect of combination is to reduce the variance. Freund and Schapire (1996a) introduced a combination algorithm, called Adaboost, that was designed to drive the training error rapidly to zero."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Breiman (1996b) showed that for trees, Adaboost reduced variance more than bagging did while keeping bias low, leading to the possible conclusion that it was a more effective variance reduction algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 93
                            }
                        ],
                        "text": "The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1849,
                                "start": 56
                            }
                        ],
                        "text": "The first well-known combination algorithm was bagging (Breiman, 1996b). The altered training sets were taken to be bootstrap samples from the original training set, and each predictor grown had equal weighting. It proved quite effective in reducing generalization error. The explanation given for its success was in terms of the bias-variance components of the generalization error. The variance is the scatter in the predictions gotten from using different training sets, each one drawn from the same distribution. Average all of these predictions (or take their most probable value in classification) and compute how much this average differs from the target function. The result is bias. Breiman (1996a) shows that tree algorithms have small bias, and the effect of combination is to reduce the variance. Freund and Schapire (1996a) introduced a combination algorithm, called Adaboost, that was designed to drive the training error rapidly to zero. But experiments showed that Adaboost kept lowering the generalization error long after the training set error was zero. The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998). The Adaboost algorithm differed significantly from bagging and begged the question of why it worked as well as it did. Breiman (1996b) showed that for trees, Adaboost reduced variance more than bagging did while keeping bias low, leading to the possible conclusion that it was a more effective variance reduction algorithm. But Schapire et al. (1997) gave examples of data where two-node trees (stumps) had high bias and the main effect of Adaboost was to reduce the bias. Another explanation for Adaboost\u2019s success was offered by Schapire et al. (1997). For any combination of classifiers with nonnegative weights c = {cm} summing to one, define the margin mg(z, c) at input z = (y, x) as mg(z, c) = \u2211 cmI(hm(x) = y)\u2212max y\u2032 6=y \u2211 cmI(hm(x) = y\u2032)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 708,
                                "start": 56
                            }
                        ],
                        "text": "The first well-known combination algorithm was bagging (Breiman, 1996b). The altered training sets were taken to be bootstrap samples from the original training set, and each predictor grown had equal weighting. It proved quite effective in reducing generalization error. The explanation given for its success was in terms of the bias-variance components of the generalization error. The variance is the scatter in the predictions gotten from using different training sets, each one drawn from the same distribution. Average all of these predictions (or take their most probable value in classification) and compute how much this average differs from the target function. The result is bias. Breiman (1996a) shows that tree algorithms have small bias, and the effect of combination is to reduce the variance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Breiman (1996a) shows that tree algorithms have small bias, and the effect of combination is to reduce the variance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1646,
                                "start": 56
                            }
                        ],
                        "text": "The first well-known combination algorithm was bagging (Breiman, 1996b). The altered training sets were taken to be bootstrap samples from the original training set, and each predictor grown had equal weighting. It proved quite effective in reducing generalization error. The explanation given for its success was in terms of the bias-variance components of the generalization error. The variance is the scatter in the predictions gotten from using different training sets, each one drawn from the same distribution. Average all of these predictions (or take their most probable value in classification) and compute how much this average differs from the target function. The result is bias. Breiman (1996a) shows that tree algorithms have small bias, and the effect of combination is to reduce the variance. Freund and Schapire (1996a) introduced a combination algorithm, called Adaboost, that was designed to drive the training error rapidly to zero. But experiments showed that Adaboost kept lowering the generalization error long after the training set error was zero. The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998). The Adaboost algorithm differed significantly from bagging and begged the question of why it worked as well as it did. Breiman (1996b) showed that for trees, Adaboost reduced variance more than bagging did while keeping bias low, leading to the possible conclusion that it was a more effective variance reduction algorithm. But Schapire et al. (1997) gave examples of data where two-node trees (stumps) had high bias and the main effect of Adaboost was to reduce the bias."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bias, variance, and arcing classifiers (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep. No. 460). Statistics Department, University of California. Available from: www.stat.berkeley.edu."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 133
                            }
                        ],
                        "text": "The resulting generalization errors were significantly lower than those produced by bagging (Breiman, 1996a; Drucker & Cortes, 1995; Quinlan, 1996; Freund & Schapire, 1996a; Kong & Dietterich, 1996; Bauer & Kohavi, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bagging, Boosting, and C4"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of AAAI'96 National Conference on Artificial Intelligence"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of online learning and an application to boosting. to appear"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 120
                            }
                        ],
                        "text": "Another was an algorithm that used hyperplanes as the class of predictors and produced low error on some hard problems (Ji & Ma, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "Example 3: Random hyperplanes (Ji & Ma, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 107
                            }
                        ],
                        "text": "Interestingly, the individual predictors can be very simple\u2014single hyperplanes in two-class classification (Ji & Ma, 1997) or two terminal-node trees (stumps) (Schapire, Freund, Bartlett, & Lee, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combinations of weak classifiers, Special Issue of Neural Networks and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "For interesting recent work in this direction see Gollea, Bartlett, Lee, and Mason (1998) and Freund (1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 94
                            }
                        ],
                        "text": "For interesting recent work in this direction see Gollea, Bartlett, Lee, and Mason (1998) and Freund (1998). My sense is that we do not understand enough about what is going on."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 814,
                                "start": 106
                            }
                        ],
                        "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost (Freund & Schapire, 1996a, 1997) and others in reducing generalization error has not been well understood. By formulating prediction as a game where one player makes a selection from instances in the training set and the other a convex linear combination of predictors from a finite set, existing arcing algorithms are shown to be algorithms for finding good game strategies. The minimax theorem is an essential ingredient of the convergence proofs. An arcing algorithm is described that converges to the optimal strategy. A bound on the generalization error for the combined predictors in terms of their maximum error is proven that is sharper than bounds to date. Schapire, Freund, Bartlett, and Lee (1997) offered an explanation of why Adaboost works in terms of its ability to produce generally high margins."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self bounding learning algorithms, (available from http://www.research.att.com/\u223cyoav; look under \u201cpublications\u201d"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 8,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Prediction-Games-and-Arcing-Algorithms-Breiman/a92684c164b0c46020a371ae5116df74bb37a412?sort=total-citations"
}